x16_t)__rev0, __p1); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vduph_laneq_s16(__p0, __p1) __extension__ ({ \
  int16x8_t __s0 = __p0; \
  int16_t __ret; \
  __ret = (int16_t) __builtin_neon_vduph_laneq_i16((int8x16_t)__s0, __p1); \
  __ret; \
})
#else
#define vduph_laneq_s16(__p0, __p1) __extension__ ({ \
  int16x8_t __s0 = __p0; \
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 7, 6, 5, 4, 3, 2, 1, 0); \
  int16_t __ret; \
  __ret = (int16_t) __builtin_neon_vduph_laneq_i16((int8x16_t)__rev0, __p1); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vdup_laneq_p8(__p0, __p1) __extension__ ({ \
  poly8x16_t __s0 = __p0; \
  poly8x8_t __ret; \
  __ret = __builtin_shufflevector(__s0, __s0, __p1, __p1, __p1, __p1, __p1, __p1, __p1, __p1); \
  __ret; \
})
#else
#define vdup_laneq_p8(__p0, __p1) __extension__ ({ \
  poly8x16_t __s0 = __p0; \
  poly8x16_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  poly8x8_t __ret; \
  __ret = __builtin_shufflevector(__rev0, __rev0, __p1, __p1, __p1, __p1, __p1, __p1, __p1, __p1); \
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vdup_laneq_p64(__p0, __p1) __extension__ ({ \
  poly64x2_t __s0 = __p0; \
  poly64x1_t __ret; \
  __ret = __builtin_shufflevector(__s0, __s0, __p1); \
  __ret; \
})
#else
#define vdup_laneq_p64(__p0, __p1) __extension__ ({ \
  poly64x2_t __s0 = __p0; \
  poly64x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  poly64x1_t __ret; \
  __ret = __builtin_shufflevector(__rev0, __rev0, __p1); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vdup_laneq_p16(__p0, __p1) __extension__ ({ \
  poly16x8_t __s0 = __p0; \
  poly16x4_t __ret; \
  __ret = __builtin_shufflevector(__s0, __s0, __p1, __p1, __p1, __p1); \
  __ret; \
})
#else
#define vdup_laneq_p16(__p0, __p1) __extension__ ({ \
  poly16x8_t __s0 = __p0; \
  poly16x8_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 7, 6, 5, 4, 3, 2, 1, 0); \
  poly16x4_t __ret; \
  __ret = __builtin_shufflevector(__rev0, __rev0, __p1, __p1, __p1, __p1); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vdupq_laneq_p8(__p0, __p1) __extension__ ({ \
  poly8x16_t __s0 = __p0; \
  poly8x16_t __ret; \
  __ret = __builtin_shufflevector(__s0, __s0, __p1, __p1, __p1, __p1, __p1, __p1, __p1, __p1, __p1, __p1, __p1, __p1, __p1, __p1, __p1, __p1); \
  __ret; \
})
#else
#define vdupq_laneq_p8(__p0, __p1) __extension__ ({ \
  poly8x16_t __s0 = __p0; \
  poly8x16_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  poly8x16_t __ret; \
  __ret = __builtin_shufflevector(__rev0, __rev0, __p1, __p1, __p1, __p1, __p1, __p1, __p1, __p1, __p1, __p1, __p1, __p1, __p1, __p1, __p1, __p1); \
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vdupq_laneq_p64(__p0, __p1) __extension__ ({ \
  poly64x2_t __s0 = __p0; \
  poly64x2_t __ret; \
  __ret = __builtin_shufflevector(__s0, __s0, __p1, __p1); \
  __ret; \
})
#else
#define vdupq_laneq_p64(__p0, __p1) __extension__ ({ \
  poly64x2_t __s0 = __p0; \
  poly64x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  poly64x2_t __ret; \
  __ret = __builtin_shufflevector(__rev0, __rev0, __p1, __p1); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vdupq_laneq_p16(__p0, __p1) __extension__ ({ \
  poly16x8_t __s0 = __p0; \
  poly16x8_t __ret; \
  __ret = __builtin_shufflevector(__s0, __s0, __p1, __p1, __p1, __p1, __p1, __p1, __p1, __p1); \
  __ret; \
})
#else
#define vdupq_laneq_p16(__p0, __p1) __extension__ ({ \
  poly16x8_t __s0 = __p0; \
  poly16x8_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 7, 6, 5, 4, 3, 2, 1, 0); \
  poly16x8_t __ret; \
  __ret = __builtin_shufflevector(__rev0, __rev0, __p1, __p1, __p1, __p1, __p1, __p1, __p1, __p1); \
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vdupq_laneq_u8(__p0, __p1) __extension__ ({ \
  uint8x16_t __s0 = __p0; \
  uint8x16_t __ret; \
  __ret = __builtin_shufflevector(__s0, __s0, __p1, __p1, __p1, __p1, __p1, __p1, __p1, __p1, __p1, __p1, __p1, __p1, __p1, __p1, __p1, __p1); \
  __ret; \
})
#else
#define vdupq_laneq_u8(__p0, __p1) __extension__ ({ \
  uint8x16_t __s0 = __p0; \
  uint8x16_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint8x16_t __ret; \
  __ret = __builtin_shufflevector(__rev0, __rev0, __p1, __p1, __p1, __p1, __p1, __p1, __p1, __p1, __p1, __p1, __p1, __p1, __p1, __p1, __p1, __p1); \
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vdupq_laneq_u32(__p0, __p1) __extension__ ({ \
  uint32x4_t __s0 = __p0; \
  uint32x4_t __ret; \
  __ret = __builtin_shufflevector(__s0, __s0, __p1, __p1, __p1, __p1); \
  __ret; \
})
#else
#define vdupq_laneq_u32(__p0, __p1) __extension__ ({ \
  uint32x4_t __s0 = __p0; \
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  uint32x4_t __ret; \
  __ret = __builtin_shufflevector(__rev0, __rev0, __p1, __p1, __p1, __p1); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vdupq_laneq_u64(__p0, __p1) __extension__ ({ \
  uint64x2_t __s0 = __p0; \
  uint64x2_t __ret; \
  __ret = __builtin_shufflevector(__s0, __s0, __p1, __p1); \
  __ret; \
})
#else
#define vdupq_laneq_u64(__p0, __p1) __extension__ ({ \
  uint64x2_t __s0 = __p0; \
  uint64x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  uint64x2_t __ret; \
  __ret = __builtin_shufflevector(__rev0, __rev0, __p1, __p1); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vdupq_laneq_u16(__p0, __p1) __extension__ ({ \
  uint16x8_t __s0 = __p0; \
  uint16x8_t __ret; \
  __ret = __builtin_shufflevector(__s0, __s0, __p1, __p1, __p1, __p1, __p1, __p1, __p1, __p1); \
  __ret; \
})
#else
#define vdupq_laneq_u16(__p0, __p1) __extension__ ({ \
  uint16x8_t __s0 = __p0; \
  uint16x8_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint16x8_t __ret; \
  __ret = __builtin_shufflevector(__rev0, __rev0, __p1, __p1, __p1, __p1, __p1, __p1, __p1, __p1); \
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vdupq_laneq_s8(__p0, __p1) __extension__ ({ \
  int8x16_t __s0 = __p0; \
  int8x16_t __ret; \
  __ret = __builtin_shufflevector(__s0, __s0, __p1, __p1, __p1, __p1, __p1, __p1, __p1, __p1, __p1, __p1, __p1, __p1, __p1, __p1, __p1, __p1); \
  __ret; \
})
#else
#define vdupq_laneq_s8(__p0, __p1) __extension__ ({ \
  int8x16_t __s0 = __p0; \
  int8x16_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  int8x16_t __ret; \
  __ret = __builtin_shufflevector(__rev0, __rev0, __p1, __p1, __p1, __p1, __p1, __p1, __p1, __p1, __p1, __p1, __p1, __p1, __p1, __p1, __p1, __p1); \
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vdupq_laneq_f64(__p0, __p1) __extension__ ({ \
  float64x2_t __s0 = __p0; \
  float64x2_t __ret; \
  __ret = __builtin_shufflevector(__s0, __s0, __p1, __p1); \
  __ret; \
})
#else
#define vdupq_laneq_f64(__p0, __p1) __extension__ ({ \
  float64x2_t __s0 = __p0; \
  float64x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  float64x2_t __ret; \
  __ret = __builtin_shufflevector(__rev0, __rev0, __p1, __p1); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vdupq_laneq_f32(__p0, __p1) __extension__ ({ \
  float32x4_t __s0 = __p0; \
  float32x4_t __ret; \
  __ret = __builtin_shufflevector(__s0, __s0, __p1, __p1, __p1, __p1); \
  __ret; \
})
#else
#define vdupq_laneq_f32(__p0, __p1) __extension__ ({ \
  float32x4_t __s0 = __p0; \
  float32x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  float32x4_t __ret; \
  __ret = __builtin_shufflevector(__rev0, __rev0, __p1, __p1, __p1, __p1); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vdupq_laneq_f16(__p0, __p1) __extension__ ({ \
  float16x8_t __s0 = __p0; \
  float16x8_t __ret; \
  __ret = __builtin_shufflevector(__s0, __s0, __p1, __p1, __p1, __p1, __p1, __p1, __p1, __p1); \
  __ret; \
})
#else
#define vdupq_laneq_f16(__p0, __p1) __extension__ ({ \
  float16x8_t __s0 = __p0; \
  float16x8_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 7, 6, 5, 4, 3, 2, 1, 0); \
  float16x8_t __ret; \
  __ret = __builtin_shufflevector(__rev0, __rev0, __p1, __p1, __p1, __p1, __p1, __p1, __p1, __p1); \
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vdupq_laneq_s32(__p0, __p1) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  int32x4_t __ret; \
  __ret = __builtin_shufflevector(__s0, __s0, __p1, __p1, __p1, __p1); \
  __ret; \
})
#else
#define vdupq_laneq_s32(__p0, __p1) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  int32x4_t __ret; \
  __ret = __builtin_shufflevector(__rev0, __rev0, __p1, __p1, __p1, __p1); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vdupq_laneq_s64(__p0, __p1) __extension__ ({ \
  int64x2_t __s0 = __p0; \
  int64x2_t __ret; \
  __ret = __builtin_shufflevector(__s0, __s0, __p1, __p1); \
  __ret; \
})
#else
#define vdupq_laneq_s64(__p0, __p1) __extension__ ({ \
  int64x2_t __s0 = __p0; \
  int64x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  int64x2_t __ret; \
  __ret = __builtin_shufflevector(__rev0, __rev0, __p1, __p1); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vdupq_laneq_s16(__p0, __p1) __extension__ ({ \
  int16x8_t __s0 = __p0; \
  int16x8_t __ret; \
  __ret = __builtin_shufflevector(__s0, __s0, __p1, __p1, __p1, __p1, __p1, __p1, __p1, __p1); \
  __ret; \
})
#else
#define vdupq_laneq_s16(__p0, __p1) __extension__ ({ \
  int16x8_t __s0 = __p0; \
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 7, 6, 5, 4, 3, 2, 1, 0); \
  int16x8_t __ret; \
  __ret = __builtin_shufflevector(__rev0, __rev0, __p1, __p1, __p1, __p1, __p1, __p1, __p1, __p1); \
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vdup_laneq_u8(__p0, __p1) __extension__ ({ \
  uint8x16_t __s0 = __p0; \
  uint8x8_t __ret; \
  __ret = __builtin_shufflevector(__s0, __s0, __p1, __p1, __p1, __p1, __p1, __p1, __p1, __p1); \
  __ret; \
})
#else
#define vdup_laneq_u8(__p0, __p1) __extension__ ({ \
  uint8x16_t __s0 = __p0; \
  uint8x16_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint8x8_t __ret; \
  __ret = __builtin_shufflevector(__rev0, __rev0, __p1, __p1, __p1, __p1, __p1, __p1, __p1, __p1); \
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vdup_laneq_u32(__p0, __p1) __extension__ ({ \
  uint32x4_t __s0 = __p0; \
  uint32x2_t __ret; \
  __ret = __builtin_shufflevector(__s0, __s0, __p1, __p1); \
  __ret; \
})
#else
#define vdup_laneq_u32(__p0, __p1) __extension__ ({ \
  uint32x4_t __s0 = __p0; \
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  uint32x2_t __ret; \
  __ret = __builtin_shufflevector(__rev0, __rev0, __p1, __p1); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vdup_laneq_u64(__p0, __p1) __extension__ ({ \
  uint64x2_t __s0 = __p0; \
  uint64x1_t __ret; \
  __ret = __builtin_shufflevector(__s0, __s0, __p1); \
  __ret; \
})
#else
#define vdup_laneq_u64(__p0, __p1) __extension__ ({ \
  uint64x2_t __s0 = __p0; \
  uint64x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  uint64x1_t __ret; \
  __ret = __builtin_shufflevector(__rev0, __rev0, __p1); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vdup_laneq_u16(__p0, __p1) __extension__ ({ \
  uint16x8_t __s0 = __p0; \
  uint16x4_t __ret; \
  __ret = __builtin_shufflevector(__s0, __s0, __p1, __p1, __p1, __p1); \
  __ret; \
})
#else
#define vdup_laneq_u16(__p0, __p1) __extension__ ({ \
  uint16x8_t __s0 = __p0; \
  uint16x8_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint16x4_t __ret; \
  __ret = __builtin_shufflevector(__rev0, __rev0, __p1, __p1, __p1, __p1); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vdup_laneq_s8(__p0, __p1) __extension__ ({ \
  int8x16_t __s0 = __p0; \
  int8x8_t __ret; \
  __ret = __builtin_shufflevector(__s0, __s0, __p1, __p1, __p1, __p1, __p1, __p1, __p1, __p1); \
  __ret; \
})
#else
#define vdup_laneq_s8(__p0, __p1) __extension__ ({ \
  int8x16_t __s0 = __p0; \
  int8x16_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  int8x8_t __ret; \
  __ret = __builtin_shufflevector(__rev0, __rev0, __p1, __p1, __p1, __p1, __p1, __p1, __p1, __p1); \
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vdup_laneq_f64(__p0, __p1) __extension__ ({ \
  float64x2_t __s0 = __p0; \
  float64x1_t __ret; \
  __ret = __builtin_shufflevector(__s0, __s0, __p1); \
  __ret; \
})
#else
#define vdup_laneq_f64(__p0, __p1) __extension__ ({ \
  float64x2_t __s0 = __p0; \
  float64x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  float64x1_t __ret; \
  __ret = __builtin_shufflevector(__rev0, __rev0, __p1); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vdup_laneq_f32(__p0, __p1) __extension__ ({ \
  float32x4_t __s0 = __p0; \
  float32x2_t __ret; \
  __ret = __builtin_shufflevector(__s0, __s0, __p1, __p1); \
  __ret; \
})
#else
#define vdup_laneq_f32(__p0, __p1) __extension__ ({ \
  float32x4_t __s0 = __p0; \
  float32x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  float32x2_t __ret; \
  __ret = __builtin_shufflevector(__rev0, __rev0, __p1, __p1); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vdup_laneq_f16(__p0, __p1) __extension__ ({ \
  float16x8_t __s0 = __p0; \
  float16x4_t __ret; \
  __ret = __builtin_shufflevector(__s0, __s0, __p1, __p1, __p1, __p1); \
  __ret; \
})
#else
#define vdup_laneq_f16(__p0, __p1) __extension__ ({ \
  float16x8_t __s0 = __p0; \
  float16x8_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 7, 6, 5, 4, 3, 2, 1, 0); \
  float16x4_t __ret; \
  __ret = __builtin_shufflevector(__rev0, __rev0, __p1, __p1, __p1, __p1); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vdup_laneq_s32(__p0, __p1) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  int32x2_t __ret; \
  __ret = __builtin_shufflevector(__s0, __s0, __p1, __p1); \
  __ret; \
})
#else
#define vdup_laneq_s32(__p0, __p1) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  int32x2_t __ret; \
  __ret = __builtin_shufflevector(__rev0, __rev0, __p1, __p1); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vdup_laneq_s64(__p0, __p1) __extension__ ({ \
  int64x2_t __s0 = __p0; \
  int64x1_t __ret; \
  __ret = __builtin_shufflevector(__s0, __s0, __p1); \
  __ret; \
})
#else
#define vdup_laneq_s64(__p0, __p1) __extension__ ({ \
  int64x2_t __s0 = __p0; \
  int64x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  int64x1_t __ret; \
  __ret = __builtin_shufflevector(__rev0, __rev0, __p1); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vdup_laneq_s16(__p0, __p1) __extension__ ({ \
  int16x8_t __s0 = __p0; \
  int16x4_t __ret; \
  __ret = __builtin_shufflevector(__s0, __s0, __p1, __p1, __p1, __p1); \
  __ret; \
})
#else
#define vdup_laneq_s16(__p0, __p1) __extension__ ({ \
  int16x8_t __s0 = __p0; \
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 7, 6, 5, 4, 3, 2, 1, 0); \
  int16x4_t __ret; \
  __ret = __builtin_shufflevector(__rev0, __rev0, __p1, __p1, __p1, __p1); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly64x1_t vdup_n_p64(poly64_t __p0) {
  poly64x1_t __ret;
  __ret = (poly64x1_t) {__p0};
  return __ret;
}
#else
__ai poly64x1_t vdup_n_p64(poly64_t __p0) {
  poly64x1_t __ret;
  __ret = (poly64x1_t) {__p0};
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly64x2_t vdupq_n_p64(poly64_t __p0) {
  poly64x2_t __ret;
  __ret = (poly64x2_t) {__p0, __p0};
  return __ret;
}
#else
__ai poly64x2_t vdupq_n_p64(poly64_t __p0) {
  poly64x2_t __ret;
  __ret = (poly64x2_t) {__p0, __p0};
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x2_t vdupq_n_f64(float64_t __p0) {
  float64x2_t __ret;
  __ret = (float64x2_t) {__p0, __p0};
  return __ret;
}
#else
__ai float64x2_t vdupq_n_f64(float64_t __p0) {
  float64x2_t __ret;
  __ret = (float64x2_t) {__p0, __p0};
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x1_t vdup_n_f64(float64_t __p0) {
  float64x1_t __ret;
  __ret = (float64x1_t) {__p0};
  return __ret;
}
#else
__ai float64x1_t vdup_n_f64(float64_t __p0) {
  float64x1_t __ret;
  __ret = (float64x1_t) {__p0};
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
#define vext_p64(__p0, __p1, __p2) __extension__ ({ \
  poly64x1_t __s0 = __p0; \
  poly64x1_t __s1 = __p1; \
  poly64x1_t __ret; \
  __ret = (poly64x1_t) __builtin_neon_vext_v((int8x8_t)__s0, (int8x8_t)__s1, __p2, 6); \
  __ret; \
})
#else
#define vext_p64(__p0, __p1, __p2) __extension__ ({ \
  poly64x1_t __s0 = __p0; \
  poly64x1_t __s1 = __p1; \
  poly64x1_t __ret; \
  __ret = (poly64x1_t) __builtin_neon_vext_v((int8x8_t)__s0, (int8x8_t)__s1, __p2, 6); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vextq_p64(__p0, __p1, __p2) __extension__ ({ \
  poly64x2_t __s0 = __p0; \
  poly64x2_t __s1 = __p1; \
  poly64x2_t __ret; \
  __ret = (poly64x2_t) __builtin_neon_vextq_v((int8x16_t)__s0, (int8x16_t)__s1, __p2, 38); \
  __ret; \
})
#else
#define vextq_p64(__p0, __p1, __p2) __extension__ ({ \
  poly64x2_t __s0 = __p0; \
  poly64x2_t __s1 = __p1; \
  poly64x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  poly64x2_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 1, 0); \
  poly64x2_t __ret; \
  __ret = (poly64x2_t) __builtin_neon_vextq_v((int8x16_t)__rev0, (int8x16_t)__rev1, __p2, 38); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vextq_f64(__p0, __p1, __p2) __extension__ ({ \
  float64x2_t __s0 = __p0; \
  float64x2_t __s1 = __p1; \
  float64x2_t __ret; \
  __ret = (float64x2_t) __builtin_neon_vextq_v((int8x16_t)__s0, (int8x16_t)__s1, __p2, 42); \
  __ret; \
})
#else
#define vextq_f64(__p0, __p1, __p2) __extension__ ({ \
  float64x2_t __s0 = __p0; \
  float64x2_t __s1 = __p1; \
  float64x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  float64x2_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 1, 0); \
  float64x2_t __ret; \
  __ret = (float64x2_t) __builtin_neon_vextq_v((int8x16_t)__rev0, (int8x16_t)__rev1, __p2, 42); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vext_f64(__p0, __p1, __p2) __extension__ ({ \
  float64x1_t __s0 = __p0; \
  float64x1_t __s1 = __p1; \
  float64x1_t __ret; \
  __ret = (float64x1_t) __builtin_neon_vext_v((int8x8_t)__s0, (int8x8_t)__s1, __p2, 10); \
  __ret; \
})
#else
#define vext_f64(__p0, __p1, __p2) __extension__ ({ \
  float64x1_t __s0 = __p0; \
  float64x1_t __s1 = __p1; \
  float64x1_t __ret; \
  __ret = (float64x1_t) __builtin_neon_vext_v((int8x8_t)__s0, (int8x8_t)__s1, __p2, 10); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x2_t vfmaq_f64(float64x2_t __p0, float64x2_t __p1, float64x2_t __p2) {
  float64x2_t __ret;
  __ret = (float64x2_t) __builtin_neon_vfmaq_v((int8x16_t)__p0, (int8x16_t)__p1, (int8x16_t)__p2, 42);
  return __ret;
}
#else
__ai float64x2_t vfmaq_f64(float64x2_t __p0, float64x2_t __p1, float64x2_t __p2) {
  float64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  float64x2_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 1, 0);
  float64x2_t __ret;
  __ret = (float64x2_t) __builtin_neon_vfmaq_v((int8x16_t)__rev0, (int8x16_t)__rev1, (int8x16_t)__rev2, 42);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
__ai float64x2_t __noswap_vfmaq_f64(float64x2_t __p0, float64x2_t __p1, float64x2_t __p2) {
  float64x2_t __ret;
  __ret = (float64x2_t) __builtin_neon_vfmaq_v((int8x16_t)__p0, (int8x16_t)__p1, (int8x16_t)__p2, 42);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x1_t vfma_f64(float64x1_t __p0, float64x1_t __p1, float64x1_t __p2) {
  float64x1_t __ret;
  __ret = (float64x1_t) __builtin_neon_vfma_v((int8x8_t)__p0, (int8x8_t)__p1, (int8x8_t)__p2, 10);
  return __ret;
}
#else
__ai float64x1_t vfma_f64(float64x1_t __p0, float64x1_t __p1, float64x1_t __p2) {
  float64x1_t __ret;
  __ret = (float64x1_t) __builtin_neon_vfma_v((int8x8_t)__p0, (int8x8_t)__p1, (int8x8_t)__p2, 10);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
#define vfmad_lane_f64(__p0, __p1, __p2, __p3) __extension__ ({ \
  float64_t __s0 = __p0; \
  float64_t __s1 = __p1; \
  float64x1_t __s2 = __p2; \
  float64_t __ret; \
  __ret = (float64_t) __builtin_neon_vfmad_lane_f64(__s0, __s1, (int8x8_t)__s2, __p3); \
  __ret; \
})
#else
#define vfmad_lane_f64(__p0, __p1, __p2, __p3) __extension__ ({ \
  float64_t __s0 = __p0; \
  float64_t __s1 = __p1; \
  float64x1_t __s2 = __p2; \
  float64_t __ret; \
  __ret = (float64_t) __builtin_neon_vfmad_lane_f64(__s0, __s1, (int8x8_t)__s2, __p3); \
  __ret; \
})
#define __noswap_vfmad_lane_f64(__p0, __p1, __p2, __p3) __extension__ ({ \
  float64_t __s0 = __p0; \
  float64_t __s1 = __p1; \
  float64x1_t __s2 = __p2; \
  float64_t __ret; \
  __ret = (float64_t) __builtin_neon_vfmad_lane_f64(__s0, __s1, (int8x8_t)__s2, __p3); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vfmas_lane_f32(__p0, __p1, __p2, __p3) __extension__ ({ \
  float32_t __s0 = __p0; \
  float32_t __s1 = __p1; \
  float32x2_t __s2 = __p2; \
  float32_t __ret; \
  __ret = (float32_t) __builtin_neon_vfmas_lane_f32(__s0, __s1, (int8x8_t)__s2, __p3); \
  __ret; \
})
#else
#define vfmas_lane_f32(__p0, __p1, __p2, __p3) __extension__ ({ \
  float32_t __s0 = __p0; \
  float32_t __s1 = __p1; \
  float32x2_t __s2 = __p2; \
  float32x2_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 1, 0); \
  float32_t __ret; \
  __ret = (float32_t) __builtin_neon_vfmas_lane_f32(__s0, __s1, (int8x8_t)__rev2, __p3); \
  __ret; \
})
#define __noswap_vfmas_lane_f32(__p0, __p1, __p2, __p3) __extension__ ({ \
  float32_t __s0 = __p0; \
  float32_t __s1 = __p1; \
  float32x2_t __s2 = __p2; \
  float32_t __ret; \
  __ret = (float32_t) __builtin_neon_vfmas_lane_f32(__s0, __s1, (int8x8_t)__s2, __p3); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vfmaq_lane_f64(__p0, __p1, __p2, __p3) __extension__ ({ \
  float64x2_t __s0 = __p0; \
  float64x2_t __s1 = __p1; \
  float64x1_t __s2 = __p2; \
  float64x2_t __ret; \
  __ret = (float64x2_t) __builtin_neon_vfmaq_lane_v((int8x16_t)__s0, (int8x16_t)__s1, (int8x8_t)__s2, __p3, 42); \
  __ret; \
})
#else
#define vfmaq_lane_f64(__p0, __p1, __p2, __p3) __extension__ ({ \
  float64x2_t __s0 = __p0; \
  float64x2_t __s1 = __p1; \
  float64x1_t __s2 = __p2; \
  float64x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  float64x2_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 1, 0); \
  float64x2_t __ret; \
  __ret = (float64x2_t) __builtin_neon_vfmaq_lane_v((int8x16_t)__rev0, (int8x16_t)__rev1, (int8x8_t)__s2, __p3, 42); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#define __noswap_vfmaq_lane_f64(__p0, __p1, __p2, __p3) __extension__ ({ \
  float64x2_t __s0 = __p0; \
  float64x2_t __s1 = __p1; \
  float64x1_t __s2 = __p2; \
  float64x2_t __ret; \
  __ret = (float64x2_t) __builtin_neon_vfmaq_lane_v((int8x16_t)__s0, (int8x16_t)__s1, (int8x8_t)__s2, __p3, 42); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vfmaq_lane_f32(__p0, __p1, __p2, __p3) __extension__ ({ \
  float32x4_t __s0 = __p0; \
  float32x4_t __s1 = __p1; \
  float32x2_t __s2 = __p2; \
  float32x4_t __ret; \
  __ret = (float32x4_t) __builtin_neon_vfmaq_lane_v((int8x16_t)__s0, (int8x16_t)__s1, (int8x8_t)__s2, __p3, 41); \
  __ret; \
})
#else
#define vfmaq_lane_f32(__p0, __p1, __p2, __p3) __extension__ ({ \
  float32x4_t __s0 = __p0; \
  float32x4_t __s1 = __p1; \
  float32x2_t __s2 = __p2; \
  float32x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  float32x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  float32x2_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 1, 0); \
  float32x4_t __ret; \
  __ret = (float32x4_t) __builtin_neon_vfmaq_lane_v((int8x16_t)__rev0, (int8x16_t)__rev1, (int8x8_t)__rev2, __p3, 41); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#define __noswap_vfmaq_lane_f32(__p0, __p1, __p2, __p3) __extension__ ({ \
  float32x4_t __s0 = __p0; \
  float32x4_t __s1 = __p1; \
  float32x2_t __s2 = __p2; \
  float32x4_t __ret; \
  __ret = (float32x4_t) __builtin_neon_vfmaq_lane_v((int8x16_t)__s0, (int8x16_t)__s1, (int8x8_t)__s2, __p3, 41); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vfma_lane_f64(__p0, __p1, __p2, __p3) __extension__ ({ \
  float64x1_t __s0 = __p0; \
  float64x1_t __s1 = __p1; \
  float64x1_t __s2 = __p2; \
  float64x1_t __ret; \
  __ret = (float64x1_t) __builtin_neon_vfma_lane_v((int8x8_t)__s0, (int8x8_t)__s1, (int8x8_t)__s2, __p3, 10); \
  __ret; \
})
#else
#define vfma_lane_f64(__p0, __p1, __p2, __p3) __extension__ ({ \
  float64x1_t __s0 = __p0; \
  float64x1_t __s1 = __p1; \
  float64x1_t __s2 = __p2; \
  float64x1_t __ret; \
  __ret = (float64x1_t) __builtin_neon_vfma_lane_v((int8x8_t)__s0, (int8x8_t)__s1, (int8x8_t)__s2, __p3, 10); \
  __ret; \
})
#define __noswap_vfma_lane_f64(__p0, __p1, __p2, __p3) __extension__ ({ \
  float64x1_t __s0 = __p0; \
  float64x1_t __s1 = __p1; \
  float64x1_t __s2 = __p2; \
  float64x1_t __ret; \
  __ret = (float64x1_t) __builtin_neon_vfma_lane_v((int8x8_t)__s0, (int8x8_t)__s1, (int8x8_t)__s2, __p3, 10); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vfma_lane_f32(__p0, __p1, __p2, __p3) __extension__ ({ \
  float32x2_t __s0 = __p0; \
  float32x2_t __s1 = __p1; \
  float32x2_t __s2 = __p2; \
  float32x2_t __ret; \
  __ret = (float32x2_t) __builtin_neon_vfma_lane_v((int8x8_t)__s0, (int8x8_t)__s1, (int8x8_t)__s2, __p3, 9); \
  __ret; \
})
#else
#define vfma_lane_f32(__p0, __p1, __p2, __p3) __extension__ ({ \
  float32x2_t __s0 = __p0; \
  float32x2_t __s1 = __p1; \
  float32x2_t __s2 = __p2; \
  float32x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  float32x2_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 1, 0); \
  float32x2_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 1, 0); \
  float32x2_t __ret; \
  __ret = (float32x2_t) __builtin_neon_vfma_lane_v((int8x8_t)__rev0, (int8x8_t)__rev1, (int8x8_t)__rev2, __p3, 9); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#define __noswap_vfma_lane_f32(__p0, __p1, __p2, __p3) __extension__ ({ \
  float32x2_t __s0 = __p0; \
  float32x2_t __s1 = __p1; \
  float32x2_t __s2 = __p2; \
  float32x2_t __ret; \
  __ret = (float32x2_t) __builtin_neon_vfma_lane_v((int8x8_t)__s0, (int8x8_t)__s1, (int8x8_t)__s2, __p3, 9); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vfmad_laneq_f64(__p0, __p1, __p2, __p3) __extension__ ({ \
  float64_t __s0 = __p0; \
  float64_t __s1 = __p1; \
  float64x2_t __s2 = __p2; \
  float64_t __ret; \
  __ret = (float64_t) __builtin_neon_vfmad_laneq_f64(__s0, __s1, (int8x16_t)__s2, __p3); \
  __ret; \
})
#else
#define vfmad_laneq_f64(__p0, __p1, __p2, __p3) __extension__ ({ \
  float64_t __s0 = __p0; \
  float64_t __s1 = __p1; \
  float64x2_t __s2 = __p2; \
  float64x2_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 1, 0); \
  float64_t __ret; \
  __ret = (float64_t) __builtin_neon_vfmad_laneq_f64(__s0, __s1, (int8x16_t)__rev2, __p3); \
  __ret; \
})
#define __noswap_vfmad_laneq_f64(__p0, __p1, __p2, __p3) __extension__ ({ \
  float64_t __s0 = __p0; \
  float64_t __s1 = __p1; \
  float64x2_t __s2 = __p2; \
  float64_t __ret; \
  __ret = (float64_t) __builtin_neon_vfmad_laneq_f64(__s0, __s1, (int8x16_t)__s2, __p3); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vfmas_laneq_f32(__p0, __p1, __p2, __p3) __extension__ ({ \
  float32_t __s0 = __p0; \
  float32_t __s1 = __p1; \
  float32x4_t __s2 = __p2; \
  float32_t __ret; \
  __ret = (float32_t) __builtin_neon_vfmas_laneq_f32(__s0, __s1, (int8x16_t)__s2, __p3); \
  __ret; \
})
#else
#define vfmas_laneq_f32(__p0, __p1, __p2, __p3) __extension__ ({ \
  float32_t __s0 = __p0; \
  float32_t __s1 = __p1; \
  float32x4_t __s2 = __p2; \
  float32x4_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 3, 2, 1, 0); \
  float32_t __ret; \
  __ret = (float32_t) __builtin_neon_vfmas_laneq_f32(__s0, __s1, (int8x16_t)__rev2, __p3); \
  __ret; \
})
#define __noswap_vfmas_laneq_f32(__p0, __p1, __p2, __p3) __extension__ ({ \
  float32_t __s0 = __p0; \
  float32_t __s1 = __p1; \
  float32x4_t __s2 = __p2; \
  float32_t __ret; \
  __ret = (float32_t) __builtin_neon_vfmas_laneq_f32(__s0, __s1, (int8x16_t)__s2, __p3); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vfmaq_laneq_f64(__p0, __p1, __p2, __p3) __extension__ ({ \
  float64x2_t __s0 = __p0; \
  float64x2_t __s1 = __p1; \
  float64x2_t __s2 = __p2; \
  float64x2_t __ret; \
  __ret = (float64x2_t) __builtin_neon_vfmaq_laneq_v((int8x16_t)__s0, (int8x16_t)__s1, (int8x16_t)__s2, __p3, 42); \
  __ret; \
})
#else
#define vfmaq_laneq_f64(__p0, __p1, __p2, __p3) __extension__ ({ \
  float64x2_t __s0 = __p0; \
  float64x2_t __s1 = __p1; \
  float64x2_t __s2 = __p2; \
  float64x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  float64x2_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 1, 0); \
  float64x2_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 1, 0); \
  float64x2_t __ret; \
  __ret = (float64x2_t) __builtin_neon_vfmaq_laneq_v((int8x16_t)__rev0, (int8x16_t)__rev1, (int8x16_t)__rev2, __p3, 42); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#define __noswap_vfmaq_laneq_f64(__p0, __p1, __p2, __p3) __extension__ ({ \
  float64x2_t __s0 = __p0; \
  float64x2_t __s1 = __p1; \
  float64x2_t __s2 = __p2; \
  float64x2_t __ret; \
  __ret = (float64x2_t) __builtin_neon_vfmaq_laneq_v((int8x16_t)__s0, (int8x16_t)__s1, (int8x16_t)__s2, __p3, 42); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vfmaq_laneq_f32(__p0, __p1, __p2, __p3) __extension__ ({ \
  float32x4_t __s0 = __p0; \
  float32x4_t __s1 = __p1; \
  float32x4_t __s2 = __p2; \
  float32x4_t __ret; \
  __ret = (float32x4_t) __builtin_neon_vfmaq_laneq_v((int8x16_t)__s0, (int8x16_t)__s1, (int8x16_t)__s2, __p3, 41); \
  __ret; \
})
#else
#define vfmaq_laneq_f32(__p0, __p1, __p2, __p3) __extension__ ({ \
  float32x4_t __s0 = __p0; \
  float32x4_t __s1 = __p1; \
  float32x4_t __s2 = __p2; \
  float32x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  float32x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  float32x4_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 3, 2, 1, 0); \
  float32x4_t __ret; \
  __ret = (float32x4_t) __builtin_neon_vfmaq_laneq_v((int8x16_t)__rev0, (int8x16_t)__rev1, (int8x16_t)__rev2, __p3, 41); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#define __noswap_vfmaq_laneq_f32(__p0, __p1, __p2, __p3) __extension__ ({ \
  float32x4_t __s0 = __p0; \
  float32x4_t __s1 = __p1; \
  float32x4_t __s2 = __p2; \
  float32x4_t __ret; \
  __ret = (float32x4_t) __builtin_neon_vfmaq_laneq_v((int8x16_t)__s0, (int8x16_t)__s1, (int8x16_t)__s2, __p3, 41); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vfma_laneq_f64(__p0, __p1, __p2, __p3) __extension__ ({ \
  float64x1_t __s0 = __p0; \
  float64x1_t __s1 = __p1; \
  float64x2_t __s2 = __p2; \
  float64x1_t __ret; \
  __ret = (float64x1_t) __builtin_neon_vfma_laneq_v((int8x8_t)__s0, (int8x8_t)__s1, (int8x16_t)__s2, __p3, 10); \
  __ret; \
})
#else
#define vfma_laneq_f64(__p0, __p1, __p2, __p3) __extension__ ({ \
  float64x1_t __s0 = __p0; \
  float64x1_t __s1 = __p1; \
  float64x2_t __s2 = __p2; \
  float64x2_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 1, 0); \
  float64x1_t __ret; \
  __ret = (float64x1_t) __builtin_neon_vfma_laneq_v((int8x8_t)__s0, (int8x8_t)__s1, (int8x16_t)__rev2, __p3, 10); \
  __ret; \
})
#define __noswap_vfma_laneq_f64(__p0, __p1, __p2, __p3) __extension__ ({ \
  float64x1_t __s0 = __p0; \
  float64x1_t __s1 = __p1; \
  float64x2_t __s2 = __p2; \
  float64x1_t __ret; \
  __ret = (float64x1_t) __builtin_neon_vfma_laneq_v((int8x8_t)__s0, (int8x8_t)__s1, (int8x16_t)__s2, __p3, 10); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vfma_laneq_f32(__p0, __p1, __p2, __p3) __extension__ ({ \
  float32x2_t __s0 = __p0; \
  float32x2_t __s1 = __p1; \
  float32x4_t __s2 = __p2; \
  float32x2_t __ret; \
  __ret = (float32x2_t) __builtin_neon_vfma_laneq_v((int8x8_t)__s0, (int8x8_t)__s1, (int8x16_t)__s2, __p3, 9); \
  __ret; \
})
#else
#define vfma_laneq_f32(__p0, __p1, __p2, __p3) __extension__ ({ \
  float32x2_t __s0 = __p0; \
  float32x2_t __s1 = __p1; \
  float32x4_t __s2 = __p2; \
  float32x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  float32x2_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 1, 0); \
  float32x4_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 3, 2, 1, 0); \
  float32x2_t __ret; \
  __ret = (float32x2_t) __builtin_neon_vfma_laneq_v((int8x8_t)__rev0, (int8x8_t)__rev1, (int8x16_t)__rev2, __p3, 9); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#define __noswap_vfma_laneq_f32(__p0, __p1, __p2, __p3) __extension__ ({ \
  float32x2_t __s0 = __p0; \
  float32x2_t __s1 = __p1; \
  float32x4_t __s2 = __p2; \
  float32x2_t __ret; \
  __ret = (float32x2_t) __builtin_neon_vfma_laneq_v((int8x8_t)__s0, (int8x8_t)__s1, (int8x16_t)__s2, __p3, 9); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x2_t vfmaq_n_f64(float64x2_t __p0, float64x2_t __p1, float64_t __p2) {
  float64x2_t __ret;
  __ret = vfmaq_f64(__p0, __p1, (float64x2_t) {__p2, __p2});
  return __ret;
}
#else
__ai float64x2_t vfmaq_n_f64(float64x2_t __p0, float64x2_t __p1, float64_t __p2) {
  float64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  float64x2_t __ret;
  __ret = __noswap_vfmaq_f64(__rev0, __rev1, (float64x2_t) {__p2, __p2});
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x4_t vfmaq_n_f32(float32x4_t __p0, float32x4_t __p1, float32_t __p2) {
  float32x4_t __ret;
  __ret = vfmaq_f32(__p0, __p1, (float32x4_t) {__p2, __p2, __p2, __p2});
  return __ret;
}
#else
__ai float32x4_t vfmaq_n_f32(float32x4_t __p0, float32x4_t __p1, float32_t __p2) {
  float32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  float32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  float32x4_t __ret;
  __ret = __noswap_vfmaq_f32(__rev0, __rev1, (float32x4_t) {__p2, __p2, __p2, __p2});
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x2_t vfma_n_f32(float32x2_t __p0, float32x2_t __p1, float32_t __p2) {
  float32x2_t __ret;
  __ret = vfma_f32(__p0, __p1, (float32x2_t) {__p2, __p2});
  return __ret;
}
#else
__ai float32x2_t vfma_n_f32(float32x2_t __p0, float32x2_t __p1, float32_t __p2) {
  float32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  float32x2_t __ret;
  __ret = __noswap_vfma_f32(__rev0, __rev1, (float32x2_t) {__p2, __p2});
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x2_t vfmsq_f64(float64x2_t __p0, float64x2_t __p1, float64x2_t __p2) {
  float64x2_t __ret;
  __ret = (float64x2_t) __builtin_neon_vfmsq_v((int8x16_t)__p0, (int8x16_t)__p1, (int8x16_t)__p2, 42);
  return __ret;
}
#else
__ai float64x2_t vfmsq_f64(float64x2_t __p0, float64x2_t __p1, float64x2_t __p2) {
  float64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  float64x2_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 1, 0);
  float64x2_t __ret;
  __ret = (float64x2_t) __builtin_neon_vfmsq_v((int8x16_t)__rev0, (int8x16_t)__rev1, (int8x16_t)__rev2, 42);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
__ai float64x2_t __noswap_vfmsq_f64(float64x2_t __p0, float64x2_t __p1, float64x2_t __p2) {
  float64x2_t __ret;
  __ret = (float64x2_t) __builtin_neon_vfmsq_v((int8x16_t)__p0, (int8x16_t)__p1, (int8x16_t)__p2, 42);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x4_t vfmsq_f32(float32x4_t __p0, float32x4_t __p1, float32x4_t __p2) {
  float32x4_t __ret;
  __ret = (float32x4_t) __builtin_neon_vfmsq_v((int8x16_t)__p0, (int8x16_t)__p1, (int8x16_t)__p2, 41);
  return __ret;
}
#else
__ai float32x4_t vfmsq_f32(float32x4_t __p0, float32x4_t __p1, float32x4_t __p2) {
  float32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  float32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  float32x4_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 3, 2, 1, 0);
  float32x4_t __ret;
  __ret = (float32x4_t) __builtin_neon_vfmsq_v((int8x16_t)__rev0, (int8x16_t)__rev1, (int8x16_t)__rev2, 41);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
__ai float32x4_t __noswap_vfmsq_f32(float32x4_t __p0, float32x4_t __p1, float32x4_t __p2) {
  float32x4_t __ret;
  __ret = (float32x4_t) __builtin_neon_vfmsq_v((int8x16_t)__p0, (int8x16_t)__p1, (int8x16_t)__p2, 41);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x1_t vfms_f64(float64x1_t __p0, float64x1_t __p1, float64x1_t __p2) {
  float64x1_t __ret;
  __ret = (float64x1_t) __builtin_neon_vfms_v((int8x8_t)__p0, (int8x8_t)__p1, (int8x8_t)__p2, 10);
  return __ret;
}
#else
__ai float64x1_t vfms_f64(float64x1_t __p0, float64x1_t __p1, float64x1_t __p2) {
  float64x1_t __ret;
  __ret = (float64x1_t) __builtin_neon_vfms_v((int8x8_t)__p0, (int8x8_t)__p1, (int8x8_t)__p2, 10);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x2_t vfms_f32(float32x2_t __p0, float32x2_t __p1, float32x2_t __p2) {
  float32x2_t __ret;
  __ret = (float32x2_t) __builtin_neon_vfms_v((int8x8_t)__p0, (int8x8_t)__p1, (int8x8_t)__p2, 9);
  return __ret;
}
#else
__ai float32x2_t vfms_f32(float32x2_t __p0, float32x2_t __p1, float32x2_t __p2) {
  float32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  float32x2_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 1, 0);
  float32x2_t __ret;
  __ret = (float32x2_t) __builtin_neon_vfms_v((int8x8_t)__rev0, (int8x8_t)__rev1, (int8x8_t)__rev2, 9);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
__ai float32x2_t __noswap_vfms_f32(float32x2_t __p0, float32x2_t __p1, float32x2_t __p2) {
  float32x2_t __ret;
  __ret = (float32x2_t) __builtin_neon_vfms_v((int8x8_t)__p0, (int8x8_t)__p1, (int8x8_t)__p2, 9);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
#define vfmsd_lane_f64(__p0_88, __p1_88, __p2_88, __p3_88) __extension__ ({ \
  float64_t __s0_88 = __p0_88; \
  float64_t __s1_88 = __p1_88; \
  float64x1_t __s2_88 = __p2_88; \
  float64_t __ret_88; \
  __ret_88 = vfmad_lane_f64(__s0_88, __s1_88, -__s2_88, __p3_88); \
  __ret_88; \
})
#else
#define vfmsd_lane_f64(__p0_89, __p1_89, __p2_89, __p3_89) __extension__ ({ \
  float64_t __s0_89 = __p0_89; \
  float64_t __s1_89 = __p1_89; \
  float64x1_t __s2_89 = __p2_89; \
  float64_t __ret_89; \
  __ret_89 = __noswap_vfmad_lane_f64(__s0_89, __s1_89, -__s2_89, __p3_89); \
  __ret_89; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vfmss_lane_f32(__p0_90, __p1_90, __p2_90, __p3_90) __extension__ ({ \
  float32_t __s0_90 = __p0_90; \
  float32_t __s1_90 = __p1_90; \
  float32x2_t __s2_90 = __p2_90; \
  float32_t __ret_90; \
  __ret_90 = vfmas_lane_f32(__s0_90, __s1_90, -__s2_90, __p3_90); \
  __ret_90; \
})
#else
#define vfmss_lane_f32(__p0_91, __p1_91, __p2_91, __p3_91) __extension__ ({ \
  float32_t __s0_91 = __p0_91; \
  float32_t __s1_91 = __p1_91; \
  float32x2_t __s2_91 = __p2_91; \
  float32x2_t __rev2_91;  __rev2_91 = __builtin_shufflevector(__s2_91, __s2_91, 1, 0); \
  float32_t __ret_91; \
  __ret_91 = __noswap_vfmas_lane_f32(__s0_91, __s1_91, -__rev2_91, __p3_91); \
  __ret_91; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vfmsq_lane_f64(__p0_92, __p1_92, __p2_92, __p3_92) __extension__ ({ \
  float64x2_t __s0_92 = __p0_92; \
  float64x2_t __s1_92 = __p1_92; \
  float64x1_t __s2_92 = __p2_92; \
  float64x2_t __ret_92; \
  __ret_92 = vfmaq_lane_f64(__s0_92, __s1_92, -__s2_92, __p3_92); \
  __ret_92; \
})
#else
#define vfmsq_lane_f64(__p0_93, __p1_93, __p2_93, __p3_93) __extension__ ({ \
  float64x2_t __s0_93 = __p0_93; \
  float64x2_t __s1_93 = __p1_93; \
  float64x1_t __s2_93 = __p2_93; \
  float64x2_t __rev0_93;  __rev0_93 = __builtin_shufflevector(__s0_93, __s0_93, 1, 0); \
  float64x2_t __rev1_93;  __rev1_93 = __builtin_shufflevector(__s1_93, __s1_93, 1, 0); \
  float64x2_t __ret_93; \
  __ret_93 = __noswap_vfmaq_lane_f64(__rev0_93, __rev1_93, -__s2_93, __p3_93); \
  __ret_93 = __builtin_shufflevector(__ret_93, __ret_93, 1, 0); \
  __ret_93; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vfmsq_lane_f32(__p0_94, __p1_94, __p2_94, __p3_94) __extension__ ({ \
  float32x4_t __s0_94 = __p0_94; \
  float32x4_t __s1_94 = __p1_94; \
  float32x2_t __s2_94 = __p2_94; \
  float32x4_t __ret_94; \
  __ret_94 = vfmaq_lane_f32(__s0_94, __s1_94, -__s2_94, __p3_94); \
  __ret_94; \
})
#else
#define vfmsq_lane_f32(__p0_95, __p1_95, __p2_95, __p3_95) __extension__ ({ \
  float32x4_t __s0_95 = __p0_95; \
  float32x4_t __s1_95 = __p1_95; \
  float32x2_t __s2_95 = __p2_95; \
  float32x4_t __rev0_95;  __rev0_95 = __builtin_shufflevector(__s0_95, __s0_95, 3, 2, 1, 0); \
  float32x4_t __rev1_95;  __rev1_95 = __builtin_shufflevector(__s1_95, __s1_95, 3, 2, 1, 0); \
  float32x2_t __rev2_95;  __rev2_95 = __builtin_shufflevector(__s2_95, __s2_95, 1, 0); \
  float32x4_t __ret_95; \
  __ret_95 = __noswap_vfmaq_lane_f32(__rev0_95, __rev1_95, -__rev2_95, __p3_95); \
  __ret_95 = __builtin_shufflevector(__ret_95, __ret_95, 3, 2, 1, 0); \
  __ret_95; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vfms_lane_f64(__p0_96, __p1_96, __p2_96, __p3_96) __extension__ ({ \
  float64x1_t __s0_96 = __p0_96; \
  float64x1_t __s1_96 = __p1_96; \
  float64x1_t __s2_96 = __p2_96; \
  float64x1_t __ret_96; \
  __ret_96 = vfma_lane_f64(__s0_96, __s1_96, -__s2_96, __p3_96); \
  __ret_96; \
})
#else
#define vfms_lane_f64(__p0_97, __p1_97, __p2_97, __p3_97) __extension__ ({ \
  float64x1_t __s0_97 = __p0_97; \
  float64x1_t __s1_97 = __p1_97; \
  float64x1_t __s2_97 = __p2_97; \
  float64x1_t __ret_97; \
  __ret_97 = __noswap_vfma_lane_f64(__s0_97, __s1_97, -__s2_97, __p3_97); \
  __ret_97; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vfms_lane_f32(__p0_98, __p1_98, __p2_98, __p3_98) __extension__ ({ \
  float32x2_t __s0_98 = __p0_98; \
  float32x2_t __s1_98 = __p1_98; \
  float32x2_t __s2_98 = __p2_98; \
  float32x2_t __ret_98; \
  __ret_98 = vfma_lane_f32(__s0_98, __s1_98, -__s2_98, __p3_98); \
  __ret_98; \
})
#else
#define vfms_lane_f32(__p0_99, __p1_99, __p2_99, __p3_99) __extension__ ({ \
  float32x2_t __s0_99 = __p0_99; \
  float32x2_t __s1_99 = __p1_99; \
  float32x2_t __s2_99 = __p2_99; \
  float32x2_t __rev0_99;  __rev0_99 = __builtin_shufflevector(__s0_99, __s0_99, 1, 0); \
  float32x2_t __rev1_99;  __rev1_99 = __builtin_shufflevector(__s1_99, __s1_99, 1, 0); \
  float32x2_t __rev2_99;  __rev2_99 = __builtin_shufflevector(__s2_99, __s2_99, 1, 0); \
  float32x2_t __ret_99; \
  __ret_99 = __noswap_vfma_lane_f32(__rev0_99, __rev1_99, -__rev2_99, __p3_99); \
  __ret_99 = __builtin_shufflevector(__ret_99, __ret_99, 1, 0); \
  __ret_99; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vfmsd_laneq_f64(__p0_100, __p1_100, __p2_100, __p3_100) __extension__ ({ \
  float64_t __s0_100 = __p0_100; \
  float64_t __s1_100 = __p1_100; \
  float64x2_t __s2_100 = __p2_100; \
  float64_t __ret_100; \
  __ret_100 = vfmad_laneq_f64(__s0_100, __s1_100, -__s2_100, __p3_100); \
  __ret_100; \
})
#else
#define vfmsd_laneq_f64(__p0_101, __p1_101, __p2_101, __p3_101) __extension__ ({ \
  float64_t __s0_101 = __p0_101; \
  float64_t __s1_101 = __p1_101; \
  float64x2_t __s2_101 = __p2_101; \
  float64x2_t __rev2_101;  __rev2_101 = __builtin_shufflevector(__s2_101, __s2_101, 1, 0); \
  float64_t __ret_101; \
  __ret_101 = __noswap_vfmad_laneq_f64(__s0_101, __s1_101, -__rev2_101, __p3_101); \
  __ret_101; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vfmss_laneq_f32(__p0_102, __p1_102, __p2_102, __p3_102) __extension__ ({ \
  float32_t __s0_102 = __p0_102; \
  float32_t __s1_102 = __p1_102; \
  float32x4_t __s2_102 = __p2_102; \
  float32_t __ret_102; \
  __ret_102 = vfmas_laneq_f32(__s0_102, __s1_102, -__s2_102, __p3_102); \
  __ret_102; \
})
#else
#define vfmss_laneq_f32(__p0_103, __p1_103, __p2_103, __p3_103) __extension__ ({ \
  float32_t __s0_103 = __p0_103; \
  float32_t __s1_103 = __p1_103; \
  float32x4_t __s2_103 = __p2_103; \
  float32x4_t __rev2_103;  __rev2_103 = __builtin_shufflevector(__s2_103, __s2_103, 3, 2, 1, 0); \
  float32_t __ret_103; \
  __ret_103 = __noswap_vfmas_laneq_f32(__s0_103, __s1_103, -__rev2_103, __p3_103); \
  __ret_103; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vfmsq_laneq_f64(__p0_104, __p1_104, __p2_104, __p3_104) __extension__ ({ \
  float64x2_t __s0_104 = __p0_104; \
  float64x2_t __s1_104 = __p1_104; \
  float64x2_t __s2_104 = __p2_104; \
  float64x2_t __ret_104; \
  __ret_104 = vfmaq_laneq_f64(__s0_104, __s1_104, -__s2_104, __p3_104); \
  __ret_104; \
})
#else
#define vfmsq_laneq_f64(__p0_105, __p1_105, __p2_105, __p3_105) __extension__ ({ \
  float64x2_t __s0_105 = __p0_105; \
  float64x2_t __s1_105 = __p1_105; \
  float64x2_t __s2_105 = __p2_105; \
  float64x2_t __rev0_105;  __rev0_105 = __builtin_shufflevector(__s0_105, __s0_105, 1, 0); \
  float64x2_t __rev1_105;  __rev1_105 = __builtin_shufflevector(__s1_105, __s1_105, 1, 0); \
  float64x2_t __rev2_105;  __rev2_105 = __builtin_shufflevector(__s2_105, __s2_105, 1, 0); \
  float64x2_t __ret_105; \
  __ret_105 = __noswap_vfmaq_laneq_f64(__rev0_105, __rev1_105, -__rev2_105, __p3_105); \
  __ret_105 = __builtin_shufflevector(__ret_105, __ret_105, 1, 0); \
  __ret_105; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vfmsq_laneq_f32(__p0_106, __p1_106, __p2_106, __p3_106) __extension__ ({ \
  float32x4_t __s0_106 = __p0_106; \
  float32x4_t __s1_106 = __p1_106; \
  float32x4_t __s2_106 = __p2_106; \
  float32x4_t __ret_106; \
  __ret_106 = vfmaq_laneq_f32(__s0_106, __s1_106, -__s2_106, __p3_106); \
  __ret_106; \
})
#else
#define vfmsq_laneq_f32(__p0_107, __p1_107, __p2_107, __p3_107) __extension__ ({ \
  float32x4_t __s0_107 = __p0_107; \
  float32x4_t __s1_107 = __p1_107; \
  float32x4_t __s2_107 = __p2_107; \
  float32x4_t __rev0_107;  __rev0_107 = __builtin_shufflevector(__s0_107, __s0_107, 3, 2, 1, 0); \
  float32x4_t __rev1_107;  __rev1_107 = __builtin_shufflevector(__s1_107, __s1_107, 3, 2, 1, 0); \
  float32x4_t __rev2_107;  __rev2_107 = __builtin_shufflevector(__s2_107, __s2_107, 3, 2, 1, 0); \
  float32x4_t __ret_107; \
  __ret_107 = __noswap_vfmaq_laneq_f32(__rev0_107, __rev1_107, -__rev2_107, __p3_107); \
  __ret_107 = __builtin_shufflevector(__ret_107, __ret_107, 3, 2, 1, 0); \
  __ret_107; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vfms_laneq_f64(__p0_108, __p1_108, __p2_108, __p3_108) __extension__ ({ \
  float64x1_t __s0_108 = __p0_108; \
  float64x1_t __s1_108 = __p1_108; \
  float64x2_t __s2_108 = __p2_108; \
  float64x1_t __ret_108; \
  __ret_108 = vfma_laneq_f64(__s0_108, __s1_108, -__s2_108, __p3_108); \
  __ret_108; \
})
#else
#define vfms_laneq_f64(__p0_109, __p1_109, __p2_109, __p3_109) __extension__ ({ \
  float64x1_t __s0_109 = __p0_109; \
  float64x1_t __s1_109 = __p1_109; \
  float64x2_t __s2_109 = __p2_109; \
  float64x2_t __rev2_109;  __rev2_109 = __builtin_shufflevector(__s2_109, __s2_109, 1, 0); \
  float64x1_t __ret_109; \
  __ret_109 = __noswap_vfma_laneq_f64(__s0_109, __s1_109, -__rev2_109, __p3_109); \
  __ret_109; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vfms_laneq_f32(__p0_110, __p1_110, __p2_110, __p3_110) __extension__ ({ \
  float32x2_t __s0_110 = __p0_110; \
  float32x2_t __s1_110 = __p1_110; \
  float32x4_t __s2_110 = __p2_110; \
  float32x2_t __ret_110; \
  __ret_110 = vfma_laneq_f32(__s0_110, __s1_110, -__s2_110, __p3_110); \
  __ret_110; \
})
#else
#define vfms_laneq_f32(__p0_111, __p1_111, __p2_111, __p3_111) __extension__ ({ \
  float32x2_t __s0_111 = __p0_111; \
  float32x2_t __s1_111 = __p1_111; \
  float32x4_t __s2_111 = __p2_111; \
  float32x2_t __rev0_111;  __rev0_111 = __builtin_shufflevector(__s0_111, __s0_111, 1, 0); \
  float32x2_t __rev1_111;  __rev1_111 = __builtin_shufflevector(__s1_111, __s1_111, 1, 0); \
  float32x4_t __rev2_111;  __rev2_111 = __builtin_shufflevector(__s2_111, __s2_111, 3, 2, 1, 0); \
  float32x2_t __ret_111; \
  __ret_111 = __noswap_vfma_laneq_f32(__rev0_111, __rev1_111, -__rev2_111, __p3_111); \
  __ret_111 = __builtin_shufflevector(__ret_111, __ret_111, 1, 0); \
  __ret_111; \
})
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x2_t vfmsq_n_f64(float64x2_t __p0, float64x2_t __p1, float64_t __p2) {
  float64x2_t __ret;
  __ret = vfmsq_f64(__p0, __p1, (float64x2_t) {__p2, __p2});
  return __ret;
}
#else
__ai float64x2_t vfmsq_n_f64(float64x2_t __p0, float64x2_t __p1, float64_t __p2) {
  float64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  float64x2_t __ret;
  __ret = __noswap_vfmsq_f64(__rev0, __rev1, (float64x2_t) {__p2, __p2});
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x4_t vfmsq_n_f32(float32x4_t __p0, float32x4_t __p1, float32_t __p2) {
  float32x4_t __ret;
  __ret = vfmsq_f32(__p0, __p1, (float32x4_t) {__p2, __p2, __p2, __p2});
  return __ret;
}
#else
__ai float32x4_t vfmsq_n_f32(float32x4_t __p0, float32x4_t __p1, float32_t __p2) {
  float32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  float32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  float32x4_t __ret;
  __ret = __noswap_vfmsq_f32(__rev0, __rev1, (float32x4_t) {__p2, __p2, __p2, __p2});
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x2_t vfms_n_f32(float32x2_t __p0, float32x2_t __p1, float32_t __p2) {
  float32x2_t __ret;
  __ret = vfms_f32(__p0, __p1, (float32x2_t) {__p2, __p2});
  return __ret;
}
#else
__ai float32x2_t vfms_n_f32(float32x2_t __p0, float32x2_t __p1, float32_t __p2) {
  float32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  float32x2_t __ret;
  __ret = __noswap_vfms_f32(__rev0, __rev1, (float32x2_t) {__p2, __p2});
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly64x1_t vget_high_p64(poly64x2_t __p0) {
  poly64x1_t __ret;
  __ret = __builtin_shufflevector(__p0, __p0, 1);
  return __ret;
}
#else
__ai poly64x1_t vget_high_p64(poly64x2_t __p0) {
  poly64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  poly64x1_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev0, 1);
  return __ret;
}
__ai poly64x1_t __noswap_vget_high_p64(poly64x2_t __p0) {
  poly64x1_t __ret;
  __ret = __builtin_shufflevector(__p0, __p0, 1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x1_t vget_high_f64(float64x2_t __p0) {
  float64x1_t __ret;
  __ret = __builtin_shufflevector(__p0, __p0, 1);
  return __ret;
}
#else
__ai float64x1_t vget_high_f64(float64x2_t __p0) {
  float64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float64x1_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev0, 1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
#define vget_lane_p64(__p0, __p1) __extension__ ({ \
  poly64x1_t __s0 = __p0; \
  poly64_t __ret; \
  __ret = (poly64_t) __builtin_neon_vget_lane_i64((int8x8_t)__s0, __p1); \
  __ret; \
})
#else
#define vget_lane_p64(__p0, __p1) __extension__ ({ \
  poly64x1_t __s0 = __p0; \
  poly64_t __ret; \
  __ret = (poly64_t) __builtin_neon_vget_lane_i64((int8x8_t)__s0, __p1); \
  __ret; \
})
#define __noswap_vget_lane_p64(__p0, __p1) __extension__ ({ \
  poly64x1_t __s0 = __p0; \
  poly64_t __ret; \
  __ret = (poly64_t) __builtin_neon_vget_lane_i64((int8x8_t)__s0, __p1); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vgetq_lane_p64(__p0, __p1) __extension__ ({ \
  poly64x2_t __s0 = __p0; \
  poly64_t __ret; \
  __ret = (poly64_t) __builtin_neon_vgetq_lane_i64((int8x16_t)__s0, __p1); \
  __ret; \
})
#else
#define vgetq_lane_p64(__p0, __p1) __extension__ ({ \
  poly64x2_t __s0 = __p0; \
  poly64x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  poly64_t __ret; \
  __ret = (poly64_t) __builtin_neon_vgetq_lane_i64((int8x16_t)__rev0, __p1); \
  __ret; \
})
#define __noswap_vgetq_lane_p64(__p0, __p1) __extension__ ({ \
  poly64x2_t __s0 = __p0; \
  poly64_t __ret; \
  __ret = (poly64_t) __builtin_neon_vgetq_lane_i64((int8x16_t)__s0, __p1); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vgetq_lane_f64(__p0, __p1) __extension__ ({ \
  float64x2_t __s0 = __p0; \
  float64_t __ret; \
  __ret = (float64_t) __builtin_neon_vgetq_lane_f64((int8x16_t)__s0, __p1); \
  __ret; \
})
#else
#define vgetq_lane_f64(__p0, __p1) __extension__ ({ \
  float64x2_t __s0 = __p0; \
  float64x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  float64_t __ret; \
  __ret = (float64_t) __builtin_neon_vgetq_lane_f64((int8x16_t)__rev0, __p1); \
  __ret; \
})
#define __noswap_vgetq_lane_f64(__p0, __p1) __extension__ ({ \
  float64x2_t __s0 = __p0; \
  float64_t __ret; \
  __ret = (float64_t) __builtin_neon_vgetq_lane_f64((int8x16_t)__s0, __p1); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vget_lane_f64(__p0, __p1) __extension__ ({ \
  float64x1_t __s0 = __p0; \
  float64_t __ret; \
  __ret = (float64_t) __builtin_neon_vget_lane_f64((int8x8_t)__s0, __p1); \
  __ret; \
})
#else
#define vget_lane_f64(__p0, __p1) __extension__ ({ \
  float64x1_t __s0 = __p0; \
  float64_t __ret; \
  __ret = (float64_t) __builtin_neon_vget_lane_f64((int8x8_t)__s0, __p1); \
  __ret; \
})
#define __noswap_vget_lane_f64(__p0, __p1) __extension__ ({ \
  float64x1_t __s0 = __p0; \
  float64_t __ret; \
  __ret = (float64_t) __builtin_neon_vget_lane_f64((int8x8_t)__s0, __p1); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vget_lane_f16(__p0_112, __p1_112) __extension__ ({ \
  float16x4_t __s0_112 = __p0_112; \
  float16_t __ret_112; \
float16x4_t __reint_112 = __s0_112; \
int16_t __reint1_112 = vget_lane_s16(*(int16x4_t *) &__reint_112, __p1_112); \
  __ret_112 = *(float16_t *) &__reint1_112; \
  __ret_112; \
})
#else
#define vget_lane_f16(__p0_113, __p1_113) __extension__ ({ \
  float16x4_t __s0_113 = __p0_113; \
  float16x4_t __rev0_113;  __rev0_113 = __builtin_shufflevector(__s0_113, __s0_113, 3, 2, 1, 0); \
  float16_t __ret_113; \
float16x4_t __reint_113 = __rev0_113; \
int16_t __reint1_113 = __noswap_vget_lane_s16(*(int16x4_t *) &__reint_113, __p1_113); \
  __ret_113 = *(float16_t *) &__reint1_113; \
  __ret_113; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vgetq_lane_f16(__p0_114, __p1_114) __extension__ ({ \
  float16x8_t __s0_114 = __p0_114; \
  float16_t __ret_114; \
float16x8_t __reint_114 = __s0_114; \
int16_t __reint1_114 = vgetq_lane_s16(*(int16x8_t *) &__reint_114, __p1_114); \
  __ret_114 = *(float16_t *) &__reint1_114; \
  __ret_114; \
})
#else
#define vgetq_lane_f16(__p0_115, __p1_115) __extension__ ({ \
  float16x8_t __s0_115 = __p0_115; \
  float16x8_t __rev0_115;  __rev0_115 = __builtin_shufflevector(__s0_115, __s0_115, 7, 6, 5, 4, 3, 2, 1, 0); \
  float16_t __ret_115; \
float16x8_t __reint_115 = __rev0_115; \
int16_t __reint1_115 = __noswap_vgetq_lane_s16(*(int16x8_t *) &__reint_115, __p1_115); \
  __ret_115 = *(float16_t *) &__reint1_115; \
  __ret_115; \
})
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly64x1_t vget_low_p64(poly64x2_t __p0) {
  poly64x1_t __ret;
  __ret = __builtin_shufflevector(__p0, __p0, 0);
  return __ret;
}
#else
__ai poly64x1_t vget_low_p64(poly64x2_t __p0) {
  poly64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  poly64x1_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev0, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x1_t vget_low_f64(float64x2_t __p0) {
  float64x1_t __ret;
  __ret = __builtin_shufflevector(__p0, __p0, 0);
  return __ret;
}
#else
__ai float64x1_t vget_low_f64(float64x2_t __p0) {
  float64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float64x1_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev0, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
#define vld1_p64(__p0) __extension__ ({ \
  poly64x1_t __ret; \
  __ret = (poly64x1_t) __builtin_neon_vld1_v(__p0, 6); \
  __ret; \
})
#else
#define vld1_p64(__p0) __extension__ ({ \
  poly64x1_t __ret; \
  __ret = (poly64x1_t) __builtin_neon_vld1_v(__p0, 6); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld1q_p64(__p0) __extension__ ({ \
  poly64x2_t __ret; \
  __ret = (poly64x2_t) __builtin_neon_vld1q_v(__p0, 38); \
  __ret; \
})
#else
#define vld1q_p64(__p0) __extension__ ({ \
  poly64x2_t __ret; \
  __ret = (poly64x2_t) __builtin_neon_vld1q_v(__p0, 38); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld1q_f64(__p0) __extension__ ({ \
  float64x2_t __ret; \
  __ret = (float64x2_t) __builtin_neon_vld1q_v(__p0, 42); \
  __ret; \
})
#else
#define vld1q_f64(__p0) __extension__ ({ \
  float64x2_t __ret; \
  __ret = (float64x2_t) __builtin_neon_vld1q_v(__p0, 42); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld1_f64(__p0) __extension__ ({ \
  float64x1_t __ret; \
  __ret = (float64x1_t) __builtin_neon_vld1_v(__p0, 10); \
  __ret; \
})
#else
#define vld1_f64(__p0) __extension__ ({ \
  float64x1_t __ret; \
  __ret = (float64x1_t) __builtin_neon_vld1_v(__p0, 10); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld1_dup_p64(__p0) __extension__ ({ \
  poly64x1_t __ret; \
  __ret = (poly64x1_t) __builtin_neon_vld1_dup_v(__p0, 6); \
  __ret; \
})
#else
#define vld1_dup_p64(__p0) __extension__ ({ \
  poly64x1_t __ret; \
  __ret = (poly64x1_t) __builtin_neon_vld1_dup_v(__p0, 6); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld1q_dup_p64(__p0) __extension__ ({ \
  poly64x2_t __ret; \
  __ret = (poly64x2_t) __builtin_neon_vld1q_dup_v(__p0, 38); \
  __ret; \
})
#else
#define vld1q_dup_p64(__p0) __extension__ ({ \
  poly64x2_t __ret; \
  __ret = (poly64x2_t) __builtin_neon_vld1q_dup_v(__p0, 38); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld1q_dup_f64(__p0) __extension__ ({ \
  float64x2_t __ret; \
  __ret = (float64x2_t) __builtin_neon_vld1q_dup_v(__p0, 42); \
  __ret; \
})
#else
#define vld1q_dup_f64(__p0) __extension__ ({ \
  float64x2_t __ret; \
  __ret = (float64x2_t) __builtin_neon_vld1q_dup_v(__p0, 42); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld1_dup_f64(__p0) __extension__ ({ \
  float64x1_t __ret; \
  __ret = (float64x1_t) __builtin_neon_vld1_dup_v(__p0, 10); \
  __ret; \
})
#else
#define vld1_dup_f64(__p0) __extension__ ({ \
  float64x1_t __ret; \
  __ret = (float64x1_t) __builtin_neon_vld1_dup_v(__p0, 10); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld1_lane_p64(__p0, __p1, __p2) __extension__ ({ \
  poly64x1_t __s1 = __p1; \
  poly64x1_t __ret; \
  __ret = (poly64x1_t) __builtin_neon_vld1_lane_v(__p0, (int8x8_t)__s1, __p2, 6); \
  __ret; \
})
#else
#define vld1_lane_p64(__p0, __p1, __p2) __extension__ ({ \
  poly64x1_t __s1 = __p1; \
  poly64x1_t __ret; \
  __ret = (poly64x1_t) __builtin_neon_vld1_lane_v(__p0, (int8x8_t)__s1, __p2, 6); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld1q_lane_p64(__p0, __p1, __p2) __extension__ ({ \
  poly64x2_t __s1 = __p1; \
  poly64x2_t __ret; \
  __ret = (poly64x2_t) __builtin_neon_vld1q_lane_v(__p0, (int8x16_t)__s1, __p2, 38); \
  __ret; \
})
#else
#define vld1q_lane_p64(__p0, __p1, __p2) __extension__ ({ \
  poly64x2_t __s1 = __p1; \
  poly64x2_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 1, 0); \
  poly64x2_t __ret; \
  __ret = (poly64x2_t) __builtin_neon_vld1q_lane_v(__p0, (int8x16_t)__rev1, __p2, 38); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld1q_lane_f64(__p0, __p1, __p2) __extension__ ({ \
  float64x2_t __s1 = __p1; \
  float64x2_t __ret; \
  __ret = (float64x2_t) __builtin_neon_vld1q_lane_v(__p0, (int8x16_t)__s1, __p2, 42); \
  __ret; \
})
#else
#define vld1q_lane_f64(__p0, __p1, __p2) __extension__ ({ \
  float64x2_t __s1 = __p1; \
  float64x2_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 1, 0); \
  float64x2_t __ret; \
  __ret = (float64x2_t) __builtin_neon_vld1q_lane_v(__p0, (int8x16_t)__rev1, __p2, 42); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld1_lane_f64(__p0, __p1, __p2) __extension__ ({ \
  float64x1_t __s1 = __p1; \
  float64x1_t __ret; \
  __ret = (float64x1_t) __builtin_neon_vld1_lane_v(__p0, (int8x8_t)__s1, __p2, 10); \
  __ret; \
})
#else
#define vld1_lane_f64(__p0, __p1, __p2) __extension__ ({ \
  float64x1_t __s1 = __p1; \
  float64x1_t __ret; \
  __ret = (float64x1_t) __builtin_neon_vld1_lane_v(__p0, (int8x8_t)__s1, __p2, 10); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld1_p8_x2(__p0) __extension__ ({ \
  poly8x8x2_t __ret; \
  __builtin_neon_vld1_x2_v(&__ret, __p0, 4); \
  __ret; \
})
#else
#define vld1_p8_x2(__p0) __extension__ ({ \
  poly8x8x2_t __ret; \
  __builtin_neon_vld1_x2_v(&__ret, __p0, 4); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld1_p64_x2(__p0) __extension__ ({ \
  poly64x1x2_t __ret; \
  __builtin_neon_vld1_x2_v(&__ret, __p0, 6); \
  __ret; \
})
#else
#define vld1_p64_x2(__p0) __extension__ ({ \
  poly64x1x2_t __ret; \
  __builtin_neon_vld1_x2_v(&__ret, __p0, 6); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld1_p16_x2(__p0) __extension__ ({ \
  poly16x4x2_t __ret; \
  __builtin_neon_vld1_x2_v(&__ret, __p0, 5); \
  __ret; \
})
#else
#define vld1_p16_x2(__p0) __extension__ ({ \
  poly16x4x2_t __ret; \
  __builtin_neon_vld1_x2_v(&__ret, __p0, 5); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 3, 2, 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld1q_p8_x2(__p0) __extension__ ({ \
  poly8x16x2_t __ret; \
  __builtin_neon_vld1q_x2_v(&__ret, __p0, 36); \
  __ret; \
})
#else
#define vld1q_p8_x2(__p0) __extension__ ({ \
  poly8x16x2_t __ret; \
  __builtin_neon_vld1q_x2_v(&__ret, __p0, 36); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld1q_p64_x2(__p0) __extension__ ({ \
  poly64x2x2_t __ret; \
  __builtin_neon_vld1q_x2_v(&__ret, __p0, 38); \
  __ret; \
})
#else
#define vld1q_p64_x2(__p0) __extension__ ({ \
  poly64x2x2_t __ret; \
  __builtin_neon_vld1q_x2_v(&__ret, __p0, 38); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld1q_p16_x2(__p0) __extension__ ({ \
  poly16x8x2_t __ret; \
  __builtin_neon_vld1q_x2_v(&__ret, __p0, 37); \
  __ret; \
})
#else
#define vld1q_p16_x2(__p0) __extension__ ({ \
  poly16x8x2_t __ret; \
  __builtin_neon_vld1q_x2_v(&__ret, __p0, 37); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld1q_u8_x2(__p0) __extension__ ({ \
  uint8x16x2_t __ret; \
  __builtin_neon_vld1q_x2_v(&__ret, __p0, 48); \
  __ret; \
})
#else
#define vld1q_u8_x2(__p0) __extension__ ({ \
  uint8x16x2_t __ret; \
  __builtin_neon_vld1q_x2_v(&__ret, __p0, 48); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld1q_u32_x2(__p0) __extension__ ({ \
  uint32x4x2_t __ret; \
  __builtin_neon_vld1q_x2_v(&__ret, __p0, 50); \
  __ret; \
})
#else
#define vld1q_u32_x2(__p0) __extension__ ({ \
  uint32x4x2_t __ret; \
  __builtin_neon_vld1q_x2_v(&__ret, __p0, 50); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 3, 2, 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld1q_u64_x2(__p0) __extension__ ({ \
  uint64x2x2_t __ret; \
  __builtin_neon_vld1q_x2_v(&__ret, __p0, 51); \
  __ret; \
})
#else
#define vld1q_u64_x2(__p0) __extension__ ({ \
  uint64x2x2_t __ret; \
  __builtin_neon_vld1q_x2_v(&__ret, __p0, 51); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld1q_u16_x2(__p0) __extension__ ({ \
  uint16x8x2_t __ret; \
  __builtin_neon_vld1q_x2_v(&__ret, __p0, 49); \
  __ret; \
})
#else
#define vld1q_u16_x2(__p0) __extension__ ({ \
  uint16x8x2_t __ret; \
  __builtin_neon_vld1q_x2_v(&__ret, __p0, 49); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld1q_s8_x2(__p0) __extension__ ({ \
  int8x16x2_t __ret; \
  __builtin_neon_vld1q_x2_v(&__ret, __p0, 32); \
  __ret; \
})
#else
#define vld1q_s8_x2(__p0) __extension__ ({ \
  int8x16x2_t __ret; \
  __builtin_neon_vld1q_x2_v(&__ret, __p0, 32); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld1q_f64_x2(__p0) __extension__ ({ \
  float64x2x2_t __ret; \
  __builtin_neon_vld1q_x2_v(&__ret, __p0, 42); \
  __ret; \
})
#else
#define vld1q_f64_x2(__p0) __extension__ ({ \
  float64x2x2_t __ret; \
  __builtin_neon_vld1q_x2_v(&__ret, __p0, 42); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld1q_f32_x2(__p0) __extension__ ({ \
  float32x4x2_t __ret; \
  __builtin_neon_vld1q_x2_v(&__ret, __p0, 41); \
  __ret; \
})
#else
#define vld1q_f32_x2(__p0) __extension__ ({ \
  float32x4x2_t __ret; \
  __builtin_neon_vld1q_x2_v(&__ret, __p0, 41); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 3, 2, 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld1q_f16_x2(__p0) __extension__ ({ \
  float16x8x2_t __ret; \
  __builtin_neon_vld1q_x2_v(&__ret, __p0, 40); \
  __ret; \
})
#else
#define vld1q_f16_x2(__p0) __extension__ ({ \
  float16x8x2_t __ret; \
  __builtin_neon_vld1q_x2_v(&__ret, __p0, 40); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld1q_s32_x2(__p0) __extension__ ({ \
  int32x4x2_t __ret; \
  __builtin_neon_vld1q_x2_v(&__ret, __p0, 34); \
  __ret; \
})
#else
#define vld1q_s32_x2(__p0) __extension__ ({ \
  int32x4x2_t __ret; \
  __builtin_neon_vld1q_x2_v(&__ret, __p0, 34); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 3, 2, 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld1q_s64_x2(__p0) __extension__ ({ \
  int64x2x2_t __ret; \
  __builtin_neon_vld1q_x2_v(&__ret, __p0, 35); \
  __ret; \
})
#else
#define vld1q_s64_x2(__p0) __extension__ ({ \
  int64x2x2_t __ret; \
  __builtin_neon_vld1q_x2_v(&__ret, __p0, 35); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld1q_s16_x2(__p0) __extension__ ({ \
  int16x8x2_t __ret; \
  __builtin_neon_vld1q_x2_v(&__ret, __p0, 33); \
  __ret; \
})
#else
#define vld1q_s16_x2(__p0) __extension__ ({ \
  int16x8x2_t __ret; \
  __builtin_neon_vld1q_x2_v(&__ret, __p0, 33); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld1_u8_x2(__p0) __extension__ ({ \
  uint8x8x2_t __ret; \
  __builtin_neon_vld1_x2_v(&__ret, __p0, 16); \
  __ret; \
})
#else
#define vld1_u8_x2(__p0) __extension__ ({ \
  uint8x8x2_t __ret; \
  __builtin_neon_vld1_x2_v(&__ret, __p0, 16); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld1_u32_x2(__p0) __extension__ ({ \
  uint32x2x2_t __ret; \
  __builtin_neon_vld1_x2_v(&__ret, __p0, 18); \
  __ret; \
})
#else
#define vld1_u32_x2(__p0) __extension__ ({ \
  uint32x2x2_t __ret; \
  __builtin_neon_vld1_x2_v(&__ret, __p0, 18); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld1_u64_x2(__p0) __extension__ ({ \
  uint64x1x2_t __ret; \
  __builtin_neon_vld1_x2_v(&__ret, __p0, 19); \
  __ret; \
})
#else
#define vld1_u64_x2(__p0) __extension__ ({ \
  uint64x1x2_t __ret; \
  __builtin_neon_vld1_x2_v(&__ret, __p0, 19); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld1_u16_x2(__p0) __extension__ ({ \
  uint16x4x2_t __ret; \
  __builtin_neon_vld1_x2_v(&__ret, __p0, 17); \
  __ret; \
})
#else
#define vld1_u16_x2(__p0) __extension__ ({ \
  uint16x4x2_t __ret; \
  __builtin_neon_vld1_x2_v(&__ret, __p0, 17); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 3, 2, 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld1_s8_x2(__p0) __extension__ ({ \
  int8x8x2_t __ret; \
  __builtin_neon_vld1_x2_v(&__ret, __p0, 0); \
  __ret; \
})
#else
#define vld1_s8_x2(__p0) __extension__ ({ \
  int8x8x2_t __ret; \
  __builtin_neon_vld1_x2_v(&__ret, __p0, 0); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld1_f64_x2(__p0) __extension__ ({ \
  float64x1x2_t __ret; \
  __builtin_neon_vld1_x2_v(&__ret, __p0, 10); \
  __ret; \
})
#else
#define vld1_f64_x2(__p0) __extension__ ({ \
  float64x1x2_t __ret; \
  __builtin_neon_vld1_x2_v(&__ret, __p0, 10); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld1_f32_x2(__p0) __extension__ ({ \
  float32x2x2_t __ret; \
  __builtin_neon_vld1_x2_v(&__ret, __p0, 9); \
  __ret; \
})
#else
#define vld1_f32_x2(__p0) __extension__ ({ \
  float32x2x2_t __ret; \
  __builtin_neon_vld1_x2_v(&__ret, __p0, 9); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld1_f16_x2(__p0) __extension__ ({ \
  float16x4x2_t __ret; \
  __builtin_neon_vld1_x2_v(&__ret, __p0, 8); \
  __ret; \
})
#else
#define vld1_f16_x2(__p0) __extension__ ({ \
  float16x4x2_t __ret; \
  __builtin_neon_vld1_x2_v(&__ret, __p0, 8); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 3, 2, 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld1_s32_x2(__p0) __extension__ ({ \
  int32x2x2_t __ret; \
  __builtin_neon_vld1_x2_v(&__ret, __p0, 2); \
  __ret; \
})
#else
#define vld1_s32_x2(__p0) __extension__ ({ \
  int32x2x2_t __ret; \
  __builtin_neon_vld1_x2_v(&__ret, __p0, 2); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld1_s64_x2(__p0) __extension__ ({ \
  int64x1x2_t __ret; \
  __builtin_neon_vld1_x2_v(&__ret, __p0, 3); \
  __ret; \
})
#else
#define vld1_s64_x2(__p0) __extension__ ({ \
  int64x1x2_t __ret; \
  __builtin_neon_vld1_x2_v(&__ret, __p0, 3); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld1_s16_x2(__p0) __extension__ ({ \
  int16x4x2_t __ret; \
  __builtin_neon_vld1_x2_v(&__ret, __p0, 1); \
  __ret; \
})
#else
#define vld1_s16_x2(__p0) __extension__ ({ \
  int16x4x2_t __ret; \
  __builtin_neon_vld1_x2_v(&__ret, __p0, 1); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 3, 2, 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld1_p8_x3(__p0) __extension__ ({ \
  poly8x8x3_t __ret; \
  __builtin_neon_vld1_x3_v(&__ret, __p0, 4); \
  __ret; \
})
#else
#define vld1_p8_x3(__p0) __extension__ ({ \
  poly8x8x3_t __ret; \
  __builtin_neon_vld1_x3_v(&__ret, __p0, 4); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld1_p64_x3(__p0) __extension__ ({ \
  poly64x1x3_t __ret; \
  __builtin_neon_vld1_x3_v(&__ret, __p0, 6); \
  __ret; \
})
#else
#define vld1_p64_x3(__p0) __extension__ ({ \
  poly64x1x3_t __ret; \
  __builtin_neon_vld1_x3_v(&__ret, __p0, 6); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld1_p16_x3(__p0) __extension__ ({ \
  poly16x4x3_t __ret; \
  __builtin_neon_vld1_x3_v(&__ret, __p0, 5); \
  __ret; \
})
#else
#define vld1_p16_x3(__p0) __extension__ ({ \
  poly16x4x3_t __ret; \
  __builtin_neon_vld1_x3_v(&__ret, __p0, 5); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 3, 2, 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 3, 2, 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld1q_p8_x3(__p0) __extension__ ({ \
  poly8x16x3_t __ret; \
  __builtin_neon_vld1q_x3_v(&__ret, __p0, 36); \
  __ret; \
})
#else
#define vld1q_p8_x3(__p0) __extension__ ({ \
  poly8x16x3_t __ret; \
  __builtin_neon_vld1q_x3_v(&__ret, __p0, 36); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld1q_p64_x3(__p0) __extension__ ({ \
  poly64x2x3_t __ret; \
  __builtin_neon_vld1q_x3_v(&__ret, __p0, 38); \
  __ret; \
})
#else
#define vld1q_p64_x3(__p0) __extension__ ({ \
  poly64x2x3_t __ret; \
  __builtin_neon_vld1q_x3_v(&__ret, __p0, 38); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld1q_p16_x3(__p0) __extension__ ({ \
  poly16x8x3_t __ret; \
  __builtin_neon_vld1q_x3_v(&__ret, __p0, 37); \
  __ret; \
})
#else
#define vld1q_p16_x3(__p0) __extension__ ({ \
  poly16x8x3_t __ret; \
  __builtin_neon_vld1q_x3_v(&__ret, __p0, 37); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld1q_u8_x3(__p0) __extension__ ({ \
  uint8x16x3_t __ret; \
  __builtin_neon_vld1q_x3_v(&__ret, __p0, 48); \
  __ret; \
})
#else
#define vld1q_u8_x3(__p0) __extension__ ({ \
  uint8x16x3_t __ret; \
  __builtin_neon_vld1q_x3_v(&__ret, __p0, 48); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld1q_u32_x3(__p0) __extension__ ({ \
  uint32x4x3_t __ret; \
  __builtin_neon_vld1q_x3_v(&__ret, __p0, 50); \
  __ret; \
})
#else
#define vld1q_u32_x3(__p0) __extension__ ({ \
  uint32x4x3_t __ret; \
  __builtin_neon_vld1q_x3_v(&__ret, __p0, 50); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 3, 2, 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 3, 2, 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld1q_u64_x3(__p0) __extension__ ({ \
  uint64x2x3_t __ret; \
  __builtin_neon_vld1q_x3_v(&__ret, __p0, 51); \
  __ret; \
})
#else
#define vld1q_u64_x3(__p0) __extension__ ({ \
  uint64x2x3_t __ret; \
  __builtin_neon_vld1q_x3_v(&__ret, __p0, 51); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld1q_u16_x3(__p0) __extension__ ({ \
  uint16x8x3_t __ret; \
  __builtin_neon_vld1q_x3_v(&__ret, __p0, 49); \
  __ret; \
})
#else
#define vld1q_u16_x3(__p0) __extension__ ({ \
  uint16x8x3_t __ret; \
  __builtin_neon_vld1q_x3_v(&__ret, __p0, 49); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld1q_s8_x3(__p0) __extension__ ({ \
  int8x16x3_t __ret; \
  __builtin_neon_vld1q_x3_v(&__ret, __p0, 32); \
  __ret; \
})
#else
#define vld1q_s8_x3(__p0) __extension__ ({ \
  int8x16x3_t __ret; \
  __builtin_neon_vld1q_x3_v(&__ret, __p0, 32); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld1q_f64_x3(__p0) __extension__ ({ \
  float64x2x3_t __ret; \
  __builtin_neon_vld1q_x3_v(&__ret, __p0, 42); \
  __ret; \
})
#else
#define vld1q_f64_x3(__p0) __extension__ ({ \
  float64x2x3_t __ret; \
  __builtin_neon_vld1q_x3_v(&__ret, __p0, 42); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld1q_f32_x3(__p0) __extension__ ({ \
  float32x4x3_t __ret; \
  __builtin_neon_vld1q_x3_v(&__ret, __p0, 41); \
  __ret; \
})
#else
#define vld1q_f32_x3(__p0) __extension__ ({ \
  float32x4x3_t __ret; \
  __builtin_neon_vld1q_x3_v(&__ret, __p0, 41); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 3, 2, 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 3, 2, 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld1q_f16_x3(__p0) __extension__ ({ \
  float16x8x3_t __ret; \
  __builtin_neon_vld1q_x3_v(&__ret, __p0, 40); \
  __ret; \
})
#else
#define vld1q_f16_x3(__p0) __extension__ ({ \
  float16x8x3_t __ret; \
  __builtin_neon_vld1q_x3_v(&__ret, __p0, 40); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld1q_s32_x3(__p0) __extension__ ({ \
  int32x4x3_t __ret; \
  __builtin_neon_vld1q_x3_v(&__ret, __p0, 34); \
  __ret; \
})
#else
#define vld1q_s32_x3(__p0) __extension__ ({ \
  int32x4x3_t __ret; \
  __builtin_neon_vld1q_x3_v(&__ret, __p0, 34); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 3, 2, 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 3, 2, 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld1q_s64_x3(__p0) __extension__ ({ \
  int64x2x3_t __ret; \
  __builtin_neon_vld1q_x3_v(&__ret, __p0, 35); \
  __ret; \
})
#else
#define vld1q_s64_x3(__p0) __extension__ ({ \
  int64x2x3_t __ret; \
  __builtin_neon_vld1q_x3_v(&__ret, __p0, 35); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld1q_s16_x3(__p0) __extension__ ({ \
  int16x8x3_t __ret; \
  __builtin_neon_vld1q_x3_v(&__ret, __p0, 33); \
  __ret; \
})
#else
#define vld1q_s16_x3(__p0) __extension__ ({ \
  int16x8x3_t __ret; \
  __builtin_neon_vld1q_x3_v(&__ret, __p0, 33); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld1_u8_x3(__p0) __extension__ ({ \
  uint8x8x3_t __ret; \
  __builtin_neon_vld1_x3_v(&__ret, __p0, 16); \
  __ret; \
})
#else
#define vld1_u8_x3(__p0) __extension__ ({ \
  uint8x8x3_t __ret; \
  __builtin_neon_vld1_x3_v(&__ret, __p0, 16); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld1_u32_x3(__p0) __extension__ ({ \
  uint32x2x3_t __ret; \
  __builtin_neon_vld1_x3_v(&__ret, __p0, 18); \
  __ret; \
})
#else
#define vld1_u32_x3(__p0) __extension__ ({ \
  uint32x2x3_t __ret; \
  __builtin_neon_vld1_x3_v(&__ret, __p0, 18); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld1_u64_x3(__p0) __extension__ ({ \
  uint64x1x3_t __ret; \
  __builtin_neon_vld1_x3_v(&__ret, __p0, 19); \
  __ret; \
})
#else
#define vld1_u64_x3(__p0) __extension__ ({ \
  uint64x1x3_t __ret; \
  __builtin_neon_vld1_x3_v(&__ret, __p0, 19); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld1_u16_x3(__p0) __extension__ ({ \
  uint16x4x3_t __ret; \
  __builtin_neon_vld1_x3_v(&__ret, __p0, 17); \
  __ret; \
})
#else
#define vld1_u16_x3(__p0) __extension__ ({ \
  uint16x4x3_t __ret; \
  __builtin_neon_vld1_x3_v(&__ret, __p0, 17); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 3, 2, 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 3, 2, 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld1_s8_x3(__p0) __extension__ ({ \
  int8x8x3_t __ret; \
  __builtin_neon_vld1_x3_v(&__ret, __p0, 0); \
  __ret; \
})
#else
#define vld1_s8_x3(__p0) __extension__ ({ \
  int8x8x3_t __ret; \
  __builtin_neon_vld1_x3_v(&__ret, __p0, 0); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld1_f64_x3(__p0) __extension__ ({ \
  float64x1x3_t __ret; \
  __builtin_neon_vld1_x3_v(&__ret, __p0, 10); \
  __ret; \
})
#else
#define vld1_f64_x3(__p0) __extension__ ({ \
  float64x1x3_t __ret; \
  __builtin_neon_vld1_x3_v(&__ret, __p0, 10); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld1_f32_x3(__p0) __extension__ ({ \
  float32x2x3_t __ret; \
  __builtin_neon_vld1_x3_v(&__ret, __p0, 9); \
  __ret; \
})
#else
#define vld1_f32_x3(__p0) __extension__ ({ \
  float32x2x3_t __ret; \
  __builtin_neon_vld1_x3_v(&__ret, __p0, 9); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld1_f16_x3(__p0) __extension__ ({ \
  float16x4x3_t __ret; \
  __builtin_neon_vld1_x3_v(&__ret, __p0, 8); \
  __ret; \
})
#else
#define vld1_f16_x3(__p0) __extension__ ({ \
  float16x4x3_t __ret; \
  __builtin_neon_vld1_x3_v(&__ret, __p0, 8); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 3, 2, 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 3, 2, 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld1_s32_x3(__p0) __extension__ ({ \
  int32x2x3_t __ret; \
  __builtin_neon_vld1_x3_v(&__ret, __p0, 2); \
  __ret; \
})
#else
#define vld1_s32_x3(__p0) __extension__ ({ \
  int32x2x3_t __ret; \
  __builtin_neon_vld1_x3_v(&__ret, __p0, 2); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld1_s64_x3(__p0) __extension__ ({ \
  int64x1x3_t __ret; \
  __builtin_neon_vld1_x3_v(&__ret, __p0, 3); \
  __ret; \
})
#else
#define vld1_s64_x3(__p0) __extension__ ({ \
  int64x1x3_t __ret; \
  __builtin_neon_vld1_x3_v(&__ret, __p0, 3); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld1_s16_x3(__p0) __extension__ ({ \
  int16x4x3_t __ret; \
  __builtin_neon_vld1_x3_v(&__ret, __p0, 1); \
  __ret; \
})
#else
#define vld1_s16_x3(__p0) __extension__ ({ \
  int16x4x3_t __ret; \
  __builtin_neon_vld1_x3_v(&__ret, __p0, 1); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 3, 2, 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 3, 2, 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld1_p8_x4(__p0) __extension__ ({ \
  poly8x8x4_t __ret; \
  __builtin_neon_vld1_x4_v(&__ret, __p0, 4); \
  __ret; \
})
#else
#define vld1_p8_x4(__p0) __extension__ ({ \
  poly8x8x4_t __ret; \
  __builtin_neon_vld1_x4_v(&__ret, __p0, 4); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[3] = __builtin_shufflevector(__ret.val[3], __ret.val[3], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld1_p64_x4(__p0) __extension__ ({ \
  poly64x1x4_t __ret; \
  __builtin_neon_vld1_x4_v(&__ret, __p0, 6); \
  __ret; \
})
#else
#define vld1_p64_x4(__p0) __extension__ ({ \
  poly64x1x4_t __ret; \
  __builtin_neon_vld1_x4_v(&__ret, __p0, 6); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld1_p16_x4(__p0) __extension__ ({ \
  poly16x4x4_t __ret; \
  __builtin_neon_vld1_x4_v(&__ret, __p0, 5); \
  __ret; \
})
#else
#define vld1_p16_x4(__p0) __extension__ ({ \
  poly16x4x4_t __ret; \
  __builtin_neon_vld1_x4_v(&__ret, __p0, 5); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 3, 2, 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 3, 2, 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 3, 2, 1, 0); \
  __ret.val[3] = __builtin_shufflevector(__ret.val[3], __ret.val[3], 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld1q_p8_x4(__p0) __extension__ ({ \
  poly8x16x4_t __ret; \
  __builtin_neon_vld1q_x4_v(&__ret, __p0, 36); \
  __ret; \
})
#else
#define vld1q_p8_x4(__p0) __extension__ ({ \
  poly8x16x4_t __ret; \
  __builtin_neon_vld1q_x4_v(&__ret, __p0, 36); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[3] = __builtin_shufflevector(__ret.val[3], __ret.val[3], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld1q_p64_x4(__p0) __extension__ ({ \
  poly64x2x4_t __ret; \
  __builtin_neon_vld1q_x4_v(&__ret, __p0, 38); \
  __ret; \
})
#else
#define vld1q_p64_x4(__p0) __extension__ ({ \
  poly64x2x4_t __ret; \
  __builtin_neon_vld1q_x4_v(&__ret, __p0, 38); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 1, 0); \
  __ret.val[3] = __builtin_shufflevector(__ret.val[3], __ret.val[3], 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld1q_p16_x4(__p0) __extension__ ({ \
  poly16x8x4_t __ret; \
  __builtin_neon_vld1q_x4_v(&__ret, __p0, 37); \
  __ret; \
})
#else
#define vld1q_p16_x4(__p0) __extension__ ({ \
  poly16x8x4_t __ret; \
  __builtin_neon_vld1q_x4_v(&__ret, __p0, 37); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[3] = __builtin_shufflevector(__ret.val[3], __ret.val[3], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld1q_u8_x4(__p0) __extension__ ({ \
  uint8x16x4_t __ret; \
  __builtin_neon_vld1q_x4_v(&__ret, __p0, 48); \
  __ret; \
})
#else
#define vld1q_u8_x4(__p0) __extension__ ({ \
  uint8x16x4_t __ret; \
  __builtin_neon_vld1q_x4_v(&__ret, __p0, 48); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[3] = __builtin_shufflevector(__ret.val[3], __ret.val[3], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld1q_u32_x4(__p0) __extension__ ({ \
  uint32x4x4_t __ret; \
  __builtin_neon_vld1q_x4_v(&__ret, __p0, 50); \
  __ret; \
})
#else
#define vld1q_u32_x4(__p0) __extension__ ({ \
  uint32x4x4_t __ret; \
  __builtin_neon_vld1q_x4_v(&__ret, __p0, 50); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 3, 2, 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 3, 2, 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 3, 2, 1, 0); \
  __ret.val[3] = __builtin_shufflevector(__ret.val[3], __ret.val[3], 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld1q_u64_x4(__p0) __extension__ ({ \
  uint64x2x4_t __ret; \
  __builtin_neon_vld1q_x4_v(&__ret, __p0, 51); \
  __ret; \
})
#else
#define vld1q_u64_x4(__p0) __extension__ ({ \
  uint64x2x4_t __ret; \
  __builtin_neon_vld1q_x4_v(&__ret, __p0, 51); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 1, 0); \
  __ret.val[3] = __builtin_shufflevector(__ret.val[3], __ret.val[3], 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld1q_u16_x4(__p0) __extension__ ({ \
  uint16x8x4_t __ret; \
  __builtin_neon_vld1q_x4_v(&__ret, __p0, 49); \
  __ret; \
})
#else
#define vld1q_u16_x4(__p0) __extension__ ({ \
  uint16x8x4_t __ret; \
  __builtin_neon_vld1q_x4_v(&__ret, __p0, 49); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[3] = __builtin_shufflevector(__ret.val[3], __ret.val[3], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld1q_s8_x4(__p0) __extension__ ({ \
  int8x16x4_t __ret; \
  __builtin_neon_vld1q_x4_v(&__ret, __p0, 32); \
  __ret; \
})
#else
#define vld1q_s8_x4(__p0) __extension__ ({ \
  int8x16x4_t __ret; \
  __builtin_neon_vld1q_x4_v(&__ret, __p0, 32); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[3] = __builtin_shufflevector(__ret.val[3], __ret.val[3], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld1q_f64_x4(__p0) __extension__ ({ \
  float64x2x4_t __ret; \
  __builtin_neon_vld1q_x4_v(&__ret, __p0, 42); \
  __ret; \
})
#else
#define vld1q_f64_x4(__p0) __extension__ ({ \
  float64x2x4_t __ret; \
  __builtin_neon_vld1q_x4_v(&__ret, __p0, 42); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 1, 0); \
  __ret.val[3] = __builtin_shufflevector(__ret.val[3], __ret.val[3], 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld1q_f32_x4(__p0) __extension__ ({ \
  float32x4x4_t __ret; \
  __builtin_neon_vld1q_x4_v(&__ret, __p0, 41); \
  __ret; \
})
#else
#define vld1q_f32_x4(__p0) __extension__ ({ \
  float32x4x4_t __ret; \
  __builtin_neon_vld1q_x4_v(&__ret, __p0, 41); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 3, 2, 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 3, 2, 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 3, 2, 1, 0); \
  __ret.val[3] = __builtin_shufflevector(__ret.val[3], __ret.val[3], 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld1q_f16_x4(__p0) __extension__ ({ \
  float16x8x4_t __ret; \
  __builtin_neon_vld1q_x4_v(&__ret, __p0, 40); \
  __ret; \
})
#else
#define vld1q_f16_x4(__p0) __extension__ ({ \
  float16x8x4_t __ret; \
  __builtin_neon_vld1q_x4_v(&__ret, __p0, 40); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[3] = __builtin_shufflevector(__ret.val[3], __ret.val[3], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld1q_s32_x4(__p0) __extension__ ({ \
  int32x4x4_t __ret; \
  __builtin_neon_vld1q_x4_v(&__ret, __p0, 34); \
  __ret; \
})
#else
#define vld1q_s32_x4(__p0) __extension__ ({ \
  int32x4x4_t __ret; \
  __builtin_neon_vld1q_x4_v(&__ret, __p0, 34); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 3, 2, 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 3, 2, 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 3, 2, 1, 0); \
  __ret.val[3] = __builtin_shufflevector(__ret.val[3], __ret.val[3], 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld1q_s64_x4(__p0) __extension__ ({ \
  int64x2x4_t __ret; \
  __builtin_neon_vld1q_x4_v(&__ret, __p0, 35); \
  __ret; \
})
#else
#define vld1q_s64_x4(__p0) __extension__ ({ \
  int64x2x4_t __ret; \
  __builtin_neon_vld1q_x4_v(&__ret, __p0, 35); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 1, 0); \
  __ret.val[3] = __builtin_shufflevector(__ret.val[3], __ret.val[3], 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld1q_s16_x4(__p0) __extension__ ({ \
  int16x8x4_t __ret; \
  __builtin_neon_vld1q_x4_v(&__ret, __p0, 33); \
  __ret; \
})
#else
#define vld1q_s16_x4(__p0) __extension__ ({ \
  int16x8x4_t __ret; \
  __builtin_neon_vld1q_x4_v(&__ret, __p0, 33); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[3] = __builtin_shufflevector(__ret.val[3], __ret.val[3], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld1_u8_x4(__p0) __extension__ ({ \
  uint8x8x4_t __ret; \
  __builtin_neon_vld1_x4_v(&__ret, __p0, 16); \
  __ret; \
})
#else
#define vld1_u8_x4(__p0) __extension__ ({ \
  uint8x8x4_t __ret; \
  __builtin_neon_vld1_x4_v(&__ret, __p0, 16); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[3] = __builtin_shufflevector(__ret.val[3], __ret.val[3], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld1_u32_x4(__p0) __extension__ ({ \
  uint32x2x4_t __ret; \
  __builtin_neon_vld1_x4_v(&__ret, __p0, 18); \
  __ret; \
})
#else
#define vld1_u32_x4(__p0) __extension__ ({ \
  uint32x2x4_t __ret; \
  __builtin_neon_vld1_x4_v(&__ret, __p0, 18); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 1, 0); \
  __ret.val[3] = __builtin_shufflevector(__ret.val[3], __ret.val[3], 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld1_u64_x4(__p0) __extension__ ({ \
  uint64x1x4_t __ret; \
  __builtin_neon_vld1_x4_v(&__ret, __p0, 19); \
  __ret; \
})
#else
#define vld1_u64_x4(__p0) __extension__ ({ \
  uint64x1x4_t __ret; \
  __builtin_neon_vld1_x4_v(&__ret, __p0, 19); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld1_u16_x4(__p0) __extension__ ({ \
  uint16x4x4_t __ret; \
  __builtin_neon_vld1_x4_v(&__ret, __p0, 17); \
  __ret; \
})
#else
#define vld1_u16_x4(__p0) __extension__ ({ \
  uint16x4x4_t __ret; \
  __builtin_neon_vld1_x4_v(&__ret, __p0, 17); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 3, 2, 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 3, 2, 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 3, 2, 1, 0); \
  __ret.val[3] = __builtin_shufflevector(__ret.val[3], __ret.val[3], 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld1_s8_x4(__p0) __extension__ ({ \
  int8x8x4_t __ret; \
  __builtin_neon_vld1_x4_v(&__ret, __p0, 0); \
  __ret; \
})
#else
#define vld1_s8_x4(__p0) __extension__ ({ \
  int8x8x4_t __ret; \
  __builtin_neon_vld1_x4_v(&__ret, __p0, 0); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[3] = __builtin_shufflevector(__ret.val[3], __ret.val[3], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld1_f64_x4(__p0) __extension__ ({ \
  float64x1x4_t __ret; \
  __builtin_neon_vld1_x4_v(&__ret, __p0, 10); \
  __ret; \
})
#else
#define vld1_f64_x4(__p0) __extension__ ({ \
  float64x1x4_t __ret; \
  __builtin_neon_vld1_x4_v(&__ret, __p0, 10); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld1_f32_x4(__p0) __extension__ ({ \
  float32x2x4_t __ret; \
  __builtin_neon_vld1_x4_v(&__ret, __p0, 9); \
  __ret; \
})
#else
#define vld1_f32_x4(__p0) __extension__ ({ \
  float32x2x4_t __ret; \
  __builtin_neon_vld1_x4_v(&__ret, __p0, 9); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 1, 0); \
  __ret.val[3] = __builtin_shufflevector(__ret.val[3], __ret.val[3], 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld1_f16_x4(__p0) __extension__ ({ \
  float16x4x4_t __ret; \
  __builtin_neon_vld1_x4_v(&__ret, __p0, 8); \
  __ret; \
})
#else
#define vld1_f16_x4(__p0) __extension__ ({ \
  float16x4x4_t __ret; \
  __builtin_neon_vld1_x4_v(&__ret, __p0, 8); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 3, 2, 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 3, 2, 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 3, 2, 1, 0); \
  __ret.val[3] = __builtin_shufflevector(__ret.val[3], __ret.val[3], 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld1_s32_x4(__p0) __extension__ ({ \
  int32x2x4_t __ret; \
  __builtin_neon_vld1_x4_v(&__ret, __p0, 2); \
  __ret; \
})
#else
#define vld1_s32_x4(__p0) __extension__ ({ \
  int32x2x4_t __ret; \
  __builtin_neon_vld1_x4_v(&__ret, __p0, 2); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 1, 0); \
  __ret.val[3] = __builtin_shufflevector(__ret.val[3], __ret.val[3], 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld1_s64_x4(__p0) __extension__ ({ \
  int64x1x4_t __ret; \
  __builtin_neon_vld1_x4_v(&__ret, __p0, 3); \
  __ret; \
})
#else
#define vld1_s64_x4(__p0) __extension__ ({ \
  int64x1x4_t __ret; \
  __builtin_neon_vld1_x4_v(&__ret, __p0, 3); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld1_s16_x4(__p0) __extension__ ({ \
  int16x4x4_t __ret; \
  __builtin_neon_vld1_x4_v(&__ret, __p0, 1); \
  __ret; \
})
#else
#define vld1_s16_x4(__p0) __extension__ ({ \
  int16x4x4_t __ret; \
  __builtin_neon_vld1_x4_v(&__ret, __p0, 1); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 3, 2, 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 3, 2, 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 3, 2, 1, 0); \
  __ret.val[3] = __builtin_shufflevector(__ret.val[3], __ret.val[3], 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld2_p64(__p0) __extension__ ({ \
  poly64x1x2_t __ret; \
  __builtin_neon_vld2_v(&__ret, __p0, 6); \
  __ret; \
})
#else
#define vld2_p64(__p0) __extension__ ({ \
  poly64x1x2_t __ret; \
  __builtin_neon_vld2_v(&__ret, __p0, 6); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld2q_p64(__p0) __extension__ ({ \
  poly64x2x2_t __ret; \
  __builtin_neon_vld2q_v(&__ret, __p0, 38); \
  __ret; \
})
#else
#define vld2q_p64(__p0) __extension__ ({ \
  poly64x2x2_t __ret; \
  __builtin_neon_vld2q_v(&__ret, __p0, 38); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld2q_u64(__p0) __extension__ ({ \
  uint64x2x2_t __ret; \
  __builtin_neon_vld2q_v(&__ret, __p0, 51); \
  __ret; \
})
#else
#define vld2q_u64(__p0) __extension__ ({ \
  uint64x2x2_t __ret; \
  __builtin_neon_vld2q_v(&__ret, __p0, 51); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld2q_f64(__p0) __extension__ ({ \
  float64x2x2_t __ret; \
  __builtin_neon_vld2q_v(&__ret, __p0, 42); \
  __ret; \
})
#else
#define vld2q_f64(__p0) __extension__ ({ \
  float64x2x2_t __ret; \
  __builtin_neon_vld2q_v(&__ret, __p0, 42); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld2q_s64(__p0) __extension__ ({ \
  int64x2x2_t __ret; \
  __builtin_neon_vld2q_v(&__ret, __p0, 35); \
  __ret; \
})
#else
#define vld2q_s64(__p0) __extension__ ({ \
  int64x2x2_t __ret; \
  __builtin_neon_vld2q_v(&__ret, __p0, 35); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld2_f64(__p0) __extension__ ({ \
  float64x1x2_t __ret; \
  __builtin_neon_vld2_v(&__ret, __p0, 10); \
  __ret; \
})
#else
#define vld2_f64(__p0) __extension__ ({ \
  float64x1x2_t __ret; \
  __builtin_neon_vld2_v(&__ret, __p0, 10); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld2_dup_p64(__p0) __extension__ ({ \
  poly64x1x2_t __ret; \
  __builtin_neon_vld2_dup_v(&__ret, __p0, 6); \
  __ret; \
})
#else
#define vld2_dup_p64(__p0) __extension__ ({ \
  poly64x1x2_t __ret; \
  __builtin_neon_vld2_dup_v(&__ret, __p0, 6); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld2q_dup_p8(__p0) __extension__ ({ \
  poly8x16x2_t __ret; \
  __builtin_neon_vld2q_dup_v(&__ret, __p0, 36); \
  __ret; \
})
#else
#define vld2q_dup_p8(__p0) __extension__ ({ \
  poly8x16x2_t __ret; \
  __builtin_neon_vld2q_dup_v(&__ret, __p0, 36); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld2q_dup_p64(__p0) __extension__ ({ \
  poly64x2x2_t __ret; \
  __builtin_neon_vld2q_dup_v(&__ret, __p0, 38); \
  __ret; \
})
#else
#define vld2q_dup_p64(__p0) __extension__ ({ \
  poly64x2x2_t __ret; \
  __builtin_neon_vld2q_dup_v(&__ret, __p0, 38); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld2q_dup_p16(__p0) __extension__ ({ \
  poly16x8x2_t __ret; \
  __builtin_neon_vld2q_dup_v(&__ret, __p0, 37); \
  __ret; \
})
#else
#define vld2q_dup_p16(__p0) __extension__ ({ \
  poly16x8x2_t __ret; \
  __builtin_neon_vld2q_dup_v(&__ret, __p0, 37); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld2q_dup_u8(__p0) __extension__ ({ \
  uint8x16x2_t __ret; \
  __builtin_neon_vld2q_dup_v(&__ret, __p0, 48); \
  __ret; \
})
#else
#define vld2q_dup_u8(__p0) __extension__ ({ \
  uint8x16x2_t __ret; \
  __builtin_neon_vld2q_dup_v(&__ret, __p0, 48); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld2q_dup_u32(__p0) __extension__ ({ \
  uint32x4x2_t __ret; \
  __builtin_neon_vld2q_dup_v(&__ret, __p0, 50); \
  __ret; \
})
#else
#define vld2q_dup_u32(__p0) __extension__ ({ \
  uint32x4x2_t __ret; \
  __builtin_neon_vld2q_dup_v(&__ret, __p0, 50); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 3, 2, 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld2q_dup_u64(__p0) __extension__ ({ \
  uint64x2x2_t __ret; \
  __builtin_neon_vld2q_dup_v(&__ret, __p0, 51); \
  __ret; \
})
#else
#define vld2q_dup_u64(__p0) __extension__ ({ \
  uint64x2x2_t __ret; \
  __builtin_neon_vld2q_dup_v(&__ret, __p0, 51); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld2q_dup_u16(__p0) __extension__ ({ \
  uint16x8x2_t __ret; \
  __builtin_neon_vld2q_dup_v(&__ret, __p0, 49); \
  __ret; \
})
#else
#define vld2q_dup_u16(__p0) __extension__ ({ \
  uint16x8x2_t __ret; \
  __builtin_neon_vld2q_dup_v(&__ret, __p0, 49); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld2q_dup_s8(__p0) __extension__ ({ \
  int8x16x2_t __ret; \
  __builtin_neon_vld2q_dup_v(&__ret, __p0, 32); \
  __ret; \
})
#else
#define vld2q_dup_s8(__p0) __extension__ ({ \
  int8x16x2_t __ret; \
  __builtin_neon_vld2q_dup_v(&__ret, __p0, 32); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld2q_dup_f64(__p0) __extension__ ({ \
  float64x2x2_t __ret; \
  __builtin_neon_vld2q_dup_v(&__ret, __p0, 42); \
  __ret; \
})
#else
#define vld2q_dup_f64(__p0) __extension__ ({ \
  float64x2x2_t __ret; \
  __builtin_neon_vld2q_dup_v(&__ret, __p0, 42); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld2q_dup_f32(__p0) __extension__ ({ \
  float32x4x2_t __ret; \
  __builtin_neon_vld2q_dup_v(&__ret, __p0, 41); \
  __ret; \
})
#else
#define vld2q_dup_f32(__p0) __extension__ ({ \
  float32x4x2_t __ret; \
  __builtin_neon_vld2q_dup_v(&__ret, __p0, 41); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 3, 2, 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld2q_dup_f16(__p0) __extension__ ({ \
  float16x8x2_t __ret; \
  __builtin_neon_vld2q_dup_v(&__ret, __p0, 40); \
  __ret; \
})
#else
#define vld2q_dup_f16(__p0) __extension__ ({ \
  float16x8x2_t __ret; \
  __builtin_neon_vld2q_dup_v(&__ret, __p0, 40); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld2q_dup_s32(__p0) __extension__ ({ \
  int32x4x2_t __ret; \
  __builtin_neon_vld2q_dup_v(&__ret, __p0, 34); \
  __ret; \
})
#else
#define vld2q_dup_s32(__p0) __extension__ ({ \
  int32x4x2_t __ret; \
  __builtin_neon_vld2q_dup_v(&__ret, __p0, 34); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 3, 2, 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld2q_dup_s64(__p0) __extension__ ({ \
  int64x2x2_t __ret; \
  __builtin_neon_vld2q_dup_v(&__ret, __p0, 35); \
  __ret; \
})
#else
#define vld2q_dup_s64(__p0) __extension__ ({ \
  int64x2x2_t __ret; \
  __builtin_neon_vld2q_dup_v(&__ret, __p0, 35); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld2q_dup_s16(__p0) __extension__ ({ \
  int16x8x2_t __ret; \
  __builtin_neon_vld2q_dup_v(&__ret, __p0, 33); \
  __ret; \
})
#else
#define vld2q_dup_s16(__p0) __extension__ ({ \
  int16x8x2_t __ret; \
  __builtin_neon_vld2q_dup_v(&__ret, __p0, 33); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld2_dup_f64(__p0) __extension__ ({ \
  float64x1x2_t __ret; \
  __builtin_neon_vld2_dup_v(&__ret, __p0, 10); \
  __ret; \
})
#else
#define vld2_dup_f64(__p0) __extension__ ({ \
  float64x1x2_t __ret; \
  __builtin_neon_vld2_dup_v(&__ret, __p0, 10); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld2_lane_p64(__p0, __p1, __p2) __extension__ ({ \
  poly64x1x2_t __s1 = __p1; \
  poly64x1x2_t __ret; \
  __builtin_neon_vld2_lane_v(&__ret, __p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], __p2, 6); \
  __ret; \
})
#else
#define vld2_lane_p64(__p0, __p1, __p2) __extension__ ({ \
  poly64x1x2_t __s1 = __p1; \
  poly64x1x2_t __ret; \
  __builtin_neon_vld2_lane_v(&__ret, __p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], __p2, 6); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld2q_lane_p8(__p0, __p1, __p2) __extension__ ({ \
  poly8x16x2_t __s1 = __p1; \
  poly8x16x2_t __ret; \
  __builtin_neon_vld2q_lane_v(&__ret, __p0, (int8x16_t)__s1.val[0], (int8x16_t)__s1.val[1], __p2, 36); \
  __ret; \
})
#else
#define vld2q_lane_p8(__p0, __p1, __p2) __extension__ ({ \
  poly8x16x2_t __s1 = __p1; \
  poly8x16x2_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  poly8x16x2_t __ret; \
  __builtin_neon_vld2q_lane_v(&__ret, __p0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], __p2, 36); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld2q_lane_p64(__p0, __p1, __p2) __extension__ ({ \
  poly64x2x2_t __s1 = __p1; \
  poly64x2x2_t __ret; \
  __builtin_neon_vld2q_lane_v(&__ret, __p0, (int8x16_t)__s1.val[0], (int8x16_t)__s1.val[1], __p2, 38); \
  __ret; \
})
#else
#define vld2q_lane_p64(__p0, __p1, __p2) __extension__ ({ \
  poly64x2x2_t __s1 = __p1; \
  poly64x2x2_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 1, 0); \
  poly64x2x2_t __ret; \
  __builtin_neon_vld2q_lane_v(&__ret, __p0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], __p2, 38); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld2q_lane_u8(__p0, __p1, __p2) __extension__ ({ \
  uint8x16x2_t __s1 = __p1; \
  uint8x16x2_t __ret; \
  __builtin_neon_vld2q_lane_v(&__ret, __p0, (int8x16_t)__s1.val[0], (int8x16_t)__s1.val[1], __p2, 48); \
  __ret; \
})
#else
#define vld2q_lane_u8(__p0, __p1, __p2) __extension__ ({ \
  uint8x16x2_t __s1 = __p1; \
  uint8x16x2_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint8x16x2_t __ret; \
  __builtin_neon_vld2q_lane_v(&__ret, __p0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], __p2, 48); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld2q_lane_u64(__p0, __p1, __p2) __extension__ ({ \
  uint64x2x2_t __s1 = __p1; \
  uint64x2x2_t __ret; \
  __builtin_neon_vld2q_lane_v(&__ret, __p0, (int8x16_t)__s1.val[0], (int8x16_t)__s1.val[1], __p2, 51); \
  __ret; \
})
#else
#define vld2q_lane_u64(__p0, __p1, __p2) __extension__ ({ \
  uint64x2x2_t __s1 = __p1; \
  uint64x2x2_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 1, 0); \
  uint64x2x2_t __ret; \
  __builtin_neon_vld2q_lane_v(&__ret, __p0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], __p2, 51); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld2q_lane_s8(__p0, __p1, __p2) __extension__ ({ \
  int8x16x2_t __s1 = __p1; \
  int8x16x2_t __ret; \
  __builtin_neon_vld2q_lane_v(&__ret, __p0, (int8x16_t)__s1.val[0], (int8x16_t)__s1.val[1], __p2, 32); \
  __ret; \
})
#else
#define vld2q_lane_s8(__p0, __p1, __p2) __extension__ ({ \
  int8x16x2_t __s1 = __p1; \
  int8x16x2_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  int8x16x2_t __ret; \
  __builtin_neon_vld2q_lane_v(&__ret, __p0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], __p2, 32); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld2q_lane_f64(__p0, __p1, __p2) __extension__ ({ \
  float64x2x2_t __s1 = __p1; \
  float64x2x2_t __ret; \
  __builtin_neon_vld2q_lane_v(&__ret, __p0, __s1.val[0], __s1.val[1], __p2, 42); \
  __ret; \
})
#else
#define vld2q_lane_f64(__p0, __p1, __p2) __extension__ ({ \
  float64x2x2_t __s1 = __p1; \
  float64x2x2_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 1, 0); \
  float64x2x2_t __ret; \
  __builtin_neon_vld2q_lane_v(&__ret, __p0, __rev1.val[0], __rev1.val[1], __p2, 42); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld2q_lane_s64(__p0, __p1, __p2) __extension__ ({ \
  int64x2x2_t __s1 = __p1; \
  int64x2x2_t __ret; \
  __builtin_neon_vld2q_lane_v(&__ret, __p0, __s1.val[0], __s1.val[1], __p2, 35); \
  __ret; \
})
#else
#define vld2q_lane_s64(__p0, __p1, __p2) __extension__ ({ \
  int64x2x2_t __s1 = __p1; \
  int64x2x2_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 1, 0); \
  int64x2x2_t __ret; \
  __builtin_neon_vld2q_lane_v(&__ret, __p0, __rev1.val[0], __rev1.val[1], __p2, 35); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld2_lane_u64(__p0, __p1, __p2) __extension__ ({ \
  uint64x1x2_t __s1 = __p1; \
  uint64x1x2_t __ret; \
  __builtin_neon_vld2_lane_v(&__ret, __p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], __p2, 19); \
  __ret; \
})
#else
#define vld2_lane_u64(__p0, __p1, __p2) __extension__ ({ \
  uint64x1x2_t __s1 = __p1; \
  uint64x1x2_t __ret; \
  __builtin_neon_vld2_lane_v(&__ret, __p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], __p2, 19); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld2_lane_f64(__p0, __p1, __p2) __extension__ ({ \
  float64x1x2_t __s1 = __p1; \
  float64x1x2_t __ret; \
  __builtin_neon_vld2_lane_v(&__ret, __p0, __s1.val[0], __s1.val[1], __p2, 10); \
  __ret; \
})
#else
#define vld2_lane_f64(__p0, __p1, __p2) __extension__ ({ \
  float64x1x2_t __s1 = __p1; \
  float64x1x2_t __ret; \
  __builtin_neon_vld2_lane_v(&__ret, __p0, __s1.val[0], __s1.val[1], __p2, 10); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld2_lane_s64(__p0, __p1, __p2) __extension__ ({ \
  int64x1x2_t __s1 = __p1; \
  int64x1x2_t __ret; \
  __builtin_neon_vld2_lane_v(&__ret, __p0, __s1.val[0], __s1.val[1], __p2, 3); \
  __ret; \
})
#else
#define vld2_lane_s64(__p0, __p1, __p2) __extension__ ({ \
  int64x1x2_t __s1 = __p1; \
  int64x1x2_t __ret; \
  __builtin_neon_vld2_lane_v(&__ret, __p0, __s1.val[0], __s1.val[1], __p2, 3); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld3_p64(__p0) __extension__ ({ \
  poly64x1x3_t __ret; \
  __builtin_neon_vld3_v(&__ret, __p0, 6); \
  __ret; \
})
#else
#define vld3_p64(__p0) __extension__ ({ \
  poly64x1x3_t __ret; \
  __builtin_neon_vld3_v(&__ret, __p0, 6); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld3q_p64(__p0) __extension__ ({ \
  poly64x2x3_t __ret; \
  __builtin_neon_vld3q_v(&__ret, __p0, 38); \
  __ret; \
})
#else
#define vld3q_p64(__p0) __extension__ ({ \
  poly64x2x3_t __ret; \
  __builtin_neon_vld3q_v(&__ret, __p0, 38); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld3q_u64(__p0) __extension__ ({ \
  uint64x2x3_t __ret; \
  __builtin_neon_vld3q_v(&__ret, __p0, 51); \
  __ret; \
})
#else
#define vld3q_u64(__p0) __extension__ ({ \
  uint64x2x3_t __ret; \
  __builtin_neon_vld3q_v(&__ret, __p0, 51); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld3q_f64(__p0) __extension__ ({ \
  float64x2x3_t __ret; \
  __builtin_neon_vld3q_v(&__ret, __p0, 42); \
  __ret; \
})
#else
#define vld3q_f64(__p0) __extension__ ({ \
  float64x2x3_t __ret; \
  __builtin_neon_vld3q_v(&__ret, __p0, 42); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld3q_s64(__p0) __extension__ ({ \
  int64x2x3_t __ret; \
  __builtin_neon_vld3q_v(&__ret, __p0, 35); \
  __ret; \
})
#else
#define vld3q_s64(__p0) __extension__ ({ \
  int64x2x3_t __ret; \
  __builtin_neon_vld3q_v(&__ret, __p0, 35); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld3_f64(__p0) __extension__ ({ \
  float64x1x3_t __ret; \
  __builtin_neon_vld3_v(&__ret, __p0, 10); \
  __ret; \
})
#else
#define vld3_f64(__p0) __extension__ ({ \
  float64x1x3_t __ret; \
  __builtin_neon_vld3_v(&__ret, __p0, 10); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld3_dup_p64(__p0) __extension__ ({ \
  poly64x1x3_t __ret; \
  __builtin_neon_vld3_dup_v(&__ret, __p0, 6); \
  __ret; \
})
#else
#define vld3_dup_p64(__p0) __extension__ ({ \
  poly64x1x3_t __ret; \
  __builtin_neon_vld3_dup_v(&__ret, __p0, 6); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld3q_dup_p8(__p0) __extension__ ({ \
  poly8x16x3_t __ret; \
  __builtin_neon_vld3q_dup_v(&__ret, __p0, 36); \
  __ret; \
})
#else
#define vld3q_dup_p8(__p0) __extension__ ({ \
  poly8x16x3_t __ret; \
  __builtin_neon_vld3q_dup_v(&__ret, __p0, 36); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld3q_dup_p64(__p0) __extension__ ({ \
  poly64x2x3_t __ret; \
  __builtin_neon_vld3q_dup_v(&__ret, __p0, 38); \
  __ret; \
})
#else
#define vld3q_dup_p64(__p0) __extension__ ({ \
  poly64x2x3_t __ret; \
  __builtin_neon_vld3q_dup_v(&__ret, __p0, 38); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld3q_dup_p16(__p0) __extension__ ({ \
  poly16x8x3_t __ret; \
  __builtin_neon_vld3q_dup_v(&__ret, __p0, 37); \
  __ret; \
})
#else
#define vld3q_dup_p16(__p0) __extension__ ({ \
  poly16x8x3_t __ret; \
  __builtin_neon_vld3q_dup_v(&__ret, __p0, 37); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld3q_dup_u8(__p0) __extension__ ({ \
  uint8x16x3_t __ret; \
  __builtin_neon_vld3q_dup_v(&__ret, __p0, 48); \
  __ret; \
})
#else
#define vld3q_dup_u8(__p0) __extension__ ({ \
  uint8x16x3_t __ret; \
  __builtin_neon_vld3q_dup_v(&__ret, __p0, 48); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld3q_dup_u32(__p0) __extension__ ({ \
  uint32x4x3_t __ret; \
  __builtin_neon_vld3q_dup_v(&__ret, __p0, 50); \
  __ret; \
})
#else
#define vld3q_dup_u32(__p0) __extension__ ({ \
  uint32x4x3_t __ret; \
  __builtin_neon_vld3q_dup_v(&__ret, __p0, 50); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 3, 2, 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 3, 2, 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld3q_dup_u64(__p0) __extension__ ({ \
  uint64x2x3_t __ret; \
  __builtin_neon_vld3q_dup_v(&__ret, __p0, 51); \
  __ret; \
})
#else
#define vld3q_dup_u64(__p0) __extension__ ({ \
  uint64x2x3_t __ret; \
  __builtin_neon_vld3q_dup_v(&__ret, __p0, 51); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld3q_dup_u16(__p0) __extension__ ({ \
  uint16x8x3_t __ret; \
  __builtin_neon_vld3q_dup_v(&__ret, __p0, 49); \
  __ret; \
})
#else
#define vld3q_dup_u16(__p0) __extension__ ({ \
  uint16x8x3_t __ret; \
  __builtin_neon_vld3q_dup_v(&__ret, __p0, 49); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld3q_dup_s8(__p0) __extension__ ({ \
  int8x16x3_t __ret; \
  __builtin_neon_vld3q_dup_v(&__ret, __p0, 32); \
  __ret; \
})
#else
#define vld3q_dup_s8(__p0) __extension__ ({ \
  int8x16x3_t __ret; \
  __builtin_neon_vld3q_dup_v(&__ret, __p0, 32); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld3q_dup_f64(__p0) __extension__ ({ \
  float64x2x3_t __ret; \
  __builtin_neon_vld3q_dup_v(&__ret, __p0, 42); \
  __ret; \
})
#else
#define vld3q_dup_f64(__p0) __extension__ ({ \
  float64x2x3_t __ret; \
  __builtin_neon_vld3q_dup_v(&__ret, __p0, 42); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld3q_dup_f32(__p0) __extension__ ({ \
  float32x4x3_t __ret; \
  __builtin_neon_vld3q_dup_v(&__ret, __p0, 41); \
  __ret; \
})
#else
#define vld3q_dup_f32(__p0) __extension__ ({ \
  float32x4x3_t __ret; \
  __builtin_neon_vld3q_dup_v(&__ret, __p0, 41); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 3, 2, 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 3, 2, 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld3q_dup_f16(__p0) __extension__ ({ \
  float16x8x3_t __ret; \
  __builtin_neon_vld3q_dup_v(&__ret, __p0, 40); \
  __ret; \
})
#else
#define vld3q_dup_f16(__p0) __extension__ ({ \
  float16x8x3_t __ret; \
  __builtin_neon_vld3q_dup_v(&__ret, __p0, 40); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld3q_dup_s32(__p0) __extension__ ({ \
  int32x4x3_t __ret; \
  __builtin_neon_vld3q_dup_v(&__ret, __p0, 34); \
  __ret; \
})
#else
#define vld3q_dup_s32(__p0) __extension__ ({ \
  int32x4x3_t __ret; \
  __builtin_neon_vld3q_dup_v(&__ret, __p0, 34); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 3, 2, 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 3, 2, 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld3q_dup_s64(__p0) __extension__ ({ \
  int64x2x3_t __ret; \
  __builtin_neon_vld3q_dup_v(&__ret, __p0, 35); \
  __ret; \
})
#else
#define vld3q_dup_s64(__p0) __extension__ ({ \
  int64x2x3_t __ret; \
  __builtin_neon_vld3q_dup_v(&__ret, __p0, 35); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld3q_dup_s16(__p0) __extension__ ({ \
  int16x8x3_t __ret; \
  __builtin_neon_vld3q_dup_v(&__ret, __p0, 33); \
  __ret; \
})
#else
#define vld3q_dup_s16(__p0) __extension__ ({ \
  int16x8x3_t __ret; \
  __builtin_neon_vld3q_dup_v(&__ret, __p0, 33); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld3_dup_f64(__p0) __extension__ ({ \
  float64x1x3_t __ret; \
  __builtin_neon_vld3_dup_v(&__ret, __p0, 10); \
  __ret; \
})
#else
#define vld3_dup_f64(__p0) __extension__ ({ \
  float64x1x3_t __ret; \
  __builtin_neon_vld3_dup_v(&__ret, __p0, 10); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld3_lane_p64(__p0, __p1, __p2) __extension__ ({ \
  poly64x1x3_t __s1 = __p1; \
  poly64x1x3_t __ret; \
  __builtin_neon_vld3_lane_v(&__ret, __p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], (int8x8_t)__s1.val[2], __p2, 6); \
  __ret; \
})
#else
#define vld3_lane_p64(__p0, __p1, __p2) __extension__ ({ \
  poly64x1x3_t __s1 = __p1; \
  poly64x1x3_t __ret; \
  __builtin_neon_vld3_lane_v(&__ret, __p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], (int8x8_t)__s1.val[2], __p2, 6); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld3q_lane_p8(__p0, __p1, __p2) __extension__ ({ \
  poly8x16x3_t __s1 = __p1; \
  poly8x16x3_t __ret; \
  __builtin_neon_vld3q_lane_v(&__ret, __p0, (int8x16_t)__s1.val[0], (int8x16_t)__s1.val[1], (int8x16_t)__s1.val[2], __p2, 36); \
  __ret; \
})
#else
#define vld3q_lane_p8(__p0, __p1, __p2) __extension__ ({ \
  poly8x16x3_t __s1 = __p1; \
  poly8x16x3_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  poly8x16x3_t __ret; \
  __builtin_neon_vld3q_lane_v(&__ret, __p0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], (int8x16_t)__rev1.val[2], __p2, 36); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld3q_lane_p64(__p0, __p1, __p2) __extension__ ({ \
  poly64x2x3_t __s1 = __p1; \
  poly64x2x3_t __ret; \
  __builtin_neon_vld3q_lane_v(&__ret, __p0, (int8x16_t)__s1.val[0], (int8x16_t)__s1.val[1], (int8x16_t)__s1.val[2], __p2, 38); \
  __ret; \
})
#else
#define vld3q_lane_p64(__p0, __p1, __p2) __extension__ ({ \
  poly64x2x3_t __s1 = __p1; \
  poly64x2x3_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 1, 0); \
  poly64x2x3_t __ret; \
  __builtin_neon_vld3q_lane_v(&__ret, __p0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], (int8x16_t)__rev1.val[2], __p2, 38); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld3q_lane_u8(__p0, __p1, __p2) __extension__ ({ \
  uint8x16x3_t __s1 = __p1; \
  uint8x16x3_t __ret; \
  __builtin_neon_vld3q_lane_v(&__ret, __p0, (int8x16_t)__s1.val[0], (int8x16_t)__s1.val[1], (int8x16_t)__s1.val[2], __p2, 48); \
  __ret; \
})
#else
#define vld3q_lane_u8(__p0, __p1, __p2) __extension__ ({ \
  uint8x16x3_t __s1 = __p1; \
  uint8x16x3_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint8x16x3_t __ret; \
  __builtin_neon_vld3q_lane_v(&__ret, __p0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], (int8x16_t)__rev1.val[2], __p2, 48); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld3q_lane_u64(__p0, __p1, __p2) __extension__ ({ \
  uint64x2x3_t __s1 = __p1; \
  uint64x2x3_t __ret; \
  __builtin_neon_vld3q_lane_v(&__ret, __p0, (int8x16_t)__s1.val[0], (int8x16_t)__s1.val[1], (int8x16_t)__s1.val[2], __p2, 51); \
  __ret; \
})
#else
#define vld3q_lane_u64(__p0, __p1, __p2) __extension__ ({ \
  uint64x2x3_t __s1 = __p1; \
  uint64x2x3_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 1, 0); \
  uint64x2x3_t __ret; \
  __builtin_neon_vld3q_lane_v(&__ret, __p0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], (int8x16_t)__rev1.val[2], __p2, 51); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld3q_lane_s8(__p0, __p1, __p2) __extension__ ({ \
  int8x16x3_t __s1 = __p1; \
  int8x16x3_t __ret; \
  __builtin_neon_vld3q_lane_v(&__ret, __p0, (int8x16_t)__s1.val[0], (int8x16_t)__s1.val[1], (int8x16_t)__s1.val[2], __p2, 32); \
  __ret; \
})
#else
#define vld3q_lane_s8(__p0, __p1, __p2) __extension__ ({ \
  int8x16x3_t __s1 = __p1; \
  int8x16x3_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  int8x16x3_t __ret; \
  __builtin_neon_vld3q_lane_v(&__ret, __p0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], (int8x16_t)__rev1.val[2], __p2, 32); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld3q_lane_f64(__p0, __p1, __p2) __extension__ ({ \
  float64x2x3_t __s1 = __p1; \
  float64x2x3_t __ret; \
  __builtin_neon_vld3q_lane_v(&__ret, __p0, __s1.val[0], __s1.val[1], __s1.val[2], __p2, 42); \
  __ret; \
})
#else
#define vld3q_lane_f64(__p0, __p1, __p2) __extension__ ({ \
  float64x2x3_t __s1 = __p1; \
  float64x2x3_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 1, 0); \
  float64x2x3_t __ret; \
  __builtin_neon_vld3q_lane_v(&__ret, __p0, __rev1.val[0], __rev1.val[1], __rev1.val[2], __p2, 42); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld3q_lane_s64(__p0, __p1, __p2) __extension__ ({ \
  int64x2x3_t __s1 = __p1; \
  int64x2x3_t __ret; \
  __builtin_neon_vld3q_lane_v(&__ret, __p0, __s1.val[0], __s1.val[1], __s1.val[2], __p2, 35); \
  __ret; \
})
#else
#define vld3q_lane_s64(__p0, __p1, __p2) __extension__ ({ \
  int64x2x3_t __s1 = __p1; \
  int64x2x3_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 1, 0); \
  int64x2x3_t __ret; \
  __builtin_neon_vld3q_lane_v(&__ret, __p0, __rev1.val[0], __rev1.val[1], __rev1.val[2], __p2, 35); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld3_lane_u64(__p0, __p1, __p2) __extension__ ({ \
  uint64x1x3_t __s1 = __p1; \
  uint64x1x3_t __ret; \
  __builtin_neon_vld3_lane_v(&__ret, __p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], (int8x8_t)__s1.val[2], __p2, 19); \
  __ret; \
})
#else
#define vld3_lane_u64(__p0, __p1, __p2) __extension__ ({ \
  uint64x1x3_t __s1 = __p1; \
  uint64x1x3_t __ret; \
  __builtin_neon_vld3_lane_v(&__ret, __p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], (int8x8_t)__s1.val[2], __p2, 19); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld3_lane_f64(__p0, __p1, __p2) __extension__ ({ \
  float64x1x3_t __s1 = __p1; \
  float64x1x3_t __ret; \
  __builtin_neon_vld3_lane_v(&__ret, __p0, __s1.val[0], __s1.val[1], __s1.val[2], __p2, 10); \
  __ret; \
})
#else
#define vld3_lane_f64(__p0, __p1, __p2) __extension__ ({ \
  float64x1x3_t __s1 = __p1; \
  float64x1x3_t __ret; \
  __builtin_neon_vld3_lane_v(&__ret, __p0, __s1.val[0], __s1.val[1], __s1.val[2], __p2, 10); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld3_lane_s64(__p0, __p1, __p2) __extension__ ({ \
  int64x1x3_t __s1 = __p1; \
  int64x1x3_t __ret; \
  __builtin_neon_vld3_lane_v(&__ret, __p0, __s1.val[0], __s1.val[1], __s1.val[2], __p2, 3); \
  __ret; \
})
#else
#define vld3_lane_s64(__p0, __p1, __p2) __extension__ ({ \
  int64x1x3_t __s1 = __p1; \
  int64x1x3_t __ret; \
  __builtin_neon_vld3_lane_v(&__ret, __p0, __s1.val[0], __s1.val[1], __s1.val[2], __p2, 3); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld4_p64(__p0) __extension__ ({ \
  poly64x1x4_t __ret; \
  __builtin_neon_vld4_v(&__ret, __p0, 6); \
  __ret; \
})
#else
#define vld4_p64(__p0) __extension__ ({ \
  poly64x1x4_t __ret; \
  __builtin_neon_vld4_v(&__ret, __p0, 6); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld4q_p64(__p0) __extension__ ({ \
  poly64x2x4_t __ret; \
  __builtin_neon_vld4q_v(&__ret, __p0, 38); \
  __ret; \
})
#else
#define vld4q_p64(__p0) __extension__ ({ \
  poly64x2x4_t __ret; \
  __builtin_neon_vld4q_v(&__ret, __p0, 38); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 1, 0); \
  __ret.val[3] = __builtin_shufflevector(__ret.val[3], __ret.val[3], 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld4q_u64(__p0) __extension__ ({ \
  uint64x2x4_t __ret; \
  __builtin_neon_vld4q_v(&__ret, __p0, 51); \
  __ret; \
})
#else
#define vld4q_u64(__p0) __extension__ ({ \
  uint64x2x4_t __ret; \
  __builtin_neon_vld4q_v(&__ret, __p0, 51); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 1, 0); \
  __ret.val[3] = __builtin_shufflevector(__ret.val[3], __ret.val[3], 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld4q_f64(__p0) __extension__ ({ \
  float64x2x4_t __ret; \
  __builtin_neon_vld4q_v(&__ret, __p0, 42); \
  __ret; \
})
#else
#define vld4q_f64(__p0) __extension__ ({ \
  float64x2x4_t __ret; \
  __builtin_neon_vld4q_v(&__ret, __p0, 42); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 1, 0); \
  __ret.val[3] = __builtin_shufflevector(__ret.val[3], __ret.val[3], 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld4q_s64(__p0) __extension__ ({ \
  int64x2x4_t __ret; \
  __builtin_neon_vld4q_v(&__ret, __p0, 35); \
  __ret; \
})
#else
#define vld4q_s64(__p0) __extension__ ({ \
  int64x2x4_t __ret; \
  __builtin_neon_vld4q_v(&__ret, __p0, 35); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 1, 0); \
  __ret.val[3] = __builtin_shufflevector(__ret.val[3], __ret.val[3], 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld4_f64(__p0) __extension__ ({ \
  float64x1x4_t __ret; \
  __builtin_neon_vld4_v(&__ret, __p0, 10); \
  __ret; \
})
#else
#define vld4_f64(__p0) __extension__ ({ \
  float64x1x4_t __ret; \
  __builtin_neon_vld4_v(&__ret, __p0, 10); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld4_dup_p64(__p0) __extension__ ({ \
  poly64x1x4_t __ret; \
  __builtin_neon_vld4_dup_v(&__ret, __p0, 6); \
  __ret; \
})
#else
#define vld4_dup_p64(__p0) __extension__ ({ \
  poly64x1x4_t __ret; \
  __builtin_neon_vld4_dup_v(&__ret, __p0, 6); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld4q_dup_p8(__p0) __extension__ ({ \
  poly8x16x4_t __ret; \
  __builtin_neon_vld4q_dup_v(&__ret, __p0, 36); \
  __ret; \
})
#else
#define vld4q_dup_p8(__p0) __extension__ ({ \
  poly8x16x4_t __ret; \
  __builtin_neon_vld4q_dup_v(&__ret, __p0, 36); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[3] = __builtin_shufflevector(__ret.val[3], __ret.val[3], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld4q_dup_p64(__p0) __extension__ ({ \
  poly64x2x4_t __ret; \
  __builtin_neon_vld4q_dup_v(&__ret, __p0, 38); \
  __ret; \
})
#else
#define vld4q_dup_p64(__p0) __extension__ ({ \
  poly64x2x4_t __ret; \
  __builtin_neon_vld4q_dup_v(&__ret, __p0, 38); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 1, 0); \
  __ret.val[3] = __builtin_shufflevector(__ret.val[3], __ret.val[3], 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld4q_dup_p16(__p0) __extension__ ({ \
  poly16x8x4_t __ret; \
  __builtin_neon_vld4q_dup_v(&__ret, __p0, 37); \
  __ret; \
})
#else
#define vld4q_dup_p16(__p0) __extension__ ({ \
  poly16x8x4_t __ret; \
  __builtin_neon_vld4q_dup_v(&__ret, __p0, 37); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[3] = __builtin_shufflevector(__ret.val[3], __ret.val[3], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld4q_dup_u8(__p0) __extension__ ({ \
  uint8x16x4_t __ret; \
  __builtin_neon_vld4q_dup_v(&__ret, __p0, 48); \
  __ret; \
})
#else
#define vld4q_dup_u8(__p0) __extension__ ({ \
  uint8x16x4_t __ret; \
  __builtin_neon_vld4q_dup_v(&__ret, __p0, 48); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[3] = __builtin_shufflevector(__ret.val[3], __ret.val[3], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld4q_dup_u32(__p0) __extension__ ({ \
  uint32x4x4_t __ret; \
  __builtin_neon_vld4q_dup_v(&__ret, __p0, 50); \
  __ret; \
})
#else
#define vld4q_dup_u32(__p0) __extension__ ({ \
  uint32x4x4_t __ret; \
  __builtin_neon_vld4q_dup_v(&__ret, __p0, 50); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 3, 2, 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 3, 2, 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 3, 2, 1, 0); \
  __ret.val[3] = __builtin_shufflevector(__ret.val[3], __ret.val[3], 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld4q_dup_u64(__p0) __extension__ ({ \
  uint64x2x4_t __ret; \
  __builtin_neon_vld4q_dup_v(&__ret, __p0, 51); \
  __ret; \
})
#else
#define vld4q_dup_u64(__p0) __extension__ ({ \
  uint64x2x4_t __ret; \
  __builtin_neon_vld4q_dup_v(&__ret, __p0, 51); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 1, 0); \
  __ret.val[3] = __builtin_shufflevector(__ret.val[3], __ret.val[3], 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld4q_dup_u16(__p0) __extension__ ({ \
  uint16x8x4_t __ret; \
  __builtin_neon_vld4q_dup_v(&__ret, __p0, 49); \
  __ret; \
})
#else
#define vld4q_dup_u16(__p0) __extension__ ({ \
  uint16x8x4_t __ret; \
  __builtin_neon_vld4q_dup_v(&__ret, __p0, 49); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[3] = __builtin_shufflevector(__ret.val[3], __ret.val[3], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld4q_dup_s8(__p0) __extension__ ({ \
  int8x16x4_t __ret; \
  __builtin_neon_vld4q_dup_v(&__ret, __p0, 32); \
  __ret; \
})
#else
#define vld4q_dup_s8(__p0) __extension__ ({ \
  int8x16x4_t __ret; \
  __builtin_neon_vld4q_dup_v(&__ret, __p0, 32); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[3] = __builtin_shufflevector(__ret.val[3], __ret.val[3], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld4q_dup_f64(__p0) __extension__ ({ \
  float64x2x4_t __ret; \
  __builtin_neon_vld4q_dup_v(&__ret, __p0, 42); \
  __ret; \
})
#else
#define vld4q_dup_f64(__p0) __extension__ ({ \
  float64x2x4_t __ret; \
  __builtin_neon_vld4q_dup_v(&__ret, __p0, 42); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 1, 0); \
  __ret.val[3] = __builtin_shufflevector(__ret.val[3], __ret.val[3], 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld4q_dup_f32(__p0) __extension__ ({ \
  float32x4x4_t __ret; \
  __builtin_neon_vld4q_dup_v(&__ret, __p0, 41); \
  __ret; \
})
#else
#define vld4q_dup_f32(__p0) __extension__ ({ \
  float32x4x4_t __ret; \
  __builtin_neon_vld4q_dup_v(&__ret, __p0, 41); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 3, 2, 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 3, 2, 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 3, 2, 1, 0); \
  __ret.val[3] = __builtin_shufflevector(__ret.val[3], __ret.val[3], 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld4q_dup_f16(__p0) __extension__ ({ \
  float16x8x4_t __ret; \
  __builtin_neon_vld4q_dup_v(&__ret, __p0, 40); \
  __ret; \
})
#else
#define vld4q_dup_f16(__p0) __extension__ ({ \
  float16x8x4_t __ret; \
  __builtin_neon_vld4q_dup_v(&__ret, __p0, 40); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[3] = __builtin_shufflevector(__ret.val[3], __ret.val[3], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld4q_dup_s32(__p0) __extension__ ({ \
  int32x4x4_t __ret; \
  __builtin_neon_vld4q_dup_v(&__ret, __p0, 34); \
  __ret; \
})
#else
#define vld4q_dup_s32(__p0) __extension__ ({ \
  int32x4x4_t __ret; \
  __builtin_neon_vld4q_dup_v(&__ret, __p0, 34); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 3, 2, 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 3, 2, 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 3, 2, 1, 0); \
  __ret.val[3] = __builtin_shufflevector(__ret.val[3], __ret.val[3], 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld4q_dup_s64(__p0) __extension__ ({ \
  int64x2x4_t __ret; \
  __builtin_neon_vld4q_dup_v(&__ret, __p0, 35); \
  __ret; \
})
#else
#define vld4q_dup_s64(__p0) __extension__ ({ \
  int64x2x4_t __ret; \
  __builtin_neon_vld4q_dup_v(&__ret, __p0, 35); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 1, 0); \
  __ret.val[3] = __builtin_shufflevector(__ret.val[3], __ret.val[3], 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld4q_dup_s16(__p0) __extension__ ({ \
  int16x8x4_t __ret; \
  __builtin_neon_vld4q_dup_v(&__ret, __p0, 33); \
  __ret; \
})
#else
#define vld4q_dup_s16(__p0) __extension__ ({ \
  int16x8x4_t __ret; \
  __builtin_neon_vld4q_dup_v(&__ret, __p0, 33); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[3] = __builtin_shufflevector(__ret.val[3], __ret.val[3], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld4_dup_f64(__p0) __extension__ ({ \
  float64x1x4_t __ret; \
  __builtin_neon_vld4_dup_v(&__ret, __p0, 10); \
  __ret; \
})
#else
#define vld4_dup_f64(__p0) __extension__ ({ \
  float64x1x4_t __ret; \
  __builtin_neon_vld4_dup_v(&__ret, __p0, 10); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld4_lane_p64(__p0, __p1, __p2) __extension__ ({ \
  poly64x1x4_t __s1 = __p1; \
  poly64x1x4_t __ret; \
  __builtin_neon_vld4_lane_v(&__ret, __p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], (int8x8_t)__s1.val[2], (int8x8_t)__s1.val[3], __p2, 6); \
  __ret; \
})
#else
#define vld4_lane_p64(__p0, __p1, __p2) __extension__ ({ \
  poly64x1x4_t __s1 = __p1; \
  poly64x1x4_t __ret; \
  __builtin_neon_vld4_lane_v(&__ret, __p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], (int8x8_t)__s1.val[2], (int8x8_t)__s1.val[3], __p2, 6); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld4q_lane_p8(__p0, __p1, __p2) __extension__ ({ \
  poly8x16x4_t __s1 = __p1; \
  poly8x16x4_t __ret; \
  __builtin_neon_vld4q_lane_v(&__ret, __p0, (int8x16_t)__s1.val[0], (int8x16_t)__s1.val[1], (int8x16_t)__s1.val[2], (int8x16_t)__s1.val[3], __p2, 36); \
  __ret; \
})
#else
#define vld4q_lane_p8(__p0, __p1, __p2) __extension__ ({ \
  poly8x16x4_t __s1 = __p1; \
  poly8x16x4_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[3] = __builtin_shufflevector(__s1.val[3], __s1.val[3], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  poly8x16x4_t __ret; \
  __builtin_neon_vld4q_lane_v(&__ret, __p0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], (int8x16_t)__rev1.val[2], (int8x16_t)__rev1.val[3], __p2, 36); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[3] = __builtin_shufflevector(__ret.val[3], __ret.val[3], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld4q_lane_p64(__p0, __p1, __p2) __extension__ ({ \
  poly64x2x4_t __s1 = __p1; \
  poly64x2x4_t __ret; \
  __builtin_neon_vld4q_lane_v(&__ret, __p0, (int8x16_t)__s1.val[0], (int8x16_t)__s1.val[1], (int8x16_t)__s1.val[2], (int8x16_t)__s1.val[3], __p2, 38); \
  __ret; \
})
#else
#define vld4q_lane_p64(__p0, __p1, __p2) __extension__ ({ \
  poly64x2x4_t __s1 = __p1; \
  poly64x2x4_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 1, 0); \
  __rev1.val[3] = __builtin_shufflevector(__s1.val[3], __s1.val[3], 1, 0); \
  poly64x2x4_t __ret; \
  __builtin_neon_vld4q_lane_v(&__ret, __p0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], (int8x16_t)__rev1.val[2], (int8x16_t)__rev1.val[3], __p2, 38); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 1, 0); \
  __ret.val[3] = __builtin_shufflevector(__ret.val[3], __ret.val[3], 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld4q_lane_u8(__p0, __p1, __p2) __extension__ ({ \
  uint8x16x4_t __s1 = __p1; \
  uint8x16x4_t __ret; \
  __builtin_neon_vld4q_lane_v(&__ret, __p0, (int8x16_t)__s1.val[0], (int8x16_t)__s1.val[1], (int8x16_t)__s1.val[2], (int8x16_t)__s1.val[3], __p2, 48); \
  __ret; \
})
#else
#define vld4q_lane_u8(__p0, __p1, __p2) __extension__ ({ \
  uint8x16x4_t __s1 = __p1; \
  uint8x16x4_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[3] = __builtin_shufflevector(__s1.val[3], __s1.val[3], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint8x16x4_t __ret; \
  __builtin_neon_vld4q_lane_v(&__ret, __p0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], (int8x16_t)__rev1.val[2], (int8x16_t)__rev1.val[3], __p2, 48); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[3] = __builtin_shufflevector(__ret.val[3], __ret.val[3], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld4q_lane_u64(__p0, __p1, __p2) __extension__ ({ \
  uint64x2x4_t __s1 = __p1; \
  uint64x2x4_t __ret; \
  __builtin_neon_vld4q_lane_v(&__ret, __p0, (int8x16_t)__s1.val[0], (int8x16_t)__s1.val[1], (int8x16_t)__s1.val[2], (int8x16_t)__s1.val[3], __p2, 51); \
  __ret; \
})
#else
#define vld4q_lane_u64(__p0, __p1, __p2) __extension__ ({ \
  uint64x2x4_t __s1 = __p1; \
  uint64x2x4_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 1, 0); \
  __rev1.val[3] = __builtin_shufflevector(__s1.val[3], __s1.val[3], 1, 0); \
  uint64x2x4_t __ret; \
  __builtin_neon_vld4q_lane_v(&__ret, __p0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], (int8x16_t)__rev1.val[2], (int8x16_t)__rev1.val[3], __p2, 51); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 1, 0); \
  __ret.val[3] = __builtin_shufflevector(__ret.val[3], __ret.val[3], 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld4q_lane_s8(__p0, __p1, __p2) __extension__ ({ \
  int8x16x4_t __s1 = __p1; \
  int8x16x4_t __ret; \
  __builtin_neon_vld4q_lane_v(&__ret, __p0, (int8x16_t)__s1.val[0], (int8x16_t)__s1.val[1], (int8x16_t)__s1.val[2], (int8x16_t)__s1.val[3], __p2, 32); \
  __ret; \
})
#else
#define vld4q_lane_s8(__p0, __p1, __p2) __extension__ ({ \
  int8x16x4_t __s1 = __p1; \
  int8x16x4_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[3] = __builtin_shufflevector(__s1.val[3], __s1.val[3], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  int8x16x4_t __ret; \
  __builtin_neon_vld4q_lane_v(&__ret, __p0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], (int8x16_t)__rev1.val[2], (int8x16_t)__rev1.val[3], __p2, 32); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[3] = __builtin_shufflevector(__ret.val[3], __ret.val[3], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld4q_lane_f64(__p0, __p1, __p2) __extension__ ({ \
  float64x2x4_t __s1 = __p1; \
  float64x2x4_t __ret; \
  __builtin_neon_vld4q_lane_v(&__ret, __p0, __s1.val[0], __s1.val[1], __s1.val[2], __s1.val[3], __p2, 42); \
  __ret; \
})
#else
#define vld4q_lane_f64(__p0, __p1, __p2) __extension__ ({ \
  float64x2x4_t __s1 = __p1; \
  float64x2x4_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 1, 0); \
  __rev1.val[3] = __builtin_shufflevector(__s1.val[3], __s1.val[3], 1, 0); \
  float64x2x4_t __ret; \
  __builtin_neon_vld4q_lane_v(&__ret, __p0, __rev1.val[0], __rev1.val[1], __rev1.val[2], __rev1.val[3], __p2, 42); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 1, 0); \
  __ret.val[3] = __builtin_shufflevector(__ret.val[3], __ret.val[3], 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld4q_lane_s64(__p0, __p1, __p2) __extension__ ({ \
  int64x2x4_t __s1 = __p1; \
  int64x2x4_t __ret; \
  __builtin_neon_vld4q_lane_v(&__ret, __p0, __s1.val[0], __s1.val[1], __s1.val[2], __s1.val[3], __p2, 35); \
  __ret; \
})
#else
#define vld4q_lane_s64(__p0, __p1, __p2) __extension__ ({ \
  int64x2x4_t __s1 = __p1; \
  int64x2x4_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 1, 0); \
  __rev1.val[3] = __builtin_shufflevector(__s1.val[3], __s1.val[3], 1, 0); \
  int64x2x4_t __ret; \
  __builtin_neon_vld4q_lane_v(&__ret, __p0, __rev1.val[0], __rev1.val[1], __rev1.val[2], __rev1.val[3], __p2, 35); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 1, 0); \
  __ret.val[3] = __builtin_shufflevector(__ret.val[3], __ret.val[3], 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld4_lane_u64(__p0, __p1, __p2) __extension__ ({ \
  uint64x1x4_t __s1 = __p1; \
  uint64x1x4_t __ret; \
  __builtin_neon_vld4_lane_v(&__ret, __p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], (int8x8_t)__s1.val[2], (int8x8_t)__s1.val[3], __p2, 19); \
  __ret; \
})
#else
#define vld4_lane_u64(__p0, __p1, __p2) __extension__ ({ \
  uint64x1x4_t __s1 = __p1; \
  uint64x1x4_t __ret; \
  __builtin_neon_vld4_lane_v(&__ret, __p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], (int8x8_t)__s1.val[2], (int8x8_t)__s1.val[3], __p2, 19); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld4_lane_f64(__p0, __p1, __p2) __extension__ ({ \
  float64x1x4_t __s1 = __p1; \
  float64x1x4_t __ret; \
  __builtin_neon_vld4_lane_v(&__ret, __p0, __s1.val[0], __s1.val[1], __s1.val[2], __s1.val[3], __p2, 10); \
  __ret; \
})
#else
#define vld4_lane_f64(__p0, __p1, __p2) __extension__ ({ \
  float64x1x4_t __s1 = __p1; \
  float64x1x4_t __ret; \
  __builtin_neon_vld4_lane_v(&__ret, __p0, __s1.val[0], __s1.val[1], __s1.val[2], __s1.val[3], __p2, 10); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld4_lane_s64(__p0, __p1, __p2) __extension__ ({ \
  int64x1x4_t __s1 = __p1; \
  int64x1x4_t __ret; \
  __builtin_neon_vld4_lane_v(&__ret, __p0, __s1.val[0], __s1.val[1], __s1.val[2], __s1.val[3], __p2, 3); \
  __ret; \
})
#else
#define vld4_lane_s64(__p0, __p1, __p2) __extension__ ({ \
  int64x1x4_t __s1 = __p1; \
  int64x1x4_t __ret; \
  __builtin_neon_vld4_lane_v(&__ret, __p0, __s1.val[0], __s1.val[1], __s1.val[2], __s1.val[3], __p2, 3); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vldrq_p128(__p0) __extension__ ({ \
  poly128_t __ret; \
  __ret = (poly128_t) __builtin_neon_vldrq_p128(__p0); \
  __ret; \
})
#else
#define vldrq_p128(__p0) __extension__ ({ \
  poly128_t __ret; \
  __ret = (poly128_t) __builtin_neon_vldrq_p128(__p0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x2_t vmaxq_f64(float64x2_t __p0, float64x2_t __p1) {
  float64x2_t __ret;
  __ret = (float64x2_t) __builtin_neon_vmaxq_v((int8x16_t)__p0, (int8x16_t)__p1, 42);
  return __ret;
}
#else
__ai float64x2_t vmaxq_f64(float64x2_t __p0, float64x2_t __p1) {
  float64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  float64x2_t __ret;
  __ret = (float64x2_t) __builtin_neon_vmaxq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 42);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x1_t vmax_f64(float64x1_t __p0, float64x1_t __p1) {
  float64x1_t __ret;
  __ret = (float64x1_t) __builtin_neon_vmax_v((int8x8_t)__p0, (int8x8_t)__p1, 10);
  return __ret;
}
#else
__ai float64x1_t vmax_f64(float64x1_t __p0, float64x1_t __p1) {
  float64x1_t __ret;
  __ret = (float64x1_t) __builtin_neon_vmax_v((int8x8_t)__p0, (int8x8_t)__p1, 10);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x2_t vmaxnmq_f64(float64x2_t __p0, float64x2_t __p1) {
  float64x2_t __ret;
  __ret = (float64x2_t) __builtin_neon_vmaxnmq_v((int8x16_t)__p0, (int8x16_t)__p1, 42);
  return __ret;
}
#else
__ai float64x2_t vmaxnmq_f64(float64x2_t __p0, float64x2_t __p1) {
  float64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  float64x2_t __ret;
  __ret = (float64x2_t) __builtin_neon_vmaxnmq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 42);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x4_t vmaxnmq_f32(float32x4_t __p0, float32x4_t __p1) {
  float32x4_t __ret;
  __ret = (float32x4_t) __builtin_neon_vmaxnmq_v((int8x16_t)__p0, (int8x16_t)__p1, 41);
  return __ret;
}
#else
__ai float32x4_t vmaxnmq_f32(float32x4_t __p0, float32x4_t __p1) {
  float32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  float32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  float32x4_t __ret;
  __ret = (float32x4_t) __builtin_neon_vmaxnmq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 41);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x1_t vmaxnm_f64(float64x1_t __p0, float64x1_t __p1) {
  float64x1_t __ret;
  __ret = (float64x1_t) __builtin_neon_vmaxnm_v((int8x8_t)__p0, (int8x8_t)__p1, 10);
  return __ret;
}
#else
__ai float64x1_t vmaxnm_f64(float64x1_t __p0, float64x1_t __p1) {
  float64x1_t __ret;
  __ret = (float64x1_t) __builtin_neon_vmaxnm_v((int8x8_t)__p0, (int8x8_t)__p1, 10);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x2_t vmaxnm_f32(float32x2_t __p0, float32x2_t __p1) {
  float32x2_t __ret;
  __ret = (float32x2_t) __builtin_neon_vmaxnm_v((int8x8_t)__p0, (int8x8_t)__p1, 9);
  return __ret;
}
#else
__ai float32x2_t vmaxnm_f32(float32x2_t __p0, float32x2_t __p1) {
  float32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  float32x2_t __ret;
  __ret = (float32x2_t) __builtin_neon_vmaxnm_v((int8x8_t)__rev0, (int8x8_t)__rev1, 9);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64_t vmaxnmvq_f64(float64x2_t __p0) {
  float64_t __ret;
  __ret = (float64_t) __builtin_neon_vmaxnmvq_f64((int8x16_t)__p0);
  return __ret;
}
#else
__ai float64_t vmaxnmvq_f64(float64x2_t __p0) {
  float64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float64_t __ret;
  __ret = (float64_t) __builtin_neon_vmaxnmvq_f64((int8x16_t)__rev0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32_t vmaxnmvq_f32(float32x4_t __p0) {
  float32_t __ret;
  __ret = (float32_t) __builtin_neon_vmaxnmvq_f32((int8x16_t)__p0);
  return __ret;
}
#else
__ai float32_t vmaxnmvq_f32(float32x4_t __p0) {
  float32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  float32_t __ret;
  __ret = (float32_t) __builtin_neon_vmaxnmvq_f32((int8x16_t)__rev0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32_t vmaxnmv_f32(float32x2_t __p0) {
  float32_t __ret;
  __ret = (float32_t) __builtin_neon_vmaxnmv_f32((int8x8_t)__p0);
  return __ret;
}
#else
__ai float32_t vmaxnmv_f32(float32x2_t __p0) {
  float32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float32_t __ret;
  __ret = (float32_t) __builtin_neon_vmaxnmv_f32((int8x8_t)__rev0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8_t vmaxvq_u8(uint8x16_t __p0) {
  uint8_t __ret;
  __ret = (uint8_t) __builtin_neon_vmaxvq_u8((int8x16_t)__p0);
  return __ret;
}
#else
__ai uint8_t vmaxvq_u8(uint8x16_t __p0) {
  uint8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8_t __ret;
  __ret = (uint8_t) __builtin_neon_vmaxvq_u8((int8x16_t)__rev0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32_t vmaxvq_u32(uint32x4_t __p0) {
  uint32_t __ret;
  __ret = (uint32_t) __builtin_neon_vmaxvq_u32((int8x16_t)__p0);
  return __ret;
}
#else
__ai uint32_t vmaxvq_u32(uint32x4_t __p0) {
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint32_t __ret;
  __ret = (uint32_t) __builtin_neon_vmaxvq_u32((int8x16_t)__rev0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16_t vmaxvq_u16(uint16x8_t __p0) {
  uint16_t __ret;
  __ret = (uint16_t) __builtin_neon_vmaxvq_u16((int8x16_t)__p0);
  return __ret;
}
#else
__ai uint16_t vmaxvq_u16(uint16x8_t __p0) {
  uint16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16_t __ret;
  __ret = (uint16_t) __builtin_neon_vmaxvq_u16((int8x16_t)__rev0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8_t vmaxvq_s8(int8x16_t __p0) {
  int8_t __ret;
  __ret = (int8_t) __builtin_neon_vmaxvq_s8((int8x16_t)__p0);
  return __ret;
}
#else
__ai int8_t vmaxvq_s8(int8x16_t __p0) {
  int8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8_t __ret;
  __ret = (int8_t) __builtin_neon_vmaxvq_s8((int8x16_t)__rev0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64_t vmaxvq_f64(float64x2_t __p0) {
  float64_t __ret;
  __ret = (float64_t) __builtin_neon_vmaxvq_f64((int8x16_t)__p0);
  return __ret;
}
#else
__ai float64_t vmaxvq_f64(float64x2_t __p0) {
  float64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float64_t __ret;
  __ret = (float64_t) __builtin_neon_vmaxvq_f64((int8x16_t)__rev0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32_t vmaxvq_f32(float32x4_t __p0) {
  float32_t __ret;
  __ret = (float32_t) __builtin_neon_vmaxvq_f32((int8x16_t)__p0);
  return __ret;
}
#else
__ai float32_t vmaxvq_f32(float32x4_t __p0) {
  float32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  float32_t __ret;
  __ret = (float32_t) __builtin_neon_vmaxvq_f32((int8x16_t)__rev0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32_t vmaxvq_s32(int32x4_t __p0) {
  int32_t __ret;
  __ret = (int32_t) __builtin_neon_vmaxvq_s32((int8x16_t)__p0);
  return __ret;
}
#else
__ai int32_t vmaxvq_s32(int32x4_t __p0) {
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int32_t __ret;
  __ret = (int32_t) __builtin_neon_vmaxvq_s32((int8x16_t)__rev0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16_t vmaxvq_s16(int16x8_t __p0) {
  int16_t __ret;
  __ret = (int16_t) __builtin_neon_vmaxvq_s16((int8x16_t)__p0);
  return __ret;
}
#else
__ai int16_t vmaxvq_s16(int16x8_t __p0) {
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int16_t __ret;
  __ret = (int16_t) __builtin_neon_vmaxvq_s16((int8x16_t)__rev0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8_t vmaxv_u8(uint8x8_t __p0) {
  uint8_t __ret;
  __ret = (uint8_t) __builtin_neon_vmaxv_u8((int8x8_t)__p0);
  return __ret;
}
#else
__ai uint8_t vmaxv_u8(uint8x8_t __p0) {
  uint8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8_t __ret;
  __ret = (uint8_t) __builtin_neon_vmaxv_u8((int8x8_t)__rev0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32_t vmaxv_u32(uint32x2_t __p0) {
  uint32_t __ret;
  __ret = (uint32_t) __builtin_neon_vmaxv_u32((int8x8_t)__p0);
  return __ret;
}
#else
__ai uint32_t vmaxv_u32(uint32x2_t __p0) {
  uint32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint32_t __ret;
  __ret = (uint32_t) __builtin_neon_vmaxv_u32((int8x8_t)__rev0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16_t vmaxv_u16(uint16x4_t __p0) {
  uint16_t __ret;
  __ret = (uint16_t) __builtin_neon_vmaxv_u16((int8x8_t)__p0);
  return __ret;
}
#else
__ai uint16_t vmaxv_u16(uint16x4_t __p0) {
  uint16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint16_t __ret;
  __ret = (uint16_t) __builtin_neon_vmaxv_u16((int8x8_t)__rev0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8_t vmaxv_s8(int8x8_t __p0) {
  int8_t __ret;
  __ret = (int8_t) __builtin_neon_vmaxv_s8((int8x8_t)__p0);
  return __ret;
}
#else
__ai int8_t vmaxv_s8(int8x8_t __p0) {
  int8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int8_t __ret;
  __ret = (int8_t) __builtin_neon_vmaxv_s8((int8x8_t)__rev0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32_t vmaxv_f32(float32x2_t __p0) {
  float32_t __ret;
  __ret = (float32_t) __builtin_neon_vmaxv_f32((int8x8_t)__p0);
  return __ret;
}
#else
__ai float32_t vmaxv_f32(float32x2_t __p0) {
  float32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float32_t __ret;
  __ret = (float32_t) __builtin_neon_vmaxv_f32((int8x8_t)__rev0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32_t vmaxv_s32(int32x2_t __p0) {
  int32_t __ret;
  __ret = (int32_t) __builtin_neon_vmaxv_s32((int8x8_t)__p0);
  return __ret;
}
#else
__ai int32_t vmaxv_s32(int32x2_t __p0) {
  int32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int32_t __ret;
  __ret = (int32_t) __builtin_neon_vmaxv_s32((int8x8_t)__rev0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16_t vmaxv_s16(int16x4_t __p0) {
  int16_t __ret;
  __ret = (int16_t) __builtin_neon_vmaxv_s16((int8x8_t)__p0);
  return __ret;
}
#else
__ai int16_t vmaxv_s16(int16x4_t __p0) {
  int16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int16_t __ret;
  __ret = (int16_t) __builtin_neon_vmaxv_s16((int8x8_t)__rev0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x2_t vminq_f64(float64x2_t __p0, float64x2_t __p1) {
  float64x2_t __ret;
  __ret = (float64x2_t) __builtin_neon_vminq_v((int8x16_t)__p0, (int8x16_t)__p1, 42);
  return __ret;
}
#else
__ai float64x2_t vminq_f64(float64x2_t __p0, float64x2_t __p1) {
  float64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  float64x2_t __ret;
  __ret = (float64x2_t) __builtin_neon_vminq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 42);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x1_t vmin_f64(float64x1_t __p0, float64x1_t __p1) {
  float64x1_t __ret;
  __ret = (float64x1_t) __builtin_neon_vmin_v((int8x8_t)__p0, (int8x8_t)__p1, 10);
  return __ret;
}
#else
__ai float64x1_t vmin_f64(float64x1_t __p0, float64x1_t __p1) {
  float64x1_t __ret;
  __ret = (float64x1_t) __builtin_neon_vmin_v((int8x8_t)__p0, (int8x8_t)__p1, 10);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x2_t vminnmq_f64(float64x2_t __p0, float64x2_t __p1) {
  float64x2_t __ret;
  __ret = (float64x2_t) __builtin_neon_vminnmq_v((int8x16_t)__p0, (int8x16_t)__p1, 42);
  return __ret;
}
#else
__ai float64x2_t vminnmq_f64(float64x2_t __p0, float64x2_t __p1) {
  float64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  float64x2_t __ret;
  __ret = (float64x2_t) __builtin_neon_vminnmq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 42);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x4_t vminnmq_f32(float32x4_t __p0, float32x4_t __p1) {
  float32x4_t __ret;
  __ret = (float32x4_t) __builtin_neon_vminnmq_v((int8x16_t)__p0, (int8x16_t)__p1, 41);
  return __ret;
}
#else
__ai float32x4_t vminnmq_f32(float32x4_t __p0, float32x4_t __p1) {
  float32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  float32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  float32x4_t __ret;
  __ret = (float32x4_t) __builtin_neon_vminnmq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 41);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x1_t vminnm_f64(float64x1_t __p0, float64x1_t __p1) {
  float64x1_t __ret;
  __ret = (float64x1_t) __builtin_neon_vminnm_v((int8x8_t)__p0, (int8x8_t)__p1, 10);
  return __ret;
}
#else
__ai float64x1_t vminnm_f64(float64x1_t __p0, float64x1_t __p1) {
  float64x1_t __ret;
  __ret = (float64x1_t) __builtin_neon_vminnm_v((int8x8_t)__p0, (int8x8_t)__p1, 10);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x2_t vminnm_f32(float32x2_t __p0, float32x2_t __p1) {
  float32x2_t __ret;
  __ret = (float32x2_t) __builtin_neon_vminnm_v((int8x8_t)__p0, (int8x8_t)__p1, 9);
  return __ret;
}
#else
__ai float32x2_t vminnm_f32(float32x2_t __p0, float32x2_t __p1) {
  float32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  float32x2_t __ret;
  __ret = (float32x2_t) __builtin_neon_vminnm_v((int8x8_t)__rev0, (int8x8_t)__rev1, 9);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64_t vminnmvq_f64(float64x2_t __p0) {
  float64_t __ret;
  __ret = (float64_t) __builtin_neon_vminnmvq_f64((int8x16_t)__p0);
  return __ret;
}
#else
__ai float64_t vminnmvq_f64(float64x2_t __p0) {
  float64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float64_t __ret;
  __ret = (float64_t) __builtin_neon_vminnmvq_f64((int8x16_t)__rev0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32_t vminnmvq_f32(float32x4_t __p0) {
  float32_t __ret;
  __ret = (float32_t) __builtin_neon_vminnmvq_f32((int8x16_t)__p0);
  return __ret;
}
#else
__ai float32_t vminnmvq_f32(float32x4_t __p0) {
  float32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  float32_t __ret;
  __ret = (float32_t) __builtin_neon_vminnmvq_f32((int8x16_t)__rev0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32_t vminnmv_f32(float32x2_t __p0) {
  float32_t __ret;
  __ret = (float32_t) __builtin_neon_vminnmv_f32((int8x8_t)__p0);
  return __ret;
}
#else
__ai float32_t vminnmv_f32(float32x2_t __p0) {
  float32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float32_t __ret;
  __ret = (float32_t) __builtin_neon_vminnmv_f32((int8x8_t)__rev0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8_t vminvq_u8(uint8x16_t __p0) {
  uint8_t __ret;
  __ret = (uint8_t) __builtin_neon_vminvq_u8((int8x16_t)__p0);
  return __ret;
}
#else
__ai uint8_t vminvq_u8(uint8x16_t __p0) {
  uint8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8_t __ret;
  __ret = (uint8_t) __builtin_neon_vminvq_u8((int8x16_t)__rev0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32_t vminvq_u32(uint32x4_t __p0) {
  uint32_t __ret;
  __ret = (uint32_t) __builtin_neon_vminvq_u32((int8x16_t)__p0);
  return __ret;
}
#else
__ai uint32_t vminvq_u32(uint32x4_t __p0) {
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint32_t __ret;
  __ret = (uint32_t) __builtin_neon_vminvq_u32((int8x16_t)__rev0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16_t vminvq_u16(uint16x8_t __p0) {
  uint16_t __ret;
  __ret = (uint16_t) __builtin_neon_vminvq_u16((int8x16_t)__p0);
  return __ret;
}
#else
__ai uint16_t vminvq_u16(uint16x8_t __p0) {
  uint16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16_t __ret;
  __ret = (uint16_t) __builtin_neon_vminvq_u16((int8x16_t)__rev0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8_t vminvq_s8(int8x16_t __p0) {
  int8_t __ret;
  __ret = (int8_t) __builtin_neon_vminvq_s8((int8x16_t)__p0);
  return __ret;
}
#else
__ai int8_t vminvq_s8(int8x16_t __p0) {
  int8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8_t __ret;
  __ret = (int8_t) __builtin_neon_vminvq_s8((int8x16_t)__rev0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64_t vminvq_f64(float64x2_t __p0) {
  float64_t __ret;
  __ret = (float64_t) __builtin_neon_vminvq_f64((int8x16_t)__p0);
  return __ret;
}
#else
__ai float64_t vminvq_f64(float64x2_t __p0) {
  float64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float64_t __ret;
  __ret = (float64_t) __builtin_neon_vminvq_f64((int8x16_t)__rev0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32_t vminvq_f32(float32x4_t __p0) {
  float32_t __ret;
  __ret = (float32_t) __builtin_neon_vminvq_f32((int8x16_t)__p0);
  return __ret;
}
#else
__ai float32_t vminvq_f32(float32x4_t __p0) {
  float32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  float32_t __ret;
  __ret = (float32_t) __builtin_neon_vminvq_f32((int8x16_t)__rev0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32_t vminvq_s32(int32x4_t __p0) {
  int32_t __ret;
  __ret = (int32_t) __builtin_neon_vminvq_s32((int8x16_t)__p0);
  return __ret;
}
#else
__ai int32_t vminvq_s32(int32x4_t __p0) {
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int32_t __ret;
  __ret = (int32_t) __builtin_neon_vminvq_s32((int8x16_t)__rev0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16_t vminvq_s16(int16x8_t __p0) {
  int16_t __ret;
  __ret = (int16_t) __builtin_neon_vminvq_s16((int8x16_t)__p0);
  return __ret;
}
#else
__ai int16_t vminvq_s16(int16x8_t __p0) {
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int16_t __ret;
  __ret = (int16_t) __builtin_neon_vminvq_s16((int8x16_t)__rev0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8_t vminv_u8(uint8x8_t __p0) {
  uint8_t __ret;
  __ret = (uint8_t) __builtin_neon_vminv_u8((int8x8_t)__p0);
  return __ret;
}
#else
__ai uint8_t vminv_u8(uint8x8_t __p0) {
  uint8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8_t __ret;
  __ret = (uint8_t) __builtin_neon_vminv_u8((int8x8_t)__rev0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32_t vminv_u32(uint32x2_t __p0) {
  uint32_t __ret;
  __ret = (uint32_t) __builtin_neon_vminv_u32((int8x8_t)__p0);
  return __ret;
}
#else
__ai uint32_t vminv_u32(uint32x2_t __p0) {
  uint32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint32_t __ret;
  __ret = (uint32_t) __builtin_neon_vminv_u32((int8x8_t)__rev0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16_t vminv_u16(uint16x4_t __p0) {
  uint16_t __ret;
  __ret = (uint16_t) __builtin_neon_vminv_u16((int8x8_t)__p0);
  return __ret;
}
#else
__ai uint16_t vminv_u16(uint16x4_t __p0) {
  uint16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint16_t __ret;
  __ret = (uint16_t) __builtin_neon_vminv_u16((int8x8_t)__rev0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8_t vminv_s8(int8x8_t __p0) {
  int8_t __ret;
  __ret = (int8_t) __builtin_neon_vminv_s8((int8x8_t)__p0);
  return __ret;
}
#else
__ai int8_t vminv_s8(int8x8_t __p0) {
  int8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int8_t __ret;
  __ret = (int8_t) __builtin_neon_vminv_s8((int8x8_t)__rev0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32_t vminv_f32(float32x2_t __p0) {
  float32_t __ret;
  __ret = (float32_t) __builtin_neon_vminv_f32((int8x8_t)__p0);
  return __ret;
}
#else
__ai float32_t vminv_f32(float32x2_t __p0) {
  float32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float32_t __ret;
  __ret = (float32_t) __builtin_neon_vminv_f32((int8x8_t)__rev0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32_t vminv_s32(int32x2_t __p0) {
  int32_t __ret;
  __ret = (int32_t) __builtin_neon_vminv_s32((int8x8_t)__p0);
  return __ret;
}
#else
__ai int32_t vminv_s32(int32x2_t __p0) {
  int32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int32_t __ret;
  __ret = (int32_t) __builtin_neon_vminv_s32((int8x8_t)__rev0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16_t vminv_s16(int16x4_t __p0) {
  int16_t __ret;
  __ret = (int16_t) __builtin_neon_vminv_s16((int8x8_t)__p0);
  return __ret;
}
#else
__ai int16_t vminv_s16(int16x4_t __p0) {
  int16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int16_t __ret;
  __ret = (int16_t) __builtin_neon_vminv_s16((int8x8_t)__rev0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x2_t vmlaq_f64(float64x2_t __p0, float64x2_t __p1, float64x2_t __p2) {
  float64x2_t __ret;
  __ret = __p0 + __p1 * __p2;
  return __ret;
}
#else
__ai float64x2_t vmlaq_f64(float64x2_t __p0, float64x2_t __p1, float64x2_t __p2) {
  float64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  float64x2_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 1, 0);
  float64x2_t __ret;
  __ret = __rev0 + __rev1 * __rev2;
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x1_t vmla_f64(float64x1_t __p0, float64x1_t __p1, float64x1_t __p2) {
  float64x1_t __ret;
  __ret = __p0 + __p1 * __p2;
  return __ret;
}
#else
__ai float64x1_t vmla_f64(float64x1_t __p0, float64x1_t __p1, float64x1_t __p2) {
  float64x1_t __ret;
  __ret = __p0 + __p1 * __p2;
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
#define vmlaq_laneq_u32(__p0, __p1, __p2, __p3) __extension__ ({ \
  uint32x4_t __s0 = __p0; \
  uint32x4_t __s1 = __p1; \
  uint32x4_t __s2 = __p2; \
  uint32x4_t __ret; \
  __ret = __s0 + __s1 * __builtin_shufflevector(__s2, __s2, __p3, __p3, __p3, __p3); \
  __ret; \
})
#else
#define vmlaq_laneq_u32(__p0, __p1, __p2, __p3) __extension__ ({ \
  uint32x4_t __s0 = __p0; \
  uint32x4_t __s1 = __p1; \
  uint32x4_t __s2 = __p2; \
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  uint32x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  uint32x4_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 3, 2, 1, 0); \
  uint32x4_t __ret; \
  __ret = __rev0 + __rev1 * __builtin_shufflevector(__rev2, __rev2, __p3, __p3, __p3, __p3); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmlaq_laneq_u16(__p0, __p1, __p2, __p3) __extension__ ({ \
  uint16x8_t __s0 = __p0; \
  uint16x8_t __s1 = __p1; \
  uint16x8_t __s2 = __p2; \
  uint16x8_t __ret; \
  __ret = __s0 + __s1 * __builtin_shufflevector(__s2, __s2, __p3, __p3, __p3, __p3, __p3, __p3, __p3, __p3); \
  __ret; \
})
#else
#define vmlaq_laneq_u16(__p0, __p1, __p2, __p3) __extension__ ({ \
  uint16x8_t __s0 = __p0; \
  uint16x8_t __s1 = __p1; \
  uint16x8_t __s2 = __p2; \
  uint16x8_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint16x8_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint16x8_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint16x8_t __ret; \
  __ret = __rev0 + __rev1 * __builtin_shufflevector(__rev2, __rev2, __p3, __p3, __p3, __p3, __p3, __p3, __p3, __p3); \
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmlaq_laneq_f32(__p0, __p1, __p2, __p3) __extension__ ({ \
  float32x4_t __s0 = __p0; \
  float32x4_t __s1 = __p1; \
  float32x4_t __s2 = __p2; \
  float32x4_t __ret; \
  __ret = __s0 + __s1 * __builtin_shufflevector(__s2, __s2, __p3, __p3, __p3, __p3); \
  __ret; \
})
#else
#define vmlaq_laneq_f32(__p0, __p1, __p2, __p3) __extension__ ({ \
  float32x4_t __s0 = __p0; \
  float32x4_t __s1 = __p1; \
  float32x4_t __s2 = __p2; \
  float32x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  float32x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  float32x4_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 3, 2, 1, 0); \
  float32x4_t __ret; \
  __ret = __rev0 + __rev1 * __builtin_shufflevector(__rev2, __rev2, __p3, __p3, __p3, __p3); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmlaq_laneq_s32(__p0, __p1, __p2, __p3) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  int32x4_t __s1 = __p1; \
  int32x4_t __s2 = __p2; \
  int32x4_t __ret; \
  __ret = __s0 + __s1 * __builtin_shufflevector(__s2, __s2, __p3, __p3, __p3, __p3); \
  __ret; \
})
#else
#define vmlaq_laneq_s32(__p0, __p1, __p2, __p3) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  int32x4_t __s1 = __p1; \
  int32x4_t __s2 = __p2; \
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  int32x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  int32x4_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 3, 2, 1, 0); \
  int32x4_t __ret; \
  __ret = __rev0 + __rev1 * __builtin_shufflevector(__rev2, __rev2, __p3, __p3, __p3, __p3); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmlaq_laneq_s16(__p0, __p1, __p2, __p3) __extension__ ({ \
  int16x8_t __s0 = __p0; \
  int16x8_t __s1 = __p1; \
  int16x8_t __s2 = __p2; \
  int16x8_t __ret; \
  __ret = __s0 + __s1 * __builtin_shufflevector(__s2, __s2, __p3, __p3, __p3, __p3, __p3, __p3, __p3, __p3); \
  __ret; \
})
#else
#define vmlaq_laneq_s16(__p0, __p1, __p2, __p3) __extension__ ({ \
  int16x8_t __s0 = __p0; \
  int16x8_t __s1 = __p1; \
  int16x8_t __s2 = __p2; \
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 7, 6, 5, 4, 3, 2, 1, 0); \
  int16x8_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 7, 6, 5, 4, 3, 2, 1, 0); \
  int16x8_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 7, 6, 5, 4, 3, 2, 1, 0); \
  int16x8_t __ret; \
  __ret = __rev0 + __rev1 * __builtin_shufflevector(__rev2, __rev2, __p3, __p3, __p3, __p3, __p3, __p3, __p3, __p3); \
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmla_laneq_u32(__p0, __p1, __p2, __p3) __extension__ ({ \
  uint32x2_t __s0 = __p0; \
  uint32x2_t __s1 = __p1; \
  uint32x4_t __s2 = __p2; \
  uint32x2_t __ret; \
  __ret = __s0 + __s1 * __builtin_shufflevector(__s2, __s2, __p3, __p3); \
  __ret; \
})
#else
#define vmla_laneq_u32(__p0, __p1, __p2, __p3) __extension__ ({ \
  uint32x2_t __s0 = __p0; \
  uint32x2_t __s1 = __p1; \
  uint32x4_t __s2 = __p2; \
  uint32x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  uint32x2_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 1, 0); \
  uint32x4_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 3, 2, 1, 0); \
  uint32x2_t __ret; \
  __ret = __rev0 + __rev1 * __builtin_shufflevector(__rev2, __rev2, __p3, __p3); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmla_laneq_u16(__p0, __p1, __p2, __p3) __extension__ ({ \
  uint16x4_t __s0 = __p0; \
  uint16x4_t __s1 = __p1; \
  uint16x8_t __s2 = __p2; \
  uint16x4_t __ret; \
  __ret = __s0 + __s1 * __builtin_shufflevector(__s2, __s2, __p3, __p3, __p3, __p3); \
  __ret; \
})
#else
#define vmla_laneq_u16(__p0, __p1, __p2, __p3) __extension__ ({ \
  uint16x4_t __s0 = __p0; \
  uint16x4_t __s1 = __p1; \
  uint16x8_t __s2 = __p2; \
  uint16x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  uint16x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  uint16x8_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint16x4_t __ret; \
  __ret = __rev0 + __rev1 * __builtin_shufflevector(__rev2, __rev2, __p3, __p3, __p3, __p3); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmla_laneq_f32(__p0, __p1, __p2, __p3) __extension__ ({ \
  float32x2_t __s0 = __p0; \
  float32x2_t __s1 = __p1; \
  float32x4_t __s2 = __p2; \
  float32x2_t __ret; \
  __ret = __s0 + __s1 * __builtin_shufflevector(__s2, __s2, __p3, __p3); \
  __ret; \
})
#else
#define vmla_laneq_f32(__p0, __p1, __p2, __p3) __extension__ ({ \
  float32x2_t __s0 = __p0; \
  float32x2_t __s1 = __p1; \
  float32x4_t __s2 = __p2; \
  float32x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  float32x2_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 1, 0); \
  float32x4_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 3, 2, 1, 0); \
  float32x2_t __ret; \
  __ret = __rev0 + __rev1 * __builtin_shufflevector(__rev2, __rev2, __p3, __p3); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmla_laneq_s32(__p0, __p1, __p2, __p3) __extension__ ({ \
  int32x2_t __s0 = __p0; \
  int32x2_t __s1 = __p1; \
  int32x4_t __s2 = __p2; \
  int32x2_t __ret; \
  __ret = __s0 + __s1 * __builtin_shufflevector(__s2, __s2, __p3, __p3); \
  __ret; \
})
#else
#define vmla_laneq_s32(__p0, __p1, __p2, __p3) __extension__ ({ \
  int32x2_t __s0 = __p0; \
  int32x2_t __s1 = __p1; \
  int32x4_t __s2 = __p2; \
  int32x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  int32x2_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 1, 0); \
  int32x4_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 3, 2, 1, 0); \
  int32x2_t __ret; \
  __ret = __rev0 + __rev1 * __builtin_shufflevector(__rev2, __rev2, __p3, __p3); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmla_laneq_s16(__p0, __p1, __p2, __p3) __extension__ ({ \
  int16x4_t __s0 = __p0; \
  int16x4_t __s1 = __p1; \
  int16x8_t __s2 = __p2; \
  int16x4_t __ret; \
  __ret = __s0 + __s1 * __builtin_shufflevector(__s2, __s2, __p3, __p3, __p3, __p3); \
  __ret; \
})
#else
#define vmla_laneq_s16(__p0, __p1, __p2, __p3) __extension__ ({ \
  int16x4_t __s0 = __p0; \
  int16x4_t __s1 = __p1; \
  int16x8_t __s2 = __p2; \
  int16x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  int16x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  int16x8_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 7, 6, 5, 4, 3, 2, 1, 0); \
  int16x4_t __ret; \
  __ret = __rev0 + __rev1 * __builtin_shufflevector(__rev2, __rev2, __p3, __p3, __p3, __p3); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x2_t vmlaq_n_f64(float64x2_t __p0, float64x2_t __p1, float64_t __p2) {
  float64x2_t __ret;
  __ret = __p0 + __p1 * (float64x2_t) {__p2, __p2};
  return __ret;
}
#else
__ai float64x2_t vmlaq_n_f64(float64x2_t __p0, float64x2_t __p1, float64_t __p2) {
  float64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  float64x2_t __ret;
  __ret = __rev0 + __rev1 * (float64x2_t) {__p2, __p2};
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
#define vmlal_high_lane_u32(__p0, __p1, __p2, __p3) __extension__ ({ \
  uint64x2_t __s0 = __p0; \
  uint32x4_t __s1 = __p1; \
  uint32x2_t __s2 = __p2; \
  uint64x2_t __ret; \
  __ret = __s0 + vmull_u32(vget_high_u32(__s1), __builtin_shufflevector(__s2, __s2, __p3, __p3)); \
  __ret; \
})
#else
#define vmlal_high_lane_u32(__p0, __p1, __p2, __p3) __extension__ ({ \
  uint64x2_t __s0 = __p0; \
  uint32x4_t __s1 = __p1; \
  uint32x2_t __s2 = __p2; \
  uint64x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  uint32x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  uint32x2_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 1, 0); \
  uint64x2_t __ret; \
  __ret = __rev0 + __noswap_vmull_u32(__noswap_vget_high_u32(__rev1), __builtin_shufflevector(__rev2, __rev2, __p3, __p3)); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmlal_high_lane_u16(__p0, __p1, __p2, __p3) __extension__ ({ \
  uint32x4_t __s0 = __p0; \
  uint16x8_t __s1 = __p1; \
  uint16x4_t __s2 = __p2; \
  uint32x4_t __ret; \
  __ret = __s0 + vmull_u16(vget_high_u16(__s1), __builtin_shufflevector(__s2, __s2, __p3, __p3, __p3, __p3)); \
  __ret; \
})
#else
#define vmlal_high_lane_u16(__p0, __p1, __p2, __p3) __extension__ ({ \
  uint32x4_t __s0 = __p0; \
  uint16x8_t __s1 = __p1; \
  uint16x4_t __s2 = __p2; \
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  uint16x8_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint16x4_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 3, 2, 1, 0); \
  uint32x4_t __ret; \
  __ret = __rev0 + __noswap_vmull_u16(__noswap_vget_high_u16(__rev1), __builtin_shufflevector(__rev2, __rev2, __p3, __p3, __p3, __p3)); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmlal_high_lane_s32(__p0, __p1, __p2, __p3) __extension__ ({ \
  int64x2_t __s0 = __p0; \
  int32x4_t __s1 = __p1; \
  int32x2_t __s2 = __p2; \
  int64x2_t __ret; \
  __ret = __s0 + vmull_s32(vget_high_s32(__s1), __builtin_shufflevector(__s2, __s2, __p3, __p3)); \
  __ret; \
})
#else
#define vmlal_high_lane_s32(__p0, __p1, __p2, __p3) __extension__ ({ \
  int64x2_t __s0 = __p0; \
  int32x4_t __s1 = __p1; \
  int32x2_t __s2 = __p2; \
  int64x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  int32x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  int32x2_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 1, 0); \
  int64x2_t __ret; \
  __ret = __rev0 + __noswap_vmull_s32(__noswap_vget_high_s32(__rev1), __builtin_shufflevector(__rev2, __rev2, __p3, __p3)); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmlal_high_lane_s16(__p0, __p1, __p2, __p3) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  int16x8_t __s1 = __p1; \
  int16x4_t __s2 = __p2; \
  int32x4_t __ret; \
  __ret = __s0 + vmull_s16(vget_high_s16(__s1), __builtin_shufflevector(__s2, __s2, __p3, __p3, __p3, __p3)); \
  __ret; \
})
#else
#define vmlal_high_lane_s16(__p0, __p1, __p2, __p3) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  int16x8_t __s1 = __p1; \
  int16x4_t __s2 = __p2; \
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  int16x8_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 7, 6, 5, 4, 3, 2, 1, 0); \
  int16x4_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 3, 2, 1, 0); \
  int32x4_t __ret; \
  __ret = __rev0 + __noswap_vmull_s16(__noswap_vget_high_s16(__rev1), __builtin_shufflevector(__rev2, __rev2, __p3, __p3, __p3, __p3)); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmlal_high_laneq_u32(__p0, __p1, __p2, __p3) __extension__ ({ \
  uint64x2_t __s0 = __p0; \
  uint32x4_t __s1 = __p1; \
  uint32x4_t __s2 = __p2; \
  uint64x2_t __ret; \
  __ret = __s0 + vmull_u32(vget_high_u32(__s1), __builtin_shufflevector(__s2, __s2, __p3, __p3)); \
  __ret; \
})
#else
#define vmlal_high_laneq_u32(__p0, __p1, __p2, __p3) __extension__ ({ \
  uint64x2_t __s0 = __p0; \
  uint32x4_t __s1 = __p1; \
  uint32x4_t __s2 = __p2; \
  uint64x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  uint32x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  uint32x4_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 3, 2, 1, 0); \
  uint64x2_t __ret; \
  __ret = __rev0 + __noswap_vmull_u32(__noswap_vget_high_u32(__rev1), __builtin_shufflevector(__rev2, __rev2, __p3, __p3)); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmlal_high_laneq_u16(__p0, __p1, __p2, __p3) __extension__ ({ \
  uint32x4_t __s0 = __p0; \
  uint16x8_t __s1 = __p1; \
  uint16x8_t __s2 = __p2; \
  uint32x4_t __ret; \
  __ret = __s0 + vmull_u16(vget_high_u16(__s1), __builtin_shufflevector(__s2, __s2, __p3, __p3, __p3, __p3)); \
  __ret; \
})
#else
#define vmlal_high_laneq_u16(__p0, __p1, __p2, __p3) __extension__ ({ \
  uint32x4_t __s0 = __p0; \
  uint16x8_t __s1 = __p1; \
  uint16x8_t __s2 = __p2; \
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  uint16x8_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint16x8_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint32x4_t __ret; \
  __ret = __rev0 + __noswap_vmull_u16(__noswap_vget_high_u16(__rev1), __builtin_shufflevector(__rev2, __rev2, __p3, __p3, __p3, __p3)); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmlal_high_laneq_s32(__p0, __p1, __p2, __p3) __extension__ ({ \
  int64x2_t __s0 = __p0; \
  int32x4_t __s1 = __p1; \
  int32x4_t __s2 = __p2; \
  int64x2_t __ret; \
  __ret = __s0 + vmull_s32(vget_high_s32(__s1), __builtin_shufflevector(__s2, __s2, __p3, __p3)); \
  __ret; \
})
#else
#define vmlal_high_laneq_s32(__p0, __p1, __p2, __p3) __extension__ ({ \
  int64x2_t __s0 = __p0; \
  int32x4_t __s1 = __p1; \
  int32x4_t __s2 = __p2; \
  int64x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  int32x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  int32x4_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 3, 2, 1, 0); \
  int64x2_t __ret; \
  __ret = __rev0 + __noswap_vmull_s32(__noswap_vget_high_s32(__rev1), __builtin_shufflevector(__rev2, __rev2, __p3, __p3)); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmlal_high_laneq_s16(__p0, __p1, __p2, __p3) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  int16x8_t __s1 = __p1; \
  int16x8_t __s2 = __p2; \
  int32x4_t __ret; \
  __ret = __s0 + vmull_s16(vget_high_s16(__s1), __builtin_shufflevector(__s2, __s2, __p3, __p3, __p3, __p3)); \
  __ret; \
})
#else
#define vmlal_high_laneq_s16(__p0, __p1, __p2, __p3) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  int16x8_t __s1 = __p1; \
  int16x8_t __s2 = __p2; \
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  int16x8_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 7, 6, 5, 4, 3, 2, 1, 0); \
  int16x8_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 7, 6, 5, 4, 3, 2, 1, 0); \
  int32x4_t __ret; \
  __ret = __rev0 + __noswap_vmull_s16(__noswap_vget_high_s16(__rev1), __builtin_shufflevector(__rev2, __rev2, __p3, __p3, __p3, __p3)); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmlal_laneq_u32(__p0, __p1, __p2, __p3) __extension__ ({ \
  uint64x2_t __s0 = __p0; \
  uint32x2_t __s1 = __p1; \
  uint32x4_t __s2 = __p2; \
  uint64x2_t __ret; \
  __ret = __s0 + vmull_u32(__s1, __builtin_shufflevector(__s2, __s2, __p3, __p3)); \
  __ret; \
})
#else
#define vmlal_laneq_u32(__p0, __p1, __p2, __p3) __extension__ ({ \
  uint64x2_t __s0 = __p0; \
  uint32x2_t __s1 = __p1; \
  uint32x4_t __s2 = __p2; \
  uint64x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  uint32x2_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 1, 0); \
  uint32x4_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 3, 2, 1, 0); \
  uint64x2_t __ret; \
  __ret = __rev0 + __noswap_vmull_u32(__rev1, __builtin_shufflevector(__rev2, __rev2, __p3, __p3)); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmlal_laneq_u16(__p0, __p1, __p2, __p3) __extension__ ({ \
  uint32x4_t __s0 = __p0; \
  uint16x4_t __s1 = __p1; \
  uint16x8_t __s2 = __p2; \
  uint32x4_t __ret; \
  __ret = __s0 + vmull_u16(__s1, __builtin_shufflevector(__s2, __s2, __p3, __p3, __p3, __p3)); \
  __ret; \
})
#else
#define vmlal_laneq_u16(__p0, __p1, __p2, __p3) __extension__ ({ \
  uint32x4_t __s0 = __p0; \
  uint16x4_t __s1 = __p1; \
  uint16x8_t __s2 = __p2; \
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  uint16x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  uint16x8_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint32x4_t __ret; \
  __ret = __rev0 + __noswap_vmull_u16(__rev1, __builtin_shufflevector(__rev2, __rev2, __p3, __p3, __p3, __p3)); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmlal_laneq_s32(__p0, __p1, __p2, __p3) __extension__ ({ \
  int64x2_t __s0 = __p0; \
  int32x2_t __s1 = __p1; \
  int32x4_t __s2 = __p2; \
  int64x2_t __ret; \
  __ret = __s0 + vmull_s32(__s1, __builtin_shufflevector(__s2, __s2, __p3, __p3)); \
  __ret; \
})
#else
#define vmlal_laneq_s32(__p0, __p1, __p2, __p3) __extension__ ({ \
  int64x2_t __s0 = __p0; \
  int32x2_t __s1 = __p1; \
  int32x4_t __s2 = __p2; \
  int64x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  int32x2_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 1, 0); \
  int32x4_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 3, 2, 1, 0); \
  int64x2_t __ret; \
  __ret = __rev0 + __noswap_vmull_s32(__rev1, __builtin_shufflevector(__rev2, __rev2, __p3, __p3)); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmlal_laneq_s16(__p0, __p1, __p2, __p3) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  int16x4_t __s1 = __p1; \
  int16x8_t __s2 = __p2; \
  int32x4_t __ret; \
  __ret = __s0 + vmull_s16(__s1, __builtin_shufflevector(__s2, __s2, __p3, __p3, __p3, __p3)); \
  __ret; \
})
#else
#define vmlal_laneq_s16(__p0, __p1, __p2, __p3) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  int16x4_t __s1 = __p1; \
  int16x8_t __s2 = __p2; \
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  int16x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  int16x8_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 7, 6, 5, 4, 3, 2, 1, 0); \
  int32x4_t __ret; \
  __ret = __rev0 + __noswap_vmull_s16(__rev1, __builtin_shufflevector(__rev2, __rev2, __p3, __p3, __p3, __p3)); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x2_t vmlsq_f64(float64x2_t __p0, float64x2_t __p1, float64x2_t __p2) {
  float64x2_t __ret;
  __ret = __p0 - __p1 * __p2;
  return __ret;
}
#else
__ai float64x2_t vmlsq_f64(float64x2_t __p0, float64x2_t __p1, float64x2_t __p2) {
  float64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  float64x2_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 1, 0);
  float64x2_t __ret;
  __ret = __rev0 - __rev1 * __rev2;
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x1_t vmls_f64(float64x1_t __p0, float64x1_t __p1, float64x1_t __p2) {
  float64x1_t __ret;
  __ret = __p0 - __p1 * __p2;
  return __ret;
}
#else
__ai float64x1_t vmls_f64(float64x1_t __p0, float64x1_t __p1, float64x1_t __p2) {
  float64x1_t __ret;
  __ret = __p0 - __p1 * __p2;
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
#define vmlsq_laneq_u32(__p0, __p1, __p2, __p3) __extension__ ({ \
  uint32x4_t __s0 = __p0; \
  uint32x4_t __s1 = __p1; \
  uint32x4_t __s2 = __p2; \
  uint32x4_t __ret; \
  __ret = __s0 - __s1 * __builtin_shufflevector(__s2, __s2, __p3, __p3, __p3, __p3); \
  __ret; \
})
#else
#define vmlsq_laneq_u32(__p0, __p1, __p2, __p3) __extension__ ({ \
  uint32x4_t __s0 = __p0; \
  uint32x4_t __s1 = __p1; \
  uint32x4_t __s2 = __p2; \
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  uint32x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  uint32x4_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 3, 2, 1, 0); \
  uint32x4_t __ret; \
  __ret = __rev0 - __rev1 * __builtin_shufflevector(__rev2, __rev2, __p3, __p3, __p3, __p3); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmlsq_laneq_u16(__p0, __p1, __p2, __p3) __extension__ ({ \
  uint16x8_t __s0 = __p0; \
  uint16x8_t __s1 = __p1; \
  uint16x8_t __s2 = __p2; \
  uint16x8_t __ret; \
  __ret = __s0 - __s1 * __builtin_shufflevector(__s2, __s2, __p3, __p3, __p3, __p3, __p3, __p3, __p3, __p3); \
  __ret; \
})
#else
#define vmlsq_laneq_u16(__p0, __p1, __p2, __p3) __extension__ ({ \
  uint16x8_t __s0 = __p0; \
  uint16x8_t __s1 = __p1; \
  uint16x8_t __s2 = __p2; \
  uint16x8_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint16x8_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint16x8_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint16x8_t __ret; \
  __ret = __rev0 - __rev1 * __builtin_shufflevector(__rev2, __rev2, __p3, __p3, __p3, __p3, __p3, __p3, __p3, __p3); \
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmlsq_laneq_f32(__p0, __p1, __p2, __p3) __extension__ ({ \
  float32x4_t __s0 = __p0; \
  float32x4_t __s1 = __p1; \
  float32x4_t __s2 = __p2; \
  float32x4_t __ret; \
  __ret = __s0 - __s1 * __builtin_shufflevector(__s2, __s2, __p3, __p3, __p3, __p3); \
  __ret; \
})
#else
#define vmlsq_laneq_f32(__p0, __p1, __p2, __p3) __extension__ ({ \
  float32x4_t __s0 = __p0; \
  float32x4_t __s1 = __p1; \
  float32x4_t __s2 = __p2; \
  float32x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  float32x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  float32x4_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 3, 2, 1, 0); \
  float32x4_t __ret; \
  __ret = __rev0 - __rev1 * __builtin_shufflevector(__rev2, __rev2, __p3, __p3, __p3, __p3); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmlsq_laneq_s32(__p0, __p1, __p2, __p3) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  int32x4_t __s1 = __p1; \
  int32x4_t __s2 = __p2; \
  int32x4_t __ret; \
  __ret = __s0 - __s1 * __builtin_shufflevector(__s2, __s2, __p3, __p3, __p3, __p3); \
  __ret; \
})
#else
#define vmlsq_laneq_s32(__p0, __p1, __p2, __p3) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  int32x4_t __s1 = __p1; \
  int32x4_t __s2 = __p2; \
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  int32x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  int32x4_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 3, 2, 1, 0); \
  int32x4_t __ret; \
  __ret = __rev0 - __rev1 * __builtin_shufflevector(__rev2, __rev2, __p3, __p3, __p3, __p3); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmlsq_laneq_s16(__p0, __p1, __p2, __p3) __extension__ ({ \
  int16x8_t __s0 = __p0; \
  int16x8_t __s1 = __p1; \
  int16x8_t __s2 = __p2; \
  int16x8_t __ret; \
  __ret = __s0 - __s1 * __builtin_shufflevector(__s2, __s2, __p3, __p3, __p3, __p3, __p3, __p3, __p3, __p3); \
  __ret; \
})
#else
#define vmlsq_laneq_s16(__p0, __p1, __p2, __p3) __extension__ ({ \
  int16x8_t __s0 = __p0; \
  int16x8_t __s1 = __p1; \
  int16x8_t __s2 = __p2; \
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 7, 6, 5, 4, 3, 2, 1, 0); \
  int16x8_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 7, 6, 5, 4, 3, 2, 1, 0); \
  int16x8_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 7, 6, 5, 4, 3, 2, 1, 0); \
  int16x8_t __ret; \
  __ret = __rev0 - __rev1 * __builtin_shufflevector(__rev2, __rev2, __p3, __p3, __p3, __p3, __p3, __p3, __p3, __p3); \
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmls_laneq_u32(__p0, __p1, __p2, __p3) __extension__ ({ \
  uint32x2_t __s0 = __p0; \
  uint32x2_t __s1 = __p1; \
  uint32x4_t __s2 = __p2; \
  uint32x2_t __ret; \
  __ret = __s0 - __s1 * __builtin_shufflevector(__s2, __s2, __p3, __p3); \
  __ret; \
})
#else
#define vmls_laneq_u32(__p0, __p1, __p2, __p3) __extension__ ({ \
  uint32x2_t __s0 = __p0; \
  uint32x2_t __s1 = __p1; \
  uint32x4_t __s2 = __p2; \
  uint32x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  uint32x2_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 1, 0); \
  uint32x4_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 3, 2, 1, 0); \
  uint32x2_t __ret; \
  __ret = __rev0 - __rev1 * __builtin_shufflevector(__rev2, __rev2, __p3, __p3); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmls_laneq_u16(__p0, __p1, __p2, __p3) __extension__ ({ \
  uint16x4_t __s0 = __p0; \
  uint16x4_t __s1 = __p1; \
  uint16x8_t __s2 = __p2; \
  uint16x4_t __ret; \
  __ret = __s0 - __s1 * __builtin_shufflevector(__s2, __s2, __p3, __p3, __p3, __p3); \
  __ret; \
})
#else
#define vmls_laneq_u16(__p0, __p1, __p2, __p3) __extension__ ({ \
  uint16x4_t __s0 = __p0; \
  uint16x4_t __s1 = __p1; \
  uint16x8_t __s2 = __p2; \
  uint16x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  uint16x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  uint16x8_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint16x4_t __ret; \
  __ret = __rev0 - __rev1 * __builtin_shufflevector(__rev2, __rev2, __p3, __p3, __p3, __p3); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmls_laneq_f32(__p0, __p1, __p2, __p3) __extension__ ({ \
  float32x2_t __s0 = __p0; \
  float32x2_t __s1 = __p1; \
  float32x4_t __s2 = __p2; \
  float32x2_t __ret; \
  __ret = __s0 - __s1 * __builtin_shufflevector(__s2, __s2, __p3, __p3); \
  __ret; \
})
#else
#define vmls_laneq_f32(__p0, __p1, __p2, __p3) __extension__ ({ \
  float32x2_t __s0 = __p0; \
  float32x2_t __s1 = __p1; \
  float32x4_t __s2 = __p2; \
  float32x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  float32x2_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 1, 0); \
  float32x4_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 3, 2, 1, 0); \
  float32x2_t __ret; \
  __ret = __rev0 - __rev1 * __builtin_shufflevector(__rev2, __rev2, __p3, __p3); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmls_laneq_s32(__p0, __p1, __p2, __p3) __extension__ ({ \
  int32x2_t __s0 = __p0; \
  int32x2_t __s1 = __p1; \
  int32x4_t __s2 = __p2; \
  int32x2_t __ret; \
  __ret = __s0 - __s1 * __builtin_shufflevector(__s2, __s2, __p3, __p3); \
  __ret; \
})
#else
#define vmls_laneq_s32(__p0, __p1, __p2, __p3) __extension__ ({ \
  int32x2_t __s0 = __p0; \
  int32x2_t __s1 = __p1; \
  int32x4_t __s2 = __p2; \
  int32x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  int32x2_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 1, 0); \
  int32x4_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 3, 2, 1, 0); \
  int32x2_t __ret; \
  __ret = __rev0 - __rev1 * __builtin_shufflevector(__rev2, __rev2, __p3, __p3); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmls_laneq_s16(__p0, __p1, __p2, __p3) __extension__ ({ \
  int16x4_t __s0 = __p0; \
  int16x4_t __s1 = __p1; \
  int16x8_t __s2 = __p2; \
  int16x4_t __ret; \
  __ret = __s0 - __s1 * __builtin_shufflevector(__s2, __s2, __p3, __p3, __p3, __p3); \
  __ret; \
})
#else
#define vmls_laneq_s16(__p0, __p1, __p2, __p3) __extension__ ({ \
  int16x4_t __s0 = __p0; \
  int16x4_t __s1 = __p1; \
  int16x8_t __s2 = __p2; \
  int16x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  int16x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  int16x8_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 7, 6, 5, 4, 3, 2, 1, 0); \
  int16x4_t __ret; \
  __ret = __rev0 - __rev1 * __builtin_shufflevector(__rev2, __rev2, __p3, __p3, __p3, __p3); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x2_t vmlsq_n_f64(float64x2_t __p0, float64x2_t __p1, float64_t __p2) {
  float64x2_t __ret;
  __ret = __p0 - __p1 * (float64x2_t) {__p2, __p2};
  return __ret;
}
#else
__ai float64x2_t vmlsq_n_f64(float64x2_t __p0, float64x2_t __p1, float64_t __p2) {
  float64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  float64x2_t __ret;
  __ret = __rev0 - __rev1 * (float64x2_t) {__p2, __p2};
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
#define vmlsl_high_lane_u32(__p0, __p1, __p2, __p3) __extension__ ({ \
  uint64x2_t __s0 = __p0; \
  uint32x4_t __s1 = __p1; \
  uint32x2_t __s2 = __p2; \
  uint64x2_t __ret; \
  __ret = __s0 - vmull_u32(vget_high_u32(__s1), __builtin_shufflevector(__s2, __s2, __p3, __p3)); \
  __ret; \
})
#else
#define vmlsl_high_lane_u32(__p0, __p1, __p2, __p3) __extension__ ({ \
  uint64x2_t __s0 = __p0; \
  uint32x4_t __s1 = __p1; \
  uint32x2_t __s2 = __p2; \
  uint64x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  uint32x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  uint32x2_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 1, 0); \
  uint64x2_t __ret; \
  __ret = __rev0 - __noswap_vmull_u32(__noswap_vget_high_u32(__rev1), __builtin_shufflevector(__rev2, __rev2, __p3, __p3)); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmlsl_high_lane_u16(__p0, __p1, __p2, __p3) __extension__ ({ \
  uint32x4_t __s0 = __p0; \
  uint16x8_t __s1 = __p1; \
  uint16x4_t __s2 = __p2; \
  uint32x4_t __ret; \
  __ret = __s0 - vmull_u16(vget_high_u16(__s1), __builtin_shufflevector(__s2, __s2, __p3, __p3, __p3, __p3)); \
  __ret; \
})
#else
#define vmlsl_high_lane_u16(__p0, __p1, __p2, __p3) __extension__ ({ \
  uint32x4_t __s0 = __p0; \
  uint16x8_t __s1 = __p1; \
  uint16x4_t __s2 = __p2; \
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  uint16x8_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint16x4_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 3, 2, 1, 0); \
  uint32x4_t __ret; \
  __ret = __rev0 - __noswap_vmull_u16(__noswap_vget_high_u16(__rev1), __builtin_shufflevector(__rev2, __rev2, __p3, __p3, __p3, __p3)); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmlsl_high_lane_s32(__p0, __p1, __p2, __p3) __extension__ ({ \
  int64x2_t __s0 = __p0; \
  int32x4_t __s1 = __p1; \
  int32x2_t __s2 = __p2; \
  int64x2_t __ret; \
  __ret = __s0 - vmull_s32(vget_high_s32(__s1), __builtin_shufflevector(__s2, __s2, __p3, __p3)); \
  __ret; \
})
#else
#define vmlsl_high_lane_s32(__p0, __p1, __p2, __p3) __extension__ ({ \
  int64x2_t __s0 = __p0; \
  int32x4_t __s1 = __p1; \
  int32x2_t __s2 = __p2; \
  int64x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  int32x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  int32x2_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 1, 0); \
  int64x2_t __ret; \
  __ret = __rev0 - __noswap_vmull_s32(__noswap_vget_high_s32(__rev1), __builtin_shufflevector(__rev2, __rev2, __p3, __p3)); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmlsl_high_lane_s16(__p0, __p1, __p2, __p3) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  int16x8_t __s1 = __p1; \
  int16x4_t __s2 = __p2; \
  int32x4_t __ret; \
  __ret = __s0 - vmull_s16(vget_high_s16(__s1), __builtin_shufflevector(__s2, __s2, __p3, __p3, __p3, __p3)); \
  __ret; \
})
#else
#define vmlsl_high_lane_s16(__p0, __p1, __p2, __p3) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  int16x8_t __s1 = __p1; \
  int16x4_t __s2 = __p2; \
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  int16x8_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 7, 6, 5, 4, 3, 2, 1, 0); \
  int16x4_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 3, 2, 1, 0); \
  int32x4_t __ret; \
  __ret = __rev0 - __noswap_vmull_s16(__noswap_vget_high_s16(__rev1), __builtin_shufflevector(__rev2, __rev2, __p3, __p3, __p3, __p3)); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmlsl_high_laneq_u32(__p0, __p1, __p2, __p3) __extension__ ({ \
  uint64x2_t __s0 = __p0; \
  uint32x4_t __s1 = __p1; \
  uint32x4_t __s2 = __p2; \
  uint64x2_t __ret; \
  __ret = __s0 - vmull_u32(vget_high_u32(__s1), __builtin_shufflevector(__s2, __s2, __p3, __p3)); \
  __ret; \
})
#else
#define vmlsl_high_laneq_u32(__p0, __p1, __p2, __p3) __extension__ ({ \
  uint64x2_t __s0 = __p0; \
  uint32x4_t __s1 = __p1; \
  uint32x4_t __s2 = __p2; \
  uint64x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  uint32x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  uint32x4_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 3, 2, 1, 0); \
  uint64x2_t __ret; \
  __ret = __rev0 - __noswap_vmull_u32(__noswap_vget_high_u32(__rev1), __builtin_shufflevector(__rev2, __rev2, __p3, __p3)); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmlsl_high_laneq_u16(__p0, __p1, __p2, __p3) __extension__ ({ \
  uint32x4_t __s0 = __p0; \
  uint16x8_t __s1 = __p1; \
  uint16x8_t __s2 = __p2; \
  uint32x4_t __ret; \
  __ret = __s0 - vmull_u16(vget_high_u16(__s1), __builtin_shufflevector(__s2, __s2, __p3, __p3, __p3, __p3)); \
  __ret; \
})
#else
#define vmlsl_high_laneq_u16(__p0, __p1, __p2, __p3) __extension__ ({ \
  uint32x4_t __s0 = __p0; \
  uint16x8_t __s1 = __p1; \
  uint16x8_t __s2 = __p2; \
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  uint16x8_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint16x8_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint32x4_t __ret; \
  __ret = __rev0 - __noswap_vmull_u16(__noswap_vget_high_u16(__rev1), __builtin_shufflevector(__rev2, __rev2, __p3, __p3, __p3, __p3)); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmlsl_high_laneq_s32(__p0, __p1, __p2, __p3) __extension__ ({ \
  int64x2_t __s0 = __p0; \
  int32x4_t __s1 = __p1; \
  int32x4_t __s2 = __p2; \
  int64x2_t __ret; \
  __ret = __s0 - vmull_s32(vget_high_s32(__s1), __builtin_shufflevector(__s2, __s2, __p3, __p3)); \
  __ret; \
})
#else
#define vmlsl_high_laneq_s32(__p0, __p1, __p2, __p3) __extension__ ({ \
  int64x2_t __s0 = __p0; \
  int32x4_t __s1 = __p1; \
  int32x4_t __s2 = __p2; \
  int64x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  int32x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  int32x4_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 3, 2, 1, 0); \
  int64x2_t __ret; \
  __ret = __rev0 - __noswap_vmull_s32(__noswap_vget_high_s32(__rev1), __builtin_shufflevector(__rev2, __rev2, __p3, __p3)); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmlsl_high_laneq_s16(__p0, __p1, __p2, __p3) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  int16x8_t __s1 = __p1; \
  int16x8_t __s2 = __p2; \
  int32x4_t __ret; \
  __ret = __s0 - vmull_s16(vget_high_s16(__s1), __builtin_shufflevector(__s2, __s2, __p3, __p3, __p3, __p3)); \
  __ret; \
})
#else
#define vmlsl_high_laneq_s16(__p0, __p1, __p2, __p3) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  int16x8_t __s1 = __p1; \
  int16x8_t __s2 = __p2; \
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  int16x8_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 7, 6, 5, 4, 3, 2, 1, 0); \
  int16x8_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 7, 6, 5, 4, 3, 2, 1, 0); \
  int32x4_t __ret; \
  __ret = __rev0 - __noswap_vmull_s16(__noswap_vget_high_s16(__rev1), __builtin_shufflevector(__rev2, __rev2, __p3, __p3, __p3, __p3)); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmlsl_laneq_u32(__p0, __p1, __p2, __p3) __extension__ ({ \
  uint64x2_t __s0 = __p0; \
  uint32x2_t __s1 = __p1; \
  uint32x4_t __s2 = __p2; \
  uint64x2_t __ret; \
  __ret = __s0 - vmull_u32(__s1, __builtin_shufflevector(__s2, __s2, __p3, __p3)); \
  __ret; \
})
#else
#define vmlsl_laneq_u32(__p0, __p1, __p2, __p3) __extension__ ({ \
  uint64x2_t __s0 = __p0; \
  uint32x2_t __s1 = __p1; \
  uint32x4_t __s2 = __p2; \
  uint64x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  uint32x2_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 1, 0); \
  uint32x4_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 3, 2, 1, 0); \
  uint64x2_t __ret; \
  __ret = __rev0 - __noswap_vmull_u32(__rev1, __builtin_shufflevector(__rev2, __rev2, __p3, __p3)); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmlsl_laneq_u16(__p0, __p1, __p2, __p3) __extension__ ({ \
  uint32x4_t __s0 = __p0; \
  uint16x4_t __s1 = __p1; \
  uint16x8_t __s2 = __p2; \
  uint32x4_t __ret; \
  __ret = __s0 - vmull_u16(__s1, __builtin_shufflevector(__s2, __s2, __p3, __p3, __p3, __p3)); \
  __ret; \
})
#else
#define vmlsl_laneq_u16(__p0, __p1, __p2, __p3) __extension__ ({ \
  uint32x4_t __s0 = __p0; \
  uint16x4_t __s1 = __p1; \
  uint16x8_t __s2 = __p2; \
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  uint16x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  uint16x8_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint32x4_t __ret; \
  __ret = __rev0 - __noswap_vmull_u16(__rev1, __builtin_shufflevector(__rev2, __rev2, __p3, __p3, __p3, __p3)); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmlsl_laneq_s32(__p0, __p1, __p2, __p3) __extension__ ({ \
  int64x2_t __s0 = __p0; \
  int32x2_t __s1 = __p1; \
  int32x4_t __s2 = __p2; \
  int64x2_t __ret; \
  __ret = __s0 - vmull_s32(__s1, __builtin_shufflevector(__s2, __s2, __p3, __p3)); \
  __ret; \
})
#else
#define vmlsl_laneq_s32(__p0, __p1, __p2, __p3) __extension__ ({ \
  int64x2_t __s0 = __p0; \
  int32x2_t __s1 = __p1; \
  int32x4_t __s2 = __p2; \
  int64x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  int32x2_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 1, 0); \
  int32x4_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 3, 2, 1, 0); \
  int64x2_t __ret; \
  __ret = __rev0 - __noswap_vmull_s32(__rev1, __builtin_shufflevector(__rev2, __rev2, __p3, __p3)); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmlsl_laneq_s16(__p0, __p1, __p2, __p3) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  int16x4_t __s1 = __p1; \
  int16x8_t __s2 = __p2; \
  int32x4_t __ret; \
  __ret = __s0 - vmull_s16(__s1, __builtin_shufflevector(__s2, __s2, __p3, __p3, __p3, __p3)); \
  __ret; \
})
#else
#define vmlsl_laneq_s16(__p0, __p1, __p2, __p3) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  int16x4_t __s1 = __p1; \
  int16x8_t __s2 = __p2; \
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  int16x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  int16x8_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 7, 6, 5, 4, 3, 2, 1, 0); \
  int32x4_t __ret; \
  __ret = __rev0 - __noswap_vmull_s16(__rev1, __builtin_shufflevector(__rev2, __rev2, __p3, __p3, __p3, __p3)); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x2_t vmovq_n_f64(float64_t __p0) {
  float64x2_t __ret;
  __ret = (float64x2_t) {__p0, __p0};
  return __ret;
}
#else
__ai float64x2_t vmovq_n_f64(float64_t __p0) {
  float64x2_t __ret;
  __ret = (float64x2_t) {__p0, __p0};
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x1_t vmov_n_f64(float64_t __p0) {
  float64x1_t __ret;
  __ret = (float64x1_t) {__p0};
  return __ret;
}
#else
__ai float64x1_t vmov_n_f64(float64_t __p0) {
  float64x1_t __ret;
  __ret = (float64x1_t) {__p0};
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x8_t vmovl_high_u8(uint8x16_t __p0_116) {
  uint16x8_t __ret_116;
  uint8x8_t __a1_116 = vget_high_u8(__p0_116);
  __ret_116 = (uint16x8_t)(vshll_n_u8(__a1_116, 0));
  return __ret_116;
}
#else
__ai uint16x8_t vmovl_high_u8(uint8x16_t __p0_117) {
  uint8x16_t __rev0_117;  __rev0_117 = __builtin_shufflevector(__p0_117, __p0_117, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __ret_117;
  uint8x8_t __a1_117 = __noswap_vget_high_u8(__rev0_117);
  __ret_117 = (uint16x8_t)(__noswap_vshll_n_u8(__a1_117, 0));
  __ret_117 = __builtin_shufflevector(__ret_117, __ret_117, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret_117;
}
__ai uint16x8_t __noswap_vmovl_high_u8(uint8x16_t __p0_118) {
  uint16x8_t __ret_118;
  uint8x8_t __a1_118 = __noswap_vget_high_u8(__p0_118);
  __ret_118 = (uint16x8_t)(__noswap_vshll_n_u8(__a1_118, 0));
  return __ret_118;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vmovl_high_u32(uint32x4_t __p0_119) {
  uint64x2_t __ret_119;
  uint32x2_t __a1_119 = vget_high_u32(__p0_119);
  __ret_119 = (uint64x2_t)(vshll_n_u32(__a1_119, 0));
  return __ret_119;
}
#else
__ai uint64x2_t vmovl_high_u32(uint32x4_t __p0_120) {
  uint32x4_t __rev0_120;  __rev0_120 = __builtin_shufflevector(__p0_120, __p0_120, 3, 2, 1, 0);
  uint64x2_t __ret_120;
  uint32x2_t __a1_120 = __noswap_vget_high_u32(__rev0_120);
  __ret_120 = (uint64x2_t)(__noswap_vshll_n_u32(__a1_120, 0));
  __ret_120 = __builtin_shufflevector(__ret_120, __ret_120, 1, 0);
  return __ret_120;
}
__ai uint64x2_t __noswap_vmovl_high_u32(uint32x4_t __p0_121) {
  uint64x2_t __ret_121;
  uint32x2_t __a1_121 = __noswap_vget_high_u32(__p0_121);
  __ret_121 = (uint64x2_t)(__noswap_vshll_n_u32(__a1_121, 0));
  return __ret_121;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vmovl_high_u16(uint16x8_t __p0_122) {
  uint32x4_t __ret_122;
  uint16x4_t __a1_122 = vget_high_u16(__p0_122);
  __ret_122 = (uint32x4_t)(vshll_n_u16(__a1_122, 0));
  return __ret_122;
}
#else
__ai uint32x4_t vmovl_high_u16(uint16x8_t __p0_123) {
  uint16x8_t __rev0_123;  __rev0_123 = __builtin_shufflevector(__p0_123, __p0_123, 7, 6, 5, 4, 3, 2, 1, 0);
  uint32x4_t __ret_123;
  uint16x4_t __a1_123 = __noswap_vget_high_u16(__rev0_123);
  __ret_123 = (uint32x4_t)(__noswap_vshll_n_u16(__a1_123, 0));
  __ret_123 = __builtin_shufflevector(__ret_123, __ret_123, 3, 2, 1, 0);
  return __ret_123;
}
__ai uint32x4_t __noswap_vmovl_high_u16(uint16x8_t __p0_124) {
  uint32x4_t __ret_124;
  uint16x4_t __a1_124 = __noswap_vget_high_u16(__p0_124);
  __ret_124 = (uint32x4_t)(__noswap_vshll_n_u16(__a1_124, 0));
  return __ret_124;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x8_t vmovl_high_s8(int8x16_t __p0_125) {
  int16x8_t __ret_125;
  int8x8_t __a1_125 = vget_high_s8(__p0_125);
  __ret_125 = (int16x8_t)(vshll_n_s8(__a1_125, 0));
  return __ret_125;
}
#else
__ai int16x8_t vmovl_high_s8(int8x16_t __p0_126) {
  int8x16_t __rev0_126;  __rev0_126 = __builtin_shufflevector(__p0_126, __p0_126, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __ret_126;
  int8x8_t __a1_126 = __noswap_vget_high_s8(__rev0_126);
  __ret_126 = (int16x8_t)(__noswap_vshll_n_s8(__a1_126, 0));
  __ret_126 = __builtin_shufflevector(__ret_126, __ret_126, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret_126;
}
__ai int16x8_t __noswap_vmovl_high_s8(int8x16_t __p0_127) {
  int16x8_t __ret_127;
  int8x8_t __a1_127 = __noswap_vget_high_s8(__p0_127);
  __ret_127 = (int16x8_t)(__noswap_vshll_n_s8(__a1_127, 0));
  return __ret_127;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x2_t vmovl_high_s32(int32x4_t __p0_128) {
  int64x2_t __ret_128;
  int32x2_t __a1_128 = vget_high_s32(__p0_128);
  __ret_128 = (int64x2_t)(vshll_n_s32(__a1_128, 0));
  return __ret_128;
}
#else
__ai int64x2_t vmovl_high_s32(int32x4_t __p0_129) {
  int32x4_t __rev0_129;  __rev0_129 = __builtin_shufflevector(__p0_129, __p0_129, 3, 2, 1, 0);
  int64x2_t __ret_129;
  int32x2_t __a1_129 = __noswap_vget_high_s32(__rev0_129);
  __ret_129 = (int64x2_t)(__noswap_vshll_n_s32(__a1_129, 0));
  __ret_129 = __builtin_shufflevector(__ret_129, __ret_129, 1, 0);
  return __ret_129;
}
__ai int64x2_t __noswap_vmovl_high_s32(int32x4_t __p0_130) {
  int64x2_t __ret_130;
  int32x2_t __a1_130 = __noswap_vget_high_s32(__p0_130);
  __ret_130 = (int64x2_t)(__noswap_vshll_n_s32(__a1_130, 0));
  return __ret_130;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vmovl_high_s16(int16x8_t __p0_131) {
  int32x4_t __ret_131;
  int16x4_t __a1_131 = vget_high_s16(__p0_131);
  __ret_131 = (int32x4_t)(vshll_n_s16(__a1_131, 0));
  return __ret_131;
}
#else
__ai int32x4_t vmovl_high_s16(int16x8_t __p0_132) {
  int16x8_t __rev0_132;  __rev0_132 = __builtin_shufflevector(__p0_132, __p0_132, 7, 6, 5, 4, 3, 2, 1, 0);
  int32x4_t __ret_132;
  int16x4_t __a1_132 = __noswap_vget_high_s16(__rev0_132);
  __ret_132 = (int32x4_t)(__noswap_vshll_n_s16(__a1_132, 0));
  __ret_132 = __builtin_shufflevector(__ret_132, __ret_132, 3, 2, 1, 0);
  return __ret_132;
}
__ai int32x4_t __noswap_vmovl_high_s16(int16x8_t __p0_133) {
  int32x4_t __ret_133;
  int16x4_t __a1_133 = __noswap_vget_high_s16(__p0_133);
  __ret_133 = (int32x4_t)(__noswap_vshll_n_s16(__a1_133, 0));
  return __ret_133;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x8_t vmovn_high_u32(uint16x4_t __p0, uint32x4_t __p1) {
  uint16x8_t __ret;
  __ret = vcombine_u16(__p0, vmovn_u32(__p1));
  return __ret;
}
#else
__ai uint16x8_t vmovn_high_u32(uint16x4_t __p0, uint32x4_t __p1) {
  uint16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint16x8_t __ret;
  __ret = __noswap_vcombine_u16(__rev0, __noswap_vmovn_u32(__rev1));
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vmovn_high_u64(uint32x2_t __p0, uint64x2_t __p1) {
  uint32x4_t __ret;
  __ret = vcombine_u32(__p0, vmovn_u64(__p1));
  return __ret;
}
#else
__ai uint32x4_t vmovn_high_u64(uint32x2_t __p0, uint64x2_t __p1) {
  uint32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  uint32x4_t __ret;
  __ret = __noswap_vcombine_u32(__rev0, __noswap_vmovn_u64(__rev1));
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x16_t vmovn_high_u16(uint8x8_t __p0, uint16x8_t __p1) {
  uint8x16_t __ret;
  __ret = vcombine_u8(__p0, vmovn_u16(__p1));
  return __ret;
}
#else
__ai uint8x16_t vmovn_high_u16(uint8x8_t __p0, uint16x8_t __p1) {
  uint8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __ret;
  __ret = __noswap_vcombine_u8(__rev0, __noswap_vmovn_u16(__rev1));
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x8_t vmovn_high_s32(int16x4_t __p0, int32x4_t __p1) {
  int16x8_t __ret;
  __ret = vcombine_s16(__p0, vmovn_s32(__p1));
  return __ret;
}
#else
__ai int16x8_t vmovn_high_s32(int16x4_t __p0, int32x4_t __p1) {
  int16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int16x8_t __ret;
  __ret = __noswap_vcombine_s16(__rev0, __noswap_vmovn_s32(__rev1));
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vmovn_high_s64(int32x2_t __p0, int64x2_t __p1) {
  int32x4_t __ret;
  __ret = vcombine_s32(__p0, vmovn_s64(__p1));
  return __ret;
}
#else
__ai int32x4_t vmovn_high_s64(int32x2_t __p0, int64x2_t __p1) {
  int32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  int32x4_t __ret;
  __ret = __noswap_vcombine_s32(__rev0, __noswap_vmovn_s64(__rev1));
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x16_t vmovn_high_s16(int8x8_t __p0, int16x8_t __p1) {
  int8x16_t __ret;
  __ret = vcombine_s8(__p0, vmovn_s16(__p1));
  return __ret;
}
#else
__ai int8x16_t vmovn_high_s16(int8x8_t __p0, int16x8_t __p1) {
  int8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __ret;
  __ret = __noswap_vcombine_s8(__rev0, __noswap_vmovn_s16(__rev1));
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x2_t vmulq_f64(float64x2_t __p0, float64x2_t __p1) {
  float64x2_t __ret;
  __ret = __p0 * __p1;
  return __ret;
}
#else
__ai float64x2_t vmulq_f64(float64x2_t __p0, float64x2_t __p1) {
  float64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  float64x2_t __ret;
  __ret = __rev0 * __rev1;
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x1_t vmul_f64(float64x1_t __p0, float64x1_t __p1) {
  float64x1_t __ret;
  __ret = __p0 * __p1;
  return __ret;
}
#else
__ai float64x1_t vmul_f64(float64x1_t __p0, float64x1_t __p1) {
  float64x1_t __ret;
  __ret = __p0 * __p1;
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
#define vmuld_lane_f64(__p0_134, __p1_134, __p2_134) __extension__ ({ \
  float64_t __s0_134 = __p0_134; \
  float64x1_t __s1_134 = __p1_134; \
  float64_t __ret_134; \
  __ret_134 = __s0_134 * vget_lane_f64(__s1_134, __p2_134); \
  __ret_134; \
})
#else
#define vmuld_lane_f64(__p0_135, __p1_135, __p2_135) __extension__ ({ \
  float64_t __s0_135 = __p0_135; \
  float64x1_t __s1_135 = __p1_135; \
  float64_t __ret_135; \
  __ret_135 = __s0_135 * __noswap_vget_lane_f64(__s1_135, __p2_135); \
  __ret_135; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmuls_lane_f32(__p0_136, __p1_136, __p2_136) __extension__ ({ \
  float32_t __s0_136 = __p0_136; \
  float32x2_t __s1_136 = __p1_136; \
  float32_t __ret_136; \
  __ret_136 = __s0_136 * vget_lane_f32(__s1_136, __p2_136); \
  __ret_136; \
})
#else
#define vmuls_lane_f32(__p0_137, __p1_137, __p2_137) __extension__ ({ \
  float32_t __s0_137 = __p0_137; \
  float32x2_t __s1_137 = __p1_137; \
  float32x2_t __rev1_137;  __rev1_137 = __builtin_shufflevector(__s1_137, __s1_137, 1, 0); \
  float32_t __ret_137; \
  __ret_137 = __s0_137 * __noswap_vget_lane_f32(__rev1_137, __p2_137); \
  __ret_137; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmul_lane_f64(__p0, __p1, __p2) __extension__ ({ \
  float64x1_t __s0 = __p0; \
  float64x1_t __s1 = __p1; \
  float64x1_t __ret; \
  __ret = (float64x1_t) __builtin_neon_vmul_lane_v((int8x8_t)__s0, (int8x8_t)__s1, __p2, 10); \
  __ret; \
})
#else
#define vmul_lane_f64(__p0, __p1, __p2) __extension__ ({ \
  float64x1_t __s0 = __p0; \
  float64x1_t __s1 = __p1; \
  float64x1_t __ret; \
  __ret = (float64x1_t) __builtin_neon_vmul_lane_v((int8x8_t)__s0, (int8x8_t)__s1, __p2, 10); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmulq_lane_f64(__p0, __p1, __p2) __extension__ ({ \
  float64x2_t __s0 = __p0; \
  float64x1_t __s1 = __p1; \
  float64x2_t __ret; \
  __ret = __s0 * __builtin_shufflevector(__s1, __s1, __p2, __p2); \
  __ret; \
})
#else
#define vmulq_lane_f64(__p0, __p1, __p2) __extension__ ({ \
  float64x2_t __s0 = __p0; \
  float64x1_t __s1 = __p1; \
  float64x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  float64x2_t __ret; \
  __ret = __rev0 * __builtin_shufflevector(__s1, __s1, __p2, __p2); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmuld_laneq_f64(__p0_138, __p1_138, __p2_138) __extension__ ({ \
  float64_t __s0_138 = __p0_138; \
  float64x2_t __s1_138 = __p1_138; \
  float64_t __ret_138; \
  __ret_138 = __s0_138 * vgetq_lane_f64(__s1_138, __p2_138); \
  __ret_138; \
})
#else
#define vmuld_laneq_f64(__p0_139, __p1_139, __p2_139) __extension__ ({ \
  float64_t __s0_139 = __p0_139; \
  float64x2_t __s1_139 = __p1_139; \
  float64x2_t __rev1_139;  __rev1_139 = __builtin_shufflevector(__s1_139, __s1_139, 1, 0); \
  float64_t __ret_139; \
  __ret_139 = __s0_139 * __noswap_vgetq_lane_f64(__rev1_139, __p2_139); \
  __ret_139; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmuls_laneq_f32(__p0_140, __p1_140, __p2_140) __extension__ ({ \
  float32_t __s0_140 = __p0_140; \
  float32x4_t __s1_140 = __p1_140; \
  float32_t __ret_140; \
  __ret_140 = __s0_140 * vgetq_lane_f32(__s1_140, __p2_140); \
  __ret_140; \
})
#else
#define vmuls_laneq_f32(__p0_141, __p1_141, __p2_141) __extension__ ({ \
  float32_t __s0_141 = __p0_141; \
  float32x4_t __s1_141 = __p1_141; \
  float32x4_t __rev1_141;  __rev1_141 = __builtin_shufflevector(__s1_141, __s1_141, 3, 2, 1, 0); \
  float32_t __ret_141; \
  __ret_141 = __s0_141 * __noswap_vgetq_lane_f32(__rev1_141, __p2_141); \
  __ret_141; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmul_laneq_f64(__p0, __p1, __p2) __extension__ ({ \
  float64x1_t __s0 = __p0; \
  float64x2_t __s1 = __p1; \
  float64x1_t __ret; \
  __ret = (float64x1_t) __builtin_neon_vmul_laneq_v((int8x8_t)__s0, (int8x16_t)__s1, __p2, 10); \
  __ret; \
})
#else
#define vmul_laneq_f64(__p0, __p1, __p2) __extension__ ({ \
  float64x1_t __s0 = __p0; \
  float64x2_t __s1 = __p1; \
  float64x2_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 1, 0); \
  float64x1_t __ret; \
  __ret = (float64x1_t) __builtin_neon_vmul_laneq_v((int8x8_t)__s0, (int8x16_t)__rev1, __p2, 10); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmulq_laneq_u32(__p0, __p1, __p2) __extension__ ({ \
  uint32x4_t __s0 = __p0; \
  uint32x4_t __s1 = __p1; \
  uint32x4_t __ret; \
  __ret = __s0 * __builtin_shufflevector(__s1, __s1, __p2, __p2, __p2, __p2); \
  __ret; \
})
#else
#define vmulq_laneq_u32(__p0, __p1, __p2) __extension__ ({ \
  uint32x4_t __s0 = __p0; \
  uint32x4_t __s1 = __p1; \
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  uint32x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  uint32x4_t __ret; \
  __ret = __rev0 * __builtin_shufflevector(__rev1, __rev1, __p2, __p2, __p2, __p2); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmulq_laneq_u16(__p0, __p1, __p2) __extension__ ({ \
  uint16x8_t __s0 = __p0; \
  uint16x8_t __s1 = __p1; \
  uint16x8_t __ret; \
  __ret = __s0 * __builtin_shufflevector(__s1, __s1, __p2, __p2, __p2, __p2, __p2, __p2, __p2, __p2); \
  __ret; \
})
#else
#define vmulq_laneq_u16(__p0, __p1, __p2) __extension__ ({ \
  uint16x8_t __s0 = __p0; \
  uint16x8_t __s1 = __p1; \
  uint16x8_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint16x8_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint16x8_t __ret; \
  __ret = __rev0 * __builtin_shufflevector(__rev1, __rev1, __p2, __p2, __p2, __p2, __p2, __p2, __p2, __p2); \
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmulq_laneq_f64(__p0, __p1, __p2) __extension__ ({ \
  float64x2_t __s0 = __p0; \
  float64x2_t __s1 = __p1; \
  float64x2_t __ret; \
  __ret = __s0 * __builtin_shufflevector(__s1, __s1, __p2, __p2); \
  __ret; \
})
#else
#define vmulq_laneq_f64(__p0, __p1, __p2) __extension__ ({ \
  float64x2_t __s0 = __p0; \
  float64x2_t __s1 = __p1; \
  float64x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  float64x2_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 1, 0); \
  float64x2_t __ret; \
  __ret = __rev0 * __builtin_shufflevector(__rev1, __rev1, __p2, __p2); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmulq_laneq_f32(__p0, __p1, __p2) __extension__ ({ \
  float32x4_t __s0 = __p0; \
  float32x4_t __s1 = __p1; \
  float32x4_t __ret; \
  __ret = __s0 * __builtin_shufflevector(__s1, __s1, __p2, __p2, __p2, __p2); \
  __ret; \
})
#else
#define vmulq_laneq_f32(__p0, __p1, __p2) __extension__ ({ \
  float32x4_t __s0 = __p0; \
  float32x4_t __s1 = __p1; \
  float32x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  float32x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  float32x4_t __ret; \
  __ret = __rev0 * __builtin_shufflevector(__rev1, __rev1, __p2, __p2, __p2, __p2); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmulq_laneq_s32(__p0, __p1, __p2) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  int32x4_t __s1 = __p1; \
  int32x4_t __ret; \
  __ret = __s0 * __builtin_shufflevector(__s1, __s1, __p2, __p2, __p2, __p2); \
  __ret; \
})
#else
#define vmulq_laneq_s32(__p0, __p1, __p2) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  int32x4_t __s1 = __p1; \
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  int32x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  int32x4_t __ret; \
  __ret = __rev0 * __builtin_shufflevector(__rev1, __rev1, __p2, __p2, __p2, __p2); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmulq_laneq_s16(__p0, __p1, __p2) __extension__ ({ \
  int16x8_t __s0 = __p0; \
  int16x8_t __s1 = __p1; \
  int16x8_t __ret; \
  __ret = __s0 * __builtin_shufflevector(__s1, __s1, __p2, __p2, __p2, __p2, __p2, __p2, __p2, __p2); \
  __ret; \
})
#else
#define vmulq_laneq_s16(__p0, __p1, __p2) __extension__ ({ \
  int16x8_t __s0 = __p0; \
  int16x8_t __s1 = __p1; \
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 7, 6, 5, 4, 3, 2, 1, 0); \
  int16x8_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 7, 6, 5, 4, 3, 2, 1, 0); \
  int16x8_t __ret; \
  __ret = __rev0 * __builtin_shufflevector(__rev1, __rev1, __p2, __p2, __p2, __p2, __p2, __p2, __p2, __p2); \
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmul_laneq_u32(__p0, __p1, __p2) __extension__ ({ \
  uint32x2_t __s0 = __p0; \
  uint32x4_t __s1 = __p1; \
  uint32x2_t __ret; \
  __ret = __s0 * __builtin_shufflevector(__s1, __s1, __p2, __p2); \
  __ret; \
})
#else
#define vmul_laneq_u32(__p0, __p1, __p2) __extension__ ({ \
  uint32x2_t __s0 = __p0; \
  uint32x4_t __s1 = __p1; \
  uint32x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  uint32x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  uint32x2_t __ret; \
  __ret = __rev0 * __builtin_shufflevector(__rev1, __rev1, __p2, __p2); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmul_laneq_u16(__p0, __p1, __p2) __extension__ ({ \
  uint16x4_t __s0 = __p0; \
  uint16x8_t __s1 = __p1; \
  uint16x4_t __ret; \
  __ret = __s0 * __builtin_shufflevector(__s1, __s1, __p2, __p2, __p2, __p2); \
  __ret; \
})
#else
#define vmul_laneq_u16(__p0, __p1, __p2) __extension__ ({ \
  uint16x4_t __s0 = __p0; \
  uint16x8_t __s1 = __p1; \
  uint16x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  uint16x8_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint16x4_t __ret; \
  __ret = __rev0 * __builtin_shufflevector(__rev1, __rev1, __p2, __p2, __p2, __p2); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmul_laneq_f32(__p0, __p1, __p2) __extension__ ({ \
  float32x2_t __s0 = __p0; \
  float32x4_t __s1 = __p1; \
  float32x2_t __ret; \
  __ret = __s0 * __builtin_shufflevector(__s1, __s1, __p2, __p2); \
  __ret; \
})
#else
#define vmul_laneq_f32(__p0, __p1, __p2) __extension__ ({ \
  float32x2_t __s0 = __p0; \
  float32x4_t __s1 = __p1; \
  float32x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  float32x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  float32x2_t __ret; \
  __ret = __rev0 * __builtin_shufflevector(__rev1, __rev1, __p2, __p2); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmul_laneq_s32(__p0, __p1, __p2) __extension__ ({ \
  int32x2_t __s0 = __p0; \
  int32x4_t __s1 = __p1; \
  int32x2_t __ret; \
  __ret = __s0 * __builtin_shufflevector(__s1, __s1, __p2, __p2); \
  __ret; \
})
#else
#define vmul_laneq_s32(__p0, __p1, __p2) __extension__ ({ \
  int32x2_t __s0 = __p0; \
  int32x4_t __s1 = __p1; \
  int32x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  int32x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  int32x2_t __ret; \
  __ret = __rev0 * __builtin_shufflevector(__rev1, __rev1, __p2, __p2); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmul_laneq_s16(__p0, __p1, __p2) __extension__ ({ \
  int16x4_t __s0 = __p0; \
  int16x8_t __s1 = __p1; \
  int16x4_t __ret; \
  __ret = __s0 * __builtin_shufflevector(__s1, __s1, __p2, __p2, __p2, __p2); \
  __ret; \
})
#else
#define vmul_laneq_s16(__p0, __p1, __p2) __extension__ ({ \
  int16x4_t __s0 = __p0; \
  int16x8_t __s1 = __p1; \
  int16x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  int16x8_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 7, 6, 5, 4, 3, 2, 1, 0); \
  int16x4_t __ret; \
  __ret = __rev0 * __builtin_shufflevector(__rev1, __rev1, __p2, __p2, __p2, __p2); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x1_t vmul_n_f64(float64x1_t __p0, float64_t __p1) {
  float64x1_t __ret;
  __ret = (float64x1_t) __builtin_neon_vmul_n_f64((int8x8_t)__p0, __p1);
  return __ret;
}
#else
__ai float64x1_t vmul_n_f64(float64x1_t __p0, float64_t __p1) {
  float64x1_t __ret;
  __ret = (float64x1_t) __builtin_neon_vmul_n_f64((int8x8_t)__p0, __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x2_t vmulq_n_f64(float64x2_t __p0, float64_t __p1) {
  float64x2_t __ret;
  __ret = __p0 * (float64x2_t) {__p1, __p1};
  return __ret;
}
#else
__ai float64x2_t vmulq_n_f64(float64x2_t __p0, float64_t __p1) {
  float64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float64x2_t __ret;
  __ret = __rev0 * (float64x2_t) {__p1, __p1};
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly128_t vmull_p64(poly64_t __p0, poly64_t __p1) {
  poly128_t __ret;
  __ret = (poly128_t) __builtin_neon_vmull_p64(__p0, __p1);
  return __ret;
}
#else
__ai poly128_t vmull_p64(poly64_t __p0, poly64_t __p1) {
  poly128_t __ret;
  __ret = (poly128_t) __builtin_neon_vmull_p64(__p0, __p1);
  return __ret;
}
__ai poly128_t __noswap_vmull_p64(poly64_t __p0, poly64_t __p1) {
  poly128_t __ret;
  __ret = (poly128_t) __builtin_neon_vmull_p64(__p0, __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly16x8_t vmull_high_p8(poly8x16_t __p0, poly8x16_t __p1) {
  poly16x8_t __ret;
  __ret = vmull_p8(vget_high_p8(__p0), vget_high_p8(__p1));
  return __ret;
}
#else
__ai poly16x8_t vmull_high_p8(poly8x16_t __p0, poly8x16_t __p1) {
  poly8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  poly8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  poly16x8_t __ret;
  __ret = __noswap_vmull_p8(__noswap_vget_high_p8(__rev0), __noswap_vget_high_p8(__rev1));
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x8_t vmull_high_u8(uint8x16_t __p0, uint8x16_t __p1) {
  uint16x8_t __ret;
  __ret = vmull_u8(vget_high_u8(__p0), vget_high_u8(__p1));
  return __ret;
}
#else
__ai uint16x8_t vmull_high_u8(uint8x16_t __p0, uint8x16_t __p1) {
  uint8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __ret;
  __ret = __noswap_vmull_u8(__noswap_vget_high_u8(__rev0), __noswap_vget_high_u8(__rev1));
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vmull_high_u32(uint32x4_t __p0, uint32x4_t __p1) {
  uint64x2_t __ret;
  __ret = vmull_u32(vget_high_u32(__p0), vget_high_u32(__p1));
  return __ret;
}
#else
__ai uint64x2_t vmull_high_u32(uint32x4_t __p0, uint32x4_t __p1) {
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint64x2_t __ret;
  __ret = __noswap_vmull_u32(__noswap_vget_high_u32(__rev0), __noswap_vget_high_u32(__rev1));
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vmull_high_u16(uint16x8_t __p0, uint16x8_t __p1) {
  uint32x4_t __ret;
  __ret = vmull_u16(vget_high_u16(__p0), vget_high_u16(__p1));
  return __ret;
}
#else
__ai uint32x4_t vmull_high_u16(uint16x8_t __p0, uint16x8_t __p1) {
  uint16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint32x4_t __ret;
  __ret = __noswap_vmull_u16(__noswap_vget_high_u16(__rev0), __noswap_vget_high_u16(__rev1));
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x8_t vmull_high_s8(int8x16_t __p0, int8x16_t __p1) {
  int16x8_t __ret;
  __ret = vmull_s8(vget_high_s8(__p0), vget_high_s8(__p1));
  return __ret;
}
#else
__ai int16x8_t vmull_high_s8(int8x16_t __p0, int8x16_t __p1) {
  int8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __ret;
  __ret = __noswap_vmull_s8(__noswap_vget_high_s8(__rev0), __noswap_vget_high_s8(__rev1));
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x2_t vmull_high_s32(int32x4_t __p0, int32x4_t __p1) {
  int64x2_t __ret;
  __ret = vmull_s32(vget_high_s32(__p0), vget_high_s32(__p1));
  return __ret;
}
#else
__ai int64x2_t vmull_high_s32(int32x4_t __p0, int32x4_t __p1) {
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int64x2_t __ret;
  __ret = __noswap_vmull_s32(__noswap_vget_high_s32(__rev0), __noswap_vget_high_s32(__rev1));
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vmull_high_s16(int16x8_t __p0, int16x8_t __p1) {
  int32x4_t __ret;
  __ret = vmull_s16(vget_high_s16(__p0), vget_high_s16(__p1));
  return __ret;
}
#else
__ai int32x4_t vmull_high_s16(int16x8_t __p0, int16x8_t __p1) {
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int32x4_t __ret;
  __ret = __noswap_vmull_s16(__noswap_vget_high_s16(__rev0), __noswap_vget_high_s16(__rev1));
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly128_t vmull_high_p64(poly64x2_t __p0, poly64x2_t __p1) {
  poly128_t __ret;
  __ret = vmull_p64((poly64_t)(vget_high_p64(__p0)), (poly64_t)(vget_high_p64(__p1)));
  return __ret;
}
#else
__ai poly128_t vmull_high_p64(poly64x2_t __p0, poly64x2_t __p1) {
  poly64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  poly64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  poly128_t __ret;
  __ret = __noswap_vmull_p64((poly64_t)(__noswap_vget_high_p64(__rev0)), (poly64_t)(__noswap_vget_high_p64(__rev1)));
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
#define vmull_high_lane_u32(__p0, __p1, __p2) __extension__ ({ \
  uint32x4_t __s0 = __p0; \
  uint32x2_t __s1 = __p1; \
  uint64x2_t __ret; \
  __ret = vmull_u32(vget_high_u32(__s0), __builtin_shufflevector(__s1, __s1, __p2, __p2)); \
  __ret; \
})
#else
#define vmull_high_lane_u32(__p0, __p1, __p2) __extension__ ({ \
  uint32x4_t __s0 = __p0; \
  uint32x2_t __s1 = __p1; \
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  uint32x2_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 1, 0); \
  uint64x2_t __ret; \
  __ret = __noswap_vmull_u32(__noswap_vget_high_u32(__rev0), __builtin_shufflevector(__rev1, __rev1, __p2, __p2)); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmull_high_lane_u16(__p0, __p1, __p2) __extension__ ({ \
  uint16x8_t __s0 = __p0; \
  uint16x4_t __s1 = __p1; \
  uint32x4_t __ret; \
  __ret = vmull_u16(vget_high_u16(__s0), __builtin_shufflevector(__s1, __s1, __p2, __p2, __p2, __p2)); \
  __ret; \
})
#else
#define vmull_high_lane_u16(__p0, __p1, __p2) __extension__ ({ \
  uint16x8_t __s0 = __p0; \
  uint16x4_t __s1 = __p1; \
  uint16x8_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint16x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  uint32x4_t __ret; \
  __ret = __noswap_vmull_u16(__noswap_vget_high_u16(__rev0), __builtin_shufflevector(__rev1, __rev1, __p2, __p2, __p2, __p2)); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmull_high_lane_s32(__p0, __p1, __p2) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  int32x2_t __s1 = __p1; \
  int64x2_t __ret; \
  __ret = vmull_s32(vget_high_s32(__s0), __builtin_shufflevector(__s1, __s1, __p2, __p2)); \
  __ret; \
})
#else
#define vmull_high_lane_s32(__p0, __p1, __p2) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  int32x2_t __s1 = __p1; \
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  int32x2_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 1, 0); \
  int64x2_t __ret; \
  __ret = __noswap_vmull_s32(__noswap_vget_high_s32(__rev0), __builtin_shufflevector(__rev1, __rev1, __p2, __p2)); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmull_high_lane_s16(__p0, __p1, __p2) __extension__ ({ \
  int16x8_t __s0 = __p0; \
  int16x4_t __s1 = __p1; \
  int32x4_t __ret; \
  __ret = vmull_s16(vget_high_s16(__s0), __builtin_shufflevector(__s1, __s1, __p2, __p2, __p2, __p2)); \
  __ret; \
})
#else
#define vmull_high_lane_s16(__p0, __p1, __p2) __extension__ ({ \
  int16x8_t __s0 = __p0; \
  int16x4_t __s1 = __p1; \
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 7, 6, 5, 4, 3, 2, 1, 0); \
  int16x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  int32x4_t __ret; \
  __ret = __noswap_vmull_s16(__noswap_vget_high_s16(__rev0), __builtin_shufflevector(__rev1, __rev1, __p2, __p2, __p2, __p2)); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmull_high_laneq_u32(__p0, __p1, __p2) __extension__ ({ \
  uint32x4_t __s0 = __p0; \
  uint32x4_t __s1 = __p1; \
  uint64x2_t __ret; \
  __ret = vmull_u32(vget_high_u32(__s0), __builtin_shufflevector(__s1, __s1, __p2, __p2)); \
  __ret; \
})
#else
#define vmull_high_laneq_u32(__p0, __p1, __p2) __extension__ ({ \
  uint32x4_t __s0 = __p0; \
  uint32x4_t __s1 = __p1; \
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  uint32x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  uint64x2_t __ret; \
  __ret = __noswap_vmull_u32(__noswap_vget_high_u32(__rev0), __builtin_shufflevector(__rev1, __rev1, __p2, __p2)); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmull_high_laneq_u16(__p0, __p1, __p2) __extension__ ({ \
  uint16x8_t __s0 = __p0; \
  uint16x8_t __s1 = __p1; \
  uint32x4_t __ret; \
  __ret = vmull_u16(vget_high_u16(__s0), __builtin_shufflevector(__s1, __s1, __p2, __p2, __p2, __p2)); \
  __ret; \
})
#else
#define vmull_high_laneq_u16(__p0, __p1, __p2) __extension__ ({ \
  uint16x8_t __s0 = __p0; \
  uint16x8_t __s1 = __p1; \
  uint16x8_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint16x8_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint32x4_t __ret; \
  __ret = __noswap_vmull_u16(__noswap_vget_high_u16(__rev0), __builtin_shufflevector(__rev1, __rev1, __p2, __p2, __p2, __p2)); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmull_high_laneq_s32(__p0, __p1, __p2) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  int32x4_t __s1 = __p1; \
  int64x2_t __ret; \
  __ret = vmull_s32(vget_high_s32(__s0), __builtin_shufflevector(__s1, __s1, __p2, __p2)); \
  __ret; \
})
#else
#define vmull_high_laneq_s32(__p0, __p1, __p2) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  int32x4_t __s1 = __p1; \
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  int32x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  int64x2_t __ret; \
  __ret = __noswap_vmull_s32(__noswap_vget_high_s32(__rev0), __builtin_shufflevector(__rev1, __rev1, __p2, __p2)); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmull_high_laneq_s16(__p0, __p1, __p2) __extension__ ({ \
  int16x8_t __s0 = __p0; \
  int16x8_t __s1 = __p1; \
  int32x4_t __ret; \
  __ret = vmull_s16(vget_high_s16(__s0), __builtin_shufflevector(__s1, __s1, __p2, __p2, __p2, __p2)); \
  __ret; \
})
#else
#define vmull_high_laneq_s16(__p0, __p1, __p2) __extension__ ({ \
  int16x8_t __s0 = __p0; \
  int16x8_t __s1 = __p1; \
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 7, 6, 5, 4, 3, 2, 1, 0); \
  int16x8_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 7, 6, 5, 4, 3, 2, 1, 0); \
  int32x4_t __ret; \
  __ret = __noswap_vmull_s16(__noswap_vget_high_s16(__rev0), __builtin_shufflevector(__rev1, __rev1, __p2, __p2, __p2, __p2)); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vmull_high_n_u32(uint32x4_t __p0, uint32_t __p1) {
  uint64x2_t __ret;
  __ret = vmull_n_u32(vget_high_u32(__p0), __p1);
  return __ret;
}
#else
__ai uint64x2_t vmull_high_n_u32(uint32x4_t __p0, uint32_t __p1) {
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint64x2_t __ret;
  __ret = __noswap_vmull_n_u32(__noswap_vget_high_u32(__rev0), __p1);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vmull_high_n_u16(uint16x8_t __p0, uint16_t __p1) {
  uint32x4_t __ret;
  __ret = vmull_n_u16(vget_high_u16(__p0), __p1);
  return __ret;
}
#else
__ai uint32x4_t vmull_high_n_u16(uint16x8_t __p0, uint16_t __p1) {
  uint16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint32x4_t __ret;
  __ret = __noswap_vmull_n_u16(__noswap_vget_high_u16(__rev0), __p1);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x2_t vmull_high_n_s32(int32x4_t __p0, int32_t __p1) {
  int64x2_t __ret;
  __ret = vmull_n_s32(vget_high_s32(__p0), __p1);
  return __ret;
}
#else
__ai int64x2_t vmull_high_n_s32(int32x4_t __p0, int32_t __p1) {
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int64x2_t __ret;
  __ret = __noswap_vmull_n_s32(__noswap_vget_high_s32(__rev0), __p1);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vmull_high_n_s16(int16x8_t __p0, int16_t __p1) {
  int32x4_t __ret;
  __ret = vmull_n_s16(vget_high_s16(__p0), __p1);
  return __ret;
}
#else
__ai int32x4_t vmull_high_n_s16(int16x8_t __p0, int16_t __p1) {
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int32x4_t __ret;
  __ret = __noswap_vmull_n_s16(__noswap_vget_high_s16(__rev0), __p1);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
#define vmull_laneq_u32(__p0, __p1, __p2) __extension__ ({ \
  uint32x2_t __s0 = __p0; \
  uint32x4_t __s1 = __p1; \
  uint64x2_t __ret; \
  __ret = vmull_u32(__s0, __builtin_shufflevector(__s1, __s1, __p2, __p2)); \
  __ret; \
})
#else
#define vmull_laneq_u32(__p0, __p1, __p2) __extension__ ({ \
  uint32x2_t __s0 = __p0; \
  uint32x4_t __s1 = __p1; \
  uint32x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  uint32x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  uint64x2_t __ret; \
  __ret = __noswap_vmull_u32(__rev0, __builtin_shufflevector(__rev1, __rev1, __p2, __p2)); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmull_laneq_u16(__p0, __p1, __p2) __extension__ ({ \
  uint16x4_t __s0 = __p0; \
  uint16x8_t __s1 = __p1; \
  uint32x4_t __ret; \
  __ret = vmull_u16(__s0, __builtin_shufflevector(__s1, __s1, __p2, __p2, __p2, __p2)); \
  __ret; \
})
#else
#define vmull_laneq_u16(__p0, __p1, __p2) __extension__ ({ \
  uint16x4_t __s0 = __p0; \
  uint16x8_t __s1 = __p1; \
  uint16x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  uint16x8_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint32x4_t __ret; \
  __ret = __noswap_vmull_u16(__rev0, __builtin_shufflevector(__rev1, __rev1, __p2, __p2, __p2, __p2)); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmull_laneq_s32(__p0, __p1, __p2) __extension__ ({ \
  int32x2_t __s0 = __p0; \
  int32x4_t __s1 = __p1; \
  int64x2_t __ret; \
  __ret = vmull_s32(__s0, __builtin_shufflevector(__s1, __s1, __p2, __p2)); \
  __ret; \
})
#else
#define vmull_laneq_s32(__p0, __p1, __p2) __extension__ ({ \
  int32x2_t __s0 = __p0; \
  int32x4_t __s1 = __p1; \
  int32x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  int32x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  int64x2_t __ret; \
  __ret = __noswap_vmull_s32(__rev0, __builtin_shufflevector(__rev1, __rev1, __p2, __p2)); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmull_laneq_s16(__p0, __p1, __p2) __extension__ ({ \
  int16x4_t __s0 = __p0; \
  int16x8_t __s1 = __p1; \
  int32x4_t __ret; \
  __ret = vmull_s16(__s0, __builtin_shufflevector(__s1, __s1, __p2, __p2, __p2, __p2)); \
  __ret; \
})
#else
#define vmull_laneq_s16(__p0, __p1, __p2) __extension__ ({ \
  int16x4_t __s0 = __p0; \
  int16x8_t __s1 = __p1; \
  int16x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  int16x8_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 7, 6, 5, 4, 3, 2, 1, 0); \
  int32x4_t __ret; \
  __ret = __noswap_vmull_s16(__rev0, __builtin_shufflevector(__rev1, __rev1, __p2, __p2, __p2, __p2)); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x2_t vmulxq_f64(float64x2_t __p0, float64x2_t __p1) {
  float64x2_t __ret;
  __ret = (float64x2_t) __builtin_neon_vmulxq_v((int8x16_t)__p0, (int8x16_t)__p1, 42);
  return __ret;
}
#else
__ai float64x2_t vmulxq_f64(float64x2_t __p0, float64x2_t __p1) {
  float64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  float64x2_t __ret;
  __ret = (float64x2_t) __builtin_neon_vmulxq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 42);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
__ai float64x2_t __noswap_vmulxq_f64(float64x2_t __p0, float64x2_t __p1) {
  float64x2_t __ret;
  __ret = (float64x2_t) __builtin_neon_vmulxq_v((int8x16_t)__p0, (int8x16_t)__p1, 42);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x4_t vmulxq_f32(float32x4_t __p0, float32x4_t __p1) {
  float32x4_t __ret;
  __ret = (float32x4_t) __builtin_neon_vmulxq_v((int8x16_t)__p0, (int8x16_t)__p1, 41);
  return __ret;
}
#else
__ai float32x4_t vmulxq_f32(float32x4_t __p0, float32x4_t __p1) {
  float32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  float32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  float32x4_t __ret;
  __ret = (float32x4_t) __builtin_neon_vmulxq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 41);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
__ai float32x4_t __noswap_vmulxq_f32(float32x4_t __p0, float32x4_t __p1) {
  float32x4_t __ret;
  __ret = (float32x4_t) __builtin_neon_vmulxq_v((int8x16_t)__p0, (int8x16_t)__p1, 41);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x1_t vmulx_f64(float64x1_t __p0, float64x1_t __p1) {
  float64x1_t __ret;
  __ret = (float64x1_t) __builtin_neon_vmulx_v((int8x8_t)__p0, (int8x8_t)__p1, 10);
  return __ret;
}
#else
__ai float64x1_t vmulx_f64(float64x1_t __p0, float64x1_t __p1) {
  float64x1_t __ret;
  __ret = (float64x1_t) __builtin_neon_vmulx_v((int8x8_t)__p0, (int8x8_t)__p1, 10);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x2_t vmulx_f32(float32x2_t __p0, float32x2_t __p1) {
  float32x2_t __ret;
  __ret = (float32x2_t) __builtin_neon_vmulx_v((int8x8_t)__p0, (int8x8_t)__p1, 9);
  return __ret;
}
#else
__ai float32x2_t vmulx_f32(float32x2_t __p0, float32x2_t __p1) {
  float32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  float32x2_t __ret;
  __ret = (float32x2_t) __builtin_neon_vmulx_v((int8x8_t)__rev0, (int8x8_t)__rev1, 9);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
__ai float32x2_t __noswap_vmulx_f32(float32x2_t __p0, float32x2_t __p1) {
  float32x2_t __ret;
  __ret = (float32x2_t) __builtin_neon_vmulx_v((int8x8_t)__p0, (int8x8_t)__p1, 9);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64_t vmulxd_f64(float64_t __p0, float64_t __p1) {
  float64_t __ret;
  __ret = (float64_t) __builtin_neon_vmulxd_f64(__p0, __p1);
  return __ret;
}
#else
__ai float64_t vmulxd_f64(float64_t __p0, float64_t __p1) {
  float64_t __ret;
  __ret = (float64_t) __builtin_neon_vmulxd_f64(__p0, __p1);
  return __ret;
}
__ai float64_t __noswap_vmulxd_f64(float64_t __p0, float64_t __p1) {
  float64_t __ret;
  __ret = (float64_t) __builtin_neon_vmulxd_f64(__p0, __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32_t vmulxs_f32(float32_t __p0, float32_t __p1) {
  float32_t __ret;
  __ret = (float32_t) __builtin_neon_vmulxs_f32(__p0, __p1);
  return __ret;
}
#else
__ai float32_t vmulxs_f32(float32_t __p0, float32_t __p1) {
  float32_t __ret;
  __ret = (float32_t) __builtin_neon_vmulxs_f32(__p0, __p1);
  return __ret;
}
__ai float32_t __noswap_vmulxs_f32(float32_t __p0, float32_t __p1) {
  float32_t __ret;
  __ret = (float32_t) __builtin_neon_vmulxs_f32(__p0, __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
#define vmulxd_lane_f64(__p0_142, __p1_142, __p2_142) __extension__ ({ \
  float64_t __s0_142 = __p0_142; \
  float64x1_t __s1_142 = __p1_142; \
  float64_t __ret_142; \
  __ret_142 = vmulxd_f64(__s0_142, vget_lane_f64(__s1_142, __p2_142)); \
  __ret_142; \
})
#else
#define vmulxd_lane_f64(__p0_143, __p1_143, __p2_143) __extension__ ({ \
  float64_t __s0_143 = __p0_143; \
  float64x1_t __s1_143 = __p1_143; \
  float64_t __ret_143; \
  __ret_143 = __noswap_vmulxd_f64(__s0_143, __noswap_vget_lane_f64(__s1_143, __p2_143)); \
  __ret_143; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmulxs_lane_f32(__p0_144, __p1_144, __p2_144) __extension__ ({ \
  float32_t __s0_144 = __p0_144; \
  float32x2_t __s1_144 = __p1_144; \
  float32_t __ret_144; \
  __ret_144 = vmulxs_f32(__s0_144, vget_lane_f32(__s1_144, __p2_144)); \
  __ret_144; \
})
#else
#define vmulxs_lane_f32(__p0_145, __p1_145, __p2_145) __extension__ ({ \
  float32_t __s0_145 = __p0_145; \
  float32x2_t __s1_145 = __p1_145; \
  float32x2_t __rev1_145;  __rev1_145 = __builtin_shufflevector(__s1_145, __s1_145, 1, 0); \
  float32_t __ret_145; \
  __ret_145 = __noswap_vmulxs_f32(__s0_145, __noswap_vget_lane_f32(__rev1_145, __p2_145)); \
  __ret_145; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmulxq_lane_f64(__p0, __p1, __p2) __extension__ ({ \
  float64x2_t __s0 = __p0; \
  float64x1_t __s1 = __p1; \
  float64x2_t __ret; \
  __ret = vmulxq_f64(__s0, __builtin_shufflevector(__s1, __s1, __p2, __p2)); \
  __ret; \
})
#else
#define vmulxq_lane_f64(__p0, __p1, __p2) __extension__ ({ \
  float64x2_t __s0 = __p0; \
  float64x1_t __s1 = __p1; \
  float64x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  float64x2_t __ret; \
  __ret = __noswap_vmulxq_f64(__rev0, __builtin_shufflevector(__s1, __s1, __p2, __p2)); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmulxq_lane_f32(__p0, __p1, __p2) __extension__ ({ \
  float32x4_t __s0 = __p0; \
  float32x2_t __s1 = __p1; \
  float32x4_t __ret; \
  __ret = vmulxq_f32(__s0, __builtin_shufflevector(__s1, __s1, __p2, __p2, __p2, __p2)); \
  __ret; \
})
#else
#define vmulxq_lane_f32(__p0, __p1, __p2) __extension__ ({ \
  float32x4_t __s0 = __p0; \
  float32x2_t __s1 = __p1; \
  float32x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  float32x2_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 1, 0); \
  float32x4_t __ret; \
  __ret = __noswap_vmulxq_f32(__rev0, __builtin_shufflevector(__rev1, __rev1, __p2, __p2, __p2, __p2)); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmulx_lane_f32(__p0, __p1, __p2) __extension__ ({ \
  float32x2_t __s0 = __p0; \
  float32x2_t __s1 = __p1; \
  float32x2_t __ret; \
  __ret = vmulx_f32(__s0, __builtin_shufflevector(__s1, __s1, __p2, __p2)); \
  __ret; \
})
#else
#define vmulx_lane_f32(__p0, __p1, __p2) __extension__ ({ \
  float32x2_t __s0 = __p0; \
  float32x2_t __s1 = __p1; \
  float32x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  float32x2_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 1, 0); \
  float32x2_t __ret; \
  __ret = __noswap_vmulx_f32(__rev0, __builtin_shufflevector(__rev1, __rev1, __p2, __p2)); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmulxd_laneq_f64(__p0_146, __p1_146, __p2_146) __extension__ ({ \
  float64_t __s0_146 = __p0_146; \
  float64x2_t __s1_146 = __p1_146; \
  float64_t __ret_146; \
  __ret_146 = vmulxd_f64(__s0_146, vgetq_lane_f64(__s1_146, __p2_146)); \
  __ret_146; \
})
#else
#define vmulxd_laneq_f64(__p0_147, __p1_147, __p2_147) __extension__ ({ \
  float64_t __s0_147 = __p0_147; \
  float64x2_t __s1_147 = __p1_147; \
  float64x2_t __rev1_147;  __rev1_147 = __builtin_shufflevector(__s1_147, __s1_147, 1, 0); \
  float64_t __ret_147; \
  __ret_147 = __noswap_vmulxd_f64(__s0_147, __noswap_vgetq_lane_f64(__rev1_147, __p2_147)); \
  __ret_147; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmulxs_laneq_f32(__p0_148, __p1_148, __p2_148) __extension__ ({ \
  float32_t __s0_148 = __p0_148; \
  float32x4_t __s1_148 = __p1_148; \
  float32_t __ret_148; \
  __ret_148 = vmulxs_f32(__s0_148, vgetq_lane_f32(__s1_148, __p2_148)); \
  __ret_148; \
})
#else
#define vmulxs_laneq_f32(__p0_149, __p1_149, __p2_149) __extension__ ({ \
  float32_t __s0_149 = __p0_149; \
  float32x4_t __s1_149 = __p1_149; \
  float32x4_t __rev1_149;  __rev1_149 = __builtin_shufflevector(__s1_149, __s1_149, 3, 2, 1, 0); \
  float32_t __ret_149; \
  __ret_149 = __noswap_vmulxs_f32(__s0_149, __noswap_vgetq_lane_f32(__rev1_149, __p2_149)); \
  __ret_149; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmulxq_laneq_f64(__p0, __p1, __p2) __extension__ ({ \
  float64x2_t __s0 = __p0; \
  float64x2_t __s1 = __p1; \
  float64x2_t __ret; \
  __ret = vmulxq_f64(__s0, __builtin_shufflevector(__s1, __s1, __p2, __p2)); \
  __ret; \
})
#else
#define vmulxq_laneq_f64(__p0, __p1, __p2) __extension__ ({ \
  float64x2_t __s0 = __p0; \
  float64x2_t __s1 = __p1; \
  float64x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  float64x2_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 1, 0); \
  float64x2_t __ret; \
  __ret = __noswap_vmulxq_f64(__rev0, __builtin_shufflevector(__rev1, __rev1, __p2, __p2)); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmulxq_laneq_f32(__p0, __p1, __p2) __extension__ ({ \
  float32x4_t __s0 = __p0; \
  float32x4_t __s1 = __p1; \
  float32x4_t __ret; \
  __ret = vmulxq_f32(__s0, __builtin_shufflevector(__s1, __s1, __p2, __p2, __p2, __p2)); \
  __ret; \
})
#else
#define vmulxq_laneq_f32(__p0, __p1, __p2) __extension__ ({ \
  float32x4_t __s0 = __p0; \
  float32x4_t __s1 = __p1; \
  float32x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  float32x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  float32x4_t __ret; \
  __ret = __noswap_vmulxq_f32(__rev0, __builtin_shufflevector(__rev1, __rev1, __p2, __p2, __p2, __p2)); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmulx_laneq_f32(__p0, __p1, __p2) __extension__ ({ \
  float32x2_t __s0 = __p0; \
  float32x4_t __s1 = __p1; \
  float32x2_t __ret; \
  __ret = vmulx_f32(__s0, __builtin_shufflevector(__s1, __s1, __p2, __p2)); \
  __ret; \
})
#else
#define vmulx_laneq_f32(__p0, __p1, __p2) __extension__ ({ \
  float32x2_t __s0 = __p0; \
  float32x4_t __s1 = __p1; \
  float32x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  float32x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  float32x2_t __ret; \
  __ret = __noswap_vmulx_f32(__rev0, __builtin_shufflevector(__rev1, __rev1, __p2, __p2)); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x2_t vnegq_f64(float64x2_t __p0) {
  float64x2_t __ret;
  __ret = -__p0;
  return __ret;
}
#else
__ai float64x2_t vnegq_f64(float64x2_t __p0) {
  float64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float64x2_t __ret;
  __ret = -__rev0;
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x2_t vnegq_s64(int64x2_t __p0) {
  int64x2_t __ret;
  __ret = -__p0;
  return __ret;
}
#else
__ai int64x2_t vnegq_s64(int64x2_t __p0) {
  int64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int64x2_t __ret;
  __ret = -__rev0;
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x1_t vneg_f64(float64x1_t __p0) {
  float64x1_t __ret;
  __ret = -__p0;
  return __ret;
}
#else
__ai float64x1_t vneg_f64(float64x1_t __p0) {
  float64x1_t __ret;
  __ret = -__p0;
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x1_t vneg_s64(int64x1_t __p0) {
  int64x1_t __ret;
  __ret = -__p0;
  return __ret;
}
#else
__ai int64x1_t vneg_s64(int64x1_t __p0) {
  int64x1_t __ret;
  __ret = -__p0;
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64_t vnegd_s64(int64_t __p0) {
  int64_t __ret;
  __ret = (int64_t) __builtin_neon_vnegd_s64(__p0);
  return __ret;
}
#else
__ai int64_t vnegd_s64(int64_t __p0) {
  int64_t __ret;
  __ret = (int64_t) __builtin_neon_vnegd_s64(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x16_t vpaddq_u8(uint8x16_t __p0, uint8x16_t __p1) {
  uint8x16_t __ret;
  __ret = (uint8x16_t) __builtin_neon_vpaddq_v((int8x16_t)__p0, (int8x16_t)__p1, 48);
  return __ret;
}
#else
__ai uint8x16_t vpaddq_u8(uint8x16_t __p0, uint8x16_t __p1) {
  uint8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __ret;
  __ret = (uint8x16_t) __builtin_neon_vpaddq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 48);
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vpaddq_u32(uint32x4_t __p0, uint32x4_t __p1) {
  uint32x4_t __ret;
  __ret = (uint32x4_t) __builtin_neon_vpaddq_v((int8x16_t)__p0, (int8x16_t)__p1, 50);
  return __ret;
}
#else
__ai uint32x4_t vpaddq_u32(uint32x4_t __p0, uint32x4_t __p1) {
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint32x4_t __ret;
  __ret = (uint32x4_t) __builtin_neon_vpaddq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 50);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vpaddq_u64(uint64x2_t __p0, uint64x2_t __p1) {
  uint64x2_t __ret;
  __ret = (uint64x2_t) __builtin_neon_vpaddq_v((int8x16_t)__p0, (int8x16_t)__p1, 51);
  return __ret;
}
#else
__ai uint64x2_t vpaddq_u64(uint64x2_t __p0, uint64x2_t __p1) {
  uint64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  uint64x2_t __ret;
  __ret = (uint64x2_t) __builtin_neon_vpaddq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 51);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x8_t vpaddq_u16(uint16x8_t __p0, uint16x8_t __p1) {
  uint16x8_t __ret;
  __ret = (uint16x8_t) __builtin_neon_vpaddq_v((int8x16_t)__p0, (int8x16_t)__p1, 49);
  return __ret;
}
#else
__ai uint16x8_t vpaddq_u16(uint16x8_t __p0, uint16x8_t __p1) {
  uint16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __ret;
  __ret = (uint16x8_t) __builtin_neon_vpaddq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 49);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x16_t vpaddq_s8(int8x16_t __p0, int8x16_t __p1) {
  int8x16_t __ret;
  __ret = (int8x16_t) __builtin_neon_vpaddq_v((int8x16_t)__p0, (int8x16_t)__p1, 32);
  return __ret;
}
#else
__ai int8x16_t vpaddq_s8(int8x16_t __p0, int8x16_t __p1) {
  int8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __ret;
  __ret = (int8x16_t) __builtin_neon_vpaddq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 32);
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x2_t vpaddq_f64(float64x2_t __p0, float64x2_t __p1) {
  float64x2_t __ret;
  __ret = (float64x2_t) __builtin_neon_vpaddq_v((int8x16_t)__p0, (int8x16_t)__p1, 42);
  return __ret;
}
#else
__ai float64x2_t vpaddq_f64(float64x2_t __p0, float64x2_t __p1) {
  float64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  float64x2_t __ret;
  __ret = (float64x2_t) __builtin_neon_vpaddq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 42);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x4_t vpaddq_f32(float32x4_t __p0, float32x4_t __p1) {
  float32x4_t __ret;
  __ret = (float32x4_t) __builtin_neon_vpaddq_v((int8x16_t)__p0, (int8x16_t)__p1, 41);
  return __ret;
}
#else
__ai float32x4_t vpaddq_f32(float32x4_t __p0, float32x4_t __p1) {
  float32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  float32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  float32x4_t __ret;
  __ret = (float32x4_t) __builtin_neon_vpaddq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 41);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vpaddq_s32(int32x4_t __p0, int32x4_t __p1) {
  int32x4_t __ret;
  __ret = (int32x4_t) __builtin_neon_vpaddq_v((int8x16_t)__p0, (int8x16_t)__p1, 34);
  return __ret;
}
#else
__ai int32x4_t vpaddq_s32(int32x4_t __p0, int32x4_t __p1) {
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int32x4_t __ret;
  __ret = (int32x4_t) __builtin_neon_vpaddq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 34);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x2_t vpaddq_s64(int64x2_t __p0, int64x2_t __p1) {
  int64x2_t __ret;
  __ret = (int64x2_t) __builtin_neon_vpaddq_v((int8x16_t)__p0, (int8x16_t)__p1, 35);
  return __ret;
}
#else
__ai int64x2_t vpaddq_s64(int64x2_t __p0, int64x2_t __p1) {
  int64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  int64x2_t __ret;
  __ret = (int64x2_t) __builtin_neon_vpaddq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 35);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x8_t vpaddq_s16(int16x8_t __p0, int16x8_t __p1) {
  int16x8_t __ret;
  __ret = (int16x8_t) __builtin_neon_vpaddq_v((int8x16_t)__p0, (int8x16_t)__p1, 33);
  return __ret;
}
#else
__ai int16x8_t vpaddq_s16(int16x8_t __p0, int16x8_t __p1) {
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __ret;
  __ret = (int16x8_t) __builtin_neon_vpaddq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 33);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64_t vpaddd_u64(uint64x2_t __p0) {
  uint64_t __ret;
  __ret = (uint64_t) __builtin_neon_vpaddd_u64((int8x16_t)__p0);
  return __ret;
}
#else
__ai uint64_t vpaddd_u64(uint64x2_t __p0) {
  uint64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint64_t __ret;
  __ret = (uint64_t) __builtin_neon_vpaddd_u64((int8x16_t)__rev0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64_t vpaddd_f64(float64x2_t __p0) {
  float64_t __ret;
  __ret = (float64_t) __builtin_neon_vpaddd_f64((int8x16_t)__p0);
  return __ret;
}
#else
__ai float64_t vpaddd_f64(float64x2_t __p0) {
  float64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float64_t __ret;
  __ret = (float64_t) __builtin_neon_vpaddd_f64((int8x16_t)__rev0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64_t vpaddd_s64(int64x2_t __p0) {
  int64_t __ret;
  __ret = (int64_t) __builtin_neon_vpaddd_s64((int8x16_t)__p0);
  return __ret;
}
#else
__ai int64_t vpaddd_s64(int64x2_t __p0) {
  int64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int64_t __ret;
  __ret = (int64_t) __builtin_neon_vpaddd_s64((int8x16_t)__rev0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32_t vpadds_f32(float32x2_t __p0) {
  float32_t __ret;
  __ret = (float32_t) __builtin_neon_vpadds_f32((int8x8_t)__p0);
  return __ret;
}
#else
__ai float32_t vpadds_f32(float32x2_t __p0) {
  float32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float32_t __ret;
  __ret = (float32_t) __builtin_neon_vpadds_f32((int8x8_t)__rev0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x16_t vpmaxq_u8(uint8x16_t __p0, uint8x16_t __p1) {
  uint8x16_t __ret;
  __ret = (uint8x16_t) __builtin_neon_vpmaxq_v((int8x16_t)__p0, (int8x16_t)__p1, 48);
  return __ret;
}
#else
__ai uint8x16_t vpmaxq_u8(uint8x16_t __p0, uint8x16_t __p1) {
  uint8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __ret;
  __ret = (uint8x16_t) __builtin_neon_vpmaxq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 48);
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vpmaxq_u32(uint32x4_t __p0, uint32x4_t __p1) {
  uint32x4_t __ret;
  __ret = (uint32x4_t) __builtin_neon_vpmaxq_v((int8x16_t)__p0, (int8x16_t)__p1, 50);
  return __ret;
}
#else
__ai uint32x4_t vpmaxq_u32(uint32x4_t __p0, uint32x4_t __p1) {
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint32x4_t __ret;
  __ret = (uint32x4_t) __builtin_neon_vpmaxq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 50);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x8_t vpmaxq_u16(uint16x8_t __p0, uint16x8_t __p1) {
  uint16x8_t __ret;
  __ret = (uint16x8_t) __builtin_neon_vpmaxq_v((int8x16_t)__p0, (int8x16_t)__p1, 49);
  return __ret;
}
#else
__ai uint16x8_t vpmaxq_u16(uint16x8_t __p0, uint16x8_t __p1) {
  uint16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __ret;
  __ret = (uint16x8_t) __builtin_neon_vpmaxq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 49);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x16_t vpmaxq_s8(int8x16_t __p0, int8x16_t __p1) {
  int8x16_t __ret;
  __ret = (int8x16_t) __builtin_neon_vpmaxq_v((int8x16_t)__p0, (int8x16_t)__p1, 32);
  return __ret;
}
#else
__ai int8x16_t vpmaxq_s8(int8x16_t __p0, int8x16_t __p1) {
  int8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __ret;
  __ret = (int8x16_t) __builtin_neon_vpmaxq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 32);
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x2_t vpmaxq_f64(float64x2_t __p0, float64x2_t __p1) {
  float64x2_t __ret;
  __ret = (float64x2_t) __builtin_neon_vpmaxq_v((int8x16_t)__p0, (int8x16_t)__p1, 42);
  return __ret;
}
#else
__ai float64x2_t vpmaxq_f64(float64x2_t __p0, float64x2_t __p1) {
  float64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  float64x2_t __ret;
  __ret = (float64x2_t) __builtin_neon_vpmaxq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 42);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x4_t vpmaxq_f32(float32x4_t __p0, float32x4_t __p1) {
  float32x4_t __ret;
  __ret = (float32x4_t) __builtin_neon_vpmaxq_v((int8x16_t)__p0, (int8x16_t)__p1, 41);
  return __ret;
}
#else
__ai float32x4_t vpmaxq_f32(float32x4_t __p0, float32x4_t __p1) {
  float32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  float32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  float32x4_t __ret;
  __ret = (float32x4_t) __builtin_neon_vpmaxq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 41);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vpmaxq_s32(int32x4_t __p0, int32x4_t __p1) {
  int32x4_t __ret;
  __ret = (int32x4_t) __builtin_neon_vpmaxq_v((int8x16_t)__p0, (int8x16_t)__p1, 34);
  return __ret;
}
#else
__ai int32x4_t vpmaxq_s32(int32x4_t __p0, int32x4_t __p1) {
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int32x4_t __ret;
  __ret = (int32x4_t) __builtin_neon_vpmaxq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 34);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x8_t vpmaxq_s16(int16x8_t __p0, int16x8_t __p1) {
  int16x8_t __ret;
  __ret = (int16x8_t) __builtin_neon_vpmaxq_v((int8x16_t)__p0, (int8x16_t)__p1, 33);
  return __ret;
}
#else
__ai int16x8_t vpmaxq_s16(int16x8_t __p0, int16x8_t __p1) {
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __ret;
  __ret = (int16x8_t) __builtin_neon_vpmaxq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 33);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64_t vpmaxqd_f64(float64x2_t __p0) {
  float64_t __ret;
  __ret = (float64_t) __builtin_neon_vpmaxqd_f64((int8x16_t)__p0);
  return __ret;
}
#else
__ai float64_t vpmaxqd_f64(float64x2_t __p0) {
  float64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float64_t __ret;
  __ret = (float64_t) __builtin_neon_vpmaxqd_f64((int8x16_t)__rev0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32_t vpmaxs_f32(float32x2_t __p0) {
  float32_t __ret;
  __ret = (float32_t) __builtin_neon_vpmaxs_f32((int8x8_t)__p0);
  return __ret;
}
#else
__ai float32_t vpmaxs_f32(float32x2_t __p0) {
  float32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float32_t __ret;
  __ret = (float32_t) __builtin_neon_vpmaxs_f32((int8x8_t)__rev0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x2_t vpmaxnmq_f64(float64x2_t __p0, float64x2_t __p1) {
  float64x2_t __ret;
  __ret = (float64x2_t) __builtin_neon_vpmaxnmq_v((int8x16_t)__p0, (int8x16_t)__p1, 42);
  return __ret;
}
#else
__ai float64x2_t vpmaxnmq_f64(float64x2_t __p0, float64x2_t __p1) {
  float64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  float64x2_t __ret;
  __ret = (float64x2_t) __builtin_neon_vpmaxnmq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 42);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x4_t vpmaxnmq_f32(float32x4_t __p0, float32x4_t __p1) {
  float32x4_t __ret;
  __ret = (float32x4_t) __builtin_neon_vpmaxnmq_v((int8x16_t)__p0, (int8x16_t)__p1, 41);
  return __ret;
}
#else
__ai float32x4_t vpmaxnmq_f32(float32x4_t __p0, float32x4_t __p1) {
  float32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  float32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  float32x4_t __ret;
  __ret = (float32x4_t) __builtin_neon_vpmaxnmq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 41);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x2_t vpmaxnm_f32(float32x2_t __p0, float32x2_t __p1) {
  float32x2_t __ret;
  __ret = (float32x2_t) __builtin_neon_vpmaxnm_v((int8x8_t)__p0, (int8x8_t)__p1, 9);
  return __ret;
}
#else
__ai float32x2_t vpmaxnm_f32(float32x2_t __p0, float32x2_t __p1) {
  float32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  float32x2_t __ret;
  __ret = (float32x2_t) __builtin_neon_vpmaxnm_v((int8x8_t)__rev0, (int8x8_t)__rev1, 9);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64_t vpmaxnmqd_f64(float64x2_t __p0) {
  float64_t __ret;
  __ret = (float64_t) __builtin_neon_vpmaxnmqd_f64((int8x16_t)__p0);
  return __ret;
}
#else
__ai float64_t vpmaxnmqd_f64(float64x2_t __p0) {
  float64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float64_t __ret;
  __ret = (float64_t) __builtin_neon_vpmaxnmqd_f64((int8x16_t)__rev0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32_t vpmaxnms_f32(float32x2_t __p0) {
  float32_t __ret;
  __ret = (float32_t) __builtin_neon_vpmaxnms_f32((int8x8_t)__p0);
  return __ret;
}
#else
__ai float32_t vpmaxnms_f32(float32x2_t __p0) {
  float32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float32_t __ret;
  __ret = (float32_t) __builtin_neon_vpmaxnms_f32((int8x8_t)__rev0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x16_t vpminq_u8(uint8x16_t __p0, uint8x16_t __p1) {
  uint8x16_t __ret;
  __ret = (uint8x16_t) __builtin_neon_vpminq_v((int8x16_t)__p0, (int8x16_t)__p1, 48);
  return __ret;
}
#else
__ai uint8x16_t vpminq_u8(uint8x16_t __p0, uint8x16_t __p1) {
  uint8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __ret;
  __ret = (uint8x16_t) __builtin_neon_vpminq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 48);
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vpminq_u32(uint32x4_t __p0, uint32x4_t __p1) {
  uint32x4_t __ret;
  __ret = (uint32x4_t) __builtin_neon_vpminq_v((int8x16_t)__p0, (int8x16_t)__p1, 50);
  return __ret;
}
#else
__ai uint32x4_t vpminq_u32(uint32x4_t __p0, uint32x4_t __p1) {
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint32x4_t __ret;
  __ret = (uint32x4_t) __builtin_neon_vpminq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 50);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x8_t vpminq_u16(uint16x8_t __p0, uint16x8_t __p1) {
  uint16x8_t __ret;
  __ret = (uint16x8_t) __builtin_neon_vpminq_v((int8x16_t)__p0, (int8x16_t)__p1, 49);
  return __ret;
}
#else
__ai uint16x8_t vpminq_u16(uint16x8_t __p0, uint16x8_t __p1) {
  uint16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __ret;
  __ret = (uint16x8_t) __builtin_neon_vpminq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 49);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x16_t vpminq_s8(int8x16_t __p0, int8x16_t __p1) {
  int8x16_t __ret;
  __ret = (int8x16_t) __builtin_neon_vpminq_v((int8x16_t)__p0, (int8x16_t)__p1, 32);
  return __ret;
}
#else
__ai int8x16_t vpminq_s8(int8x16_t __p0, int8x16_t __p1) {
  int8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __ret;
  __ret = (int8x16_t) __builtin_neon_vpminq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 32);
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x2_t vpminq_f64(float64x2_t __p0, float64x2_t __p1) {
  float64x2_t __ret;
  __ret = (float64x2_t) __builtin_neon_vpminq_v((int8x16_t)__p0, (int8x16_t)__p1, 42);
  return __ret;
}
#else
__ai float64x2_t vpminq_f64(float64x2_t __p0, float64x2_t __p1) {
  float64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  float64x2_t __ret;
  __ret = (float64x2_t) __builtin_neon_vpminq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 42);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x4_t vpminq_f32(float32x4_t __p0, float32x4_t __p1) {
  float32x4_t __ret;
  __ret = (float32x4_t) __builtin_neon_vpminq_v((int8x16_t)__p0, (int8x16_t)__p1, 41);
  return __ret;
}
#else
__ai float32x4_t vpminq_f32(float32x4_t __p0, float32x4_t __p1) {
  float32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  float32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  float32x4_t __ret;
  __ret = (float32x4_t) __builtin_neon_vpminq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 41);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vpminq_s32(int32x4_t __p0, int32x4_t __p1) {
  int32x4_t __ret;
  __ret = (int32x4_t) __builtin_neon_vpminq_v((int8x16_t)__p0, (int8x16_t)__p1, 34);
  return __ret;
}
#else
__ai int32x4_t vpminq_s32(int32x4_t __p0, int32x4_t __p1) {
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int32x4_t __ret;
  __ret = (int32x4_t) __builtin_neon_vpminq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 34);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x8_t vpminq_s16(int16x8_t __p0, int16x8_t __p1) {
  int16x8_t __ret;
  __ret = (int16x8_t) __builtin_neon_vpminq_v((int8x16_t)__p0, (int8x16_t)__p1, 33);
  return __ret;
}
#else
__ai int16x8_t vpminq_s16(int16x8_t __p0, int16x8_t __p1) {
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __ret;
  __ret = (int16x8_t) __builtin_neon_vpminq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 33);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64_t vpminqd_f64(float64x2_t __p0) {
  float64_t __ret;
  __ret = (float64_t) __builtin_neon_vpminqd_f64((int8x16_t)__p0);
  return __ret;
}
#else
__ai float64_t vpminqd_f64(float64x2_t __p0) {
  float64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float64_t __ret;
  __ret = (float64_t) __builtin_neon_vpminqd_f64((int8x16_t)__rev0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32_t vpmins_f32(float32x2_t __p0) {
  float32_t __ret;
  __ret = (float32_t) __builtin_neon_vpmins_f32((int8x8_t)__p0);
  return __ret;
}
#else
__ai float32_t vpmins_f32(float32x2_t __p0) {
  float32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float32_t __ret;
  __ret = (float32_t) __builtin_neon_vpmins_f32((int8x8_t)__rev0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x2_t vpminnmq_f64(float64x2_t __p0, float64x2_t __p1) {
  float64x2_t __ret;
  __ret = (float64x2_t) __builtin_neon_vpminnmq_v((int8x16_t)__p0, (int8x16_t)__p1, 42);
  return __ret;
}
#else
__ai float64x2_t vpminnmq_f64(float64x2_t __p0, float64x2_t __p1) {
  float64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  float64x2_t __ret;
  __ret = (float64x2_t) __builtin_neon_vpminnmq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 42);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x4_t vpminnmq_f32(float32x4_t __p0, float32x4_t __p1) {
  float32x4_t __ret;
  __ret = (float32x4_t) __builtin_neon_vpminnmq_v((int8x16_t)__p0, (int8x16_t)__p1, 41);
  return __ret;
}
#else
__ai float32x4_t vpminnmq_f32(float32x4_t __p0, float32x4_t __p1) {
  float32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  float32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  float32x4_t __ret;
  __ret = (float32x4_t) __builtin_neon_vpminnmq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 41);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x2_t vpminnm_f32(float32x2_t __p0, float32x2_t __p1) {
  float32x2_t __ret;
  __ret = (float32x2_t) __builtin_neon_vpminnm_v((int8x8_t)__p0, (int8x8_t)__p1, 9);
  return __ret;
}
#else
__ai float32x2_t vpminnm_f32(float32x2_t __p0, float32x2_t __p1) {
  float32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  float32x2_t __ret;
  __ret = (float32x2_t) __builtin_neon_vpminnm_v((int8x8_t)__rev0, (int8x8_t)__rev1, 9);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64_t vpminnmqd_f64(float64x2_t __p0) {
  float64_t __ret;
  __ret = (float64_t) __builtin_neon_vpminnmqd_f64((int8x16_t)__p0);
  return __ret;
}
#else
__ai float64_t vpminnmqd_f64(float64x2_t __p0) {
  float64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float64_t __ret;
  __ret = (float64_t) __builtin_neon_vpminnmqd_f64((int8x16_t)__rev0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32_t vpminnms_f32(float32x2_t __p0) {
  float32_t __ret;
  __ret = (float32_t) __builtin_neon_vpminnms_f32((int8x8_t)__p0);
  return __ret;
}
#else
__ai float32_t vpminnms_f32(float32x2_t __p0) {
  float32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float32_t __ret;
  __ret = (float32_t) __builtin_neon_vpminnms_f32((int8x8_t)__rev0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x2_t vqabsq_s64(int64x2_t __p0) {
  int64x2_t __ret;
  __ret = (int64x2_t) __builtin_neon_vqabsq_v((int8x16_t)__p0, 35);
  return __ret;
}
#else
__ai int64x2_t vqabsq_s64(int64x2_t __p0) {
  int64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int64x2_t __ret;
  __ret = (int64x2_t) __builtin_neon_vqabsq_v((int8x16_t)__rev0, 35);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x1_t vqabs_s64(int64x1_t __p0) {
  int64x1_t __ret;
  __ret = (int64x1_t) __builtin_neon_vqabs_v((int8x8_t)__p0, 3);
  return __ret;
}
#else
__ai int64x1_t vqabs_s64(int64x1_t __p0) {
  int64x1_t __ret;
  __ret = (int64x1_t) __builtin_neon_vqabs_v((int8x8_t)__p0, 3);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8_t vqabsb_s8(int8_t __p0) {
  int8_t __ret;
  __ret = (int8_t) __builtin_neon_vqabsb_s8(__p0);
  return __ret;
}
#else
__ai int8_t vqabsb_s8(int8_t __p0) {
  int8_t __ret;
  __ret = (int8_t) __builtin_neon_vqabsb_s8(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32_t vqabss_s32(int32_t __p0) {
  int32_t __ret;
  __ret = (int32_t) __builtin_neon_vqabss_s32(__p0);
  return __ret;
}
#else
__ai int32_t vqabss_s32(int32_t __p0) {
  int32_t __ret;
  __ret = (int32_t) __builtin_neon_vqabss_s32(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64_t vqabsd_s64(int64_t __p0) {
  int64_t __ret;
  __ret = (int64_t) __builtin_neon_vqabsd_s64(__p0);
  return __ret;
}
#else
__ai int64_t vqabsd_s64(int64_t __p0) {
  int64_t __ret;
  __ret = (int64_t) __builtin_neon_vqabsd_s64(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16_t vqabsh_s16(int16_t __p0) {
  int16_t __ret;
  __ret = (int16_t) __builtin_neon_vqabsh_s16(__p0);
  return __ret;
}
#else
__ai int16_t vqabsh_s16(int16_t __p0) {
  int16_t __ret;
  __ret = (int16_t) __builtin_neon_vqabsh_s16(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8_t vqaddb_u8(uint8_t __p0, uint8_t __p1) {
  uint8_t __ret;
  __ret = (uint8_t) __builtin_neon_vqaddb_u8(__p0, __p1);
  return __ret;
}
#else
__ai uint8_t vqaddb_u8(uint8_t __p0, uint8_t __p1) {
  uint8_t __ret;
  __ret = (uint8_t) __builtin_neon_vqaddb_u8(__p0, __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32_t vqadds_u32(uint32_t __p0, uint32_t __p1) {
  uint32_t __ret;
  __ret = (uint32_t) __builtin_neon_vqadds_u32(__p0, __p1);
  return __ret;
}
#else
__ai uint32_t vqadds_u32(uint32_t __p0, uint32_t __p1) {
  uint32_t __ret;
  __ret = (uint32_t) __builtin_neon_vqadds_u32(__p0, __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64_t vqaddd_u64(uint64_t __p0, uint64_t __p1) {
  uint64_t __ret;
  __ret = (uint64_t) __builtin_neon_vqaddd_u64(__p0, __p1);
  return __ret;
}
#else
__ai uint64_t vqaddd_u64(uint64_t __p0, uint64_t __p1) {
  uint64_t __ret;
  __ret = (uint64_t) __builtin_neon_vqaddd_u64(__p0, __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16_t vqaddh_u16(uint16_t __p0, uint16_t __p1) {
  uint16_t __ret;
  __ret = (uint16_t) __builtin_neon_vqaddh_u16(__p0, __p1);
  return __ret;
}
#else
__ai uint16_t vqaddh_u16(uint16_t __p0, uint16_t __p1) {
  uint16_t __ret;
  __ret = (uint16_t) __builtin_neon_vqaddh_u16(__p0, __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8_t vqaddb_s8(int8_t __p0, int8_t __p1) {
  int8_t __ret;
  __ret = (int8_t) __builtin_neon_vqaddb_s8(__p0, __p1);
  return __ret;
}
#else
__ai int8_t vqaddb_s8(int8_t __p0, int8_t __p1) {
  int8_t __ret;
  __ret = (int8_t) __builtin_neon_vqaddb_s8(__p0, __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32_t vqadds_s32(int32_t __p0, int32_t __p1) {
  int32_t __ret;
  __ret = (int32_t) __builtin_neon_vqadds_s32(__p0, __p1);
  return __ret;
}
#else
__ai int32_t vqadds_s32(int32_t __p0, int32_t __p1) {
  int32_t __ret;
  __ret = (int32_t) __builtin_neon_vqadds_s32(__p0, __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64_t vqaddd_s64(int64_t __p0, int64_t __p1) {
  int64_t __ret;
  __ret = (int64_t) __builtin_neon_vqaddd_s64(__p0, __p1);
  return __ret;
}
#else
__ai int64_t vqaddd_s64(int64_t __p0, int64_t __p1) {
  int64_t __ret;
  __ret = (int64_t) __builtin_neon_vqaddd_s64(__p0, __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16_t vqaddh_s16(int16_t __p0, int16_t __p1) {
  int16_t __ret;
  __ret = (int16_t) __builtin_neon_vqaddh_s16(__p0, __p1);
  return __ret;
}
#else
__ai int16_t vqaddh_s16(int16_t __p0, int16_t __p1) {
  int16_t __ret;
  __ret = (int16_t) __builtin_neon_vqaddh_s16(__p0, __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64_t vqdmlals_s32(int64_t __p0, int32_t __p1, int32_t __p2) {
  int64_t __ret;
  __ret = (int64_t) __builtin_neon_vqdmlals_s32(__p0, __p1, __p2);
  return __ret;
}
#else
__ai int64_t vqdmlals_s32(int64_t __p0, int32_t __p1, int32_t __p2) {
  int64_t __ret;
  __ret = (int64_t) __builtin_neon_vqdmlals_s32(__p0, __p1, __p2);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32_t vqdmlalh_s16(int32_t __p0, int16_t __p1, int16_t __p2) {
  int32_t __ret;
  __ret = (int32_t) __builtin_neon_vqdmlalh_s16(__p0, __p1, __p2);
  return __ret;
}
#else
__ai int32_t vqdmlalh_s16(int32_t __p0, int16_t __p1, int16_t __p2) {
  int32_t __ret;
  __ret = (int32_t) __builtin_neon_vqdmlalh_s16(__p0, __p1, __p2);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x2_t vqdmlal_high_s32(int64x2_t __p0, int32x4_t __p1, int32x4_t __p2) {
  int64x2_t __ret;
  __ret = vqdmlal_s32(__p0, vget_high_s32(__p1), vget_high_s32(__p2));
  return __ret;
}
#else
__ai int64x2_t vqdmlal_high_s32(int64x2_t __p0, int32x4_t __p1, int32x4_t __p2) {
  int64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int32x4_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 3, 2, 1, 0);
  int64x2_t __ret;
  __ret = __noswap_vqdmlal_s32(__rev0, __noswap_vget_high_s32(__rev1), __noswap_vget_high_s32(__rev2));
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vqdmlal_high_s16(int32x4_t __p0, int16x8_t __p1, int16x8_t __p2) {
  int32x4_t __ret;
  __ret = vqdmlal_s16(__p0, vget_high_s16(__p1), vget_high_s16(__p2));
  return __ret;
}
#else
__ai int32x4_t vqdmlal_high_s16(int32x4_t __p0, int16x8_t __p1, int16x8_t __p2) {
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 7, 6, 5, 4, 3, 2, 1, 0);
  int32x4_t __ret;
  __ret = __noswap_vqdmlal_s16(__rev0, __noswap_vget_high_s16(__rev1), __noswap_vget_high_s16(__rev2));
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
#define vqdmlal_high_lane_s32(__p0, __p1, __p2, __p3) __extension__ ({ \
  int64x2_t __s0 = __p0; \
  int32x4_t __s1 = __p1; \
  int32x2_t __s2 = __p2; \
  int64x2_t __ret; \
  __ret = vqdmlal_s32(__s0, vget_high_s32(__s1), __builtin_shufflevector(__s2, __s2, __p3, __p3)); \
  __ret; \
})
#else
#define vqdmlal_high_lane_s32(__p0, __p1, __p2, __p3) __extension__ ({ \
  int64x2_t __s0 = __p0; \
  int32x4_t __s1 = __p1; \
  int32x2_t __s2 = __p2; \
  int64x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  int32x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  int32x2_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 1, 0); \
  int64x2_t __ret; \
  __ret = __noswap_vqdmlal_s32(__rev0, __noswap_vget_high_s32(__rev1), __builtin_shufflevector(__rev2, __rev2, __p3, __p3)); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqdmlal_high_lane_s16(__p0, __p1, __p2, __p3) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  int16x8_t __s1 = __p1; \
  int16x4_t __s2 = __p2; \
  int32x4_t __ret; \
  __ret = vqdmlal_s16(__s0, vget_high_s16(__s1), __builtin_shufflevector(__s2, __s2, __p3, __p3, __p3, __p3)); \
  __ret; \
})
#else
#define vqdmlal_high_lane_s16(__p0, __p1, __p2, __p3) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  int16x8_t __s1 = __p1; \
  int16x4_t __s2 = __p2; \
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  int16x8_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 7, 6, 5, 4, 3, 2, 1, 0); \
  int16x4_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 3, 2, 1, 0); \
  int32x4_t __ret; \
  __ret = __noswap_vqdmlal_s16(__rev0, __noswap_vget_high_s16(__rev1), __builtin_shufflevector(__rev2, __rev2, __p3, __p3, __p3, __p3)); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqdmlal_high_laneq_s32(__p0, __p1, __p2, __p3) __extension__ ({ \
  int64x2_t __s0 = __p0; \
  int32x4_t __s1 = __p1; \
  int32x4_t __s2 = __p2; \
  int64x2_t __ret; \
  __ret = vqdmlal_s32(__s0, vget_high_s32(__s1), __builtin_shufflevector(__s2, __s2, __p3, __p3)); \
  __ret; \
})
#else
#define vqdmlal_high_laneq_s32(__p0, __p1, __p2, __p3) __extension__ ({ \
  int64x2_t __s0 = __p0; \
  int32x4_t __s1 = __p1; \
  int32x4_t __s2 = __p2; \
  int64x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  int32x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  int32x4_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 3, 2, 1, 0); \
  int64x2_t __ret; \
  __ret = __noswap_vqdmlal_s32(__rev0, __noswap_vget_high_s32(__rev1), __builtin_shufflevector(__rev2, __rev2, __p3, __p3)); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqdmlal_high_laneq_s16(__p0, __p1, __p2, __p3) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  int16x8_t __s1 = __p1; \
  int16x8_t __s2 = __p2; \
  int32x4_t __ret; \
  __ret = vqdmlal_s16(__s0, vget_high_s16(__s1), __builtin_shufflevector(__s2, __s2, __p3, __p3, __p3, __p3)); \
  __ret; \
})
#else
#define vqdmlal_high_laneq_s16(__p0, __p1, __p2, __p3) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  int16x8_t __s1 = __p1; \
  int16x8_t __s2 = __p2; \
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  int16x8_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 7, 6, 5, 4, 3, 2, 1, 0); \
  int16x8_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 7, 6, 5, 4, 3, 2, 1, 0); \
  int32x4_t __ret; \
  __ret = __noswap_vqdmlal_s16(__rev0, __noswap_vget_high_s16(__rev1), __builtin_shufflevector(__rev2, __rev2, __p3, __p3, __p3, __p3)); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x2_t vqdmlal_high_n_s32(int64x2_t __p0, int32x4_t __p1, int32_t __p2) {
  int64x2_t __ret;
  __ret = vqdmlal_n_s32(__p0, vget_high_s32(__p1), __p2);
  return __ret;
}
#else
__ai int64x2_t vqdmlal_high_n_s32(int64x2_t __p0, int32x4_t __p1, int32_t __p2) {
  int64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int64x2_t __ret;
  __ret = __noswap_vqdmlal_n_s32(__rev0, __noswap_vget_high_s32(__rev1), __p2);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vqdmlal_high_n_s16(int32x4_t __p0, int16x8_t __p1, int16_t __p2) {
  int32x4_t __ret;
  __ret = vqdmlal_n_s16(__p0, vget_high_s16(__p1), __p2);
  return __ret;
}
#else
__ai int32x4_t vqdmlal_high_n_s16(int32x4_t __p0, int16x8_t __p1, int16_t __p2) {
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int32x4_t __ret;
  __ret = __noswap_vqdmlal_n_s16(__rev0, __noswap_vget_high_s16(__rev1), __p2);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
#define vqdmlals_lane_s32(__p0, __p1, __p2, __p3) __extension__ ({ \
  int64_t __s0 = __p0; \
  int32_t __s1 = __p1; \
  int32x2_t __s2 = __p2; \
  int64_t __ret; \
  __ret = (int64_t) __builtin_neon_vqdmlals_lane_s32(__s0, __s1, (int8x8_t)__s2, __p3); \
  __ret; \
})
#else
#define vqdmlals_lane_s32(__p0, __p1, __p2, __p3) __extension__ ({ \
  int64_t __s0 = __p0; \
  int32_t __s1 = __p1; \
  int32x2_t __s2 = __p2; \
  int32x2_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 1, 0); \
  int64_t __ret; \
  __ret = (int64_t) __builtin_neon_vqdmlals_lane_s32(__s0, __s1, (int8x8_t)__rev2, __p3); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqdmlalh_lane_s16(__p0, __p1, __p2, __p3) __extension__ ({ \
  int32_t __s0 = __p0; \
  int16_t __s1 = __p1; \
  int16x4_t __s2 = __p2; \
  int32_t __ret; \
  __ret = (int32_t) __builtin_neon_vqdmlalh_lane_s16(__s0, __s1, (int8x8_t)__s2, __p3); \
  __ret; \
})
#else
#define vqdmlalh_lane_s16(__p0, __p1, __p2, __p3) __extension__ ({ \
  int32_t __s0 = __p0; \
  int16_t __s1 = __p1; \
  int16x4_t __s2 = __p2; \
  int16x4_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 3, 2, 1, 0); \
  int32_t __ret; \
  __ret = (int32_t) __builtin_neon_vqdmlalh_lane_s16(__s0, __s1, (int8x8_t)__rev2, __p3); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqdmlals_laneq_s32(__p0, __p1, __p2, __p3) __extension__ ({ \
  int64_t __s0 = __p0; \
  int32_t __s1 = __p1; \
  int32x4_t __s2 = __p2; \
  int64_t __ret; \
  __ret = (int64_t) __builtin_neon_vqdmlals_laneq_s32(__s0, __s1, (int8x16_t)__s2, __p3); \
  __ret; \
})
#else
#define vqdmlals_laneq_s32(__p0, __p1, __p2, __p3) __extension__ ({ \
  int64_t __s0 = __p0; \
  int32_t __s1 = __p1; \
  int32x4_t __s2 = __p2; \
  int32x4_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 3, 2, 1, 0); \
  int64_t __ret; \
  __ret = (int64_t) __builtin_neon_vqdmlals_laneq_s32(__s0, __s1, (int8x16_t)__rev2, __p3); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqdmlalh_laneq_s16(__p0, __p1, __p2, __p3) __extension__ ({ \
  int32_t __s0 = __p0; \
  int16_t __s1 = __p1; \
  int16x8_t __s2 = __p2; \
  int32_t __ret; \
  __ret = (int32_t) __builtin_neon_vqdmlalh_laneq_s16(__s0, __s1, (int8x16_t)__s2, __p3); \
  __ret; \
})
#else
#define vqdmlalh_laneq_s16(__p0, __p1, __p2, __p3) __extension__ ({ \
  int32_t __s0 = __p0; \
  int16_t __s1 = __p1; \
  int16x8_t __s2 = __p2; \
  int16x8_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 7, 6, 5, 4, 3, 2, 1, 0); \
  int32_t __ret; \
  __ret = (int32_t) __builtin_neon_vqdmlalh_laneq_s16(__s0, __s1, (int8x16_t)__rev2, __p3); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqdmlal_laneq_s32(__p0, __p1, __p2, __p3) __extension__ ({ \
  int64x2_t __s0 = __p0; \
  int32x2_t __s1 = __p1; \
  int32x4_t __s2 = __p2; \
  int64x2_t __ret; \
  __ret = vqdmlal_s32(__s0, __s1, __builtin_shufflevector(__s2, __s2, __p3, __p3)); \
  __ret; \
})
#else
#define vqdmlal_laneq_s32(__p0, __p1, __p2, __p3) __extension__ ({ \
  int64x2_t __s0 = __p0; \
  int32x2_t __s1 = __p1; \
  int32x4_t __s2 = __p2; \
  int64x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  int32x2_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 1, 0); \
  int32x4_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 3, 2, 1, 0); \
  int64x2_t __ret; \
  __ret = __noswap_vqdmlal_s32(__rev0, __rev1, __builtin_shufflevector(__rev2, __rev2, __p3, __p3)); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqdmlal_laneq_s16(__p0, __p1, __p2, __p3) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  int16x4_t __s1 = __p1; \
  int16x8_t __s2 = __p2; \
  int32x4_t __ret; \
  __ret = vqdmlal_s16(__s0, __s1, __builtin_shufflevector(__s2, __s2, __p3, __p3, __p3, __p3)); \
  __ret; \
})
#else
#define vqdmlal_laneq_s16(__p0, __p1, __p2, __p3) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  int16x4_t __s1 = __p1; \
  int16x8_t __s2 = __p2; \
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  int16x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  int16x8_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 7, 6, 5, 4, 3, 2, 1, 0); \
  int32x4_t __ret; \
  __ret = __noswap_vqdmlal_s16(__rev0, __rev1, __builtin_shufflevector(__rev2, __rev2, __p3, __p3, __p3, __p3)); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64_t vqdmlsls_s32(int64_t __p0, int32_t __p1, int32_t __p2) {
  int64_t __ret;
  __ret = (int64_t) __builtin_neon_vqdmlsls_s32(__p0, __p1, __p2);
  return __ret;
}
#else
__ai int64_t vqdmlsls_s32(int64_t __p0, int32_t __p1, int32_t __p2) {
  int64_t __ret;
  __ret = (int64_t) __builtin_neon_vqdmlsls_s32(__p0, __p1, __p2);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32_t vqdmlslh_s16(int32_t __p0, int16_t __p1, int16_t __p2) {
  int32_t __ret;
  __ret = (int32_t) __builtin_neon_vqdmlslh_s16(__p0, __p1, __p2);
  return __ret;
}
#else
__ai int32_t vqdmlslh_s16(int32_t __p0, int16_t __p1, int16_t __p2) {
  int32_t __ret;
  __ret = (int32_t) __builtin_neon_vqdmlslh_s16(__p0, __p1, __p2);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x2_t vqdmlsl_high_s32(int64x2_t __p0, int32x4_t __p1, int32x4_t __p2) {
  int64x2_t __ret;
  __ret = vqdmlsl_s32(__p0, vget_high_s32(__p1), vget_high_s32(__p2));
  return __ret;
}
#else
__ai int64x2_t vqdmlsl_high_s32(int64x2_t __p0, int32x4_t __p1, int32x4_t __p2) {
  int64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int32x4_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 3, 2, 1, 0);
  int64x2_t __ret;
  __ret = __noswap_vqdmlsl_s32(__rev0, __noswap_vget_high_s32(__rev1), __noswap_vget_high_s32(__rev2));
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vqdmlsl_high_s16(int32x4_t __p0, int16x8_t __p1, int16x8_t __p2) {
  int32x4_t __ret;
  __ret = vqdmlsl_s16(__p0, vget_high_s16(__p1), vget_high_s16(__p2));
  return __ret;
}
#else
__ai int32x4_t vqdmlsl_high_s16(int32x4_t __p0, int16x8_t __p1, int16x8_t __p2) {
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 7, 6, 5, 4, 3, 2, 1, 0);
  int32x4_t __ret;
  __ret = __noswap_vqdmlsl_s16(__rev0, __noswap_vget_high_s16(__rev1), __noswap_vget_high_s16(__rev2));
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
#define vqdmlsl_high_lane_s32(__p0, __p1, __p2, __p3) __extension__ ({ \
  int64x2_t __s0 = __p0; \
  int32x4_t __s1 = __p1; \
  int32x2_t __s2 = __p2; \
  int64x2_t __ret; \
  __ret = vqdmlsl_s32(__s0, vget_high_s32(__s1), __builtin_shufflevector(__s2, __s2, __p3, __p3)); \
  __ret; \
})
#else
#define vqdmlsl_high_lane_s32(__p0, __p1, __p2, __p3) __extension__ ({ \
  int64x2_t __s0 = __p0; \
  int32x4_t __s1 = __p1; \
  int32x2_t __s2 = __p2; \
  int64x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  int32x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  int32x2_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 1, 0); \
  int64x2_t __ret; \
  __ret = __noswap_vqdmlsl_s32(__rev0, __noswap_vget_high_s32(__rev1), __builtin_shufflevector(__rev2, __rev2, __p3, __p3)); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqdmlsl_high_lane_s16(__p0, __p1, __p2, __p3) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  int16x8_t __s1 = __p1; \
  int16x4_t __s2 = __p2; \
  int32x4_t __ret; \
  __ret = vqdmlsl_s16(__s0, vget_high_s16(__s1), __builtin_shufflevector(__s2, __s2, __p3, __p3, __p3, __p3)); \
  __ret; \
})
#else
#define vqdmlsl_high_lane_s16(__p0, __p1, __p2, __p3) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  int16x8_t __s1 = __p1; \
  int16x4_t __s2 = __p2; \
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  int16x8_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 7, 6, 5, 4, 3, 2, 1, 0); \
  int16x4_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 3, 2, 1, 0); \
  int32x4_t __ret; \
  __ret = __noswap_vqdmlsl_s16(__rev0, __noswap_vget_high_s16(__rev1), __builtin_shufflevector(__rev2, __rev2, __p3, __p3, __p3, __p3)); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqdmlsl_high_laneq_s32(__p0, __p1, __p2, __p3) __extension__ ({ \
  int64x2_t __s0 = __p0; \
  int32x4_t __s1 = __p1; \
  int32x4_t __s2 = __p2; \
  int64x2_t __ret; \
  __ret = vqdmlsl_s32(__s0, vget_high_s32(__s1), __builtin_shufflevector(__s2, __s2, __p3, __p3)); \
  __ret; \
})
#else
#define vqdmlsl_high_laneq_s32(__p0, __p1, __p2, __p3) __extension__ ({ \
  int64x2_t __s0 = __p0; \
  int32x4_t __s1 = __p1; \
  int32x4_t __s2 = __p2; \
  int64x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  int32x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  int32x4_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 3, 2, 1, 0); \
  int64x2_t __ret; \
  __ret = __noswap_vqdmlsl_s32(__rev0, __noswap_vget_high_s32(__rev1), __builtin_shufflevector(__rev2, __rev2, __p3, __p3)); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqdmlsl_high_laneq_s16(__p0, __p1, __p2, __p3) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  int16x8_t __s1 = __p1; \
  int16x8_t __s2 = __p2; \
  int32x4_t __ret; \
  __ret = vqdmlsl_s16(__s0, vget_high_s16(__s1), __builtin_shufflevector(__s2, __s2, __p3, __p3, __p3, __p3)); \
  __ret; \
})
#else
#define vqdmlsl_high_laneq_s16(__p0, __p1, __p2, __p3) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  int16x8_t __s1 = __p1; \
  int16x8_t __s2 = __p2; \
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  int16x8_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 7, 6, 5, 4, 3, 2, 1, 0); \
  int16x8_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 7, 6, 5, 4, 3, 2, 1, 0); \
  int32x4_t __ret; \
  __ret = __noswap_vqdmlsl_s16(__rev0, __noswap_vget_high_s16(__rev1), __builtin_shufflevector(__rev2, __rev2, __p3, __p3, __p3, __p3)); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x2_t vqdmlsl_high_n_s32(int64x2_t __p0, int32x4_t __p1, int32_t __p2) {
  int64x2_t __ret;
  __ret = vqdmlsl_n_s32(__p0, vget_high_s32(__p1), __p2);
  return __ret;
}
#else
__ai int64x2_t vqdmlsl_high_n_s32(int64x2_t __p0, int32x4_t __p1, int32_t __p2) {
  int64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int64x2_t __ret;
  __ret = __noswap_vqdmlsl_n_s32(__rev0, __noswap_vget_high_s32(__rev1), __p2);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vqdmlsl_high_n_s16(int32x4_t __p0, int16x8_t __p1, int16_t __p2) {
  int32x4_t __ret;
  __ret = vqdmlsl_n_s16(__p0, vget_high_s16(__p1), __p2);
  return __ret;
}
#else
__ai int32x4_t vqdmlsl_high_n_s16(int32x4_t __p0, int16x8_t __p1, int16_t __p2) {
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int32x4_t __ret;
  __ret = __noswap_vqdmlsl_n_s16(__rev0, __noswap_vget_high_s16(__rev1), __p2);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
#define vqdmlsls_lane_s32(__p0, __p1, __p2, __p3) __extension__ ({ \
  int64_t __s0 = __p0; \
  int32_t __s1 = __p1; \
  int32x2_t __s2 = __p2; \
  int64_t __ret; \
  __ret = (int64_t) __builtin_neon_vqdmlsls_lane_s32(__s0, __s1, (int8x8_t)__s2, __p3); \
  __ret; \
})
#else
#define vqdmlsls_lane_s32(__p0, __p1, __p2, __p3) __extension__ ({ \
  int64_t __s0 = __p0; \
  int32_t __s1 = __p1; \
  int32x2_t __s2 = __p2; \
  int32x2_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 1, 0); \
  int64_t __ret; \
  __ret = (int64_t) __builtin_neon_vqdmlsls_lane_s32(__s0, __s1, (int8x8_t)__rev2, __p3); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqdmlslh_lane_s16(__p0, __p1, __p2, __p3) __extension__ ({ \
  int32_t __s0 = __p0; \
  int16_t __s1 = __p1; \
  int16x4_t __s2 = __p2; \
  int32_t __ret; \
  __ret = (int32_t) __builtin_neon_vqdmlslh_lane_s16(__s0, __s1, (int8x8_t)__s2, __p3); \
  __ret; \
})
#else
#define vqdmlslh_lane_s16(__p0, __p1, __p2, __p3) __extension__ ({ \
  int32_t __s0 = __p0; \
  int16_t __s1 = __p1; \
  int16x4_t __s2 = __p2; \
  int16x4_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 3, 2, 1, 0); \
  int32_t __ret; \
  __ret = (int32_t) __builtin_neon_vqdmlslh_lane_s16(__s0, __s1, (int8x8_t)__rev2, __p3); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqdmlsls_laneq_s32(__p0, __p1, __p2, __p3) __extension__ ({ \
  int64_t __s0 = __p0; \
  int32_t __s1 = __p1; \
  int32x4_t __s2 = __p2; \
  int64_t __ret; \
  __ret = (int64_t) __builtin_neon_vqdmlsls_laneq_s32(__s0, __s1, (int8x16_t)__s2, __p3); \
  __ret; \
})
#else
#define vqdmlsls_laneq_s32(__p0, __p1, __p2, __p3) __extension__ ({ \
  int64_t __s0 = __p0; \
  int32_t __s1 = __p1; \
  int32x4_t __s2 = __p2; \
  int32x4_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 3, 2, 1, 0); \
  int64_t __ret; \
  __ret = (int64_t) __builtin_neon_vqdmlsls_laneq_s32(__s0, __s1, (int8x16_t)__rev2, __p3); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqdmlslh_laneq_s16(__p0, __p1, __p2, __p3) __extension__ ({ \
  int32_t __s0 = __p0; \
  int16_t __s1 = __p1; \
  int16x8_t __s2 = __p2; \
  int32_t __ret; \
  __ret = (int32_t) __builtin_neon_vqdmlslh_laneq_s16(__s0, __s1, (int8x16_t)__s2, __p3); \
  __ret; \
})
#else
#define vqdmlslh_laneq_s16(__p0, __p1, __p2, __p3) __extension__ ({ \
  int32_t __s0 = __p0; \
  int16_t __s1 = __p1; \
  int16x8_t __s2 = __p2; \
  int16x8_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 7, 6, 5, 4, 3, 2, 1, 0); \
  int32_t __ret; \
  __ret = (int32_t) __builtin_neon_vqdmlslh_laneq_s16(__s0, __s1, (int8x16_t)__rev2, __p3); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqdmlsl_laneq_s32(__p0, __p1, __p2, __p3) __extension__ ({ \
  int64x2_t __s0 = __p0; \
  int32x2_t __s1 = __p1; \
  int32x4_t __s2 = __p2; \
  int64x2_t __ret; \
  __ret = vqdmlsl_s32(__s0, __s1, __builtin_shufflevector(__s2, __s2, __p3, __p3)); \
  __ret; \
})
#else
#define vqdmlsl_laneq_s32(__p0, __p1, __p2, __p3) __extension__ ({ \
  int64x2_t __s0 = __p0; \
  int32x2_t __s1 = __p1; \
  int32x4_t __s2 = __p2; \
  int64x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  int32x2_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 1, 0); \
  int32x4_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 3, 2, 1, 0); \
  int64x2_t __ret; \
  __ret = __noswap_vqdmlsl_s32(__rev0, __rev1, __builtin_shufflevector(__rev2, __rev2, __p3, __p3)); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqdmlsl_laneq_s16(__p0, __p1, __p2, __p3) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  int16x4_t __s1 = __p1; \
  int16x8_t __s2 = __p2; \
  int32x4_t __ret; \
  __ret = vqdmlsl_s16(__s0, __s1, __builtin_shufflevector(__s2, __s2, __p3, __p3, __p3, __p3)); \
  __ret; \
})
#else
#define vqdmlsl_laneq_s16(__p0, __p1, __p2, __p3) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  int16x4_t __s1 = __p1; \
  int16x8_t __s2 = __p2; \
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  int16x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  int16x8_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 7, 6, 5, 4, 3, 2, 1, 0); \
  int32x4_t __ret; \
  __ret = __noswap_vqdmlsl_s16(__rev0, __rev1, __builtin_shufflevector(__rev2, __rev2, __p3, __p3, __p3, __p3)); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32_t vqdmulhs_s32(int32_t __p0, int32_t __p1) {
  int32_t __ret;
  __ret = (int32_t) __builtin_neon_vqdmulhs_s32(__p0, __p1);
  return __ret;
}
#else
__ai int32_t vqdmulhs_s32(int32_t __p0, int32_t __p1) {
  int32_t __ret;
  __ret = (int32_t) __builtin_neon_vqdmulhs_s32(__p0, __p1);
  return __ret;
}
__ai int32_t __noswap_vqdmulhs_s32(int32_t __p0, int32_t __p1) {
  int32_t __ret;
  __ret = (int32_t) __builtin_neon_vqdmulhs_s32(__p0, __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16_t vqdmulhh_s16(int16_t __p0, int16_t __p1) {
  int16_t __ret;
  __ret = (int16_t) __builtin_neon_vqdmulhh_s16(__p0, __p1);
  return __ret;
}
#else
__ai int16_t vqdmulhh_s16(int16_t __p0, int16_t __p1) {
  int16_t __ret;
  __ret = (int16_t) __builtin_neon_vqdmulhh_s16(__p0, __p1);
  return __ret;
}
__ai int16_t __noswap_vqdmulhh_s16(int16_t __p0, int16_t __p1) {
  int16_t __ret;
  __ret = (int16_t) __builtin_neon_vqdmulhh_s16(__p0, __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
#define vqdmulhs_lane_s32(__p0_150, __p1_150, __p2_150) __extension__ ({ \
  int32_t __s0_150 = __p0_150; \
  int32x2_t __s1_150 = __p1_150; \
  int32_t __ret_150; \
  __ret_150 = vqdmulhs_s32(__s0_150, vget_lane_s32(__s1_150, __p2_150)); \
  __ret_150; \
})
#else
#define vqdmulhs_lane_s32(__p0_151, __p1_151, __p2_151) __extension__ ({ \
  int32_t __s0_151 = __p0_151; \
  int32x2_t __s1_151 = __p1_151; \
  int32x2_t __rev1_151;  __rev1_151 = __builtin_shufflevector(__s1_151, __s1_151, 1, 0); \
  int32_t __ret_151; \
  __ret_151 = __noswap_vqdmulhs_s32(__s0_151, __noswap_vget_lane_s32(__rev1_151, __p2_151)); \
  __ret_151; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqdmulhh_lane_s16(__p0_152, __p1_152, __p2_152) __extension__ ({ \
  int16_t __s0_152 = __p0_152; \
  int16x4_t __s1_152 = __p1_152; \
  int16_t __ret_152; \
  __ret_152 = vqdmulhh_s16(__s0_152, vget_lane_s16(__s1_152, __p2_152)); \
  __ret_152; \
})
#else
#define vqdmulhh_lane_s16(__p0_153, __p1_153, __p2_153) __extension__ ({ \
  int16_t __s0_153 = __p0_153; \
  int16x4_t __s1_153 = __p1_153; \
  int16x4_t __rev1_153;  __rev1_153 = __builtin_shufflevector(__s1_153, __s1_153, 3, 2, 1, 0); \
  int16_t __ret_153; \
  __ret_153 = __noswap_vqdmulhh_s16(__s0_153, __noswap_vget_lane_s16(__rev1_153, __p2_153)); \
  __ret_153; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqdmulhs_laneq_s32(__p0_154, __p1_154, __p2_154) __extension__ ({ \
  int32_t __s0_154 = __p0_154; \
  int32x4_t __s1_154 = __p1_154; \
  int32_t __ret_154; \
  __ret_154 = vqdmulhs_s32(__s0_154, vgetq_lane_s32(__s1_154, __p2_154)); \
  __ret_154; \
})
#else
#define vqdmulhs_laneq_s32(__p0_155, __p1_155, __p2_155) __extension__ ({ \
  int32_t __s0_155 = __p0_155; \
  int32x4_t __s1_155 = __p1_155; \
  int32x4_t __rev1_155;  __rev1_155 = __builtin_shufflevector(__s1_155, __s1_155, 3, 2, 1, 0); \
  int32_t __ret_155; \
  __ret_155 = __noswap_vqdmulhs_s32(__s0_155, __noswap_vgetq_lane_s32(__rev1_155, __p2_155)); \
  __ret_155; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqdmulhh_laneq_s16(__p0_156, __p1_156, __p2_156) __extension__ ({ \
  int16_t __s0_156 = __p0_156; \
  int16x8_t __s1_156 = __p1_156; \
  int16_t __ret_156; \
  __ret_156 = vqdmulhh_s16(__s0_156, vgetq_lane_s16(__s1_156, __p2_156)); \
  __ret_156; \
})
#else
#define vqdmulhh_laneq_s16(__p0_157, __p1_157, __p2_157) __extension__ ({ \
  int16_t __s0_157 = __p0_157; \
  int16x8_t __s1_157 = __p1_157; \
  int16x8_t __rev1_157;  __rev1_157 = __builtin_shufflevector(__s1_157, __s1_157, 7, 6, 5, 4, 3, 2, 1, 0); \
  int16_t __ret_157; \
  __ret_157 = __noswap_vqdmulhh_s16(__s0_157, __noswap_vgetq_lane_s16(__rev1_157, __p2_157)); \
  __ret_157; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqdmulhq_laneq_s32(__p0, __p1, __p2) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  int32x4_t __s1 = __p1; \
  int32x4_t __ret; \
  __ret = vqdmulhq_s32(__s0, __builtin_shufflevector(__s1, __s1, __p2, __p2, __p2, __p2)); \
  __ret; \
})
#else
#define vqdmulhq_laneq_s32(__p0, __p1, __p2) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  int32x4_t __s1 = __p1; \
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  int32x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  int32x4_t __ret; \
  __ret = __noswap_vqdmulhq_s32(__rev0, __builtin_shufflevector(__rev1, __rev1, __p2, __p2, __p2, __p2)); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqdmulhq_laneq_s16(__p0, __p1, __p2) __extension__ ({ \
  int16x8_t __s0 = __p0; \
  int16x8_t __s1 = __p1; \
  int16x8_t __ret; \
  __ret = vqdmulhq_s16(__s0, __builtin_shufflevector(__s1, __s1, __p2, __p2, __p2, __p2, __p2, __p2, __p2, __p2)); \
  __ret; \
})
#else
#define vqdmulhq_laneq_s16(__p0, __p1, __p2) __extension__ ({ \
  int16x8_t __s0 = __p0; \
  int16x8_t __s1 = __p1; \
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 7, 6, 5, 4, 3, 2, 1, 0); \
  int16x8_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 7, 6, 5, 4, 3, 2, 1, 0); \
  int16x8_t __ret; \
  __ret = __noswap_vqdmulhq_s16(__rev0, __builtin_shufflevector(__rev1, __rev1, __p2, __p2, __p2, __p2, __p2, __p2, __p2, __p2)); \
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqdmulh_laneq_s32(__p0, __p1, __p2) __extension__ ({ \
  int32x2_t __s0 = __p0; \
  int32x4_t __s1 = __p1; \
  int32x2_t __ret; \
  __ret = vqdmulh_s32(__s0, __builtin_shufflevector(__s1, __s1, __p2, __p2)); \
  __ret; \
})
#else
#define vqdmulh_laneq_s32(__p0, __p1, __p2) __extension__ ({ \
  int32x2_t __s0 = __p0; \
  int32x4_t __s1 = __p1; \
  int32x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  int32x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  int32x2_t __ret; \
  __ret = __noswap_vqdmulh_s32(__rev0, __builtin_shufflevector(__rev1, __rev1, __p2, __p2)); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqdmulh_laneq_s16(__p0, __p1, __p2) __extension__ ({ \
  int16x4_t __s0 = __p0; \
  int16x8_t __s1 = __p1; \
  int16x4_t __ret; \
  __ret = vqdmulh_s16(__s0, __builtin_shufflevector(__s1, __s1, __p2, __p2, __p2, __p2)); \
  __ret; \
})
#else
#define vqdmulh_laneq_s16(__p0, __p1, __p2) __extension__ ({ \
  int16x4_t __s0 = __p0; \
  int16x8_t __s1 = __p1; \
  int16x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  int16x8_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 7, 6, 5, 4, 3, 2, 1, 0); \
  int16x4_t __ret; \
  __ret = __noswap_vqdmulh_s16(__rev0, __builtin_shufflevector(__rev1, __rev1, __p2, __p2, __p2, __p2)); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64_t vqdmulls_s32(int32_t __p0, int32_t __p1) {
  int64_t __ret;
  __ret = (int64_t) __builtin_neon_vqdmulls_s32(__p0, __p1);
  return __ret;
}
#else
__ai int64_t vqdmulls_s32(int32_t __p0, int32_t __p1) {
  int64_t __ret;
  __ret = (int64_t) __builtin_neon_vqdmulls_s32(__p0, __p1);
  return __ret;
}
__ai int64_t __noswap_vqdmulls_s32(int32_t __p0, int32_t __p1) {
  int64_t __ret;
  __ret = (int64_t) __builtin_neon_vqdmulls_s32(__p0, __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32_t vqdmullh_s16(int16_t __p0, int16_t __p1) {
  int32_t __ret;
  __ret = (int32_t) __builtin_neon_vqdmullh_s16(__p0, __p1);
  return __ret;
}
#else
__ai int32_t vqdmullh_s16(int16_t __p0, int16_t __p1) {
  int32_t __ret;
  __ret = (int32_t) __builtin_neon_vqdmullh_s16(__p0, __p1);
  return __ret;
}
__ai int32_t __noswap_vqdmullh_s16(int16_t __p0, int16_t __p1) {
  int32_t __ret;
  __ret = (int32_t) __builtin_neon_vqdmullh_s16(__p0, __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x2_t vqdmull_high_s32(int32x4_t __p0, int32x4_t __p1) {
  int64x2_t __ret;
  __ret = vqdmull_s32(vget_high_s32(__p0), vget_high_s32(__p1));
  return __ret;
}
#else
__ai int64x2_t vqdmull_high_s32(int32x4_t __p0, int32x4_t __p1) {
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int64x2_t __ret;
  __ret = __noswap_vqdmull_s32(__noswap_vget_high_s32(__rev0), __noswap_vget_high_s32(__rev1));
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vqdmull_high_s16(int16x8_t __p0, int16x8_t __p1) {
  int32x4_t __ret;
  __ret = vqdmull_s16(vget_high_s16(__p0), vget_high_s16(__p1));
  return __ret;
}
#else
__ai int32x4_t vqdmull_high_s16(int16x8_t __p0, int16x8_t __p1) {
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int32x4_t __ret;
  __ret = __noswap_vqdmull_s16(__noswap_vget_high_s16(__rev0), __noswap_vget_high_s16(__rev1));
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
#define vqdmull_high_lane_s32(__p0, __p1, __p2) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  int32x2_t __s1 = __p1; \
  int64x2_t __ret; \
  __ret = vqdmull_s32(vget_high_s32(__s0), __builtin_shufflevector(__s1, __s1, __p2, __p2)); \
  __ret; \
})
#else
#define vqdmull_high_lane_s32(__p0, __p1, __p2) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  int32x2_t __s1 = __p1; \
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  int32x2_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 1, 0); \
  int64x2_t __ret; \
  __ret = __noswap_vqdmull_s32(__noswap_vget_high_s32(__rev0), __builtin_shufflevector(__rev1, __rev1, __p2, __p2)); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqdmull_high_lane_s16(__p0, __p1, __p2) __extension__ ({ \
  int16x8_t __s0 = __p0; \
  int16x4_t __s1 = __p1; \
  int32x4_t __ret; \
  __ret = vqdmull_s16(vget_high_s16(__s0), __builtin_shufflevector(__s1, __s1, __p2, __p2, __p2, __p2)); \
  __ret; \
})
#else
#define vqdmull_high_lane_s16(__p0, __p1, __p2) __extension__ ({ \
  int16x8_t __s0 = __p0; \
  int16x4_t __s1 = __p1; \
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 7, 6, 5, 4, 3, 2, 1, 0); \
  int16x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  int32x4_t __ret; \
  __ret = __noswap_vqdmull_s16(__noswap_vget_high_s16(__rev0), __builtin_shufflevector(__rev1, __rev1, __p2, __p2, __p2, __p2)); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqdmull_high_laneq_s32(__p0, __p1, __p2) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  int32x4_t __s1 = __p1; \
  int64x2_t __ret; \
  __ret = vqdmull_s32(vget_high_s32(__s0), __builtin_shufflevector(__s1, __s1, __p2, __p2)); \
  __ret; \
})
#else
#define vqdmull_high_laneq_s32(__p0, __p1, __p2) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  int32x4_t __s1 = __p1; \
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  int32x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  int64x2_t __ret; \
  __ret = __noswap_vqdmull_s32(__noswap_vget_high_s32(__rev0), __builtin_shufflevector(__rev1, __rev1, __p2, __p2)); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqdmull_high_laneq_s16(__p0, __p1, __p2) __extension__ ({ \
  int16x8_t __s0 = __p0; \
  int16x8_t __s1 = __p1; \
  int32x4_t __ret; \
  __ret = vqdmull_s16(vget_high_s16(__s0), __builtin_shufflevector(__s1, __s1, __p2, __p2, __p2, __p2)); \
  __ret; \
})
#else
#define vqdmull_high_laneq_s16(__p0, __p1, __p2) __extension__ ({ \
  int16x8_t __s0 = __p0; \
  int16x8_t __s1 = __p1; \
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 7, 6, 5, 4, 3, 2, 1, 0); \
  int16x8_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 7, 6, 5, 4, 3, 2, 1, 0); \
  int32x4_t __ret; \
  __ret = __noswap_vqdmull_s16(__noswap_vget_high_s16(__rev0), __builtin_shufflevector(__rev1, __rev1, __p2, __p2, __p2, __p2)); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x2_t vqdmull_high_n_s32(int32x4_t __p0, int32_t __p1) {
  int64x2_t __ret;
  __ret = vqdmull_n_s32(vget_high_s32(__p0), __p1);
  return __ret;
}
#else
__ai int64x2_t vqdmull_high_n_s32(int32x4_t __p0, int32_t __p1) {
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int64x2_t __ret;
  __ret = __noswap_vqdmull_n_s32(__noswap_vget_high_s32(__rev0), __p1);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vqdmull_high_n_s16(int16x8_t __p0, int16_t __p1) {
  int32x4_t __ret;
  __ret = vqdmull_n_s16(vget_high_s16(__p0), __p1);
  return __ret;
}
#else
__ai int32x4_t vqdmull_high_n_s16(int16x8_t __p0, int16_t __p1) {
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int32x4_t __ret;
  __ret = __noswap_vqdmull_n_s16(__noswap_vget_high_s16(__rev0), __p1);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
#define vqdmulls_lane_s32(__p0_158, __p1_158, __p2_158) __extension__ ({ \
  int32_t __s0_158 = __p0_158; \
  int32x2_t __s1_158 = __p1_158; \
  int64_t __ret_158; \
  __ret_158 = vqdmulls_s32(__s0_158, vget_lane_s32(__s1_158, __p2_158)); \
  __ret_158; \
})
#else
#define vqdmulls_lane_s32(__p0_159, __p1_159, __p2_159) __extension__ ({ \
  int32_t __s0_159 = __p0_159; \
  int32x2_t __s1_159 = __p1_159; \
  int32x2_t __rev1_159;  __rev1_159 = __builtin_shufflevector(__s1_159, __s1_159, 1, 0); \
  int64_t __ret_159; \
  __ret_159 = __noswap_vqdmulls_s32(__s0_159, __noswap_vget_lane_s32(__rev1_159, __p2_159)); \
  __ret_159; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqdmullh_lane_s16(__p0_160, __p1_160, __p2_160) __extension__ ({ \
  int16_t __s0_160 = __p0_160; \
  int16x4_t __s1_160 = __p1_160; \
  int32_t __ret_160; \
  __ret_160 = vqdmullh_s16(__s0_160, vget_lane_s16(__s1_160, __p2_160)); \
  __ret_160; \
})
#else
#define vqdmullh_lane_s16(__p0_161, __p1_161, __p2_161) __extension__ ({ \
  int16_t __s0_161 = __p0_161; \
  int16x4_t __s1_161 = __p1_161; \
  int16x4_t __rev1_161;  __rev1_161 = __builtin_shufflevector(__s1_161, __s1_161, 3, 2, 1, 0); \
  int32_t __ret_161; \
  __ret_161 = __noswap_vqdmullh_s16(__s0_161, __noswap_vget_lane_s16(__rev1_161, __p2_161)); \
  __ret_161; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqdmulls_laneq_s32(__p0_162, __p1_162, __p2_162) __extension__ ({ \
  int32_t __s0_162 = __p0_162; \
  int32x4_t __s1_162 = __p1_162; \
  int64_t __ret_162; \
  __ret_162 = vqdmulls_s32(__s0_162, vgetq_lane_s32(__s1_162, __p2_162)); \
  __ret_162; \
})
#else
#define vqdmulls_laneq_s32(__p0_163, __p1_163, __p2_163) __extension__ ({ \
  int32_t __s0_163 = __p0_163; \
  int32x4_t __s1_163 = __p1_163; \
  int32x4_t __rev1_163;  __rev1_163 = __builtin_shufflevector(__s1_163, __s1_163, 3, 2, 1, 0); \
  int64_t __ret_163; \
  __ret_163 = __noswap_vqdmulls_s32(__s0_163, __noswap_vgetq_lane_s32(__rev1_163, __p2_163)); \
  __ret_163; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqdmullh_laneq_s16(__p0_164, __p1_164, __p2_164) __extension__ ({ \
  int16_t __s0_164 = __p0_164; \
  int16x8_t __s1_164 = __p1_164; \
  int32_t __ret_164; \
  __ret_164 = vqdmullh_s16(__s0_164, vgetq_lane_s16(__s1_164, __p2_164)); \
  __ret_164; \
})
#else
#define vqdmullh_laneq_s16(__p0_165, __p1_165, __p2_165) __extension__ ({ \
  int16_t __s0_165 = __p0_165; \
  int16x8_t __s1_165 = __p1_165; \
  int16x8_t __rev1_165;  __rev1_165 = __builtin_shufflevector(__s1_165, __s1_165, 7, 6, 5, 4, 3, 2, 1, 0); \
  int32_t __ret_165; \
  __ret_165 = __noswap_vqdmullh_s16(__s0_165, __noswap_vgetq_lane_s16(__rev1_165, __p2_165)); \
  __ret_165; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqdmull_laneq_s32(__p0, __p1, __p2) __extension__ ({ \
  int32x2_t __s0 = __p0; \
  int32x4_t __s1 = __p1; \
  int64x2_t __ret; \
  __ret = vqdmull_s32(__s0, __builtin_shufflevector(__s1, __s1, __p2, __p2)); \
  __ret; \
})
#else
#define vqdmull_laneq_s32(__p0, __p1, __p2) __extension__ ({ \
  int32x2_t __s0 = __p0; \
  int32x4_t __s1 = __p1; \
  int32x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  int32x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  int64x2_t __ret; \
  __ret = __noswap_vqdmull_s32(__rev0, __builtin_shufflevector(__rev1, __rev1, __p2, __p2)); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqdmull_laneq_s16(__p0, __p1, __p2) __extension__ ({ \
  int16x4_t __s0 = __p0; \
  int16x8_t __s1 = __p1; \
  int32x4_t __ret; \
  __ret = vqdmull_s16(__s0, __builtin_shufflevector(__s1, __s1, __p2, __p2, __p2, __p2)); \
  __ret; \
})
#else
#define vqdmull_laneq_s16(__p0, __p1, __p2) __extension__ ({ \
  int16x4_t __s0 = __p0; \
  int16x8_t __s1 = __p1; \
  int16x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  int16x8_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 7, 6, 5, 4, 3, 2, 1, 0); \
  int32x4_t __ret; \
  __ret = __noswap_vqdmull_s16(__rev0, __builtin_shufflevector(__rev1, __rev1, __p2, __p2, __p2, __p2)); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16_t vqmovns_s32(int32_t __p0) {
  int16_t __ret;
  __ret = (int16_t) __builtin_neon_vqmovns_s32(__p0);
  return __ret;
}
#else
__ai int16_t vqmovns_s32(int32_t __p0) {
  int16_t __ret;
  __ret = (int16_t) __builtin_neon_vqmovns_s32(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32_t vqmovnd_s64(int64_t __p0) {
  int32_t __ret;
  __ret = (int32_t) __builtin_neon_vqmovnd_s64(__p0);
  return __ret;
}
#else
__ai int32_t vqmovnd_s64(int64_t __p0) {
  int32_t __ret;
  __ret = (int32_t) __builtin_neon_vqmovnd_s64(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8_t vqmovnh_s16(int16_t __p0) {
  int8_t __ret;
  __ret = (int8_t) __builtin_neon_vqmovnh_s16(__p0);
  return __ret;
}
#else
__ai int8_t vqmovnh_s16(int16_t __p0) {
  int8_t __ret;
  __ret = (int8_t) __builtin_neon_vqmovnh_s16(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16_t vqmovns_u32(uint32_t __p0) {
  uint16_t __ret;
  __ret = (uint16_t) __builtin_neon_vqmovns_u32(__p0);
  return __ret;
}
#else
__ai uint16_t vqmovns_u32(uint32_t __p0) {
  uint16_t __ret;
  __ret = (uint16_t) __builtin_neon_vqmovns_u32(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32_t vqmovnd_u64(uint64_t __p0) {
  uint32_t __ret;
  __ret = (uint32_t) __builtin_neon_vqmovnd_u64(__p0);
  return __ret;
}
#else
__ai uint32_t vqmovnd_u64(uint64_t __p0) {
  uint32_t __ret;
  __ret = (uint32_t) __builtin_neon_vqmovnd_u64(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8_t vqmovnh_u16(uint16_t __p0) {
  uint8_t __ret;
  __ret = (uint8_t) __builtin_neon_vqmovnh_u16(__p0);
  return __ret;
}
#else
__ai uint8_t vqmovnh_u16(uint16_t __p0) {
  uint8_t __ret;
  __ret = (uint8_t) __builtin_neon_vqmovnh_u16(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x8_t vqmovn_high_u32(uint16x4_t __p0, uint32x4_t __p1) {
  uint16x8_t __ret;
  __ret = vcombine_u16(__p0, vqmovn_u32(__p1));
  return __ret;
}
#else
__ai uint16x8_t vqmovn_high_u32(uint16x4_t __p0, uint32x4_t __p1) {
  uint16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint16x8_t __ret;
  __ret = __noswap_vcombine_u16(__rev0, __noswap_vqmovn_u32(__rev1));
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vqmovn_high_u64(uint32x2_t __p0, uint64x2_t __p1) {
  uint32x4_t __ret;
  __ret = vcombine_u32(__p0, vqmovn_u64(__p1));
  return __ret;
}
#else
__ai uint32x4_t vqmovn_high_u64(uint32x2_t __p0, uint64x2_t __p1) {
  uint32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  uint32x4_t __ret;
  __ret = __noswap_vcombine_u32(__rev0, __noswap_vqmovn_u64(__rev1));
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x16_t vqmovn_high_u16(uint8x8_t __p0, uint16x8_t __p1) {
  uint8x16_t __ret;
  __ret = vcombine_u8(__p0, vqmovn_u16(__p1));
  return __ret;
}
#else
__ai uint8x16_t vqmovn_high_u16(uint8x8_t __p0, uint16x8_t __p1) {
  uint8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __ret;
  __ret = __noswap_vcombine_u8(__rev0, __noswap_vqmovn_u16(__rev1));
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x8_t vqmovn_high_s32(int16x4_t __p0, int32x4_t __p1) {
  int16x8_t __ret;
  __ret = vcombine_s16(__p0, vqmovn_s32(__p1));
  return __ret;
}
#else
__ai int16x8_t vqmovn_high_s32(int16x4_t __p0, int32x4_t __p1) {
  int16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int16x8_t __ret;
  __ret = __noswap_vcombine_s16(__rev0, __noswap_vqmovn_s32(__rev1));
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vqmovn_high_s64(int32x2_t __p0, int64x2_t __p1) {
  int32x4_t __ret;
  __ret = vcombine_s32(__p0, vqmovn_s64(__p1));
  return __ret;
}
#else
__ai int32x4_t vqmovn_high_s64(int32x2_t __p0, int64x2_t __p1) {
  int32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  int32x4_t __ret;
  __ret = __noswap_vcombine_s32(__rev0, __noswap_vqmovn_s64(__rev1));
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x16_t vqmovn_high_s16(int8x8_t __p0, int16x8_t __p1) {
  int8x16_t __ret;
  __ret = vcombine_s8(__p0, vqmovn_s16(__p1));
  return __ret;
}
#else
__ai int8x16_t vqmovn_high_s16(int8x8_t __p0, int16x8_t __p1) {
  int8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __ret;
  __ret = __noswap_vcombine_s8(__rev0, __noswap_vqmovn_s16(__rev1));
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16_t vqmovuns_s32(int32_t __p0) {
  int16_t __ret;
  __ret = (int16_t) __builtin_neon_vqmovuns_s32(__p0);
  return __ret;
}
#else
__ai int16_t vqmovuns_s32(int32_t __p0) {
  int16_t __ret;
  __ret = (int16_t) __builtin_neon_vqmovuns_s32(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32_t vqmovund_s64(int64_t __p0) {
  int32_t __ret;
  __ret = (int32_t) __builtin_neon_vqmovund_s64(__p0);
  return __ret;
}
#else
__ai int32_t vqmovund_s64(int64_t __p0) {
  int32_t __ret;
  __ret = (int32_t) __builtin_neon_vqmovund_s64(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8_t vqmovunh_s16(int16_t __p0) {
  int8_t __ret;
  __ret = (int8_t) __builtin_neon_vqmovunh_s16(__p0);
  return __ret;
}
#else
__ai int8_t vqmovunh_s16(int16_t __p0) {
  int8_t __ret;
  __ret = (int8_t) __builtin_neon_vqmovunh_s16(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x8_t vqmovun_high_s32(int16x4_t __p0, int32x4_t __p1) {
  int16x8_t __ret;
  __ret = vcombine_u16((uint16x4_t)(__p0), vqmovun_s32(__p1));
  return __ret;
}
#else
__ai int16x8_t vqmovun_high_s32(int16x4_t __p0, int32x4_t __p1) {
  int16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int16x8_t __ret;
  __ret = __noswap_vcombine_u16((uint16x4_t)(__rev0), __noswap_vqmovun_s32(__rev1));
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vqmovun_high_s64(int32x2_t __p0, int64x2_t __p1) {
  int32x4_t __ret;
  __ret = vcombine_u32((uint32x2_t)(__p0), vqmovun_s64(__p1));
  return __ret;
}
#else
__ai int32x4_t vqmovun_high_s64(int32x2_t __p0, int64x2_t __p1) {
  int32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  int32x4_t __ret;
  __ret = __noswap_vcombine_u32((uint32x2_t)(__rev0), __noswap_vqmovun_s64(__rev1));
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x16_t vqmovun_high_s16(int8x8_t __p0, int16x8_t __p1) {
  int8x16_t __ret;
  __ret = vcombine_u8((uint8x8_t)(__p0), vqmovun_s16(__p1));
  return __ret;
}
#else
__ai int8x16_t vqmovun_high_s16(int8x8_t __p0, int16x8_t __p1) {
  int8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __ret;
  __ret = __noswap_vcombine_u8((uint8x8_t)(__rev0), __noswap_vqmovun_s16(__rev1));
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x2_t vqnegq_s64(int64x2_t __p0) {
  int64x2_t __ret;
  __ret = (int64x2_t) __builtin_neon_vqnegq_v((int8x16_t)__p0, 35);
  return __ret;
}
#else
__ai int64x2_t vqnegq_s64(int64x2_t __p0) {
  int64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int64x2_t __ret;
  __ret = (int64x2_t) __builtin_neon_vqnegq_v((int8x16_t)__rev0, 35);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x1_t vqneg_s64(int64x1_t __p0) {
  int64x1_t __ret;
  __ret = (int64x1_t) __builtin_neon_vqneg_v((int8x8_t)__p0, 3);
  return __ret;
}
#else
__ai int64x1_t vqneg_s64(int64x1_t __p0) {
  int64x1_t __ret;
  __ret = (int64x1_t) __builtin_neon_vqneg_v((int8x8_t)__p0, 3);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8_t vqnegb_s8(int8_t __p0) {
  int8_t __ret;
  __ret = (int8_t) __builtin_neon_vqnegb_s8(__p0);
  return __ret;
}
#else
__ai int8_t vqnegb_s8(int8_t __p0) {
  int8_t __ret;
  __ret = (int8_t) __builtin_neon_vqnegb_s8(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32_t vqnegs_s32(int32_t __p0) {
  int32_t __ret;
  __ret = (int32_t) __builtin_neon_vqnegs_s32(__p0);
  return __ret;
}
#else
__ai int32_t vqnegs_s32(int32_t __p0) {
  int32_t __ret;
  __ret = (int32_t) __builtin_neon_vqnegs_s32(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64_t vqnegd_s64(int64_t __p0) {
  int64_t __ret;
  __ret = (int64_t) __builtin_neon_vqnegd_s64(__p0);
  return __ret;
}
#else
__ai int64_t vqnegd_s64(int64_t __p0) {
  int64_t __ret;
  __ret = (int64_t) __builtin_neon_vqnegd_s64(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16_t vqnegh_s16(int16_t __p0) {
  int16_t __ret;
  __ret = (int16_t) __builtin_neon_vqnegh_s16(__p0);
  return __ret;
}
#else
__ai int16_t vqnegh_s16(int16_t __p0) {
  int16_t __ret;
  __ret = (int16_t) __builtin_neon_vqnegh_s16(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32_t vqrdmulhs_s32(int32_t __p0, int32_t __p1) {
  int32_t __ret;
  __ret = (int32_t) __builtin_neon_vqrdmulhs_s32(__p0, __p1);
  return __ret;
}
#else
__ai int32_t vqrdmulhs_s32(int32_t __p0, int32_t __p1) {
  int32_t __ret;
  __ret = (int32_t) __builtin_neon_vqrdmulhs_s32(__p0, __p1);
  return __ret;
}
__ai int32_t __noswap_vqrdmulhs_s32(int32_t __p0, int32_t __p1) {
  int32_t __ret;
  __ret = (int32_t) __builtin_neon_vqrdmulhs_s32(__p0, __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16_t vqrdmulhh_s16(int16_t __p0, int16_t __p1) {
  int16_t __ret;
  __ret = (int16_t) __builtin_neon_vqrdmulhh_s16(__p0, __p1);
  return __ret;
}
#else
__ai int16_t vqrdmulhh_s16(int16_t __p0, int16_t __p1) {
  int16_t __ret;
  __ret = (int16_t) __builtin_neon_vqrdmulhh_s16(__p0, __p1);
  return __ret;
}
__ai int16_t __noswap_vqrdmulhh_s16(int16_t __p0, int16_t __p1) {
  int16_t __ret;
  __ret = (int16_t) __builtin_neon_vqrdmulhh_s16(__p0, __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
#define vqrdmulhs_lane_s32(__p0_166, __p1_166, __p2_166) __extension__ ({ \
  int32_t __s0_166 = __p0_166; \
  int32x2_t __s1_166 = __p1_166; \
  int32_t __ret_166; \
  __ret_166 = vqrdmulhs_s32(__s0_166, vget_lane_s32(__s1_166, __p2_166)); \
  __ret_166; \
})
#else
#define vqrdmulhs_lane_s32(__p0_167, __p1_167, __p2_167) __extension__ ({ \
  int32_t __s0_167 = __p0_167; \
  int32x2_t __s1_167 = __p1_167; \
  int32x2_t __rev1_167;  __rev1_167 = __builtin_shufflevector(__s1_167, __s1_167, 1, 0); \
  int32_t __ret_167; \
  __ret_167 = __noswap_vqrdmulhs_s32(__s0_167, __noswap_vget_lane_s32(__rev1_167, __p2_167)); \
  __ret_167; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqrdmulhh_lane_s16(__p0_168, __p1_168, __p2_168) __extension__ ({ \
  int16_t __s0_168 = __p0_168; \
  int16x4_t __s1_168 = __p1_168; \
  int16_t __ret_168; \
  __ret_168 = vqrdmulhh_s16(__s0_168, vget_lane_s16(__s1_168, __p2_168)); \
  __ret_168; \
})
#else
#define vqrdmulhh_lane_s16(__p0_169, __p1_169, __p2_169) __extension__ ({ \
  int16_t __s0_169 = __p0_169; \
  int16x4_t __s1_169 = __p1_169; \
  int16x4_t __rev1_169;  __rev1_169 = __builtin_shufflevector(__s1_169, __s1_169, 3, 2, 1, 0); \
  int16_t __ret_169; \
  __ret_169 = __noswap_vqrdmulhh_s16(__s0_169, __noswap_vget_lane_s16(__rev1_169, __p2_169)); \
  __ret_169; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqrdmulhs_laneq_s32(__p0_170, __p1_170, __p2_170) __extension__ ({ \
  int32_t __s0_170 = __p0_170; \
  int32x4_t __s1_170 = __p1_170; \
  int32_t __ret_170; \
  __ret_170 = vqrdmulhs_s32(__s0_170, vgetq_lane_s32(__s1_170, __p2_170)); \
  __ret_170; \
})
#else
#define vqrdmulhs_laneq_s32(__p0_171, __p1_171, __p2_171) __extension__ ({ \
  int32_t __s0_171 = __p0_171; \
  int32x4_t __s1_171 = __p1_171; \
  int32x4_t __rev1_171;  __rev1_171 = __builtin_shufflevector(__s1_171, __s1_171, 3, 2, 1, 0); \
  int32_t __ret_171; \
  __ret_171 = __noswap_vqrdmulhs_s32(__s0_171, __noswap_vgetq_lane_s32(__rev1_171, __p2_171)); \
  __ret_171; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqrdmulhh_laneq_s16(__p0_172, __p1_172, __p2_172) __extension__ ({ \
  int16_t __s0_172 = __p0_172; \
  int16x8_t __s1_172 = __p1_172; \
  int16_t __ret_172; \
  __ret_172 = vqrdmulhh_s16(__s0_172, vgetq_lane_s16(__s1_172, __p2_172)); \
  __ret_172; \
})
#else
#define vqrdmulhh_laneq_s16(__p0_173, __p1_173, __p2_173) __extension__ ({ \
  int16_t __s0_173 = __p0_173; \
  int16x8_t __s1_173 = __p1_173; \
  int16x8_t __rev1_173;  __rev1_173 = __builtin_shufflevector(__s1_173, __s1_173, 7, 6, 5, 4, 3, 2, 1, 0); \
  int16_t __ret_173; \
  __ret_173 = __noswap_vqrdmulhh_s16(__s0_173, __noswap_vgetq_lane_s16(__rev1_173, __p2_173)); \
  __ret_173; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqrdmulhq_laneq_s32(__p0, __p1, __p2) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  int32x4_t __s1 = __p1; \
  int32x4_t __ret; \
  __ret = vqrdmulhq_s32(__s0, __builtin_shufflevector(__s1, __s1, __p2, __p2, __p2, __p2)); \
  __ret; \
})
#else
#define vqrdmulhq_laneq_s32(__p0, __p1, __p2) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  int32x4_t __s1 = __p1; \
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  int32x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  int32x4_t __ret; \
  __ret = __noswap_vqrdmulhq_s32(__rev0, __builtin_shufflevector(__rev1, __rev1, __p2, __p2, __p2, __p2)); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqrdmulhq_laneq_s16(__p0, __p1, __p2) __extension__ ({ \
  int16x8_t __s0 = __p0; \
  int16x8_t __s1 = __p1; \
  int16x8_t __ret; \
  __ret = vqrdmulhq_s16(__s0, __builtin_shufflevector(__s1, __s1, __p2, __p2, __p2, __p2, __p2, __p2, __p2, __p2)); \
  __ret; \
})
#else
#define vqrdmulhq_laneq_s16(__p0, __p1, __p2) __extension__ ({ \
  int16x8_t __s0 = __p0; \
  int16x8_t __s1 = __p1; \
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 7, 6, 5, 4, 3, 2, 1, 0); \
  int16x8_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 7, 6, 5, 4, 3, 2, 1, 0); \
  int16x8_t __ret; \
  __ret = __noswap_vqrdmulhq_s16(__rev0, __builtin_shufflevector(__rev1, __rev1, __p2, __p2, __p2, __p2, __p2, __p2, __p2, __p2)); \
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqrdmulh_laneq_s32(__p0, __p1, __p2) __extension__ ({ \
  int32x2_t __s0 = __p0; \
  int32x4_t __s1 = __p1; \
  int32x2_t __ret; \
  __ret = vqrdmulh_s32(__s0, __builtin_shufflevector(__s1, __s1, __p2, __p2)); \
  __ret; \
})
#else
#define vqrdmulh_laneq_s32(__p0, __p1, __p2) __extension__ ({ \
  int32x2_t __s0 = __p0; \
  int32x4_t __s1 = __p1; \
  int32x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  int32x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  int32x2_t __ret; \
  __ret = __noswap_vqrdmulh_s32(__rev0, __builtin_shufflevector(__rev1, __rev1, __p2, __p2)); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqrdmulh_laneq_s16(__p0, __p1, __p2) __extension__ ({ \
  int16x4_t __s0 = __p0; \
  int16x8_t __s1 = __p1; \
  int16x4_t __ret; \
  __ret = vqrdmulh_s16(__s0, __builtin_shufflevector(__s1, __s1, __p2, __p2, __p2, __p2)); \
  __ret; \
})
#else
#define vqrdmulh_laneq_s16(__p0, __p1, __p2) __extension__ ({ \
  int16x4_t __s0 = __p0; \
  int16x8_t __s1 = __p1; \
  int16x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  int16x8_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 7, 6, 5, 4, 3, 2, 1, 0); \
  int16x4_t __ret; \
  __ret = __noswap_vqrdmulh_s16(__rev0, __builtin_shufflevector(__rev1, __rev1, __p2, __p2, __p2, __p2)); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8_t vqrshlb_u8(uint8_t __p0, uint8_t __p1) {
  uint8_t __ret;
  __ret = (uint8_t) __builtin_neon_vqrshlb_u8(__p0, __p1);
  return __ret;
}
#else
__ai uint8_t vqrshlb_u8(uint8_t __p0, uint8_t __p1) {
  uint8_t __ret;
  __ret = (uint8_t) __builtin_neon_vqrshlb_u8(__p0, __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32_t vqrshls_u32(uint32_t __p0, uint32_t __p1) {
  uint32_t __ret;
  __ret = (uint32_t) __builtin_neon_vqrshls_u32(__p0, __p1);
  return __ret;
}
#else
__ai uint32_t vqrshls_u32(uint32_t __p0, uint32_t __p1) {
  uint32_t __ret;
  __ret = (uint32_t) __builtin_neon_vqrshls_u32(__p0, __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64_t vqrshld_u64(uint64_t __p0, uint64_t __p1) {
  uint64_t __ret;
  __ret = (uint64_t) __builtin_neon_vqrshld_u64(__p0, __p1);
  return __ret;
}
#else
__ai uint64_t vqrshld_u64(uint64_t __p0, uint64_t __p1) {
  uint64_t __ret;
  __ret = (uint64_t) __builtin_neon_vqrshld_u64(__p0, __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16_t vqrshlh_u16(uint16_t __p0, uint16_t __p1) {
  uint16_t __ret;
  __ret = (uint16_t) __builtin_neon_vqrshlh_u16(__p0, __p1);
  return __ret;
}
#else
__ai uint16_t vqrshlh_u16(uint16_t __p0, uint16_t __p1) {
  uint16_t __ret;
  __ret = (uint16_t) __builtin_neon_vqrshlh_u16(__p0, __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8_t vqrshlb_s8(int8_t __p0, int8_t __p1) {
  int8_t __ret;
  __ret = (int8_t) __builtin_neon_vqrshlb_s8(__p0, __p1);
  return __ret;
}
#else
__ai int8_t vqrshlb_s8(int8_t __p0, int8_t __p1) {
  int8_t __ret;
  __ret = (int8_t) __builtin_neon_vqrshlb_s8(__p0, __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32_t vqrshls_s32(int32_t __p0, int32_t __p1) {
  int32_t __ret;
  __ret = (int32_t) __builtin_neon_vqrshls_s32(__p0, __p1);
  return __ret;
}
#else
__ai int32_t vqrshls_s32(int32_t __p0, int32_t __p1) {
  int32_t __ret;
  __ret = (int32_t) __builtin_neon_vqrshls_s32(__p0, __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64_t vqrshld_s64(int64_t __p0, int64_t __p1) {
  int64_t __ret;
  __ret = (int64_t) __builtin_neon_vqrshld_s64(__p0, __p1);
  return __ret;
}
#else
__ai int64_t vqrshld_s64(int64_t __p0, int64_t __p1) {
  int64_t __ret;
  __ret = (int64_t) __builtin_neon_vqrshld_s64(__p0, __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16_t vqrshlh_s16(int16_t __p0, int16_t __p1) {
  int16_t __ret;
  __ret = (int16_t) __builtin_neon_vqrshlh_s16(__p0, __p1);
  return __ret;
}
#else
__ai int16_t vqrshlh_s16(int16_t __p0, int16_t __p1) {
  int16_t __ret;
  __ret = (int16_t) __builtin_neon_vqrshlh_s16(__p0, __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
#define vqrshrn_high_n_u32(__p0_174, __p1_174, __p2_174) __extension__ ({ \
  uint16x4_t __s0_174 = __p0_174; \
  uint32x4_t __s1_174 = __p1_174; \
  uint16x8_t __ret_174; \
  __ret_174 = (uint16x8_t)(vcombine_u16((uint16x4_t)(__s0_174), (uint16x4_t)(vqrshrn_n_u32(__s1_174, __p2_174)))); \
  __ret_174; \
})
#else
#define vqrshrn_high_n_u32(__p0_175, __p1_175, __p2_175) __extension__ ({ \
  uint16x4_t __s0_175 = __p0_175; \
  uint32x4_t __s1_175 = __p1_175; \
  uint16x4_t __rev0_175;  __rev0_175 = __builtin_shufflevector(__s0_175, __s0_175, 3, 2, 1, 0); \
  uint32x4_t __rev1_175;  __rev1_175 = __builtin_shufflevector(__s1_175, __s1_175, 3, 2, 1, 0); \
  uint16x8_t __ret_175; \
  __ret_175 = (uint16x8_t)(__noswap_vcombine_u16((uint16x4_t)(__rev0_175), (uint16x4_t)(__noswap_vqrshrn_n_u32(__rev1_175, __p2_175)))); \
  __ret_175 = __builtin_shufflevector(__ret_175, __ret_175, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret_175; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqrshrn_high_n_u64(__p0_176, __p1_176, __p2_176) __extension__ ({ \
  uint32x2_t __s0_176 = __p0_176; \
  uint64x2_t __s1_176 = __p1_176; \
  uint32x4_t __ret_176; \
  __ret_176 = (uint32x4_t)(vcombine_u32((uint32x2_t)(__s0_176), (uint32x2_t)(vqrshrn_n_u64(__s1_176, __p2_176)))); \
  __ret_176; \
})
#else
#define vqrshrn_high_n_u64(__p0_177, __p1_177, __p2_177) __extension__ ({ \
  uint32x2_t __s0_177 = __p0_177; \
  uint64x2_t __s1_177 = __p1_177; \
  uint32x2_t __rev0_177;  __rev0_177 = __builtin_shufflevector(__s0_177, __s0_177, 1, 0); \
  uint64x2_t __rev1_177;  __rev1_177 = __builtin_shufflevector(__s1_177, __s1_177, 1, 0); \
  uint32x4_t __ret_177; \
  __ret_177 = (uint32x4_t)(__noswap_vcombine_u32((uint32x2_t)(__rev0_177), (uint32x2_t)(__noswap_vqrshrn_n_u64(__rev1_177, __p2_177)))); \
  __ret_177 = __builtin_shufflevector(__ret_177, __ret_177, 3, 2, 1, 0); \
  __ret_177; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqrshrn_high_n_u16(__p0_178, __p1_178, __p2_178) __extension__ ({ \
  uint8x8_t __s0_178 = __p0_178; \
  uint16x8_t __s1_178 = __p1_178; \
  uint8x16_t __ret_178; \
  __ret_178 = (uint8x16_t)(vcombine_u8((uint8x8_t)(__s0_178), (uint8x8_t)(vqrshrn_n_u16(__s1_178, __p2_178)))); \
  __ret_178; \
})
#else
#define vqrshrn_high_n_u16(__p0_179, __p1_179, __p2_179) __extension__ ({ \
  uint8x8_t __s0_179 = __p0_179; \
  uint16x8_t __s1_179 = __p1_179; \
  uint8x8_t __rev0_179;  __rev0_179 = __builtin_shufflevector(__s0_179, __s0_179, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint16x8_t __rev1_179;  __rev1_179 = __builtin_shufflevector(__s1_179, __s1_179, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint8x16_t __ret_179; \
  __ret_179 = (uint8x16_t)(__noswap_vcombine_u8((uint8x8_t)(__rev0_179), (uint8x8_t)(__noswap_vqrshrn_n_u16(__rev1_179, __p2_179)))); \
  __ret_179 = __builtin_shufflevector(__ret_179, __ret_179, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret_179; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqrshrn_high_n_s32(__p0_180, __p1_180, __p2_180) __extension__ ({ \
  int16x4_t __s0_180 = __p0_180; \
  int32x4_t __s1_180 = __p1_180; \
  int16x8_t __ret_180; \
  __ret_180 = (int16x8_t)(vcombine_s16((int16x4_t)(__s0_180), (int16x4_t)(vqrshrn_n_s32(__s1_180, __p2_180)))); \
  __ret_180; \
})
#else
#define vqrshrn_high_n_s32(__p0_181, __p1_181, __p2_181) __extension__ ({ \
  int16x4_t __s0_181 = __p0_181; \
  int32x4_t __s1_181 = __p1_181; \
  int16x4_t __rev0_181;  __rev0_181 = __builtin_shufflevector(__s0_181, __s0_181, 3, 2, 1, 0); \
  int32x4_t __rev1_181;  __rev1_181 = __builtin_shufflevector(__s1_181, __s1_181, 3, 2, 1, 0); \
  int16x8_t __ret_181; \
  __ret_181 = (int16x8_t)(__noswap_vcombine_s16((int16x4_t)(__rev0_181), (int16x4_t)(__noswap_vqrshrn_n_s32(__rev1_181, __p2_181)))); \
  __ret_181 = __builtin_shufflevector(__ret_181, __ret_181, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret_181; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqrshrn_high_n_s64(__p0_182, __p1_182, __p2_182) __extension__ ({ \
  int32x2_t __s0_182 = __p0_182; \
  int64x2_t __s1_182 = __p1_182; \
  int32x4_t __ret_182; \
  __ret_182 = (int32x4_t)(vcombine_s32((int32x2_t)(__s0_182), (int32x2_t)(vqrshrn_n_s64(__s1_182, __p2_182)))); \
  __ret_182; \
})
#else
#define vqrshrn_high_n_s64(__p0_183, __p1_183, __p2_183) __extension__ ({ \
  int32x2_t __s0_183 = __p0_183; \
  int64x2_t __s1_183 = __p1_183; \
  int32x2_t __rev0_183;  __rev0_183 = __builtin_shufflevector(__s0_183, __s0_183, 1, 0); \
  int64x2_t __rev1_183;  __rev1_183 = __builtin_shufflevector(__s1_183, __s1_183, 1, 0); \
  int32x4_t __ret_183; \
  __ret_183 = (int32x4_t)(__noswap_vcombine_s32((int32x2_t)(__rev0_183), (int32x2_t)(__noswap_vqrshrn_n_s64(__rev1_183, __p2_183)))); \
  __ret_183 = __builtin_shufflevector(__ret_183, __ret_183, 3, 2, 1, 0); \
  __ret_183; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqrshrn_high_n_s16(__p0_184, __p1_184, __p2_184) __extension__ ({ \
  int8x8_t __s0_184 = __p0_184; \
  int16x8_t __s1_184 = __p1_184; \
  int8x16_t __ret_184; \
  __ret_184 = (int8x16_t)(vcombine_s8((int8x8_t)(__s0_184), (int8x8_t)(vqrshrn_n_s16(__s1_184, __p2_184)))); \
  __ret_184; \
})
#else
#define vqrshrn_high_n_s16(__p0_185, __p1_185, __p2_185) __extension__ ({ \
  int8x8_t __s0_185 = __p0_185; \
  int16x8_t __s1_185 = __p1_185; \
  int8x8_t __rev0_185;  __rev0_185 = __builtin_shufflevector(__s0_185, __s0_185, 7, 6, 5, 4, 3, 2, 1, 0); \
  int16x8_t __rev1_185;  __rev1_185 = __builtin_shufflevector(__s1_185, __s1_185, 7, 6, 5, 4, 3, 2, 1, 0); \
  int8x16_t __ret_185; \
  __ret_185 = (int8x16_t)(__noswap_vcombine_s8((int8x8_t)(__rev0_185), (int8x8_t)(__noswap_vqrshrn_n_s16(__rev1_185, __p2_185)))); \
  __ret_185 = __builtin_shufflevector(__ret_185, __ret_185, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret_185; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqrshrns_n_u32(__p0, __p1) __extension__ ({ \
  uint32_t __s0 = __p0; \
  uint16_t __ret; \
  __ret = (uint16_t) __builtin_neon_vqrshrns_n_u32(__s0, __p1); \
  __ret; \
})
#else
#define vqrshrns_n_u32(__p0, __p1) __extension__ ({ \
  uint32_t __s0 = __p0; \
  uint16_t __ret; \
  __ret = (uint16_t) __builtin_neon_vqrshrns_n_u32(__s0, __p1); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqrshrnd_n_u64(__p0, __p1) __extension__ ({ \
  uint64_t __s0 = __p0; \
  uint32_t __ret; \
  __ret = (uint32_t) __builtin_neon_vqrshrnd_n_u64(__s0, __p1); \
  __ret; \
})
#else
#define vqrshrnd_n_u64(__p0, __p1) __extension__ ({ \
  uint64_t __s0 = __p0; \
  uint32_t __ret; \
  __ret = (uint32_t) __builtin_neon_vqrshrnd_n_u64(__s0, __p1); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqrshrnh_n_u16(__p0, __p1) __extension__ ({ \
  uint16_t __s0 = __p0; \
  uint8_t __ret; \
  __ret = (uint8_t) __builtin_neon_vqrshrnh_n_u16(__s0, __p1); \
  __ret; \
})
#else
#define vqrshrnh_n_u16(__p0, __p1) __extension__ ({ \
  uint16_t __s0 = __p0; \
  uint8_t __ret; \
  __ret = (uint8_t) __builtin_neon_vqrshrnh_n_u16(__s0, __p1); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqrshrns_n_s32(__p0, __p1) __extension__ ({ \
  int32_t __s0 = __p0; \
  int16_t __ret; \
  __ret = (int16_t) __builtin_neon_vqrshrns_n_s32(__s0, __p1); \
  __ret; \
})
#else
#define vqrshrns_n_s32(__p0, __p1) __extension__ ({ \
  int32_t __s0 = __p0; \
  int16_t __ret; \
  __ret = (int16_t) __builtin_neon_vqrshrns_n_s32(__s0, __p1); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqrshrnd_n_s64(__p0, __p1) __extension__ ({ \
  int64_t __s0 = __p0; \
  int32_t __ret; \
  __ret = (int32_t) __builtin_neon_vqrshrnd_n_s64(__s0, __p1); \
  __ret; \
})
#else
#define vqrshrnd_n_s64(__p0, __p1) __extension__ ({ \
  int64_t __s0 = __p0; \
  int32_t __ret; \
  __ret = (int32_t) __builtin_neon_vqrshrnd_n_s64(__s0, __p1); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqrshrnh_n_s16(__p0, __p1) __extension__ ({ \
  int16_t __s0 = __p0; \
  int8_t __ret; \
  __ret = (int8_t) __builtin_neon_vqrshrnh_n_s16(__s0, __p1); \
  __ret; \
})
#else
#define vqrshrnh_n_s16(__p0, __p1) __extension__ ({ \
  int16_t __s0 = __p0; \
  int8_t __ret; \
  __ret = (int8_t) __builtin_neon_vqrshrnh_n_s16(__s0, __p1); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqrshrun_high_n_s32(__p0_186, __p1_186, __p2_186) __extension__ ({ \
  int16x4_t __s0_186 = __p0_186; \
  int32x4_t __s1_186 = __p1_186; \
  int16x8_t __ret_186; \
  __ret_186 = (int16x8_t)(vcombine_s16((int16x4_t)(__s0_186), (int16x4_t)(vqrshrun_n_s32(__s1_186, __p2_186)))); \
  __ret_186; \
})
#else
#define vqrshrun_high_n_s32(__p0_187, __p1_187, __p2_187) __extension__ ({ \
  int16x4_t __s0_187 = __p0_187; \
  int32x4_t __s1_187 = __p1_187; \
  int16x4_t __rev0_187;  __rev0_187 = __builtin_shufflevector(__s0_187, __s0_187, 3, 2, 1, 0); \
  int32x4_t __rev1_187;  __rev1_187 = __builtin_shufflevector(__s1_187, __s1_187, 3, 2, 1, 0); \
  int16x8_t __ret_187; \
  __ret_187 = (int16x8_t)(__noswap_vcombine_s16((int16x4_t)(__rev0_187), (int16x4_t)(__noswap_vqrshrun_n_s32(__rev1_187, __p2_187)))); \
  __ret_187 = __builtin_shufflevector(__ret_187, __ret_187, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret_187; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqrshrun_high_n_s64(__p0_188, __p1_188, __p2_188) __extension__ ({ \
  int32x2_t __s0_188 = __p0_188; \
  int64x2_t __s1_188 = __p1_188; \
  int32x4_t __ret_188; \
  __ret_188 = (int32x4_t)(vcombine_s32((int32x2_t)(__s0_188), (int32x2_t)(vqrshrun_n_s64(__s1_188, __p2_188)))); \
  __ret_188; \
})
#else
#define vqrshrun_high_n_s64(__p0_189, __p1_189, __p2_189) __extension__ ({ \
  int32x2_t __s0_189 = __p0_189; \
  int64x2_t __s1_189 = __p1_189; \
  int32x2_t __rev0_189;  __rev0_189 = __builtin_shufflevector(__s0_189, __s0_189, 1, 0); \
  int64x2_t __rev1_189;  __rev1_189 = __builtin_shufflevector(__s1_189, __s1_189, 1, 0); \
  int32x4_t __ret_189; \
  __ret_189 = (int32x4_t)(__noswap_vcombine_s32((int32x2_t)(__rev0_189), (int32x2_t)(__noswap_vqrshrun_n_s64(__rev1_189, __p2_189)))); \
  __ret_189 = __builtin_shufflevector(__ret_189, __ret_189, 3, 2, 1, 0); \
  __ret_189; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqrshrun_high_n_s16(__p0_190, __p1_190, __p2_190) __extension__ ({ \
  int8x8_t __s0_190 = __p0_190; \
  int16x8_t __s1_190 = __p1_190; \
  int8x16_t __ret_190; \
  __ret_190 = (int8x16_t)(vcombine_s8((int8x8_t)(__s0_190), (int8x8_t)(vqrshrun_n_s16(__s1_190, __p2_190)))); \
  __ret_190; \
})
#else
#define vqrshrun_high_n_s16(__p0_191, __p1_191, __p2_191) __extension__ ({ \
  int8x8_t __s0_191 = __p0_191; \
  int16x8_t __s1_191 = __p1_191; \
  int8x8_t __rev0_191;  __rev0_191 = __builtin_shufflevector(__s0_191, __s0_191, 7, 6, 5, 4, 3, 2, 1, 0); \
  int16x8_t __rev1_191;  __rev1_191 = __builtin_shufflevector(__s1_191, __s1_191, 7, 6, 5, 4, 3, 2, 1, 0); \
  int8x16_t __ret_191; \
  __ret_191 = (int8x16_t)(__noswap_vcombine_s8((int8x8_t)(__rev0_191), (int8x8_t)(__noswap_vqrshrun_n_s16(__rev1_191, __p2_191)))); \
  __ret_191 = __builtin_shufflevector(__ret_191, __ret_191, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret_191; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqrshruns_n_s32(__p0, __p1) __extension__ ({ \
  int32_t __s0 = __p0; \
  int16_t __ret; \
  __ret = (int16_t) __builtin_neon_vqrshruns_n_s32(__s0, __p1); \
  __ret; \
})
#else
#define vqrshruns_n_s32(__p0, __p1) __extension__ ({ \
  int32_t __s0 = __p0; \
  int16_t __ret; \
  __ret = (int16_t) __builtin_neon_vqrshruns_n_s32(__s0, __p1); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqrshrund_n_s64(__p0, __p1) __extension__ ({ \
  int64_t __s0 = __p0; \
  int32_t __ret; \
  __ret = (int32_t) __builtin_neon_vqrshrund_n_s64(__s0, __p1); \
  __ret; \
})
#else
#define vqrshrund_n_s64(__p0, __p1) __extension__ ({ \
  int64_t __s0 = __p0; \
  int32_t __ret; \
  __ret = (int32_t) __builtin_neon_vqrshrund_n_s64(__s0, __p1); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqrshrunh_n_s16(__p0, __p1) __extension__ ({ \
  int16_t __s0 = __p0; \
  int8_t __ret; \
  __ret = (int8_t) __builtin_neon_vqrshrunh_n_s16(__s0, __p1); \
  __ret; \
})
#else
#define vqrshrunh_n_s16(__p0, __p1) __extension__ ({ \
  int16_t __s0 = __p0; \
  int8_t __ret; \
  __ret = (int8_t) __builtin_neon_vqrshrunh_n_s16(__s0, __p1); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8_t vqshlb_u8(uint8_t __p0, uint8_t __p1) {
  uint8_t __ret;
  __ret = (uint8_t) __builtin_neon_vqshlb_u8(__p0, __p1);
  return __ret;
}
#else
__ai uint8_t vqshlb_u8(uint8_t __p0, uint8_t __p1) {
  uint8_t __ret;
  __ret = (uint8_t) __builtin_neon_vqshlb_u8(__p0, __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32_t vqshls_u32(uint32_t __p0, uint32_t __p1) {
  uint32_t __ret;
  __ret = (uint32_t) __builtin_neon_vqshls_u32(__p0, __p1);
  return __ret;
}
#else
__ai uint32_t vqshls_u32(uint32_t __p0, uint32_t __p1) {
  uint32_t __ret;
  __ret = (uint32_t) __builtin_neon_vqshls_u32(__p0, __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64_t vqshld_u64(uint64_t __p0, uint64_t __p1) {
  uint64_t __ret;
  __ret = (uint64_t) __builtin_neon_vqshld_u64(__p0, __p1);
  return __ret;
}
#else
__ai uint64_t vqshld_u64(uint64_t __p0, uint64_t __p1) {
  uint64_t __ret;
  __ret = (uint64_t) __builtin_neon_vqshld_u64(__p0, __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16_t vqshlh_u16(uint16_t __p0, uint16_t __p1) {
  uint16_t __ret;
  __ret = (uint16_t) __builtin_neon_vqshlh_u16(__p0, __p1);
  return __ret;
}
#else
__ai uint16_t vqshlh_u16(uint16_t __p0, uint16_t __p1) {
  uint16_t __ret;
  __ret = (uint16_t) __builtin_neon_vqshlh_u16(__p0, __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8_t vqshlb_s8(int8_t __p0, int8_t __p1) {
  int8_t __ret;
  __ret = (int8_t) __builtin_neon_vqshlb_s8(__p0, __p1);
  return __ret;
}
#else
__ai int8_t vqshlb_s8(int8_t __p0, int8_t __p1) {
  int8_t __ret;
  __ret = (int8_t) __builtin_neon_vqshlb_s8(__p0, __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32_t vqshls_s32(int32_t __p0, int32_t __p1) {
  int32_t __ret;
  __ret = (int32_t) __builtin_neon_vqshls_s32(__p0, __p1);
  return __ret;
}
#else
__ai int32_t vqshls_s32(int32_t __p0, int32_t __p1) {
  int32_t __ret;
  __ret = (int32_t) __builtin_neon_vqshls_s32(__p0, __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64_t vqshld_s64(int64_t __p0, int64_t __p1) {
  int64_t __ret;
  __ret = (int64_t) __builtin_neon_vqshld_s64(__p0, __p1);
  return __ret;
}
#else
__ai int64_t vqshld_s64(int64_t __p0, int64_t __p1) {
  int64_t __ret;
  __ret = (int64_t) __builtin_neon_vqshld_s64(__p0, __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16_t vqshlh_s16(int16_t __p0, int16_t __p1) {
  int16_t __ret;
  __ret = (int16_t) __builtin_neon_vqshlh_s16(__p0, __p1);
  return __ret;
}
#else
__ai int16_t vqshlh_s16(int16_t __p0, int16_t __p1) {
  int16_t __ret;
  __ret = (int16_t) __builtin_neon_vqshlh_s16(__p0, __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
#define vqshlb_n_u8(__p0, __p1) __extension__ ({ \
  uint8_t __s0 = __p0; \
  uint8_t __ret; \
  __ret = (uint8_t) __builtin_neon_vqshlb_n_u8(__s0, __p1); \
  __ret; \
})
#else
#define vqshlb_n_u8(__p0, __p1) __extension__ ({ \
  uint8_t __s0 = __p0; \
  uint8_t __ret; \
  __ret = (uint8_t) __builtin_neon_vqshlb_n_u8(__s0, __p1); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqshls_n_u32(__p0, __p1) __extension__ ({ \
  uint32_t __s0 = __p0; \
  uint32_t __ret; \
  __ret = (uint32_t) __builtin_neon_vqshls_n_u32(__s0, __p1); \
  __ret; \
})
#else
#define vqshls_n_u32(__p0, __p1) __extension__ ({ \
  uint32_t __s0 = __p0; \
  uint32_t __ret; \
  __ret = (uint32_t) __builtin_neon_vqshls_n_u32(__s0, __p1); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqshld_n_u64(__p0, __p1) __extension__ ({ \
  uint64_t __s0 = __p0; \
  uint64_t __ret; \
  __ret = (uint64_t) __builtin_neon_vqshld_n_u64(__s0, __p1); \
  __ret; \
})
#else
#define vqshld_n_u64(__p0, __p1) __extension__ ({ \
  uint64_t __s0 = __p0; \
  uint64_t __ret; \
  __ret = (uint64_t) __builtin_neon_vqshld_n_u64(__s0, __p1); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqshlh_n_u16(__p0, __p1) __extension__ ({ \
  uint16_t __s0 = __p0; \
  uint16_t __ret; \
  __ret = (uint16_t) __builtin_neon_vqshlh_n_u16(__s0, __p1); \
  __ret; \
})
#else
#define vqshlh_n_u16(__p0, __p1) __extension__ ({ \
  uint16_t __s0 = __p0; \
  uint16_t __ret; \
  __ret = (uint16_t) __builtin_neon_vqshlh_n_u16(__s0, __p1); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqshlb_n_s8(__p0, __p1) __extension__ ({ \
  int8_t __s0 = __p0; \
  int8_t __ret; \
  __ret = (int8_t) __builtin_neon_vqshlb_n_s8(__s0, __p1); \
  __ret; \
})
#else
#define vqshlb_n_s8(__p0, __p1) __extension__ ({ \
  int8_t __s0 = __p0; \
  int8_t __ret; \
  __ret = (int8_t) __builtin_neon_vqshlb_n_s8(__s0, __p1); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqshls_n_s32(__p0, __p1) __extension__ ({ \
  int32_t __s0 = __p0; \
  int32_t __ret; \
  __ret = (int32_t) __builtin_neon_vqshls_n_s32(__s0, __p1); \
  __ret; \
})
#else
#define vqshls_n_s32(__p0, __p1) __extension__ ({ \
  int32_t __s0 = __p0; \
  int32_t __ret; \
  __ret = (int32_t) __builtin_neon_vqshls_n_s32(__s0, __p1); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqshld_n_s64(__p0, __p1) __extension__ ({ \
  int64_t __s0 = __p0; \
  int64_t __ret; \
  __ret = (int64_t) __builtin_neon_vqshld_n_s64(__s0, __p1); \
  __ret; \
})
#else
#define vqshld_n_s64(__p0, __p1) __extension__ ({ \
  int64_t __s0 = __p0; \
  int64_t __ret; \
  __ret = (int64_t) __builtin_neon_vqshld_n_s64(__s0, __p1); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqshlh_n_s16(__p0, __p1) __extension__ ({ \
  int16_t __s0 = __p0; \
  int16_t __ret; \
  __ret = (int16_t) __builtin_neon_vqshlh_n_s16(__s0, __p1); \
  __ret; \
})
#else
#define vqshlh_n_s16(__p0, __p1) __extension__ ({ \
  int16_t __s0 = __p0; \
  int16_t __ret; \
  __ret = (int16_t) __builtin_neon_vqshlh_n_s16(__s0, __p1); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqshlub_n_s8(__p0, __p1) __extension__ ({ \
  int8_t __s0 = __p0; \
  int8_t __ret; \
  __ret = (int8_t) __builtin_neon_vqshlub_n_s8(__s0, __p1); \
  __ret; \
})
#else
#define vqshlub_n_s8(__p0, __p1) __extension__ ({ \
  int8_t __s0 = __p0; \
  int8_t __ret; \
  __ret = (int8_t) __builtin_neon_vqshlub_n_s8(__s0, __p1); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqshlus_n_s32(__p0, __p1) __extension__ ({ \
  int32_t __s0 = __p0; \
  int32_t __ret; \
  __ret = (int32_t) __builtin_neon_vqshlus_n_s32(__s0, __p1); \
  __ret; \
})
#else
#define vqshlus_n_s32(__p0, __p1) __extension__ ({ \
  int32_t __s0 = __p0; \
  int32_t __ret; \
  __ret = (int32_t) __builtin_neon_vqshlus_n_s32(__s0, __p1); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqshlud_n_s64(__p0, __p1) __extension__ ({ \
  int64_t __s0 = __p0; \
  int64_t __ret; \
  __ret = (int64_t) __builtin_neon_vqshlud_n_s64(__s0, __p1); \
  __ret; \
})
#else
#define vqshlud_n_s64(__p0, __p1) __extension__ ({ \
  int64_t __s0 = __p0; \
  int64_t __ret; \
  __ret = (int64_t) __builtin_neon_vqshlud_n_s64(__s0, __p1); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqshluh_n_s16(__p0, __p1) __extension__ ({ \
  int16_t __s0 = __p0; \
  int16_t __ret; \
  __ret = (int16_t) __builtin_neon_vqshluh_n_s16(__s0, __p1); \
  __ret; \
})
#else
#define vqshluh_n_s16(__p0, __p1) __extension__ ({ \
  int16_t __s0 = __p0; \
  int16_t __ret; \
  __ret = (int16_t) __builtin_neon_vqshluh_n_s16(__s0, __p1); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqshrn_high_n_u32(__p0_192, __p1_192, __p2_192) __extension__ ({ \
  uint16x4_t __s0_192 = __p0_192; \
  uint32x4_t __s1_192 = __p1_192; \
  uint16x8_t __ret_192; \
  __ret_192 = (uint16x8_t)(vcombine_u16((uint16x4_t)(__s0_192), (uint16x4_t)(vqshrn_n_u32(__s1_192, __p2_192)))); \
  __ret_192; \
})
#else
#define vqshrn_high_n_u32(__p0_193, __p1_193, __p2_193) __extension__ ({ \
  uint16x4_t __s0_193 = __p0_193; \
  uint32x4_t __s1_193 = __p1_193; \
  uint16x4_t __rev0_193;  __rev0_193 = __builtin_shufflevector(__s0_193, __s0_193, 3, 2, 1, 0); \
  uint32x4_t __rev1_193;  __rev1_193 = __builtin_shufflevector(__s1_193, __s1_193, 3, 2, 1, 0); \
  uint16x8_t __ret_193; \
  __ret_193 = (uint16x8_t)(__noswap_vcombine_u16((uint16x4_t)(__rev0_193), (uint16x4_t)(__noswap_vqshrn_n_u32(__rev1_193, __p2_193)))); \
  __ret_193 = __builtin_shufflevector(__ret_193, __ret_193, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret_193; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqshrn_high_n_u64(__p0_194, __p1_194, __p2_194) __extension__ ({ \
  uint32x2_t __s0_194 = __p0_194; \
  uint64x2_t __s1_194 = __p1_194; \
  uint32x4_t __ret_194; \
  __ret_194 = (uint32x4_t)(vcombine_u32((uint32x2_t)(__s0_194), (uint32x2_t)(vqshrn_n_u64(__s1_194, __p2_194)))); \
  __ret_194; \
})
#else
#define vqshrn_high_n_u64(__p0_195, __p1_195, __p2_195) __extension__ ({ \
  uint32x2_t __s0_195 = __p0_195; \
  uint64x2_t __s1_195 = __p1_195; \
  uint32x2_t __rev0_195;  __rev0_195 = __builtin_shufflevector(__s0_195, __s0_195, 1, 0); \
  uint64x2_t __rev1_195;  __rev1_195 = __builtin_shufflevector(__s1_195, __s1_195, 1, 0); \
  uint32x4_t __ret_195; \
  __ret_195 = (uint32x4_t)(__noswap_vcombine_u32((uint32x2_t)(__rev0_195), (uint32x2_t)(__noswap_vqshrn_n_u64(__rev1_195, __p2_195)))); \
  __ret_195 = __builtin_shufflevector(__ret_195, __ret_195, 3, 2, 1, 0); \
  __ret_195; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqshrn_high_n_u16(__p0_196, __p1_196, __p2_196) __extension__ ({ \
  uint8x8_t __s0_196 = __p0_196; \
  uint16x8_t __s1_196 = __p1_196; \
  uint8x16_t __ret_196; \
  __ret_196 = (uint8x16_t)(vcombine_u8((uint8x8_t)(__s0_196), (uint8x8_t)(vqshrn_n_u16(__s1_196, __p2_196)))); \
  __ret_196; \
})
#else
#define vqshrn_high_n_u16(__p0_197, __p1_197, __p2_197) __extension__ ({ \
  uint8x8_t __s0_197 = __p0_197; \
  uint16x8_t __s1_197 = __p1_197; \
  uint8x8_t __rev0_197;  __rev0_197 = __builtin_shufflevector(__s0_197, __s0_197, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint16x8_t __rev1_197;  __rev1_197 = __builtin_shufflevector(__s1_197, __s1_197, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint8x16_t __ret_197; \
  __ret_197 = (uint8x16_t)(__noswap_vcombine_u8((uint8x8_t)(__rev0_197), (uint8x8_t)(__noswap_vqshrn_n_u16(__rev1_197, __p2_197)))); \
  __ret_197 = __builtin_shufflevector(__ret_197, __ret_197, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret_197; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqshrn_high_n_s32(__p0_198, __p1_198, __p2_198) __extension__ ({ \
  int16x4_t __s0_198 = __p0_198; \
  int32x4_t __s1_198 = __p1_198; \
  int16x8_t __ret_198; \
  __ret_198 = (int16x8_t)(vcombine_s16((int16x4_t)(__s0_198), (int16x4_t)(vqshrn_n_s32(__s1_198, __p2_198)))); \
  __ret_198; \
})
#else
#define vqshrn_high_n_s32(__p0_199, __p1_199, __p2_199) __extension__ ({ \
  int16x4_t __s0_199 = __p0_199; \
  int32x4_t __s1_199 = __p1_199; \
  int16x4_t __rev0_199;  __rev0_199 = __builtin_shufflevector(__s0_199, __s0_199, 3, 2, 1, 0); \
  int32x4_t __rev1_199;  __rev1_199 = __builtin_shufflevector(__s1_199, __s1_199, 3, 2, 1, 0); \
  int16x8_t __ret_199; \
  __ret_199 = (int16x8_t)(__noswap_vcombine_s16((int16x4_t)(__rev0_199), (int16x4_t)(__noswap_vqshrn_n_s32(__rev1_199, __p2_199)))); \
  __ret_199 = __builtin_shufflevector(__ret_199, __ret_199, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret_199; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqshrn_high_n_s64(__p0_200, __p1_200, __p2_200) __extension__ ({ \
  int32x2_t __s0_200 = __p0_200; \
  int64x2_t __s1_200 = __p1_200; \
  int32x4_t __ret_200; \
  __ret_200 = (int32x4_t)(vcombine_s32((int32x2_t)(__s0_200), (int32x2_t)(vqshrn_n_s64(__s1_200, __p2_200)))); \
  __ret_200; \
})
#else
#define vqshrn_high_n_s64(__p0_201, __p1_201, __p2_201) __extension__ ({ \
  int32x2_t __s0_201 = __p0_201; \
  int64x2_t __s1_201 = __p1_201; \
  int32x2_t __rev0_201;  __rev0_201 = __builtin_shufflevector(__s0_201, __s0_201, 1, 0); \
  int64x2_t __rev1_201;  __rev1_201 = __builtin_shufflevector(__s1_201, __s1_201, 1, 0); \
  int32x4_t __ret_201; \
  __ret_201 = (int32x4_t)(__noswap_vcombine_s32((int32x2_t)(__rev0_201), (int32x2_t)(__noswap_vqshrn_n_s64(__rev1_201, __p2_201)))); \
  __ret_201 = __builtin_shufflevector(__ret_201, __ret_201, 3, 2, 1, 0); \
  __ret_201; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqshrn_high_n_s16(__p0_202, __p1_202, __p2_202) __extension__ ({ \
  int8x8_t __s0_202 = __p0_202; \
  int16x8_t __s1_202 = __p1_202; \
  int8x16_t __ret_202; \
  __ret_202 = (int8x16_t)(vcombine_s8((int8x8_t)(__s0_202), (int8x8_t)(vqshrn_n_s16(__s1_202, __p2_202)))); \
  __ret_202; \
})
#else
#define vqshrn_high_n_s16(__p0_203, __p1_203, __p2_203) __extension__ ({ \
  int8x8_t __s0_203 = __p0_203; \
  int16x8_t __s1_203 = __p1_203; \
  int8x8_t __rev0_203;  __rev0_203 = __builtin_shufflevector(__s0_203, __s0_203, 7, 6, 5, 4, 3, 2, 1, 0); \
  int16x8_t __rev1_203;  __rev1_203 = __builtin_shufflevector(__s1_203, __s1_203, 7, 6, 5, 4, 3, 2, 1, 0); \
  int8x16_t __ret_203; \
  __ret_203 = (int8x16_t)(__noswap_vcombine_s8((int8x8_t)(__rev0_203), (int8x8_t)(__noswap_vqshrn_n_s16(__rev1_203, __p2_203)))); \
  __ret_203 = __builtin_shufflevector(__ret_203, __ret_203, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret_203; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqshrns_n_u32(__p0, __p1) __extension__ ({ \
  uint32_t __s0 = __p0; \
  uint16_t __ret; \
  __ret = (uint16_t) __builtin_neon_vqshrns_n_u32(__s0, __p1); \
  __ret; \
})
#else
#define vqshrns_n_u32(__p0, __p1) __extension__ ({ \
  uint32_t __s0 = __p0; \
  uint16_t __ret; \
  __ret = (uint16_t) __builtin_neon_vqshrns_n_u32(__s0, __p1); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqshrnd_n_u64(__p0, __p1) __extension__ ({ \
  uint64_t __s0 = __p0; \
  uint32_t __ret; \
  __ret = (uint32_t) __builtin_neon_vqshrnd_n_u64(__s0, __p1); \
  __ret; \
})
#else
#define vqshrnd_n_u64(__p0, __p1) __extension__ ({ \
  uint64_t __s0 = __p0; \
  uint32_t __ret; \
  __ret = (uint32_t) __builtin_neon_vqshrnd_n_u64(__s0, __p1); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqshrnh_n_u16(__p0, __p1) __extension__ ({ \
  uint16_t __s0 = __p0; \
  uint8_t __ret; \
  __ret = (uint8_t) __builtin_neon_vqshrnh_n_u16(__s0, __p1); \
  __ret; \
})
#else
#define vqshrnh_n_u16(__p0, __p1) __extension__ ({ \
  uint16_t __s0 = __p0; \
  uint8_t __ret; \
  __ret = (uint8_t) __builtin_neon_vqshrnh_n_u16(__s0, __p1); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqshrns_n_s32(__p0, __p1) __extension__ ({ \
  int32_t __s0 = __p0; \
  int16_t __ret; \
  __ret = (int16_t) __builtin_neon_vqshrns_n_s32(__s0, __p1); \
  __ret; \
})
#else
#define vqshrns_n_s32(__p0, __p1) __extension__ ({ \
  int32_t __s0 = __p0; \
  int16_t __ret; \
  __ret = (int16_t) __builtin_neon_vqshrns_n_s32(__s0, __p1); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqshrnd_n_s64(__p0, __p1) __extension__ ({ \
  int64_t __s0 = __p0; \
  int32_t __ret; \
  __ret = (int32_t) __builtin_neon_vqshrnd_n_s64(__s0, __p1); \
  __ret; \
})
#else
#define vqshrnd_n_s64(__p0, __p1) __extension__ ({ \
  int64_t __s0 = __p0; \
  int32_t __ret; \
  __ret = (int32_t) __builtin_neon_vqshrnd_n_s64(__s0, __p1); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqshrnh_n_s16(__p0, __p1) __extension__ ({ \
  int16_t __s0 = __p0; \
  int8_t __ret; \
  __ret = (int8_t) __builtin_neon_vqshrnh_n_s16(__s0, __p1); \
  __ret; \
})
#else
#define vqshrnh_n_s16(__p0, __p1) __extension__ ({ \
  int16_t __s0 = __p0; \
  int8_t __ret; \
  __ret = (int8_t) __builtin_neon_vqshrnh_n_s16(__s0, __p1); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqshrun_high_n_s32(__p0_204, __p1_204, __p2_204) __extension__ ({ \
  int16x4_t __s0_204 = __p0_204; \
  int32x4_t __s1_204 = __p1_204; \
  int16x8_t __ret_204; \
  __ret_204 = (int16x8_t)(vcombine_s16((int16x4_t)(__s0_204), (int16x4_t)(vqshrun_n_s32(__s1_204, __p2_204)))); \
  __ret_204; \
})
#else
#define vqshrun_high_n_s32(__p0_205, __p1_205, __p2_205) __extension__ ({ \
  int16x4_t __s0_205 = __p0_205; \
  int32x4_t __s1_205 = __p1_205; \
  int16x4_t __rev0_205;  __rev0_205 = __builtin_shufflevector(__s0_205, __s0_205, 3, 2, 1, 0); \
  int32x4_t __rev1_205;  __rev1_205 = __builtin_shufflevector(__s1_205, __s1_205, 3, 2, 1, 0); \
  int16x8_t __ret_205; \
  __ret_205 = (int16x8_t)(__noswap_vcombine_s16((int16x4_t)(__rev0_205), (int16x4_t)(__noswap_vqshrun_n_s32(__rev1_205, __p2_205)))); \
  __ret_205 = __builtin_shufflevector(__ret_205, __ret_205, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret_205; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqshrun_high_n_s64(__p0_206, __p1_206, __p2_206) __extension__ ({ \
  int32x2_t __s0_206 = __p0_206; \
  int64x2_t __s1_206 = __p1_206; \
  int32x4_t __ret_206; \
  __ret_206 = (int32x4_t)(vcombine_s32((int32x2_t)(__s0_206), (int32x2_t)(vqshrun_n_s64(__s1_206, __p2_206)))); \
  __ret_206; \
})
#else
#define vqshrun_high_n_s64(__p0_207, __p1_207, __p2_207) __extension__ ({ \
  int32x2_t __s0_207 = __p0_207; \
  int64x2_t __s1_207 = __p1_207; \
  int32x2_t __rev0_207;  __rev0_207 = __builtin_shufflevector(__s0_207, __s0_207, 1, 0); \
  int64x2_t __rev1_207;  __rev1_207 = __builtin_shufflevector(__s1_207, __s1_207, 1, 0); \
  int32x4_t __ret_207; \
  __ret_207 = (int32x4_t)(__noswap_vcombine_s32((int32x2_t)(__rev0_207), (int32x2_t)(__noswap_vqshrun_n_s64(__rev1_207, __p2_207)))); \
  __ret_207 = __builtin_shufflevector(__ret_207, __ret_207, 3, 2, 1, 0); \
  __ret_207; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqshrun_high_n_s16(__p0_208, __p1_208, __p2_208) __extension__ ({ \
  int8x8_t __s0_208 = __p0_208; \
  int16x8_t __s1_208 = __p1_208; \
  int8x16_t __ret_208; \
  __ret_208 = (int8x16_t)(vcombine_s8((int8x8_t)(__s0_208), (int8x8_t)(vqshrun_n_s16(__s1_208, __p2_208)))); \
  __ret_208; \
})
#else
#define vqshrun_high_n_s16(__p0_209, __p1_209, __p2_209) __extension__ ({ \
  int8x8_t __s0_209 = __p0_209; \
  int16x8_t __s1_209 = __p1_209; \
  int8x8_t __rev0_209;  __rev0_209 = __builtin_shufflevector(__s0_209, __s0_209, 7, 6, 5, 4, 3, 2, 1, 0); \
  int16x8_t __rev1_209;  __rev1_209 = __builtin_shufflevector(__s1_209, __s1_209, 7, 6, 5, 4, 3, 2, 1, 0); \
  int8x16_t __ret_209; \
  __ret_209 = (int8x16_t)(__noswap_vcombine_s8((int8x8_t)(__rev0_209), (int8x8_t)(__noswap_vqshrun_n_s16(__rev1_209, __p2_209)))); \
  __ret_209 = __builtin_shufflevector(__ret_209, __ret_209, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret_209; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqshruns_n_s32(__p0, __p1) __extension__ ({ \
  int32_t __s0 = __p0; \
  int16_t __ret; \
  __ret = (int16_t) __builtin_neon_vqshruns_n_s32(__s0, __p1); \
  __ret; \
})
#else
#define vqshruns_n_s32(__p0, __p1) __extension__ ({ \
  int32_t __s0 = __p0; \
  int16_t __ret; \
  __ret = (int16_t) __builtin_neon_vqshruns_n_s32(__s0, __p1); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqshrund_n_s64(__p0, __p1) __extension__ ({ \
  int64_t __s0 = __p0; \
  int32_t __ret; \
  __ret = (int32_t) __builtin_neon_vqshrund_n_s64(__s0, __p1); \
  __ret; \
})
#else
#define vqshrund_n_s64(__p0, __p1) __extension__ ({ \
  int64_t __s0 = __p0; \
  int32_t __ret; \
  __ret = (int32_t) __builtin_neon_vqshrund_n_s64(__s0, __p1); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqshrunh_n_s16(__p0, __p1) __extension__ ({ \
  int16_t __s0 = __p0; \
  int8_t __ret; \
  __ret = (int8_t) __builtin_neon_vqshrunh_n_s16(__s0, __p1); \
  __ret; \
})
#else
#define vqshrunh_n_s16(__p0, __p1) __extension__ ({ \
  int16_t __s0 = __p0; \
  int8_t __ret; \
  __ret = (int8_t) __builtin_neon_vqshrunh_n_s16(__s0, __p1); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8_t vqsubb_u8(uint8_t __p0, uint8_t __p1) {
  uint8_t __ret;
  __ret = (uint8_t) __builtin_neon_vqsubb_u8(__p0, __p1);
  return __ret;
}
#else
__ai uint8_t vqsubb_u8(uint8_t __p0, uint8_t __p1) {
  uint8_t __ret;
  __ret = (uint8_t) __builtin_neon_vqsubb_u8(__p0, __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32_t vqsubs_u32(uint32_t __p0, uint32_t __p1) {
  uint32_t __ret;
  __ret = (uint32_t) __builtin_neon_vqsubs_u32(__p0, __p1);
  return __ret;
}
#else
__ai uint32_t vqsubs_u32(uint32_t __p0, uint32_t __p1) {
  uint32_t __ret;
  __ret = (uint32_t) __builtin_neon_vqsubs_u32(__p0, __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64_t vqsubd_u64(uint64_t __p0, uint64_t __p1) {
  uint64_t __ret;
  __ret = (uint64_t) __builtin_neon_vqsubd_u64(__p0, __p1);
  return __ret;
}
#else
__ai uint64_t vqsubd_u64(uint64_t __p0, uint64_t __p1) {
  uint64_t __ret;
  __ret = (uint64_t) __builtin_neon_vqsubd_u64(__p0, __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16_t vqsubh_u16(uint16_t __p0, uint16_t __p1) {
  uint16_t __ret;
  __ret = (uint16_t) __builtin_neon_vqsubh_u16(__p0, __p1);
  return __ret;
}
#else
__ai uint16_t vqsubh_u16(uint16_t __p0, uint16_t __p1) {
  uint16_t __ret;
  __ret = (uint16_t) __builtin_neon_vqsubh_u16(__p0, __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8_t vqsubb_s8(int8_t __p0, int8_t __p1) {
  int8_t __ret;
  __ret = (int8_t) __builtin_neon_vqsubb_s8(__p0, __p1);
  return __ret;
}
#else
__ai int8_t vqsubb_s8(int8_t __p0, int8_t __p1) {
  int8_t __ret;
  __ret = (int8_t) __builtin_neon_vqsubb_s8(__p0, __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32_t vqsubs_s32(int32_t __p0, int32_t __p1) {
  int32_t __ret;
  __ret = (int32_t) __builtin_neon_vqsubs_s32(__p0, __p1);
  return __ret;
}
#else
__ai int32_t vqsubs_s32(int32_t __p0, int32_t __p1) {
  int32_t __ret;
  __ret = (int32_t) __builtin_neon_vqsubs_s32(__p0, __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64_t vqsubd_s64(int64_t __p0, int64_t __p1) {
  int64_t __ret;
  __ret = (int64_t) __builtin_neon_vqsubd_s64(__p0, __p1);
  return __ret;
}
#else
__ai int64_t vqsubd_s64(int64_t __p0, int64_t __p1) {
  int64_t __ret;
  __ret = (int64_t) __builtin_neon_vqsubd_s64(__p0, __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16_t vqsubh_s16(int16_t __p0, int16_t __p1) {
  int16_t __ret;
  __ret = (int16_t) __builtin_neon_vqsubh_s16(__p0, __p1);
  return __ret;
}
#else
__ai int16_t vqsubh_s16(int16_t __p0, int16_t __p1) {
  int16_t __ret;
  __ret = (int16_t) __builtin_neon_vqsubh_s16(__p0, __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x8_t vqtbl1_p8(poly8x16_t __p0, uint8x8_t __p1) {
  poly8x8_t __ret;
  __ret = (poly8x8_t) __builtin_neon_vqtbl1_v((int8x16_t)__p0, (int8x8_t)__p1, 4);
  return __ret;
}
#else
__ai poly8x8_t vqtbl1_p8(poly8x16_t __p0, uint8x8_t __p1) {
  poly8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  poly8x8_t __ret;
  __ret = (poly8x8_t) __builtin_neon_vqtbl1_v((int8x16_t)__rev0, (int8x8_t)__rev1, 4);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x16_t vqtbl1q_p8(poly8x16_t __p0, uint8x16_t __p1) {
  poly8x16_t __ret;
  __ret = (poly8x16_t) __builtin_neon_vqtbl1q_v((int8x16_t)__p0, (int8x16_t)__p1, 36);
  return __ret;
}
#else
__ai poly8x16_t vqtbl1q_p8(poly8x16_t __p0, uint8x16_t __p1) {
  poly8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  poly8x16_t __ret;
  __ret = (poly8x16_t) __builtin_neon_vqtbl1q_v((int8x16_t)__rev0, (int8x16_t)__rev1, 36);
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x16_t vqtbl1q_u8(uint8x16_t __p0, uint8x16_t __p1) {
  uint8x16_t __ret;
  __ret = (uint8x16_t) __builtin_neon_vqtbl1q_v((int8x16_t)__p0, (int8x16_t)__p1, 48);
  return __ret;
}
#else
__ai uint8x16_t vqtbl1q_u8(uint8x16_t __p0, uint8x16_t __p1) {
  uint8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __ret;
  __ret = (uint8x16_t) __builtin_neon_vqtbl1q_v((int8x16_t)__rev0, (int8x16_t)__rev1, 48);
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x16_t vqtbl1q_s8(int8x16_t __p0, int8x16_t __p1) {
  int8x16_t __ret;
  __ret = (int8x16_t) __builtin_neon_vqtbl1q_v((int8x16_t)__p0, (int8x16_t)__p1, 32);
  return __ret;
}
#else
__ai int8x16_t vqtbl1q_s8(int8x16_t __p0, int8x16_t __p1) {
  int8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __ret;
  __ret = (int8x16_t) __builtin_neon_vqtbl1q_v((int8x16_t)__rev0, (int8x16_t)__rev1, 32);
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x8_t vqtbl1_u8(uint8x16_t __p0, uint8x8_t __p1) {
  uint8x8_t __ret;
  __ret = (uint8x8_t) __builtin_neon_vqtbl1_v((int8x16_t)__p0, (int8x8_t)__p1, 16);
  return __ret;
}
#else
__ai uint8x8_t vqtbl1_u8(uint8x16_t __p0, uint8x8_t __p1) {
  uint8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __ret;
  __ret = (uint8x8_t) __builtin_neon_vqtbl1_v((int8x16_t)__rev0, (int8x8_t)__rev1, 16);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x8_t vqtbl1_s8(int8x16_t __p0, int8x8_t __p1) {
  int8x8_t __ret;
  __ret = (int8x8_t) __builtin_neon_vqtbl1_v((int8x16_t)__p0, (int8x8_t)__p1, 0);
  return __ret;
}
#else
__ai int8x8_t vqtbl1_s8(int8x16_t __p0, int8x8_t __p1) {
  int8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __ret;
  __ret = (int8x8_t) __builtin_neon_vqtbl1_v((int8x16_t)__rev0, (int8x8_t)__rev1, 0);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x8_t vqtbl2_p8(poly8x16x2_t __p0, uint8x8_t __p1) {
  poly8x8_t __ret;
  __ret = (poly8x8_t) __builtin_neon_vqtbl2_v((int8x16_t)__p0.val[0], (int8x16_t)__p0.val[1], (int8x8_t)__p1, 4);
  return __ret;
}
#else
__ai poly8x8_t vqtbl2_p8(poly8x16x2_t __p0, uint8x8_t __p1) {
  poly8x16x2_t __rev0;
  __rev0.val[0] = __builtin_shufflevector(__p0.val[0], __p0.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  __rev0.val[1] = __builtin_shufflevector(__p0.val[1], __p0.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  poly8x8_t __ret;
  __ret = (poly8x8_t) __builtin_neon_vqtbl2_v((int8x16_t)__rev0.val[0], (int8x16_t)__rev0.val[1], (int8x8_t)__rev1, 4);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x16_t vqtbl2q_p8(poly8x16x2_t __p0, uint8x16_t __p1) {
  poly8x16_t __ret;
  __ret = (poly8x16_t) __builtin_neon_vqtbl2q_v((int8x16_t)__p0.val[0], (int8x16_t)__p0.val[1], (int8x16_t)__p1, 36);
  return __ret;
}
#else
__ai poly8x16_t vqtbl2q_p8(poly8x16x2_t __p0, uint8x16_t __p1) {
  poly8x16x2_t __rev0;
  __rev0.val[0] = __builtin_shufflevector(__p0.val[0], __p0.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  __rev0.val[1] = __builtin_shufflevector(__p0.val[1], __p0.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  poly8x16_t __ret;
  __ret = (poly8x16_t) __builtin_neon_vqtbl2q_v((int8x16_t)__rev0.val[0], (int8x16_t)__rev0.val[1], (int8x16_t)__rev1, 36);
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x16_t vqtbl2q_u8(uint8x16x2_t __p0, uint8x16_t __p1) {
  uint8x16_t __ret;
  __ret = (uint8x16_t) __builtin_neon_vqtbl2q_v((int8x16_t)__p0.val[0], (int8x16_t)__p0.val[1], (int8x16_t)__p1, 48);
  return __ret;
}
#else
__ai uint8x16_t vqtbl2q_u8(uint8x16x2_t __p0, uint8x16_t __p1) {
  uint8x16x2_t __rev0;
  __rev0.val[0] = __builtin_shufflevector(__p0.val[0], __p0.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  __rev0.val[1] = __builtin_shufflevector(__p0.val[1], __p0.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __ret;
  __ret = (uint8x16_t) __builtin_neon_vqtbl2q_v((int8x16_t)__rev0.val[0], (int8x16_t)__rev0.val[1], (int8x16_t)__rev1, 48);
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x16_t vqtbl2q_s8(int8x16x2_t __p0, int8x16_t __p1) {
  int8x16_t __ret;
  __ret = (int8x16_t) __builtin_neon_vqtbl2q_v((int8x16_t)__p0.val[0], (int8x16_t)__p0.val[1], (int8x16_t)__p1, 32);
  return __ret;
}
#else
__ai int8x16_t vqtbl2q_s8(int8x16x2_t __p0, int8x16_t __p1) {
  int8x16x2_t __rev0;
  __rev0.val[0] = __builtin_shufflevector(__p0.val[0], __p0.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  __rev0.val[1] = __builtin_shufflevector(__p0.val[1], __p0.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __ret;
  __ret = (int8x16_t) __builtin_neon_vqtbl2q_v((int8x16_t)__rev0.val[0], (int8x16_t)__rev0.val[1], (int8x16_t)__rev1, 32);
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x8_t vqtbl2_u8(uint8x16x2_t __p0, uint8x8_t __p1) {
  uint8x8_t __ret;
  __ret = (uint8x8_t) __builtin_neon_vqtbl2_v((int8x16_t)__p0.val[0], (int8x16_t)__p0.val[1], (int8x8_t)__p1, 16);
  return __ret;
}
#else
__ai uint8x8_t vqtbl2_u8(uint8x16x2_t __p0, uint8x8_t __p1) {
  uint8x16x2_t __rev0;
  __rev0.val[0] = __builtin_shufflevector(__p0.val[0], __p0.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  __rev0.val[1] = __builtin_shufflevector(__p0.val[1], __p0.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __ret;
  __ret = (uint8x8_t) __builtin_neon_vqtbl2_v((int8x16_t)__rev0.val[0], (int8x16_t)__rev0.val[1], (int8x8_t)__rev1, 16);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x8_t vqtbl2_s8(int8x16x2_t __p0, int8x8_t __p1) {
  int8x8_t __ret;
  __ret = (int8x8_t) __builtin_neon_vqtbl2_v((int8x16_t)__p0.val[0], (int8x16_t)__p0.val[1], (int8x8_t)__p1, 0);
  return __ret;
}
#else
__ai int8x8_t vqtbl2_s8(int8x16x2_t __p0, int8x8_t __p1) {
  int8x16x2_t __rev0;
  __rev0.val[0] = __builtin_shufflevector(__p0.val[0], __p0.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  __rev0.val[1] = __builtin_shufflevector(__p0.val[1], __p0.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __ret;
  __ret = (int8x8_t) __builtin_neon_vqtbl2_v((int8x16_t)__rev0.val[0], (int8x16_t)__rev0.val[1], (int8x8_t)__rev1, 0);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x8_t vqtbl3_p8(poly8x16x3_t __p0, uint8x8_t __p1) {
  poly8x8_t __ret;
  __ret = (poly8x8_t) __builtin_neon_vqtbl3_v((int8x16_t)__p0.val[0], (int8x16_t)__p0.val[1], (int8x16_t)__p0.val[2], (int8x8_t)__p1, 4);
  return __ret;
}
#else
__ai poly8x8_t vqtbl3_p8(poly8x16x3_t __p0, uint8x8_t __p1) {
  poly8x16x3_t __rev0;
  __rev0.val[0] = __builtin_shufflevector(__p0.val[0], __p0.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  __rev0.val[1] = __builtin_shufflevector(__p0.val[1], __p0.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  __rev0.val[2] = __builtin_shufflevector(__p0.val[2], __p0.val[2], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  poly8x8_t __ret;
  __ret = (poly8x8_t) __builtin_neon_vqtbl3_v((int8x16_t)__rev0.val[0], (int8x16_t)__rev0.val[1], (int8x16_t)__rev0.val[2], (int8x8_t)__rev1, 4);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x16_t vqtbl3q_p8(poly8x16x3_t __p0, uint8x16_t __p1) {
  poly8x16_t __ret;
  __ret = (poly8x16_t) __builtin_neon_vqtbl3q_v((int8x16_t)__p0.val[0], (int8x16_t)__p0.val[1], (int8x16_t)__p0.val[2], (int8x16_t)__p1, 36);
  return __ret;
}
#else
__ai poly8x16_t vqtbl3q_p8(poly8x16x3_t __p0, uint8x16_t __p1) {
  poly8x16x3_t __rev0;
  __rev0.val[0] = __builtin_shufflevector(__p0.val[0], __p0.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  __rev0.val[1] = __builtin_shufflevector(__p0.val[1], __p0.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  __rev0.val[2] = __builtin_shufflevector(__p0.val[2], __p0.val[2], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  poly8x16_t __ret;
  __ret = (poly8x16_t) __builtin_neon_vqtbl3q_v((int8x16_t)__rev0.val[0], (int8x16_t)__rev0.val[1], (int8x16_t)__rev0.val[2], (int8x16_t)__rev1, 36);
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x16_t vqtbl3q_u8(uint8x16x3_t __p0, uint8x16_t __p1) {
  uint8x16_t __ret;
  __ret = (uint8x16_t) __builtin_neon_vqtbl3q_v((int8x16_t)__p0.val[0], (int8x16_t)__p0.val[1], (int8x16_t)__p0.val[2], (int8x16_t)__p1, 48);
  return __ret;
}
#else
__ai uint8x16_t vqtbl3q_u8(uint8x16x3_t __p0, uint8x16_t __p1) {
  uint8x16x3_t __rev0;
  __rev0.val[0] = __builtin_shufflevector(__p0.val[0], __p0.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  __rev0.val[1] = __builtin_shufflevector(__p0.val[1], __p0.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  __rev0.val[2] = __builtin_shufflevector(__p0.val[2], __p0.val[2], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __ret;
  __ret = (uint8x16_t) __builtin_neon_vqtbl3q_v((int8x16_t)__rev0.val[0], (int8x16_t)__rev0.val[1], (int8x16_t)__rev0.val[2], (int8x16_t)__rev1, 48);
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x16_t vqtbl3q_s8(int8x16x3_t __p0, int8x16_t __p1) {
  int8x16_t __ret;
  __ret = (int8x16_t) __builtin_neon_vqtbl3q_v((int8x16_t)__p0.val[0], (int8x16_t)__p0.val[1], (int8x16_t)__p0.val[2], (int8x16_t)__p1, 32);
  return __ret;
}
#else
__ai int8x16_t vqtbl3q_s8(int8x16x3_t __p0, int8x16_t __p1) {
  int8x16x3_t __rev0;
  __rev0.val[0] = __builtin_shufflevector(__p0.val[0], __p0.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  __rev0.val[1] = __builtin_shufflevector(__p0.val[1], __p0.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  __rev0.val[2] = __builtin_shufflevector(__p0.val[2], __p0.val[2], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __ret;
  __ret = (int8x16_t) __builtin_neon_vqtbl3q_v((int8x16_t)__rev0.val[0], (int8x16_t)__rev0.val[1], (int8x16_t)__rev0.val[2], (int8x16_t)__rev1, 32);
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x8_t vqtbl3_u8(uint8x16x3_t __p0, uint8x8_t __p1) {
  uint8x8_t __ret;
  __ret = (uint8x8_t) __builtin_neon_vqtbl3_v((int8x16_t)__p0.val[0], (int8x16_t)__p0.val[1], (int8x16_t)__p0.val[2], (int8x8_t)__p1, 16);
  return __ret;
}
#else
__ai uint8x8_t vqtbl3_u8(uint8x16x3_t __p0, uint8x8_t __p1) {
  uint8x16x3_t __rev0;
  __rev0.val[0] = __builtin_shufflevector(__p0.val[0], __p0.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  __rev0.val[1] = __builtin_shufflevector(__p0.val[1], __p0.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  __rev0.val[2] = __builtin_shufflevector(__p0.val[2], __p0.val[2], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __ret;
  __ret = (uint8x8_t) __builtin_neon_vqtbl3_v((int8x16_t)__rev0.val[0], (int8x16_t)__rev0.val[1], (int8x16_t)__rev0.val[2], (int8x8_t)__rev1, 16);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x8_t vqtbl3_s8(int8x16x3_t __p0, int8x8_t __p1) {
  int8x8_t __ret;
  __ret = (int8x8_t) __builtin_neon_vqtbl3_v((int8x16_t)__p0.val[0], (int8x16_t)__p0.val[1], (int8x16_t)__p0.val[2], (int8x8_t)__p1, 0);
  return __ret;
}
#else
__ai int8x8_t vqtbl3_s8(int8x16x3_t __p0, int8x8_t __p1) {
  int8x16x3_t __rev0;
  __rev0.val[0] = __builtin_shufflevector(__p0.val[0], __p0.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  __rev0.val[1] = __builtin_shufflevector(__p0.val[1], __p0.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  __rev0.val[2] = __builtin_shufflevector(__p0.val[2], __p0.val[2], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __ret;
  __ret = (int8x8_t) __builtin_neon_vqtbl3_v((int8x16_t)__rev0.val[0], (int8x16_t)__rev0.val[1], (int8x16_t)__rev0.val[2], (int8x8_t)__rev1, 0);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x8_t vqtbl4_p8(poly8x16x4_t __p0, uint8x8_t __p1) {
  poly8x8_t __ret;
  __ret = (poly8x8_t) __builtin_neon_vqtbl4_v((int8x16_t)__p0.val[0], (int8x16_t)__p0.val[1], (int8x16_t)__p0.val[2], (int8x16_t)__p0.val[3], (int8x8_t)__p1, 4);
  return __ret;
}
#else
__ai poly8x8_t vqtbl4_p8(poly8x16x4_t __p0, uint8x8_t __p1) {
  poly8x16x4_t __rev0;
  __rev0.val[0] = __builtin_shufflevector(__p0.val[0], __p0.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  __rev0.val[1] = __builtin_shufflevector(__p0.val[1], __p0.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  __rev0.val[2] = __builtin_shufflevector(__p0.val[2], __p0.val[2], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  __rev0.val[3] = __builtin_shufflevector(__p0.val[3], __p0.val[3], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  poly8x8_t __ret;
  __ret = (poly8x8_t) __builtin_neon_vqtbl4_v((int8x16_t)__rev0.val[0], (int8x16_t)__rev0.val[1], (int8x16_t)__rev0.val[2], (int8x16_t)__rev0.val[3], (int8x8_t)__rev1, 4);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x16_t vqtbl4q_p8(poly8x16x4_t __p0, uint8x16_t __p1) {
  poly8x16_t __ret;
  __ret = (poly8x16_t) __builtin_neon_vqtbl4q_v((int8x16_t)__p0.val[0], (int8x16_t)__p0.val[1], (int8x16_t)__p0.val[2], (int8x16_t)__p0.val[3], (int8x16_t)__p1, 36);
  return __ret;
}
#else
__ai poly8x16_t vqtbl4q_p8(poly8x16x4_t __p0, uint8x16_t __p1) {
  poly8x16x4_t __rev0;
  __rev0.val[0] = __builtin_shufflevector(__p0.val[0], __p0.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  __rev0.val[1] = __builtin_shufflevector(__p0.val[1], __p0.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  __rev0.val[2] = __builtin_shufflevector(__p0.val[2], __p0.val[2], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  __rev0.val[3] = __builtin_shufflevector(__p0.val[3], __p0.val[3], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  poly8x16_t __ret;
  __ret = (poly8x16_t) __builtin_neon_vqtbl4q_v((int8x16_t)__rev0.val[0], (int8x16_t)__rev0.val[1], (int8x16_t)__rev0.val[2], (int8x16_t)__rev0.val[3], (int8x16_t)__rev1, 36);
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x16_t vqtbl4q_u8(uint8x16x4_t __p0, uint8x16_t __p1) {
  uint8x16_t __ret;
  __ret = (uint8x16_t) __builtin_neon_vqtbl4q_v((int8x16_t)__p0.val[0], (int8x16_t)__p0.val[1], (int8x16_t)__p0.val[2], (int8x16_t)__p0.val[3], (int8x16_t)__p1, 48);
  return __ret;
}
#else
__ai uint8x16_t vqtbl4q_u8(uint8x16x4_t __p0, uint8x16_t __p1) {
  uint8x16x4_t __rev0;
  __rev0.val[0] = __builtin_shufflevector(__p0.val[0], __p0.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  __rev0.val[1] = __builtin_shufflevector(__p0.val[1], __p0.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  __rev0.val[2] = __builtin_shufflevector(__p0.val[2], __p0.val[2], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  __rev0.val[3] = __builtin_shufflevector(__p0.val[3], __p0.val[3], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __ret;
  __ret = (uint8x16_t) __builtin_neon_vqtbl4q_v((int8x16_t)__rev0.val[0], (int8x16_t)__rev0.val[1], (int8x16_t)__rev0.val[2], (int8x16_t)__rev0.val[3], (int8x16_t)__rev1, 48);
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x16_t vqtbl4q_s8(int8x16x4_t __p0, int8x16_t __p1) {
  int8x16_t __ret;
  __ret = (int8x16_t) __builtin_neon_vqtbl4q_v((int8x16_t)__p0.val[0], (int8x16_t)__p0.val[1], (int8x16_t)__p0.val[2], (int8x16_t)__p0.val[3], (int8x16_t)__p1, 32);
  return __ret;
}
#else
__ai int8x16_t vqtbl4q_s8(int8x16x4_t __p0, int8x16_t __p1) {
  int8x16x4_t __rev0;
  __rev0.val[0] = __builtin_shufflevector(__p0.val[0], __p0.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  __rev0.val[1] = __builtin_shufflevector(__p0.val[1], __p0.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  __rev0.val[2] = __builtin_shufflevector(__p0.val[2], __p0.val[2], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  __rev0.val[3] = __builtin_shufflevector(__p0.val[3], __p0.val[3], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __ret;
  __ret = (int8x16_t) __builtin_neon_vqtbl4q_v((int8x16_t)__rev0.val[0], (int8x16_t)__rev0.val[1], (int8x16_t)__rev0.val[2], (int8x16_t)__rev0.val[3], (int8x16_t)__rev1, 32);
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x8_t vqtbl4_u8(uint8x16x4_t __p0, uint8x8_t __p1) {
  uint8x8_t __ret;
  __ret = (uint8x8_t) __builtin_neon_vqtbl4_v((int8x16_t)__p0.val[0], (int8x16_t)__p0.val[1], (int8x16_t)__p0.val[2], (int8x16_t)__p0.val[3], (int8x8_t)__p1, 16);
  return __ret;
}
#else
__ai uint8x8_t vqtbl4_u8(uint8x16x4_t __p0, uint8x8_t __p1) {
  uint8x16x4_t __rev0;
  __rev0.val[0] = __builtin_shufflevector(__p0.val[0], __p0.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  __rev0.val[1] = __builtin_shufflevector(__p0.val[1], __p0.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  __rev0.val[2] = __builtin_shufflevector(__p0.val[2], __p0.val[2], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  __rev0.val[3] = __builtin_shufflevector(__p0.val[3], __p0.val[3], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __ret;
  __ret = (uint8x8_t) __builtin_neon_vqtbl4_v((int8x16_t)__rev0.val[0], (int8x16_t)__rev0.val[1], (int8x16_t)__rev0.val[2], (int8x16_t)__rev0.val[3], (int8x8_t)__rev1, 16);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x8_t vqtbl4_s8(int8x16x4_t __p0, int8x8_t __p1) {
  int8x8_t __ret;
  __ret = (int8x8_t) __builtin_neon_vqtbl4_v((int8x16_t)__p0.val[0], (int8x16_t)__p0.val[1], (int8x16_t)__p0.val[2], (int8x16_t)__p0.val[3], (int8x8_t)__p1, 0);
  return __ret;
}
#else
__ai int8x8_t vqtbl4_s8(int8x16x4_t __p0, int8x8_t __p1) {
  int8x16x4_t __rev0;
  __rev0.val[0] = __builtin_shufflevector(__p0.val[0], __p0.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  __rev0.val[1] = __builtin_shufflevector(__p0.val[1], __p0.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  __rev0.val[2] = __builtin_shufflevector(__p0.val[2], __p0.val[2], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  __rev0.val[3] = __builtin_shufflevector(__p0.val[3], __p0.val[3], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __ret;
  __ret = (int8x8_t) __builtin_neon_vqtbl4_v((int8x16_t)__rev0.val[0], (int8x16_t)__rev0.val[1], (int8x16_t)__rev0.val[2], (int8x16_t)__rev0.val[3], (int8x8_t)__rev1, 0);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x8_t vqtbx1_p8(poly8x8_t __p0, poly8x16_t __p1, uint8x8_t __p2) {
  poly8x8_t __ret;
  __ret = (poly8x8_t) __builtin_neon_vqtbx1_v((int8x8_t)__p0, (int8x16_t)__p1, (int8x8_t)__p2, 4);
  return __ret;
}
#else
__ai poly8x8_t vqtbx1_p8(poly8x8_t __p0, poly8x16_t __p1, uint8x8_t __p2) {
  poly8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  poly8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 7, 6, 5, 4, 3, 2, 1, 0);
  poly8x8_t __ret;
  __ret = (poly8x8_t) __builtin_neon_vqtbx1_v((int8x8_t)__rev0, (int8x16_t)__rev1, (int8x8_t)__rev2, 4);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x16_t vqtbx1q_p8(poly8x16_t __p0, poly8x16_t __p1, uint8x16_t __p2) {
  poly8x16_t __ret;
  __ret = (poly8x16_t) __builtin_neon_vqtbx1q_v((int8x16_t)__p0, (int8x16_t)__p1, (int8x16_t)__p2, 36);
  return __ret;
}
#else
__ai poly8x16_t vqtbx1q_p8(poly8x16_t __p0, poly8x16_t __p1, uint8x16_t __p2) {
  poly8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  poly8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  poly8x16_t __ret;
  __ret = (poly8x16_t) __builtin_neon_vqtbx1q_v((int8x16_t)__rev0, (int8x16_t)__rev1, (int8x16_t)__rev2, 36);
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x16_t vqtbx1q_u8(uint8x16_t __p0, uint8x16_t __p1, uint8x16_t __p2) {
  uint8x16_t __ret;
  __ret = (uint8x16_t) __builtin_neon_vqtbx1q_v((int8x16_t)__p0, (int8x16_t)__p1, (int8x16_t)__p2, 48);
  return __ret;
}
#else
__ai uint8x16_t vqtbx1q_u8(uint8x16_t __p0, uint8x16_t __p1, uint8x16_t __p2) {
  uint8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __ret;
  __ret = (uint8x16_t) __builtin_neon_vqtbx1q_v((int8x16_t)__rev0, (int8x16_t)__rev1, (int8x16_t)__rev2, 48);
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x16_t vqtbx1q_s8(int8x16_t __p0, int8x16_t __p1, int8x16_t __p2) {
  int8x16_t __ret;
  __ret = (int8x16_t) __builtin_neon_vqtbx1q_v((int8x16_t)__p0, (int8x16_t)__p1, (int8x16_t)__p2, 32);
  return __ret;
}
#else
__ai int8x16_t vqtbx1q_s8(int8x16_t __p0, int8x16_t __p1, int8x16_t __p2) {
  int8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __ret;
  __ret = (int8x16_t) __builtin_neon_vqtbx1q_v((int8x16_t)__rev0, (int8x16_t)__rev1, (int8x16_t)__rev2, 32);
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x8_t vqtbx1_u8(uint8x8_t __p0, uint8x16_t __p1, uint8x8_t __p2) {
  uint8x8_t __ret;
  __ret = (uint8x8_t) __builtin_neon_vqtbx1_v((int8x8_t)__p0, (int8x16_t)__p1, (int8x8_t)__p2, 16);
  return __ret;
}
#else
__ai uint8x8_t vqtbx1_u8(uint8x8_t __p0, uint8x16_t __p1, uint8x8_t __p2) {
  uint8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __ret;
  __ret = (uint8x8_t) __builtin_neon_vqtbx1_v((int8x8_t)__rev0, (int8x16_t)__rev1, (int8x8_t)__rev2, 16);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x8_t vqtbx1_s8(int8x8_t __p0, int8x16_t __p1, int8x8_t __p2) {
  int8x8_t __ret;
  __ret = (int8x8_t) __builtin_neon_vqtbx1_v((int8x8_t)__p0, (int8x16_t)__p1, (int8x8_t)__p2, 0);
  return __ret;
}
#else
__ai int8x8_t vqtbx1_s8(int8x8_t __p0, int8x16_t __p1, int8x8_t __p2) {
  int8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __ret;
  __ret = (int8x8_t) __builtin_neon_vqtbx1_v((int8x8_t)__rev0, (int8x16_t)__rev1, (int8x8_t)__rev2, 0);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x8_t vqtbx2_p8(poly8x8_t __p0, poly8x16x2_t __p1, uint8x8_t __p2) {
  poly8x8_t __ret;
  __ret = (poly8x8_t) __builtin_neon_vqtbx2_v((int8x8_t)__p0, (int8x16_t)__p1.val[0], (int8x16_t)__p1.val[1], (int8x8_t)__p2, 4);
  return __ret;
}
#else
__ai poly8x8_t vqtbx2_p8(poly8x8_t __p0, poly8x16x2_t __p1, uint8x8_t __p2) {
  poly8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  poly8x16x2_t __rev1;
  __rev1.val[0] = __builtin_shufflevector(__p1.val[0], __p1.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  __rev1.val[1] = __builtin_shufflevector(__p1.val[1], __p1.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 7, 6, 5, 4, 3, 2, 1, 0);
  poly8x8_t __ret;
  __ret = (poly8x8_t) __builtin_neon_vqtbx2_v((int8x8_t)__rev0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], (int8x8_t)__rev2, 4);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x16_t vqtbx2q_p8(poly8x16_t __p0, poly8x16x2_t __p1, uint8x16_t __p2) {
  poly8x16_t __ret;
  __ret = (poly8x16_t) __builtin_neon_vqtbx2q_v((int8x16_t)__p0, (int8x16_t)__p1.val[0], (int8x16_t)__p1.val[1], (int8x16_t)__p2, 36);
  return __ret;
}
#else
__ai poly8x16_t vqtbx2q_p8(poly8x16_t __p0, poly8x16x2_t __p1, uint8x16_t __p2) {
  poly8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  poly8x16x2_t __rev1;
  __rev1.val[0] = __builtin_shufflevector(__p1.val[0], __p1.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  __rev1.val[1] = __builtin_shufflevector(__p1.val[1], __p1.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  poly8x16_t __ret;
  __ret = (poly8x16_t) __builtin_neon_vqtbx2q_v((int8x16_t)__rev0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], (int8x16_t)__rev2, 36);
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x16_t vqtbx2q_u8(uint8x16_t __p0, uint8x16x2_t __p1, uint8x16_t __p2) {
  uint8x16_t __ret;
  __ret = (uint8x16_t) __builtin_neon_vqtbx2q_v((int8x16_t)__p0, (int8x16_t)__p1.val[0], (int8x16_t)__p1.val[1], (int8x16_t)__p2, 48);
  return __ret;
}
#else
__ai uint8x16_t vqtbx2q_u8(uint8x16_t __p0, uint8x16x2_t __p1, uint8x16_t __p2) {
  uint8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16x2_t __rev1;
  __rev1.val[0] = __builtin_shufflevector(__p1.val[0], __p1.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  __rev1.val[1] = __builtin_shufflevector(__p1.val[1], __p1.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __ret;
  __ret = (uint8x16_t) __builtin_neon_vqtbx2q_v((int8x16_t)__rev0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], (int8x16_t)__rev2, 48);
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x16_t vqtbx2q_s8(int8x16_t __p0, int8x16x2_t __p1, int8x16_t __p2) {
  int8x16_t __ret;
  __ret = (int8x16_t) __builtin_neon_vqtbx2q_v((int8x16_t)__p0, (int8x16_t)__p1.val[0], (int8x16_t)__p1.val[1], (int8x16_t)__p2, 32);
  return __ret;
}
#else
__ai int8x16_t vqtbx2q_s8(int8x16_t __p0, int8x16x2_t __p1, int8x16_t __p2) {
  int8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16x2_t __rev1;
  __rev1.val[0] = __builtin_shufflevector(__p1.val[0], __p1.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  __rev1.val[1] = __builtin_shufflevector(__p1.val[1], __p1.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __ret;
  __ret = (int8x16_t) __builtin_neon_vqtbx2q_v((int8x16_t)__rev0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], (int8x16_t)__rev2, 32);
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x8_t vqtbx2_u8(uint8x8_t __p0, uint8x16x2_t __p1, uint8x8_t __p2) {
  uint8x8_t __ret;
  __ret = (uint8x8_t) __builtin_neon_vqtbx2_v((int8x8_t)__p0, (int8x16_t)__p1.val[0], (int8x16_t)__p1.val[1], (int8x8_t)__p2, 16);
  return __ret;
}
#else
__ai uint8x8_t vqtbx2_u8(uint8x8_t __p0, uint8x16x2_t __p1, uint8x8_t __p2) {
  uint8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16x2_t __rev1;
  __rev1.val[0] = __builtin_shufflevector(__p1.val[0], __p1.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  __rev1.val[1] = __builtin_shufflevector(__p1.val[1], __p1.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __ret;
  __ret = (uint8x8_t) __builtin_neon_vqtbx2_v((int8x8_t)__rev0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], (int8x8_t)__rev2, 16);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x8_t vqtbx2_s8(int8x8_t __p0, int8x16x2_t __p1, int8x8_t __p2) {
  int8x8_t __ret;
  __ret = (int8x8_t) __builtin_neon_vqtbx2_v((int8x8_t)__p0, (int8x16_t)__p1.val[0], (int8x16_t)__p1.val[1], (int8x8_t)__p2, 0);
  return __ret;
}
#else
__ai int8x8_t vqtbx2_s8(int8x8_t __p0, int8x16x2_t __p1, int8x8_t __p2) {
  int8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16x2_t __rev1;
  __rev1.val[0] = __builtin_shufflevector(__p1.val[0], __p1.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  __rev1.val[1] = __builtin_shufflevector(__p1.val[1], __p1.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __ret;
  __ret = (int8x8_t) __builtin_neon_vqtbx2_v((int8x8_t)__rev0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], (int8x8_t)__rev2, 0);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x8_t vqtbx3_p8(poly8x8_t __p0, poly8x16x3_t __p1, uint8x8_t __p2) {
  poly8x8_t __ret;
  __ret = (poly8x8_t) __builtin_neon_vqtbx3_v((int8x8_t)__p0, (int8x16_t)__p1.val[0], (int8x16_t)__p1.val[1], (int8x16_t)__p1.val[2], (int8x8_t)__p2, 4);
  return __ret;
}
#else
__ai poly8x8_t vqtbx3_p8(poly8x8_t __p0, poly8x16x3_t __p1, uint8x8_t __p2) {
  poly8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  poly8x16x3_t __rev1;
  __rev1.val[0] = __builtin_shufflevector(__p1.val[0], __p1.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  __rev1.val[1] = __builtin_shufflevector(__p1.val[1], __p1.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  __rev1.val[2] = __builtin_shufflevector(__p1.val[2], __p1.val[2], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 7, 6, 5, 4, 3, 2, 1, 0);
  poly8x8_t __ret;
  __ret = (poly8x8_t) __builtin_neon_vqtbx3_v((int8x8_t)__rev0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], (int8x16_t)__rev1.val[2], (int8x8_t)__rev2, 4);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x16_t vqtbx3q_p8(poly8x16_t __p0, poly8x16x3_t __p1, uint8x16_t __p2) {
  poly8x16_t __ret;
  __ret = (poly8x16_t) __builtin_neon_vqtbx3q_v((int8x16_t)__p0, (int8x16_t)__p1.val[0], (int8x16_t)__p1.val[1], (int8x16_t)__p1.val[2], (int8x16_t)__p2, 36);
  return __ret;
}
#else
__ai poly8x16_t vqtbx3q_p8(poly8x16_t __p0, poly8x16x3_t __p1, uint8x16_t __p2) {
  poly8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  poly8x16x3_t __rev1;
  __rev1.val[0] = __builtin_shufflevector(__p1.val[0], __p1.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  __rev1.val[1] = __builtin_shufflevector(__p1.val[1], __p1.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  __rev1.val[2] = __builtin_shufflevector(__p1.val[2], __p1.val[2], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  poly8x16_t __ret;
  __ret = (poly8x16_t) __builtin_neon_vqtbx3q_v((int8x16_t)__rev0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], (int8x16_t)__rev1.val[2], (int8x16_t)__rev2, 36);
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x16_t vqtbx3q_u8(uint8x16_t __p0, uint8x16x3_t __p1, uint8x16_t __p2) {
  uint8x16_t __ret;
  __ret = (uint8x16_t) __builtin_neon_vqtbx3q_v((int8x16_t)__p0, (int8x16_t)__p1.val[0], (int8x16_t)__p1.val[1], (int8x16_t)__p1.val[2], (int8x16_t)__p2, 48);
  return __ret;
}
#else
__ai uint8x16_t vqtbx3q_u8(uint8x16_t __p0, uint8x16x3_t __p1, uint8x16_t __p2) {
  uint8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16x3_t __rev1;
  __rev1.val[0] = __builtin_shufflevector(__p1.val[0], __p1.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  __rev1.val[1] = __builtin_shufflevector(__p1.val[1], __p1.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  __rev1.val[2] = __builtin_shufflevector(__p1.val[2], __p1.val[2], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __ret;
  __ret = (uint8x16_t) __builtin_neon_vqtbx3q_v((int8x16_t)__rev0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], (int8x16_t)__rev1.val[2], (int8x16_t)__rev2, 48);
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x16_t vqtbx3q_s8(int8x16_t __p0, int8x16x3_t __p1, int8x16_t __p2) {
  int8x16_t __ret;
  __ret = (int8x16_t) __builtin_neon_vqtbx3q_v((int8x16_t)__p0, (int8x16_t)__p1.val[0], (int8x16_t)__p1.val[1], (int8x16_t)__p1.val[2], (int8x16_t)__p2, 32);
  return __ret;
}
#else
__ai int8x16_t vqtbx3q_s8(int8x16_t __p0, int8x16x3_t __p1, int8x16_t __p2) {
  int8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16x3_t __rev1;
  __rev1.val[0] = __builtin_shufflevector(__p1.val[0], __p1.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  __rev1.val[1] = __builtin_shufflevector(__p1.val[1], __p1.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  __rev1.val[2] = __builtin_shufflevector(__p1.val[2], __p1.val[2], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __ret;
  __ret = (int8x16_t) __builtin_neon_vqtbx3q_v((int8x16_t)__rev0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], (int8x16_t)__rev1.val[2], (int8x16_t)__rev2, 32);
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x8_t vqtbx3_u8(uint8x8_t __p0, uint8x16x3_t __p1, uint8x8_t __p2) {
  uint8x8_t __ret;
  __ret = (uint8x8_t) __builtin_neon_vqtbx3_v((int8x8_t)__p0, (int8x16_t)__p1.val[0], (int8x16_t)__p1.val[1], (int8x16_t)__p1.val[2], (int8x8_t)__p2, 16);
  return __ret;
}
#else
__ai uint8x8_t vqtbx3_u8(uint8x8_t __p0, uint8x16x3_t __p1, uint8x8_t __p2) {
  uint8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16x3_t __rev1;
  __rev1.val[0] = __builtin_shufflevector(__p1.val[0], __p1.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  __rev1.val[1] = __builtin_shufflevector(__p1.val[1], __p1.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  __rev1.val[2] = __builtin_shufflevector(__p1.val[2], __p1.val[2], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __ret;
  __ret = (uint8x8_t) __builtin_neon_vqtbx3_v((int8x8_t)__rev0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], (int8x16_t)__rev1.val[2], (int8x8_t)__rev2, 16);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x8_t vqtbx3_s8(int8x8_t __p0, int8x16x3_t __p1, int8x8_t __p2) {
  int8x8_t __ret;
  __ret = (int8x8_t) __builtin_neon_vqtbx3_v((int8x8_t)__p0, (int8x16_t)__p1.val[0], (int8x16_t)__p1.val[1], (int8x16_t)__p1.val[2], (int8x8_t)__p2, 0);
  return __ret;
}
#else
__ai int8x8_t vqtbx3_s8(int8x8_t __p0, int8x16x3_t __p1, int8x8_t __p2) {
  int8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16x3_t __rev1;
  __rev1.val[0] = __builtin_shufflevector(__p1.val[0], __p1.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  __rev1.val[1] = __builtin_shufflevector(__p1.val[1], __p1.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  __rev1.val[2] = __builtin_shufflevector(__p1.val[2], __p1.val[2], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __ret;
  __ret = (int8x8_t) __builtin_neon_vqtbx3_v((int8x8_t)__rev0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], (int8x16_t)__rev1.val[2], (int8x8_t)__rev2, 0);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x8_t vqtbx4_p8(poly8x8_t __p0, poly8x16x4_t __p1, uint8x8_t __p2) {
  poly8x8_t __ret;
  __ret = (poly8x8_t) __builtin_neon_vqtbx4_v((int8x8_t)__p0, (int8x16_t)__p1.val[0], (int8x16_t)__p1.val[1], (int8x16_t)__p1.val[2], (int8x16_t)__p1.val[3], (int8x8_t)__p2, 4);
  return __ret;
}
#else
__ai poly8x8_t vqtbx4_p8(poly8x8_t __p0, poly8x16x4_t __p1, uint8x8_t __p2) {
  poly8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  poly8x16x4_t __rev1;
  __rev1.val[0] = __builtin_shufflevector(__p1.val[0], __p1.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  __rev1.val[1] = __builtin_shufflevector(__p1.val[1], __p1.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  __rev1.val[2] = __builtin_shufflevector(__p1.val[2], __p1.val[2], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  __rev1.val[3] = __builtin_shufflevector(__p1.val[3], __p1.val[3], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 7, 6, 5, 4, 3, 2, 1, 0);
  poly8x8_t __ret;
  __ret = (poly8x8_t) __builtin_neon_vqtbx4_v((int8x8_t)__rev0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], (int8x16_t)__rev1.val[2], (int8x16_t)__rev1.val[3], (int8x8_t)__rev2, 4);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x16_t vqtbx4q_p8(poly8x16_t __p0, poly8x16x4_t __p1, uint8x16_t __p2) {
  poly8x16_t __ret;
  __ret = (poly8x16_t) __builtin_neon_vqtbx4q_v((int8x16_t)__p0, (int8x16_t)__p1.val[0], (int8x16_t)__p1.val[1], (int8x16_t)__p1.val[2], (int8x16_t)__p1.val[3], (int8x16_t)__p2, 36);
  return __ret;
}
#else
__ai poly8x16_t vqtbx4q_p8(poly8x16_t __p0, poly8x16x4_t __p1, uint8x16_t __p2) {
  poly8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  poly8x16x4_t __rev1;
  __rev1.val[0] = __builtin_shufflevector(__p1.val[0], __p1.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  __rev1.val[1] = __builtin_shufflevector(__p1.val[1], __p1.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  __rev1.val[2] = __builtin_shufflevector(__p1.val[2], __p1.val[2], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  __rev1.val[3] = __builtin_shufflevector(__p1.val[3], __p1.val[3], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  poly8x16_t __ret;
  __ret = (poly8x16_t) __builtin_neon_vqtbx4q_v((int8x16_t)__rev0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], (int8x16_t)__rev1.val[2], (int8x16_t)__rev1.val[3], (int8x16_t)__rev2, 36);
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x16_t vqtbx4q_u8(uint8x16_t __p0, uint8x16x4_t __p1, uint8x16_t __p2) {
  uint8x16_t __ret;
  __ret = (uint8x16_t) __builtin_neon_vqtbx4q_v((int8x16_t)__p0, (int8x16_t)__p1.val[0], (int8x16_t)__p1.val[1], (int8x16_t)__p1.val[2], (int8x16_t)__p1.val[3], (int8x16_t)__p2, 48);
  return __ret;
}
#else
__ai uint8x16_t vqtbx4q_u8(uint8x16_t __p0, uint8x16x4_t __p1, uint8x16_t __p2) {
  uint8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16x4_t __rev1;
  __rev1.val[0] = __builtin_shufflevector(__p1.val[0], __p1.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  __rev1.val[1] = __builtin_shufflevector(__p1.val[1], __p1.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  __rev1.val[2] = __builtin_shufflevector(__p1.val[2], __p1.val[2], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  __rev1.val[3] = __builtin_shufflevector(__p1.val[3], __p1.val[3], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __ret;
  __ret = (uint8x16_t) __builtin_neon_vqtbx4q_v((int8x16_t)__rev0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], (int8x16_t)__rev1.val[2], (int8x16_t)__rev1.val[3], (int8x16_t)__rev2, 48);
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x16_t vqtbx4q_s8(int8x16_t __p0, int8x16x4_t __p1, int8x16_t __p2) {
  int8x16_t __ret;
  __ret = (int8x16_t) __builtin_neon_vqtbx4q_v((int8x16_t)__p0, (int8x16_t)__p1.val[0], (int8x16_t)__p1.val[1], (int8x16_t)__p1.val[2], (int8x16_t)__p1.val[3], (int8x16_t)__p2, 32);
  return __ret;
}
#else
__ai int8x16_t vqtbx4q_s8(int8x16_t __p0, int8x16x4_t __p1, int8x16_t __p2) {
  int8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16x4_t __rev1;
  __rev1.val[0] = __builtin_shufflevector(__p1.val[0], __p1.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  __rev1.val[1] = __builtin_shufflevector(__p1.val[1], __p1.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  __rev1.val[2] = __builtin_shufflevector(__p1.val[2], __p1.val[2], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  __rev1.val[3] = __builtin_shufflevector(__p1.val[3], __p1.val[3], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __ret;
  __ret = (int8x16_t) __builtin_neon_vqtbx4q_v((int8x16_t)__rev0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], (int8x16_t)__rev1.val[2], (int8x16_t)__rev1.val[3], (int8x16_t)__rev2, 32);
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x8_t vqtbx4_u8(uint8x8_t __p0, uint8x16x4_t __p1, uint8x8_t __p2) {
  uint8x8_t __ret;
  __ret = (uint8x8_t) __builtin_neon_vqtbx4_v((int8x8_t)__p0, (int8x16_t)__p1.val[0], (int8x16_t)__p1.val[1], (int8x16_t)__p1.val[2], (int8x16_t)__p1.val[3], (int8x8_t)__p2, 16);
  return __ret;
}
#else
__ai uint8x8_t vqtbx4_u8(uint8x8_t __p0, uint8x16x4_t __p1, uint8x8_t __p2) {
  uint8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16x4_t __rev1;
  __rev1.val[0] = __builtin_shufflevector(__p1.val[0], __p1.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  __rev1.val[1] = __builtin_shufflevector(__p1.val[1], __p1.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  __rev1.val[2] = __builtin_shufflevector(__p1.val[2], __p1.val[2], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  __rev1.val[3] = __builtin_shufflevector(__p1.val[3], __p1.val[3], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __ret;
  __ret = (uint8x8_t) __builtin_neon_vqtbx4_v((int8x8_t)__rev0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], (int8x16_t)__rev1.val[2], (int8x16_t)__rev1.val[3], (int8x8_t)__rev2, 16);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x8_t vqtbx4_s8(int8x8_t __p0, int8x16x4_t __p1, int8x8_t __p2) {
  int8x8_t __ret;
  __ret = (int8x8_t) __builtin_neon_vqtbx4_v((int8x8_t)__p0, (int8x16_t)__p1.val[0], (int8x16_t)__p1.val[1], (int8x16_t)__p1.val[2], (int8x16_t)__p1.val[3], (int8x8_t)__p2, 0);
  return __ret;
}
#else
__ai int8x8_t vqtbx4_s8(int8x8_t __p0, int8x16x4_t __p1, int8x8_t __p2) {
  int8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16x4_t __rev1;
  __rev1.val[0] = __builtin_shufflevector(__p1.val[0], __p1.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  __rev1.val[1] = __builtin_shufflevector(__p1.val[1], __p1.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  __rev1.val[2] = __builtin_shufflevector(__p1.val[2], __p1.val[2], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  __rev1.val[3] = __builtin_shufflevector(__p1.val[3], __p1.val[3], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __ret;
  __ret = (int8x8_t) __builtin_neon_vqtbx4_v((int8x8_t)__rev0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], (int8x16_t)__rev1.val[2], (int8x16_t)__rev1.val[3], (int8x8_t)__rev2, 0);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x8_t vraddhn_high_u32(uint16x4_t __p0, uint32x4_t __p1, uint32x4_t __p2) {
  uint16x8_t __ret;
  __ret = vcombine_u16(__p0, vraddhn_u32(__p1, __p2));
  return __ret;
}
#else
__ai uint16x8_t vraddhn_high_u32(uint16x4_t __p0, uint32x4_t __p1, uint32x4_t __p2) {
  uint16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint32x4_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 3, 2, 1, 0);
  uint16x8_t __ret;
  __ret = __noswap_vcombine_u16(__rev0, __noswap_vraddhn_u32(__rev1, __rev2));
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vraddhn_high_u64(uint32x2_t __p0, uint64x2_t __p1, uint64x2_t __p2) {
  uint32x4_t __ret;
  __ret = vcombine_u32(__p0, vraddhn_u64(__p1, __p2));
  return __ret;
}
#else
__ai uint32x4_t vraddhn_high_u64(uint32x2_t __p0, uint64x2_t __p1, uint64x2_t __p2) {
  uint32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  uint64x2_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 1, 0);
  uint32x4_t __ret;
  __ret = __noswap_vcombine_u32(__rev0, __noswap_vraddhn_u64(__rev1, __rev2));
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x16_t vraddhn_high_u16(uint8x8_t __p0, uint16x8_t __p1, uint16x8_t __p2) {
  uint8x16_t __ret;
  __ret = vcombine_u8(__p0, vraddhn_u16(__p1, __p2));
  return __ret;
}
#else
__ai uint8x16_t vraddhn_high_u16(uint8x8_t __p0, uint16x8_t __p1, uint16x8_t __p2) {
  uint8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __ret;
  __ret = __noswap_vcombine_u8(__rev0, __noswap_vraddhn_u16(__rev1, __rev2));
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x8_t vraddhn_high_s32(int16x4_t __p0, int32x4_t __p1, int32x4_t __p2) {
  int16x8_t __ret;
  __ret = vcombine_s16(__p0, vraddhn_s32(__p1, __p2));
  return __ret;
}
#else
__ai int16x8_t vraddhn_high_s32(int16x4_t __p0, int32x4_t __p1, int32x4_t __p2) {
  int16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int32x4_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 3, 2, 1, 0);
  int16x8_t __ret;
  __ret = __noswap_vcombine_s16(__rev0, __noswap_vraddhn_s32(__rev1, __rev2));
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vraddhn_high_s64(int32x2_t __p0, int64x2_t __p1, int64x2_t __p2) {
  int32x4_t __ret;
  __ret = vcombine_s32(__p0, vraddhn_s64(__p1, __p2));
  return __ret;
}
#else
__ai int32x4_t vraddhn_high_s64(int32x2_t __p0, int64x2_t __p1, int64x2_t __p2) {
  int32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  int64x2_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 1, 0);
  int32x4_t __ret;
  __ret = __noswap_vcombine_s32(__rev0, __noswap_vraddhn_s64(__rev1, __rev2));
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x16_t vraddhn_high_s16(int8x8_t __p0, int16x8_t __p1, int16x8_t __p2) {
  int8x16_t __ret;
  __ret = vcombine_s8(__p0, vraddhn_s16(__p1, __p2));
  return __ret;
}
#else
__ai int8x16_t vraddhn_high_s16(int8x8_t __p0, int16x8_t __p1, int16x8_t __p2) {
  int8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __ret;
  __ret = __noswap_vcombine_s8(__rev0, __noswap_vraddhn_s16(__rev1, __rev2));
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x8_t vrbit_p8(poly8x8_t __p0) {
  poly8x8_t __ret;
  __ret = (poly8x8_t) __builtin_neon_vrbit_v((int8x8_t)__p0, 4);
  return __ret;
}
#else
__ai poly8x8_t vrbit_p8(poly8x8_t __p0) {
  poly8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  poly8x8_t __ret;
  __ret = (poly8x8_t) __builtin_neon_vrbit_v((int8x8_t)__rev0, 4);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x16_t vrbitq_p8(poly8x16_t __p0) {
  poly8x16_t __ret;
  __ret = (poly8x16_t) __builtin_neon_vrbitq_v((int8x16_t)__p0, 36);
  return __ret;
}
#else
__ai poly8x16_t vrbitq_p8(poly8x16_t __p0) {
  poly8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  poly8x16_t __ret;
  __ret = (poly8x16_t) __builtin_neon_vrbitq_v((int8x16_t)__rev0, 36);
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x16_t vrbitq_u8(uint8x16_t __p0) {
  uint8x16_t __ret;
  __ret = (uint8x16_t) __builtin_neon_vrbitq_v((int8x16_t)__p0, 48);
  return __ret;
}
#else
__ai uint8x16_t vrbitq_u8(uint8x16_t __p0) {
  uint8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __ret;
  __ret = (uint8x16_t) __builtin_neon_vrbitq_v((int8x16_t)__rev0, 48);
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x16_t vrbitq_s8(int8x16_t __p0) {
  int8x16_t __ret;
  __ret = (int8x16_t) __builtin_neon_vrbitq_v((int8x16_t)__p0, 32);
  return __ret;
}
#else
__ai int8x16_t vrbitq_s8(int8x16_t __p0) {
  int8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __ret;
  __ret = (int8x16_t) __builtin_neon_vrbitq_v((int8x16_t)__rev0, 32);
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x8_t vrbit_u8(uint8x8_t __p0) {
  uint8x8_t __ret;
  __ret = (uint8x8_t) __builtin_neon_vrbit_v((int8x8_t)__p0, 16);
  return __ret;
}
#else
__ai uint8x8_t vrbit_u8(uint8x8_t __p0) {
  uint8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __ret;
  __ret = (uint8x8_t) __builtin_neon_vrbit_v((int8x8_t)__rev0, 16);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x8_t vrbit_s8(int8x8_t __p0) {
  int8x8_t __ret;
  __ret = (int8x8_t) __builtin_neon_vrbit_v((int8x8_t)__p0, 0);
  return __ret;
}
#else
__ai int8x8_t vrbit_s8(int8x8_t __p0) {
  int8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __ret;
  __ret = (int8x8_t) __builtin_neon_vrbit_v((int8x8_t)__rev0, 0);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x2_t vrecpeq_f64(float64x2_t __p0) {
  float64x2_t __ret;
  __ret = (float64x2_t) __builtin_neon_vrecpeq_v((int8x16_t)__p0, 42);
  return __ret;
}
#else
__ai float64x2_t vrecpeq_f64(float64x2_t __p0) {
  float64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float64x2_t __ret;
  __ret = (float64x2_t) __builtin_neon_vrecpeq_v((int8x16_t)__rev0, 42);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x1_t vrecpe_f64(float64x1_t __p0) {
  float64x1_t __ret;
  __ret = (float64x1_t) __builtin_neon_vrecpe_v((int8x8_t)__p0, 10);
  return __ret;
}
#else
__ai float64x1_t vrecpe_f64(float64x1_t __p0) {
  float64x1_t __ret;
  __ret = (float64x1_t) __builtin_neon_vrecpe_v((int8x8_t)__p0, 10);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64_t vrecped_f64(float64_t __p0) {
  float64_t __ret;
  __ret = (float64_t) __builtin_neon_vrecped_f64(__p0);
  return __ret;
}
#else
__ai float64_t vrecped_f64(float64_t __p0) {
  float64_t __ret;
  __ret = (float64_t) __builtin_neon_vrecped_f64(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32_t vrecpes_f32(float32_t __p0) {
  float32_t __ret;
  __ret = (float32_t) __builtin_neon_vrecpes_f32(__p0);
  return __ret;
}
#else
__ai float32_t vrecpes_f32(float32_t __p0) {
  float32_t __ret;
  __ret = (float32_t) __builtin_neon_vrecpes_f32(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x2_t vrecpsq_f64(float64x2_t __p0, float64x2_t __p1) {
  float64x2_t __ret;
  __ret = (float64x2_t) __builtin_neon_vrecpsq_v((int8x16_t)__p0, (int8x16_t)__p1, 42);
  return __ret;
}
#else
__ai float64x2_t vrecpsq_f64(float64x2_t __p0, float64x2_t __p1) {
  float64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  float64x2_t __ret;
  __ret = (float64x2_t) __builtin_neon_vrecpsq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 42);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x1_t vrecps_f64(float64x1_t __p0, float64x1_t __p1) {
  float64x1_t __ret;
  __ret = (float64x1_t) __builtin_neon_vrecps_v((int8x8_t)__p0, (int8x8_t)__p1, 10);
  return __ret;
}
#else
__ai float64x1_t vrecps_f64(float64x1_t __p0, float64x1_t __p1) {
  float64x1_t __ret;
  __ret = (float64x1_t) __builtin_neon_vrecps_v((int8x8_t)__p0, (int8x8_t)__p1, 10);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64_t vrecpsd_f64(float64_t __p0, float64_t __p1) {
  float64_t __ret;
  __ret = (float64_t) __builtin_neon_vrecpsd_f64(__p0, __p1);
  return __ret;
}
#else
__ai float64_t vrecpsd_f64(float64_t __p0, float64_t __p1) {
  float64_t __ret;
  __ret = (float64_t) __builtin_neon_vrecpsd_f64(__p0, __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32_t vrecpss_f32(float32_t __p0, float32_t __p1) {
  float32_t __ret;
  __ret = (float32_t) __builtin_neon_vrecpss_f32(__p0, __p1);
  return __ret;
}
#else
__ai float32_t vrecpss_f32(float32_t __p0, float32_t __p1) {
  float32_t __ret;
  __ret = (float32_t) __builtin_neon_vrecpss_f32(__p0, __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64_t vrecpxd_f64(float64_t __p0) {
  float64_t __ret;
  __ret = (float64_t) __builtin_neon_vrecpxd_f64(__p0);
  return __ret;
}
#else
__ai float64_t vrecpxd_f64(float64_t __p0) {
  float64_t __ret;
  __ret = (float64_t) __builtin_neon_vrecpxd_f64(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32_t vrecpxs_f32(float32_t __p0) {
  float32_t __ret;
  __ret = (float32_t) __builtin_neon_vrecpxs_f32(__p0);
  return __ret;
}
#else
__ai float32_t vrecpxs_f32(float32_t __p0) {
  float32_t __ret;
  __ret = (float32_t) __builtin_neon_vrecpxs_f32(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x2_t vrndq_f64(float64x2_t __p0) {
  float64x2_t __ret;
  __ret = (float64x2_t) __builtin_neon_vrndq_v((int8x16_t)__p0, 42);
  return __ret;
}
#else
__ai float64x2_t vrndq_f64(float64x2_t __p0) {
  float64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float64x2_t __ret;
  __ret = (float64x2_t) __builtin_neon_vrndq_v((int8x16_t)__rev0, 42);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x4_t vrndq_f32(float32x4_t __p0) {
  float32x4_t __ret;
  __ret = (float32x4_t) __builtin_neon_vrndq_v((int8x16_t)__p0, 41);
  return __ret;
}
#else
__ai float32x4_t vrndq_f32(float32x4_t __p0) {
  float32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  float32x4_t __ret;
  __ret = (float32x4_t) __builtin_neon_vrndq_v((int8x16_t)__rev0, 41);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x1_t vrnd_f64(float64x1_t __p0) {
  float64x1_t __ret;
  __ret = (float64x1_t) __builtin_neon_vrnd_v((int8x8_t)__p0, 10);
  return __ret;
}
#else
__ai float64x1_t vrnd_f64(float64x1_t __p0) {
  float64x1_t __ret;
  __ret = (float64x1_t) __builtin_neon_vrnd_v((int8x8_t)__p0, 10);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x2_t vrnd_f32(float32x2_t __p0) {
  float32x2_t __ret;
  __ret = (float32x2_t) __builtin_neon_vrnd_v((int8x8_t)__p0, 9);
  return __ret;
}
#else
__ai float32x2_t vrnd_f32(float32x2_t __p0) {
  float32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float32x2_t __ret;
  __ret = (float32x2_t) __builtin_neon_vrnd_v((int8x8_t)__rev0, 9);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x2_t vrndaq_f64(float64x2_t __p0) {
  float64x2_t __ret;
  __ret = (float64x2_t) __builtin_neon_vrndaq_v((int8x16_t)__p0, 42);
  return __ret;
}
#else
__ai float64x2_t vrndaq_f64(float64x2_t __p0) {
  float64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float64x2_t __ret;
  __ret = (float64x2_t) __builtin_neon_vrndaq_v((int8x16_t)__rev0, 42);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x4_t vrndaq_f32(float32x4_t __p0) {
  float32x4_t __ret;
  __ret = (float32x4_t) __builtin_neon_vrndaq_v((int8x16_t)__p0, 41);
  return __ret;
}
#else
__ai float32x4_t vrndaq_f32(float32x4_t __p0) {
  float32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  float32x4_t __ret;
  __ret = (float32x4_t) __builtin_neon_vrndaq_v((int8x16_t)__rev0, 41);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x1_t vrnda_f64(float64x1_t __p0) {
  float64x1_t __ret;
  __ret = (float64x1_t) __builtin_neon_vrnda_v((int8x8_t)__p0, 10);
  return __ret;
}
#else
__ai float64x1_t vrnda_f64(float64x1_t __p0) {
  float64x1_t __ret;
  __ret = (float64x1_t) __builtin_neon_vrnda_v((int8x8_t)__p0, 10);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x2_t vrnda_f32(float32x2_t __p0) {
  float32x2_t __ret;
  __ret = (float32x2_t) __builtin_neon_vrnda_v((int8x8_t)__p0, 9);
  return __ret;
}
#else
__ai float32x2_t vrnda_f32(float32x2_t __p0) {
  float32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float32x2_t __ret;
  __ret = (float32x2_t) __builtin_neon_vrnda_v((int8x8_t)__rev0, 9);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x2_t vrndiq_f64(float64x2_t __p0) {
  float64x2_t __ret;
  __ret = (float64x2_t) __builtin_neon_vrndiq_v((int8x16_t)__p0, 42);
  return __ret;
}
#else
__ai float64x2_t vrndiq_f64(float64x2_t __p0) {
  float64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float64x2_t __ret;
  __ret = (float64x2_t) __builtin_neon_vrndiq_v((int8x16_t)__rev0, 42);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x4_t vrndiq_f32(float32x4_t __p0) {
  float32x4_t __ret;
  __ret = (float32x4_t) __builtin_neon_vrndiq_v((int8x16_t)__p0, 41);
  return __ret;
}
#else
__ai float32x4_t vrndiq_f32(float32x4_t __p0) {
  float32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  float32x4_t __ret;
  __ret = (float32x4_t) __builtin_neon_vrndiq_v((int8x16_t)__rev0, 41);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x1_t vrndi_f64(float64x1_t __p0) {
  float64x1_t __ret;
  __ret = (float64x1_t) __builtin_neon_vrndi_v((int8x8_t)__p0, 10);
  return __ret;
}
#else
__ai float64x1_t vrndi_f64(float64x1_t __p0) {
  float64x1_t __ret;
  __ret = (float64x1_t) __builtin_neon_vrndi_v((int8x8_t)__p0, 10);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x2_t vrndi_f32(float32x2_t __p0) {
  float32x2_t __ret;
  __ret = (float32x2_t) __builtin_neon_vrndi_v((int8x8_t)__p0, 9);
  return __ret;
}
#else
__ai float32x2_t vrndi_f32(float32x2_t __p0) {
  float32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float32x2_t __ret;
  __ret = (float32x2_t) __builtin_neon_vrndi_v((int8x8_t)__rev0, 9);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x2_t vrndmq_f64(float64x2_t __p0) {
  float64x2_t __ret;
  __ret = (float64x2_t) __builtin_neon_vrndmq_v((int8x16_t)__p0, 42);
  return __ret;
}
#else
__ai float64x2_t vrndmq_f64(float64x2_t __p0) {
  float64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float64x2_t __ret;
  __ret = (float64x2_t) __builtin_neon_vrndmq_v((int8x16_t)__rev0, 42);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x4_t vrndmq_f32(float32x4_t __p0) {
  float32x4_t __ret;
  __ret = (float32x4_t) __builtin_neon_vrndmq_v((int8x16_t)__p0, 41);
  return __ret;
}
#else
__ai float32x4_t vrndmq_f32(float32x4_t __p0) {
  float32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  float32x4_t __ret;
  __ret = (float32x4_t) __builtin_neon_vrndmq_v((int8x16_t)__rev0, 41);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x1_t vrndm_f64(float64x1_t __p0) {
  float64x1_t __ret;
  __ret = (float64x1_t) __builtin_neon_vrndm_v((int8x8_t)__p0, 10);
  return __ret;
}
#else
__ai float64x1_t vrndm_f64(float64x1_t __p0) {
  float64x1_t __ret;
  __ret = (float64x1_t) __builtin_neon_vrndm_v((int8x8_t)__p0, 10);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x2_t vrndm_f32(float32x2_t __p0) {
  float32x2_t __ret;
  __ret = (float32x2_t) __builtin_neon_vrndm_v((int8x8_t)__p0, 9);
  return __ret;
}
#else
__ai float32x2_t vrndm_f32(float32x2_t __p0) {
  float32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float32x2_t __ret;
  __ret = (float32x2_t) __builtin_neon_vrndm_v((int8x8_t)__rev0, 9);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x2_t vrndnq_f64(float64x2_t __p0) {
  float64x2_t __ret;
  __ret = (float64x2_t) __builtin_neon_vrndnq_v((int8x16_t)__p0, 42);
  return __ret;
}
#else
__ai float64x2_t vrndnq_f64(float64x2_t __p0) {
  float64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float64x2_t __ret;
  __ret = (float64x2_t) __builtin_neon_vrndnq_v((int8x16_t)__rev0, 42);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x4_t vrndnq_f32(float32x4_t __p0) {
  float32x4_t __ret;
  __ret = (float32x4_t) __builtin_neon_vrndnq_v((int8x16_t)__p0, 41);
  return __ret;
}
#else
__ai float32x4_t vrndnq_f32(float32x4_t __p0) {
  float32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  float32x4_t __ret;
  __ret = (float32x4_t) __builtin_neon_vrndnq_v((int8x16_t)__rev0, 41);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x1_t vrndn_f64(float64x1_t __p0) {
  float64x1_t __ret;
  __ret = (float64x1_t) __builtin_neon_vrndn_v((int8x8_t)__p0, 10);
  return __ret;
}
#else
__ai float64x1_t vrndn_f64(float64x1_t __p0) {
  float64x1_t __ret;
  __ret = (float64x1_t) __builtin_neon_vrndn_v((int8x8_t)__p0, 10);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x2_t vrndn_f32(float32x2_t __p0) {
  float32x2_t __ret;
  __ret = (float32x2_t) __builtin_neon_vrndn_v((int8x8_t)__p0, 9);
  return __ret;
}
#else
__ai float32x2_t vrndn_f32(float32x2_t __p0) {
  float32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float32x2_t __ret;
  __ret = (float32x2_t) __builtin_neon_vrndn_v((int8x8_t)__rev0, 9);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x2_t vrndpq_f64(float64x2_t __p0) {
  float64x2_t __ret;
  __ret = (float64x2_t) __builtin_neon_vrndpq_v((int8x16_t)__p0, 42);
  return __ret;
}
#else
__ai float64x2_t vrndpq_f64(float64x2_t __p0) {
  float64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float64x2_t __ret;
  __ret = (float64x2_t) __builtin_neon_vrndpq_v((int8x16_t)__rev0, 42);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x4_t vrndpq_f32(float32x4_t __p0) {
  float32x4_t __ret;
  __ret = (float32x4_t) __builtin_neon_vrndpq_v((int8x16_t)__p0, 41);
  return __ret;
}
#else
__ai float32x4_t vrndpq_f32(float32x4_t __p0) {
  float32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  float32x4_t __ret;
  __ret = (float32x4_t) __builtin_neon_vrndpq_v((int8x16_t)__rev0, 41);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x1_t vrndp_f64(float64x1_t __p0) {
  float64x1_t __ret;
  __ret = (float64x1_t) __builtin_neon_vrndp_v((int8x8_t)__p0, 10);
  return __ret;
}
#else
__ai float64x1_t vrndp_f64(float64x1_t __p0) {
  float64x1_t __ret;
  __ret = (float64x1_t) __builtin_neon_vrndp_v((int8x8_t)__p0, 10);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x2_t vrndp_f32(float32x2_t __p0) {
  float32x2_t __ret;
  __ret = (float32x2_t) __builtin_neon_vrndp_v((int8x8_t)__p0, 9);
  return __ret;
}
#else
__ai float32x2_t vrndp_f32(float32x2_t __p0) {
  float32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float32x2_t __ret;
  __ret = (float32x2_t) __builtin_neon_vrndp_v((int8x8_t)__rev0, 9);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x2_t vrndxq_f64(float64x2_t __p0) {
  float64x2_t __ret;
  __ret = (float64x2_t) __builtin_neon_vrndxq_v((int8x16_t)__p0, 42);
  return __ret;
}
#else
__ai float64x2_t vrndxq_f64(float64x2_t __p0) {
  float64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float64x2_t __ret;
  __ret = (float64x2_t) __builtin_neon_vrndxq_v((int8x16_t)__rev0, 42);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x4_t vrndxq_f32(float32x4_t __p0) {
  float32x4_t __ret;
  __ret = (float32x4_t) __builtin_neon_vrndxq_v((int8x16_t)__p0, 41);
  return __ret;
}
#else
__ai float32x4_t vrndxq_f32(float32x4_t __p0) {
  float32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  float32x4_t __ret;
  __ret = (float32x4_t) __builtin_neon_vrndxq_v((int8x16_t)__rev0, 41);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x1_t vrndx_f64(float64x1_t __p0) {
  float64x1_t __ret;
  __ret = (float64x1_t) __builtin_neon_vrndx_v((int8x8_t)__p0, 10);
  return __ret;
}
#else
__ai float64x1_t vrndx_f64(float64x1_t __p0) {
  float64x1_t __ret;
  __ret = (float64x1_t) __builtin_neon_vrndx_v((int8x8_t)__p0, 10);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x2_t vrndx_f32(float32x2_t __p0) {
  float32x2_t __ret;
  __ret = (float32x2_t) __builtin_neon_vrndx_v((int8x8_t)__p0, 9);
  return __ret;
}
#else
__ai float32x2_t vrndx_f32(float32x2_t __p0) {
  float32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float32x2_t __ret;
  __ret = (float32x2_t) __builtin_neon_vrndx_v((int8x8_t)__rev0, 9);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64_t vrshld_u64(uint64_t __p0, uint64_t __p1) {
  uint64_t __ret;
  __ret = (uint64_t) __builtin_neon_vrshld_u64(__p0, __p1);
  return __ret;
}
#else
__ai uint64_t vrshld_u64(uint64_t __p0, uint64_t __p1) {
  uint64_t __ret;
  __ret = (uint64_t) __builtin_neon_vrshld_u64(__p0, __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64_t vrshld_s64(int64_t __p0, int64_t __p1) {
  int64_t __ret;
  __ret = (int64_t) __builtin_neon_vrshld_s64(__p0, __p1);
  return __ret;
}
#else
__ai int64_t vrshld_s64(int64_t __p0, int64_t __p1) {
  int64_t __ret;
  __ret = (int64_t) __builtin_neon_vrshld_s64(__p0, __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
#define vrshrd_n_u64(__p0, __p1) __extension__ ({ \
  uint64_t __s0 = __p0; \
  uint64_t __ret; \
  __ret = (uint64_t) __builtin_neon_vrshrd_n_u64(__s0, __p1); \
  __ret; \
})
#else
#define vrshrd_n_u64(__p0, __p1) __extension__ ({ \
  uint64_t __s0 = __p0; \
  uint64_t __ret; \
  __ret = (uint64_t) __builtin_neon_vrshrd_n_u64(__s0, __p1); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vrshrd_n_s64(__p0, __p1) __extension__ ({ \
  int64_t __s0 = __p0; \
  int64_t __ret; \
  __ret = (int64_t) __builtin_neon_vrshrd_n_s64(__s0, __p1); \
  __ret; \
})
#else
#define vrshrd_n_s64(__p0, __p1) __extension__ ({ \
  int64_t __s0 = __p0; \
  int64_t __ret; \
  __ret = (int64_t) __builtin_neon_vrshrd_n_s64(__s0, __p1); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vrshrn_high_n_u32(__p0_210, __p1_210, __p2_210) __extension__ ({ \
  uint16x4_t __s0_210 = __p0_210; \
  uint32x4_t __s1_210 = __p1_210; \
  uint16x8_t __ret_210; \
  __ret_210 = (uint16x8_t)(vcombine_u16((uint16x4_t)(__s0_210), (uint16x4_t)(vrshrn_n_u32(__s1_210, __p2_210)))); \
  __ret_210; \
})
#else
#define vrshrn_high_n_u32(__p0_211, __p1_211, __p2_211) __extension__ ({ \
  uint16x4_t __s0_211 = __p0_211; \
  uint32x4_t __s1_211 = __p1_211; \
  uint16x4_t __rev0_211;  __rev0_211 = __builtin_shufflevector(__s0_211, __s0_211, 3, 2, 1, 0); \
  uint32x4_t __rev1_211;  __rev1_211 = __builtin_shufflevector(__s1_211, __s1_211, 3, 2, 1, 0); \
  uint16x8_t __ret_211; \
  __ret_211 = (uint16x8_t)(__noswap_vcombine_u16((uint16x4_t)(__rev0_211), (uint16x4_t)(__noswap_vrshrn_n_u32(__rev1_211, __p2_211)))); \
  __ret_211 = __builtin_shufflevector(__ret_211, __ret_211, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret_211; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vrshrn_high_n_u64(__p0_212, __p1_212, __p2_212) __extension__ ({ \
  uint32x2_t __s0_212 = __p0_212; \
  uint64x2_t __s1_212 = __p1_212; \
  uint32x4_t __ret_212; \
  __ret_212 = (uint32x4_t)(vcombine_u32((uint32x2_t)(__s0_212), (uint32x2_t)(vrshrn_n_u64(__s1_212, __p2_212)))); \
  __ret_212; \
})
#else
#define vrshrn_high_n_u64(__p0_213, __p1_213, __p2_213) __extension__ ({ \
  uint32x2_t __s0_213 = __p0_213; \
  uint64x2_t __s1_213 = __p1_213; \
  uint32x2_t __rev0_213;  __rev0_213 = __builtin_shufflevector(__s0_213, __s0_213, 1, 0); \
  uint64x2_t __rev1_213;  __rev1_213 = __builtin_shufflevector(__s1_213, __s1_213, 1, 0); \
  uint32x4_t __ret_213; \
  __ret_213 = (uint32x4_t)(__noswap_vcombine_u32((uint32x2_t)(__rev0_213), (uint32x2_t)(__noswap_vrshrn_n_u64(__rev1_213, __p2_213)))); \
  __ret_213 = __builtin_shufflevector(__ret_213, __ret_213, 3, 2, 1, 0); \
  __ret_213; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vrshrn_high_n_u16(__p0_214, __p1_214, __p2_214) __extension__ ({ \
  uint8x8_t __s0_214 = __p0_214; \
  uint16x8_t __s1_214 = __p1_214; \
  uint8x16_t __ret_214; \
  __ret_214 = (uint8x16_t)(vcombine_u8((uint8x8_t)(__s0_214), (uint8x8_t)(vrshrn_n_u16(__s1_214, __p2_214)))); \
  __ret_214; \
})
#else
#define vrshrn_high_n_u16(__p0_215, __p1_215, __p2_215) __extension__ ({ \
  uint8x8_t __s0_215 = __p0_215; \
  uint16x8_t __s1_215 = __p1_215; \
  uint8x8_t __rev0_215;  __rev0_215 = __builtin_shufflevector(__s0_215, __s0_215, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint16x8_t __rev1_215;  __rev1_215 = __builtin_shufflevector(__s1_215, __s1_215, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint8x16_t __ret_215; \
  __ret_215 = (uint8x16_t)(__noswap_vcombine_u8((uint8x8_t)(__rev0_215), (uint8x8_t)(__noswap_vrshrn_n_u16(__rev1_215, __p2_215)))); \
  __ret_215 = __builtin_shufflevector(__ret_215, __ret_215, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret_215; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vrshrn_high_n_s32(__p0_216, __p1_216, __p2_216) __extension__ ({ \
  int16x4_t __s0_216 = __p0_216; \
  int32x4_t __s1_216 = __p1_216; \
  int16x8_t __ret_216; \
  __ret_216 = (int16x8_t)(vcombine_s16((int16x4_t)(__s0_216), (int16x4_t)(vrshrn_n_s32(__s1_216, __p2_216)))); \
  __ret_216; \
})
#else
#define vrshrn_high_n_s32(__p0_217, __p1_217, __p2_217) __extension__ ({ \
  int16x4_t __s0_217 = __p0_217; \
  int32x4_t __s1_217 = __p1_217; \
  int16x4_t __rev0_217;  __rev0_217 = __builtin_shufflevector(__s0_217, __s0_217, 3, 2, 1, 0); \
  int32x4_t __rev1_217;  __rev1_217 = __builtin_shufflevector(__s1_217, __s1_217, 3, 2, 1, 0); \
  int16x8_t __ret_217; \
  __ret_217 = (int16x8_t)(__noswap_vcombine_s16((int16x4_t)(__rev0_217), (int16x4_t)(__noswap_vrshrn_n_s32(__rev1_217, __p2_217)))); \
  __ret_217 = __builtin_shufflevector(__ret_217, __ret_217, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret_217; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vrshrn_high_n_s64(__p0_218, __p1_218, __p2_218) __extension__ ({ \
  int32x2_t __s0_218 = __p0_218; \
  int64x2_t __s1_218 = __p1_218; \
  int32x4_t __ret_218; \
  __ret_218 = (int32x4_t)(vcombine_s32((int32x2_t)(__s0_218), (int32x2_t)(vrshrn_n_s64(__s1_218, __p2_218)))); \
  __ret_218; \
})
#else
#define vrshrn_high_n_s64(__p0_219, __p1_219, __p2_219) __extension__ ({ \
  int32x2_t __s0_219 = __p0_219; \
  int64x2_t __s1_219 = __p1_219; \
  int32x2_t __rev0_219;  __rev0_219 = __builtin_shufflevector(__s0_219, __s0_219, 1, 0); \
  int64x2_t __rev1_219;  __rev1_219 = __builtin_shufflevector(__s1_219, __s1_219, 1, 0); \
  int32x4_t __ret_219; \
  __ret_219 = (int32x4_t)(__noswap_vcombine_s32((int32x2_t)(__rev0_219), (int32x2_t)(__noswap_vrshrn_n_s64(__rev1_219, __p2_219)))); \
  __ret_219 = __builtin_shufflevector(__ret_219, __ret_219, 3, 2, 1, 0); \
  __ret_219; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vrshrn_high_n_s16(__p0_220, __p1_220, __p2_220) __extension__ ({ \
  int8x8_t __s0_220 = __p0_220; \
  int16x8_t __s1_220 = __p1_220; \
  int8x16_t __ret_220; \
  __ret_220 = (int8x16_t)(vcombine_s8((int8x8_t)(__s0_220), (int8x8_t)(vrshrn_n_s16(__s1_220, __p2_220)))); \
  __ret_220; \
})
#else
#define vrshrn_high_n_s16(__p0_221, __p1_221, __p2_221) __extension__ ({ \
  int8x8_t __s0_221 = __p0_221; \
  int16x8_t __s1_221 = __p1_221; \
  int8x8_t __rev0_221;  __rev0_221 = __builtin_shufflevector(__s0_221, __s0_221, 7, 6, 5, 4, 3, 2, 1, 0); \
  int16x8_t __rev1_221;  __rev1_221 = __builtin_shufflevector(__s1_221, __s1_221, 7, 6, 5, 4, 3, 2, 1, 0); \
  int8x16_t __ret_221; \
  __ret_221 = (int8x16_t)(__noswap_vcombine_s8((int8x8_t)(__rev0_221), (int8x8_t)(__noswap_vrshrn_n_s16(__rev1_221, __p2_221)))); \
  __ret_221 = __builtin_shufflevector(__ret_221, __ret_221, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret_221; \
})
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x2_t vrsqrteq_f64(float64x2_t __p0) {
  float64x2_t __ret;
  __ret = (float64x2_t) __builtin_neon_vrsqrteq_v((int8x16_t)__p0, 42);
  return __ret;
}
#else
__ai float64x2_t vrsqrteq_f64(float64x2_t __p0) {
  float64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float64x2_t __ret;
  __ret = (float64x2_t) __builtin_neon_vrsqrteq_v((int8x16_t)__rev0, 42);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x1_t vrsqrte_f64(float64x1_t __p0) {
  float64x1_t __ret;
  __ret = (float64x1_t) __builtin_neon_vrsqrte_v((int8x8_t)__p0, 10);
  return __ret;
}
#else
__ai float64x1_t vrsqrte_f64(float64x1_t __p0) {
  float64x1_t __ret;
  __ret = (float64x1_t) __builtin_neon_vrsqrte_v((int8x8_t)__p0, 10);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64_t vrsqrted_f64(float64_t __p0) {
  float64_t __ret;
  __ret = (float64_t) __builtin_neon_vrsqrted_f64(__p0);
  return __ret;
}
#else
__ai float64_t vrsqrted_f64(float64_t __p0) {
  float64_t __ret;
  __ret = (float64_t) __builtin_neon_vrsqrted_f64(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32_t vrsqrtes_f32(float32_t __p0) {
  float32_t __ret;
  __ret = (float32_t) __builtin_neon_vrsqrtes_f32(__p0);
  return __ret;
}
#else
__ai float32_t vrsqrtes_f32(float32_t __p0) {
  float32_t __ret;
  __ret = (float32_t) __builtin_neon_vrsqrtes_f32(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x2_t vrsqrtsq_f64(float64x2_t __p0, float64x2_t __p1) {
  float64x2_t __ret;
  __ret = (float64x2_t) __builtin_neon_vrsqrtsq_v((int8x16_t)__p0, (int8x16_t)__p1, 42);
  return __ret;
}
#else
__ai float64x2_t vrsqrtsq_f64(float64x2_t __p0, float64x2_t __p1) {
  float64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  float64x2_t __ret;
  __ret = (float64x2_t) __builtin_neon_vrsqrtsq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 42);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x1_t vrsqrts_f64(float64x1_t __p0, float64x1_t __p1) {
  float64x1_t __ret;
  __ret = (float64x1_t) __builtin_neon_vrsqrts_v((int8x8_t)__p0, (int8x8_t)__p1, 10);
  return __ret;
}
#else
__ai float64x1_t vrsqrts_f64(float64x1_t __p0, float64x1_t __p1) {
  float64x1_t __ret;
  __ret = (float64x1_t) __builtin_neon_vrsqrts_v((int8x8_t)__p0, (int8x8_t)__p1, 10);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64_t vrsqrtsd_f64(float64_t __p0, float64_t __p1) {
  float64_t __ret;
  __ret = (float64_t) __builtin_neon_vrsqrtsd_f64(__p0, __p1);
  return __ret;
}
#else
__ai float64_t vrsqrtsd_f64(float64_t __p0, float64_t __p1) {
  float64_t __ret;
  __ret = (float64_t) __builtin_neon_vrsqrtsd_f64(__p0, __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32_t vrsqrtss_f32(float32_t __p0, float32_t __p1) {
  float32_t __ret;
  __ret = (float32_t) __builtin_neon_vrsqrtss_f32(__p0, __p1);
  return __ret;
}
#else
__ai float32_t vrsqrtss_f32(float32_t __p0, float32_t __p1) {
  float32_t __ret;
  __ret = (float32_t) __builtin_neon_vrsqrtss_f32(__p0, __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
#define vrsrad_n_u64(__p0, __p1, __p2) __extension__ ({ \
  uint64_t __s0 = __p0; \
  uint64_t __s1 = __p1; \
  uint64_t __ret; \
  __ret = (uint64_t) __builtin_neon_vrsrad_n_u64(__s0, __s1, __p2); \
  __ret; \
})
#else
#define vrsrad_n_u64(__p0, __p1, __p2) __extension__ ({ \
  uint64_t __s0 = __p0; \
  uint64_t __s1 = __p1; \
  uint64_t __ret; \
  __ret = (uint64_t) __builtin_neon_vrsrad_n_u64(__s0, __s1, __p2); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vrsrad_n_s64(__p0, __p1, __p2) __extension__ ({ \
  int64_t __s0 = __p0; \
  int64_t __s1 = __p1; \
  int64_t __ret; \
  __ret = (int64_t) __builtin_neon_vrsrad_n_s64(__s0, __s1, __p2); \
  __ret; \
})
#else
#define vrsrad_n_s64(__p0, __p1, __p2) __extension__ ({ \
  int64_t __s0 = __p0; \
  int64_t __s1 = __p1; \
  int64_t __ret; \
  __ret = (int64_t) __builtin_neon_vrsrad_n_s64(__s0, __s1, __p2); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x8_t vrsubhn_high_u32(uint16x4_t __p0, uint32x4_t __p1, uint32x4_t __p2) {
  uint16x8_t __ret;
  __ret = vcombine_u16(__p0, vrsubhn_u32(__p1, __p2));
  return __ret;
}
#else
__ai uint16x8_t vrsubhn_high_u32(uint16x4_t __p0, uint32x4_t __p1, uint32x4_t __p2) {
  uint16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint32x4_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 3, 2, 1, 0);
  uint16x8_t __ret;
  __ret = __noswap_vcombine_u16(__rev0, __noswap_vrsubhn_u32(__rev1, __rev2));
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vrsubhn_high_u64(uint32x2_t __p0, uint64x2_t __p1, uint64x2_t __p2) {
  uint32x4_t __ret;
  __ret = vcombine_u32(__p0, vrsubhn_u64(__p1, __p2));
  return __ret;
}
#else
__ai uint32x4_t vrsubhn_high_u64(uint32x2_t __p0, uint64x2_t __p1, uint64x2_t __p2) {
  uint32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  uint64x2_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 1, 0);
  uint32x4_t __ret;
  __ret = __noswap_vcombine_u32(__rev0, __noswap_vrsubhn_u64(__rev1, __rev2));
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x16_t vrsubhn_high_u16(uint8x8_t __p0, uint16x8_t __p1, uint16x8_t __p2) {
  uint8x16_t __ret;
  __ret = vcombine_u8(__p0, vrsubhn_u16(__p1, __p2));
  return __ret;
}
#else
__ai uint8x16_t vrsubhn_high_u16(uint8x8_t __p0, uint16x8_t __p1, uint16x8_t __p2) {
  uint8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __ret;
  __ret = __noswap_vcombine_u8(__rev0, __noswap_vrsubhn_u16(__rev1, __rev2));
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x8_t vrsubhn_high_s32(int16x4_t __p0, int32x4_t __p1, int32x4_t __p2) {
  int16x8_t __ret;
  __ret = vcombine_s16(__p0, vrsubhn_s32(__p1, __p2));
  return __ret;
}
#else
__ai int16x8_t vrsubhn_high_s32(int16x4_t __p0, int32x4_t __p1, int32x4_t __p2) {
  int16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int32x4_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 3, 2, 1, 0);
  int16x8_t __ret;
  __ret = __noswap_vcombine_s16(__rev0, __noswap_vrsubhn_s32(__rev1, __rev2));
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vrsubhn_high_s64(int32x2_t __p0, int64x2_t __p1, int64x2_t __p2) {
  int32x4_t __ret;
  __ret = vcombine_s32(__p0, vrsubhn_s64(__p1, __p2));
  return __ret;
}
#else
__ai int32x4_t vrsubhn_high_s64(int32x2_t __p0, int64x2_t __p1, int64x2_t __p2) {
  int32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  int64x2_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 1, 0);
  int32x4_t __ret;
  __ret = __noswap_vcombine_s32(__rev0, __noswap_vrsubhn_s64(__rev1, __rev2));
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x16_t vrsubhn_high_s16(int8x8_t __p0, int16x8_t __p1, int16x8_t __p2) {
  int8x16_t __ret;
  __ret = vcombine_s8(__p0, vrsubhn_s16(__p1, __p2));
  return __ret;
}
#else
__ai int8x16_t vrsubhn_high_s16(int8x8_t __p0, int16x8_t __p1, int16x8_t __p2) {
  int8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __ret;
  __ret = __noswap_vcombine_s8(__rev0, __noswap_vrsubhn_s16(__rev1, __rev2));
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
#define vset_lane_f16(__p0_222, __p1_222, __p2_222) __extension__ ({ \
  float16_t __s0_222 = __p0_222; \
  float16x4_t __s1_222 = __p1_222; \
  float16x4_t __ret_222; \
float16_t __reint_222 = __s0_222; \
float16x4_t __reint1_222 = __s1_222; \
int16x4_t __reint2_222 = vset_lane_s16(*(int16_t *) &__reint_222, *(int16x4_t *) &__reint1_222, __p2_222); \
  __ret_222 = *(float16x4_t *) &__reint2_222; \
  __ret_222; \
})
#else
#define vset_lane_f16(__p0_223, __p1_223, __p2_223) __extension__ ({ \
  float16_t __s0_223 = __p0_223; \
  float16x4_t __s1_223 = __p1_223; \
  float16x4_t __rev1_223;  __rev1_223 = __builtin_shufflevector(__s1_223, __s1_223, 3, 2, 1, 0); \
  float16x4_t __ret_223; \
float16_t __reint_223 = __s0_223; \
float16x4_t __reint1_223 = __rev1_223; \
int16x4_t __reint2_223 = __noswap_vset_lane_s16(*(int16_t *) &__reint_223, *(int16x4_t *) &__reint1_223, __p2_223); \
  __ret_223 = *(float16x4_t *) &__reint2_223; \
  __ret_223 = __builtin_shufflevector(__ret_223, __ret_223, 3, 2, 1, 0); \
  __ret_223; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vsetq_lane_f16(__p0_224, __p1_224, __p2_224) __extension__ ({ \
  float16_t __s0_224 = __p0_224; \
  float16x8_t __s1_224 = __p1_224; \
  float16x8_t __ret_224; \
float16_t __reint_224 = __s0_224; \
float16x8_t __reint1_224 = __s1_224; \
int16x8_t __reint2_224 = vsetq_lane_s16(*(int16_t *) &__reint_224, *(int16x8_t *) &__reint1_224, __p2_224); \
  __ret_224 = *(float16x8_t *) &__reint2_224; \
  __ret_224; \
})
#else
#define vsetq_lane_f16(__p0_225, __p1_225, __p2_225) __extension__ ({ \
  float16_t __s0_225 = __p0_225; \
  float16x8_t __s1_225 = __p1_225; \
  float16x8_t __rev1_225;  __rev1_225 = __builtin_shufflevector(__s1_225, __s1_225, 7, 6, 5, 4, 3, 2, 1, 0); \
  float16x8_t __ret_225; \
float16_t __reint_225 = __s0_225; \
float16x8_t __reint1_225 = __rev1_225; \
int16x8_t __reint2_225 = __noswap_vsetq_lane_s16(*(int16_t *) &__reint_225, *(int16x8_t *) &__reint1_225, __p2_225); \
  __ret_225 = *(float16x8_t *) &__reint2_225; \
  __ret_225 = __builtin_shufflevector(__ret_225, __ret_225, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret_225; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vset_lane_p64(__p0, __p1, __p2) __extension__ ({ \
  poly64_t __s0 = __p0; \
  poly64x1_t __s1 = __p1; \
  poly64x1_t __ret; \
  __ret = (poly64x1_t) __builtin_neon_vset_lane_i64(__s0, (int8x8_t)__s1, __p2); \
  __ret; \
})
#else
#define vset_lane_p64(__p0, __p1, __p2) __extension__ ({ \
  poly64_t __s0 = __p0; \
  poly64x1_t __s1 = __p1; \
  poly64x1_t __ret; \
  __ret = (poly64x1_t) __builtin_neon_vset_lane_i64(__s0, (int8x8_t)__s1, __p2); \
  __ret; \
})
#define __noswap_vset_lane_p64(__p0, __p1, __p2) __extension__ ({ \
  poly64_t __s0 = __p0; \
  poly64x1_t __s1 = __p1; \
  poly64x1_t __ret; \
  __ret = (poly64x1_t) __builtin_neon_vset_lane_i64(__s0, (int8x8_t)__s1, __p2); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vsetq_lane_p64(__p0, __p1, __p2) __extension__ ({ \
  poly64_t __s0 = __p0; \
  poly64x2_t __s1 = __p1; \
  poly64x2_t __ret; \
  __ret = (poly64x2_t) __builtin_neon_vsetq_lane_i64(__s0, (int8x16_t)__s1, __p2); \
  __ret; \
})
#else
#define vsetq_lane_p64(__p0, __p1, __p2) __extension__ ({ \
  poly64_t __s0 = __p0; \
  poly64x2_t __s1 = __p1; \
  poly64x2_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 1, 0); \
  poly64x2_t __ret; \
  __ret = (poly64x2_t) __builtin_neon_vsetq_lane_i64(__s0, (int8x16_t)__rev1, __p2); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#define __noswap_vsetq_lane_p64(__p0, __p1, __p2) __extension__ ({ \
  poly64_t __s0 = __p0; \
  poly64x2_t __s1 = __p1; \
  poly64x2_t __ret; \
  __ret = (poly64x2_t) __builtin_neon_vsetq_lane_i64(__s0, (int8x16_t)__s1, __p2); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vsetq_lane_f64(__p0, __p1, __p2) __extension__ ({ \
  float64_t __s0 = __p0; \
  float64x2_t __s1 = __p1; \
  float64x2_t __ret; \
  __ret = (float64x2_t) __builtin_neon_vsetq_lane_f64(__s0, (int8x16_t)__s1, __p2); \
  __ret; \
})
#else
#define vsetq_lane_f64(__p0, __p1, __p2) __extension__ ({ \
  float64_t __s0 = __p0; \
  float64x2_t __s1 = __p1; \
  float64x2_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 1, 0); \
  float64x2_t __ret; \
  __ret = (float64x2_t) __builtin_neon_vsetq_lane_f64(__s0, (int8x16_t)__rev1, __p2); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#define __noswap_vsetq_lane_f64(__p0, __p1, __p2) __extension__ ({ \
  float64_t __s0 = __p0; \
  float64x2_t __s1 = __p1; \
  float64x2_t __ret; \
  __ret = (float64x2_t) __builtin_neon_vsetq_lane_f64(__s0, (int8x16_t)__s1, __p2); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vset_lane_f64(__p0, __p1, __p2) __extension__ ({ \
  float64_t __s0 = __p0; \
  float64x1_t __s1 = __p1; \
  float64x1_t __ret; \
  __ret = (float64x1_t) __builtin_neon_vset_lane_f64(__s0, (int8x8_t)__s1, __p2); \
  __ret; \
})
#else
#define vset_lane_f64(__p0, __p1, __p2) __extension__ ({ \
  float64_t __s0 = __p0; \
  float64x1_t __s1 = __p1; \
  float64x1_t __ret; \
  __ret = (float64x1_t) __builtin_neon_vset_lane_f64(__s0, (int8x8_t)__s1, __p2); \
  __ret; \
})
#define __noswap_vset_lane_f64(__p0, __p1, __p2) __extension__ ({ \
  float64_t __s0 = __p0; \
  float64x1_t __s1 = __p1; \
  float64x1_t __ret; \
  __ret = (float64x1_t) __builtin_neon_vset_lane_f64(__s0, (int8x8_t)__s1, __p2); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64_t vshld_u64(uint64_t __p0, uint64_t __p1) {
  uint64_t __ret;
  __ret = (uint64_t) __builtin_neon_vshld_u64(__p0, __p1);
  return __ret;
}
#else
__ai uint64_t vshld_u64(uint64_t __p0, uint64_t __p1) {
  uint64_t __ret;
  __ret = (uint64_t) __builtin_neon_vshld_u64(__p0, __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64_t vshld_s64(int64_t __p0, int64_t __p1) {
  int64_t __ret;
  __ret = (int64_t) __builtin_neon_vshld_s64(__p0, __p1);
  return __ret;
}
#else
__ai int64_t vshld_s64(int64_t __p0, int64_t __p1) {
  int64_t __ret;
  __ret = (int64_t) __builtin_neon_vshld_s64(__p0, __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
#define vshld_n_u64(__p0, __p1) __extension__ ({ \
  uint64_t __s0 = __p0; \
  uint64_t __ret; \
  __ret = (uint64_t) __builtin_neon_vshld_n_u64(__s0, __p1); \
  __ret; \
})
#else
#define vshld_n_u64(__p0, __p1) __extension__ ({ \
  uint64_t __s0 = __p0; \
  uint64_t __ret; \
  __ret = (uint64_t) __builtin_neon_vshld_n_u64(__s0, __p1); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vshld_n_s64(__p0, __p1) __extension__ ({ \
  int64_t __s0 = __p0; \
  int64_t __ret; \
  __ret = (int64_t) __builtin_neon_vshld_n_s64(__s0, __p1); \
  __ret; \
})
#else
#define vshld_n_s64(__p0, __p1) __extension__ ({ \
  int64_t __s0 = __p0; \
  int64_t __ret; \
  __ret = (int64_t) __builtin_neon_vshld_n_s64(__s0, __p1); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vshll_high_n_u8(__p0_226, __p1_226) __extension__ ({ \
  uint8x16_t __s0_226 = __p0_226; \
  uint16x8_t __ret_226; \
  __ret_226 = (uint16x8_t)(vshll_n_u8(vget_high_u8(__s0_226), __p1_226)); \
  __ret_226; \
})
#else
#define vshll_high_n_u8(__p0_227, __p1_227) __extension__ ({ \
  uint8x16_t __s0_227 = __p0_227; \
  uint8x16_t __rev0_227;  __rev0_227 = __builtin_shufflevector(__s0_227, __s0_227, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint16x8_t __ret_227; \
  __ret_227 = (uint16x8_t)(__noswap_vshll_n_u8(__noswap_vget_high_u8(__rev0_227), __p1_227)); \
  __ret_227 = __builtin_shufflevector(__ret_227, __ret_227, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret_227; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vshll_high_n_u32(__p0_228, __p1_228) __extension__ ({ \
  uint32x4_t __s0_228 = __p0_228; \
  uint64x2_t __ret_228; \
  __ret_228 = (uint64x2_t)(vshll_n_u32(vget_high_u32(__s0_228), __p1_228)); \
  __ret_228; \
})
#else
#define vshll_high_n_u32(__p0_229, __p1_229) __extension__ ({ \
  uint32x4_t __s0_229 = __p0_229; \
  uint32x4_t __rev0_229;  __rev0_229 = __builtin_shufflevector(__s0_229, __s0_229, 3, 2, 1, 0); \
  uint64x2_t __ret_229; \
  __ret_229 = (uint64x2_t)(__noswap_vshll_n_u32(__noswap_vget_high_u32(__rev0_229), __p1_229)); \
  __ret_229 = __builtin_shufflevector(__ret_229, __ret_229, 1, 0); \
  __ret_229; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vshll_high_n_u16(__p0_230, __p1_230) __extension__ ({ \
  uint16x8_t __s0_230 = __p0_230; \
  uint32x4_t __ret_230; \
  __ret_230 = (uint32x4_t)(vshll_n_u16(vget_high_u16(__s0_230), __p1_230)); \
  __ret_230; \
})
#else
#define vshll_high_n_u16(__p0_231, __p1_231) __extension__ ({ \
  uint16x8_t __s0_231 = __p0_231; \
  uint16x8_t __rev0_231;  __rev0_231 = __builtin_shufflevector(__s0_231, __s0_231, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint32x4_t __ret_231; \
  __ret_231 = (uint32x4_t)(__noswap_vshll_n_u16(__noswap_vget_high_u16(__rev0_231), __p1_231)); \
  __ret_231 = __builtin_shufflevector(__ret_231, __ret_231, 3, 2, 1, 0); \
  __ret_231; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vshll_high_n_s8(__p0_232, __p1_232) __extension__ ({ \
  int8x16_t __s0_232 = __p0_232; \
  int16x8_t __ret_232; \
  __ret_232 = (int16x8_t)(vshll_n_s8(vget_high_s8(__s0_232), __p1_232)); \
  __ret_232; \
})
#else
#define vshll_high_n_s8(__p0_233, __p1_233) __extension__ ({ \
  int8x16_t __s0_233 = __p0_233; \
  int8x16_t __rev0_233;  __rev0_233 = __builtin_shufflevector(__s0_233, __s0_233, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  int16x8_t __ret_233; \
  __ret_233 = (int16x8_t)(__noswap_vshll_n_s8(__noswap_vget_high_s8(__rev0_233), __p1_233)); \
  __ret_233 = __builtin_shufflevector(__ret_233, __ret_233, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret_233; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vshll_high_n_s32(__p0_234, __p1_234) __extension__ ({ \
  int32x4_t __s0_234 = __p0_234; \
  int64x2_t __ret_234; \
  __ret_234 = (int64x2_t)(vshll_n_s32(vget_high_s32(__s0_234), __p1_234)); \
  __ret_234; \
})
#else
#define vshll_high_n_s32(__p0_235, __p1_235) __extension__ ({ \
  int32x4_t __s0_235 = __p0_235; \
  int32x4_t __rev0_235;  __rev0_235 = __builtin_shufflevector(__s0_235, __s0_235, 3, 2, 1, 0); \
  int64x2_t __ret_235; \
  __ret_235 = (int64x2_t)(__noswap_vshll_n_s32(__noswap_vget_high_s32(__rev0_235), __p1_235)); \
  __ret_235 = __builtin_shufflevector(__ret_235, __ret_235, 1, 0); \
  __ret_235; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vshll_high_n_s16(__p0_236, __p1_236) __extension__ ({ \
  int16x8_t __s0_236 = __p0_236; \
  int32x4_t __ret_236; \
  __ret_236 = (int32x4_t)(vshll_n_s16(vget_high_s16(__s0_236), __p1_236)); \
  __ret_236; \
})
#else
#define vshll_high_n_s16(__p0_237, __p1_237) __extension__ ({ \
  int16x8_t __s0_237 = __p0_237; \
  int16x8_t __rev0_237;  __rev0_237 = __builtin_shufflevector(__s0_237, __s0_237, 7, 6, 5, 4, 3, 2, 1, 0); \
  int32x4_t __ret_237; \
  __ret_237 = (int32x4_t)(__noswap_vshll_n_s16(__noswap_vget_high_s16(__rev0_237), __p1_237)); \
  __ret_237 = __builtin_shufflevector(__ret_237, __ret_237, 3, 2, 1, 0); \
  __ret_237; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vshrd_n_u64(__p0, __p1) __extension__ ({ \
  uint64_t __s0 = __p0; \
  uint64_t __ret; \
  __ret = (uint64_t) __builtin_neon_vshrd_n_u64(__s0, __p1); \
  __ret; \
})
#else
#define vshrd_n_u64(__p0, __p1) __extension__ ({ \
  uint64_t __s0 = __p0; \
  uint64_t __ret; \
  __ret = (uint64_t) __builtin_neon_vshrd_n_u64(__s0, __p1); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vshrd_n_s64(__p0, __p1) __extension__ ({ \
  int64_t __s0 = __p0; \
  int64_t __ret; \
  __ret = (int64_t) __builtin_neon_vshrd_n_s64(__s0, __p1); \
  __ret; \
})
#else
#define vshrd_n_s64(__p0, __p1) __extension__ ({ \
  int64_t __s0 = __p0; \
  int64_t __ret; \
  __ret = (int64_t) __builtin_neon_vshrd_n_s64(__s0, __p1); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vshrn_high_n_u32(__p0_238, __p1_238, __p2_238) __extension__ ({ \
  uint16x4_t __s0_238 = __p0_238; \
  uint32x4_t __s1_238 = __p1_238; \
  uint16x8_t __ret_238; \
  __ret_238 = (uint16x8_t)(vcombine_u16((uint16x4_t)(__s0_238), (uint16x4_t)(vshrn_n_u32(__s1_238, __p2_238)))); \
  __ret_238; \
})
#else
#define vshrn_high_n_u32(__p0_239, __p1_239, __p2_239) __extension__ ({ \
  uint16x4_t __s0_239 = __p0_239; \
  uint32x4_t __s1_239 = __p1_239; \
  uint16x4_t __rev0_239;  __rev0_239 = __builtin_shufflevector(__s0_239, __s0_239, 3, 2, 1, 0); \
  uint32x4_t __rev1_239;  __rev1_239 = __builtin_shufflevector(__s1_239, __s1_239, 3, 2, 1, 0); \
  uint16x8_t __ret_239; \
  __ret_239 = (uint16x8_t)(__noswap_vcombine_u16((uint16x4_t)(__rev0_239), (uint16x4_t)(__noswap_vshrn_n_u32(__rev1_239, __p2_239)))); \
  __ret_239 = __builtin_shufflevector(__ret_239, __ret_239, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret_239; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vshrn_high_n_u64(__p0_240, __p1_240, __p2_240) __extension__ ({ \
  uint32x2_t __s0_240 = __p0_240; \
  uint64x2_t __s1_240 = __p1_240; \
  uint32x4_t __ret_240; \
  __ret_240 = (uint32x4_t)(vcombine_u32((uint32x2_t)(__s0_240), (uint32x2_t)(vshrn_n_u64(__s1_240, __p2_240)))); \
  __ret_240; \
})
#else
#define vshrn_high_n_u64(__p0_241, __p1_241, __p2_241) __extension__ ({ \
  uint32x2_t __s0_241 = __p0_241; \
  uint64x2_t __s1_241 = __p1_241; \
  uint32x2_t __rev0_241;  __rev0_241 = __builtin_shufflevector(__s0_241, __s0_241, 1, 0); \
  uint64x2_t __rev1_241;  __rev1_241 = __builtin_shufflevector(__s1_241, __s1_241, 1, 0); \
  uint32x4_t __ret_241; \
  __ret_241 = (uint32x4_t)(__noswap_vcombine_u32((uint32x2_t)(__rev0_241), (uint32x2_t)(__noswap_vshrn_n_u64(__rev1_241, __p2_241)))); \
  __ret_241 = __builtin_shufflevector(__ret_241, __ret_241, 3, 2, 1, 0); \
  __ret_241; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vshrn_high_n_u16(__p0_242, __p1_242, __p2_242) __extension__ ({ \
  uint8x8_t __s0_242 = __p0_242; \
  uint16x8_t __s1_242 = __p1_242; \
  uint8x16_t __ret_242; \
  __ret_242 = (uint8x16_t)(vcombine_u8((uint8x8_t)(__s0_242), (uint8x8_t)(vshrn_n_u16(__s1_242, __p2_242)))); \
  __ret_242; \
})
#else
#define vshrn_high_n_u16(__p0_243, __p1_243, __p2_243) __extension__ ({ \
  uint8x8_t __s0_243 = __p0_243; \
  uint16x8_t __s1_243 = __p1_243; \
  uint8x8_t __rev0_243;  __rev0_243 = __builtin_shufflevector(__s0_243, __s0_243, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint16x8_t __rev1_243;  __rev1_243 = __builtin_shufflevector(__s1_243, __s1_243, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint8x16_t __ret_243; \
  __ret_243 = (uint8x16_t)(__noswap_vcombine_u8((uint8x8_t)(__rev0_243), (uint8x8_t)(__noswap_vshrn_n_u16(__rev1_243, __p2_243)))); \
  __ret_243 = __builtin_shufflevector(__ret_243, __ret_243, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret_243; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vshrn_high_n_s32(__p0_244, __p1_244, __p2_244) __extension__ ({ \
  int16x4_t __s0_244 = __p0_244; \
  int32x4_t __s1_244 = __p1_244; \
  int16x8_t __ret_244; \
  __ret_244 = (int16x8_t)(vcombine_s16((int16x4_t)(__s0_244), (int16x4_t)(vshrn_n_s32(__s1_244, __p2_244)))); \
  __ret_244; \
})
#else
#define vshrn_high_n_s32(__p0_245, __p1_245, __p2_245) __extension__ ({ \
  int16x4_t __s0_245 = __p0_245; \
  int32x4_t __s1_245 = __p1_245; \
  int16x4_t __rev0_245;  __rev0_245 = __builtin_shufflevector(__s0_245, __s0_245, 3, 2, 1, 0); \
  int32x4_t __rev1_245;  __rev1_245 = __builtin_shufflevector(__s1_245, __s1_245, 3, 2, 1, 0); \
  int16x8_t __ret_245; \
  __ret_245 = (int16x8_t)(__noswap_vcombine_s16((int16x4_t)(__rev0_245), (int16x4_t)(__noswap_vshrn_n_s32(__rev1_245, __p2_245)))); \
  __ret_245 = __builtin_shufflevector(__ret_245, __ret_245, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret_245; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vshrn_high_n_s64(__p0_246, __p1_246, __p2_246) __extension__ ({ \
  int32x2_t __s0_246 = __p0_246; \
  int64x2_t __s1_246 = __p1_246; \
  int32x4_t __ret_246; \
  __ret_246 = (int32x4_t)(vcombine_s32((int32x2_t)(__s0_246), (int32x2_t)(vshrn_n_s64(__s1_246, __p2_246)))); \
  __ret_246; \
})
#else
#define vshrn_high_n_s64(__p0_247, __p1_247, __p2_247) __extension__ ({ \
  int32x2_t __s0_247 = __p0_247; \
  int64x2_t __s1_247 = __p1_247; \
  int32x2_t __rev0_247;  __rev0_247 = __builtin_shufflevector(__s0_247, __s0_247, 1, 0); \
  int64x2_t __rev1_247;  __rev1_247 = __builtin_shufflevector(__s1_247, __s1_247, 1, 0); \
  int32x4_t __ret_247; \
  __ret_247 = (int32x4_t)(__noswap_vcombine_s32((int32x2_t)(__rev0_247), (int32x2_t)(__noswap_vshrn_n_s64(__rev1_247, __p2_247)))); \
  __ret_247 = __builtin_shufflevector(__ret_247, __ret_247, 3, 2, 1, 0); \
  __ret_247; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vshrn_high_n_s16(__p0_248, __p1_248, __p2_248) __extension__ ({ \
  int8x8_t __s0_248 = __p0_248; \
  int16x8_t __s1_248 = __p1_248; \
  int8x16_t __ret_248; \
  __ret_248 = (int8x16_t)(vcombine_s8((int8x8_t)(__s0_248), (int8x8_t)(vshrn_n_s16(__s1_248, __p2_248)))); \
  __ret_248; \
})
#else
#define vshrn_high_n_s16(__p0_249, __p1_249, __p2_249) __extension__ ({ \
  int8x8_t __s0_249 = __p0_249; \
  int16x8_t __s1_249 = __p1_249; \
  int8x8_t __rev0_249;  __rev0_249 = __builtin_shufflevector(__s0_249, __s0_249, 7, 6, 5, 4, 3, 2, 1, 0); \
  int16x8_t __rev1_249;  __rev1_249 = __builtin_shufflevector(__s1_249, __s1_249, 7, 6, 5, 4, 3, 2, 1, 0); \
  int8x16_t __ret_249; \
  __ret_249 = (int8x16_t)(__noswap_vcombine_s8((int8x8_t)(__rev0_249), (int8x8_t)(__noswap_vshrn_n_s16(__rev1_249, __p2_249)))); \
  __ret_249 = __builtin_shufflevector(__ret_249, __ret_249, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret_249; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vslid_n_u64(__p0, __p1, __p2) __extension__ ({ \
  uint64_t __s0 = __p0; \
  uint64_t __s1 = __p1; \
  uint64_t __ret; \
  __ret = (uint64_t) __builtin_neon_vslid_n_u64(__s0, __s1, __p2); \
  __ret; \
})
#else
#define vslid_n_u64(__p0, __p1, __p2) __extension__ ({ \
  uint64_t __s0 = __p0; \
  uint64_t __s1 = __p1; \
  uint64_t __ret; \
  __ret = (uint64_t) __builtin_neon_vslid_n_u64(__s0, __s1, __p2); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vslid_n_s64(__p0, __p1, __p2) __extension__ ({ \
  int64_t __s0 = __p0; \
  int64_t __s1 = __p1; \
  int64_t __ret; \
  __ret = (int64_t) __builtin_neon_vslid_n_s64(__s0, __s1, __p2); \
  __ret; \
})
#else
#define vslid_n_s64(__p0, __p1, __p2) __extension__ ({ \
  int64_t __s0 = __p0; \
  int64_t __s1 = __p1; \
  int64_t __ret; \
  __ret = (int64_t) __builtin_neon_vslid_n_s64(__s0, __s1, __p2); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vsli_n_p64(__p0, __p1, __p2) __extension__ ({ \
  poly64x1_t __s0 = __p0; \
  poly64x1_t __s1 = __p1; \
  poly64x1_t __ret; \
  __ret = (poly64x1_t) __builtin_neon_vsli_n_v((int8x8_t)__s0, (int8x8_t)__s1, __p2, 6); \
  __ret; \
})
#else
#define vsli_n_p64(__p0, __p1, __p2) __extension__ ({ \
  poly64x1_t __s0 = __p0; \
  poly64x1_t __s1 = __p1; \
  poly64x1_t __ret; \
  __ret = (poly64x1_t) __builtin_neon_vsli_n_v((int8x8_t)__s0, (int8x8_t)__s1, __p2, 6); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vsliq_n_p64(__p0, __p1, __p2) __extension__ ({ \
  poly64x2_t __s0 = __p0; \
  poly64x2_t __s1 = __p1; \
  poly64x2_t __ret; \
  __ret = (poly64x2_t) __builtin_neon_vsliq_n_v((int8x16_t)__s0, (int8x16_t)__s1, __p2, 38); \
  __ret; \
})
#else
#define vsliq_n_p64(__p0, __p1, __p2) __extension__ ({ \
  poly64x2_t __s0 = __p0; \
  poly64x2_t __s1 = __p1; \
  poly64x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  poly64x2_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 1, 0); \
  poly64x2_t __ret; \
  __ret = (poly64x2_t) __builtin_neon_vsliq_n_v((int8x16_t)__rev0, (int8x16_t)__rev1, __p2, 38); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8_t vsqaddb_u8(uint8_t __p0, uint8_t __p1) {
  uint8_t __ret;
  __ret = (uint8_t) __builtin_neon_vsqaddb_u8(__p0, __p1);
  return __ret;
}
#else
__ai uint8_t vsqaddb_u8(uint8_t __p0, uint8_t __p1) {
  uint8_t __ret;
  __ret = (uint8_t) __builtin_neon_vsqaddb_u8(__p0, __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32_t vsqadds_u32(uint32_t __p0, uint32_t __p1) {
  uint32_t __ret;
  __ret = (uint32_t) __builtin_neon_vsqadds_u32(__p0, __p1);
  return __ret;
}
#else
__ai uint32_t vsqadds_u32(uint32_t __p0, uint32_t __p1) {
  uint32_t __ret;
  __ret = (uint32_t) __builtin_neon_vsqadds_u32(__p0, __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64_t vsqaddd_u64(uint64_t __p0, uint64_t __p1) {
  uint64_t __ret;
  __ret = (uint64_t) __builtin_neon_vsqaddd_u64(__p0, __p1);
  return __ret;
}
#else
__ai uint64_t vsqaddd_u64(uint64_t __p0, uint64_t __p1) {
  uint64_t __ret;
  __ret = (uint64_t) __builtin_neon_vsqaddd_u64(__p0, __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16_t vsqaddh_u16(uint16_t __p0, uint16_t __p1) {
  uint16_t __ret;
  __ret = (uint16_t) __builtin_neon_vsqaddh_u16(__p0, __p1);
  return __ret;
}
#else
__ai uint16_t vsqaddh_u16(uint16_t __p0, uint16_t __p1) {
  uint16_t __ret;
  __ret = (uint16_t) __builtin_neon_vsqaddh_u16(__p0, __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x16_t vsqaddq_u8(uint8x16_t __p0, uint8x16_t __p1) {
  uint8x16_t __ret;
  __ret = (uint8x16_t) __builtin_neon_vsqaddq_v((int8x16_t)__p0, (int8x16_t)__p1, 48);
  return __ret;
}
#else
__ai uint8x16_t vsqaddq_u8(uint8x16_t __p0, uint8x16_t __p1) {
  uint8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __ret;
  __ret = (uint8x16_t) __builtin_neon_vsqaddq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 48);
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vsqaddq_u32(uint32x4_t __p0, uint32x4_t __p1) {
  uint32x4_t __ret;
  __ret = (uint32x4_t) __builtin_neon_vsqaddq_v((int8x16_t)__p0, (int8x16_t)__p1, 50);
  return __ret;
}
#else
__ai uint32x4_t vsqaddq_u32(uint32x4_t __p0, uint32x4_t __p1) {
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint32x4_t __ret;
  __ret = (uint32x4_t) __builtin_neon_vsqaddq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 50);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vsqaddq_u64(uint64x2_t __p0, uint64x2_t __p1) {
  uint64x2_t __ret;
  __ret = (uint64x2_t) __builtin_neon_vsqaddq_v((int8x16_t)__p0, (int8x16_t)__p1, 51);
  return __ret;
}
#else
__ai uint64x2_t vsqaddq_u64(uint64x2_t __p0, uint64x2_t __p1) {
  uint64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  uint64x2_t __ret;
  __ret = (uint64x2_t) __builtin_neon_vsqaddq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 51);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x8_t vsqaddq_u16(uint16x8_t __p0, uint16x8_t __p1) {
  uint16x8_t __ret;
  __ret = (uint16x8_t) __builtin_neon_vsqaddq_v((int8x16_t)__p0, (int8x16_t)__p1, 49);
  return __ret;
}
#else
__ai uint16x8_t vsqaddq_u16(uint16x8_t __p0, uint16x8_t __p1) {
  uint16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __ret;
  __ret = (uint16x8_t) __builtin_neon_vsqaddq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 49);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x8_t vsqadd_u8(uint8x8_t __p0, uint8x8_t __p1) {
  uint8x8_t __ret;
  __ret = (uint8x8_t) __builtin_neon_vsqadd_v((int8x8_t)__p0, (int8x8_t)__p1, 16);
  return __ret;
}
#else
__ai uint8x8_t vsqadd_u8(uint8x8_t __p0, uint8x8_t __p1) {
  uint8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __ret;
  __ret = (uint8x8_t) __builtin_neon_vsqadd_v((int8x8_t)__rev0, (int8x8_t)__rev1, 16);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x2_t vsqadd_u32(uint32x2_t __p0, uint32x2_t __p1) {
  uint32x2_t __ret;
  __ret = (uint32x2_t) __builtin_neon_vsqadd_v((int8x8_t)__p0, (int8x8_t)__p1, 18);
  return __ret;
}
#else
__ai uint32x2_t vsqadd_u32(uint32x2_t __p0, uint32x2_t __p1) {
  uint32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  uint32x2_t __ret;
  __ret = (uint32x2_t) __builtin_neon_vsqadd_v((int8x8_t)__rev0, (int8x8_t)__rev1, 18);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x1_t vsqadd_u64(uint64x1_t __p0, uint64x1_t __p1) {
  uint64x1_t __ret;
  __ret = (uint64x1_t) __builtin_neon_vsqadd_v((int8x8_t)__p0, (int8x8_t)__p1, 19);
  return __ret;
}
#else
__ai uint64x1_t vsqadd_u64(uint64x1_t __p0, uint64x1_t __p1) {
  uint64x1_t __ret;
  __ret = (uint64x1_t) __builtin_neon_vsqadd_v((int8x8_t)__p0, (int8x8_t)__p1, 19);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x4_t vsqadd_u16(uint16x4_t __p0, uint16x4_t __p1) {
  uint16x4_t __ret;
  __ret = (uint16x4_t) __builtin_neon_vsqadd_v((int8x8_t)__p0, (int8x8_t)__p1, 17);
  return __ret;
}
#else
__ai uint16x4_t vsqadd_u16(uint16x4_t __p0, uint16x4_t __p1) {
  uint16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint16x4_t __ret;
  __ret = (uint16x4_t) __builtin_neon_vsqadd_v((int8x8_t)__rev0, (int8x8_t)__rev1, 17);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x2_t vsqrtq_f64(float64x2_t __p0) {
  float64x2_t __ret;
  __ret = (float64x2_t) __builtin_neon_vsqrtq_v((int8x16_t)__p0, 42);
  return __ret;
}
#else
__ai float64x2_t vsqrtq_f64(float64x2_t __p0) {
  float64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float64x2_t __ret;
  __ret = (float64x2_t) __builtin_neon_vsqrtq_v((int8x16_t)__rev0, 42);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x4_t vsqrtq_f32(float32x4_t __p0) {
  float32x4_t __ret;
  __ret = (float32x4_t) __builtin_neon_vsqrtq_v((int8x16_t)__p0, 41);
  return __ret;
}
#else
__ai float32x4_t vsqrtq_f32(float32x4_t __p0) {
  float32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  float32x4_t __ret;
  __ret = (float32x4_t) __builtin_neon_vsqrtq_v((int8x16_t)__rev0, 41);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x1_t vsqrt_f64(float64x1_t __p0) {
  float64x1_t __ret;
  __ret = (float64x1_t) __builtin_neon_vsqrt_v((int8x8_t)__p0, 10);
  return __ret;
}
#else
__ai float64x1_t vsqrt_f64(float64x1_t __p0) {
  float64x1_t __ret;
  __ret = (float64x1_t) __builtin_neon_vsqrt_v((int8x8_t)__p0, 10);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x2_t vsqrt_f32(float32x2_t __p0) {
  float32x2_t __ret;
  __ret = (float32x2_t) __builtin_neon_vsqrt_v((int8x8_t)__p0, 9);
  return __ret;
}
#else
__ai float32x2_t vsqrt_f32(float32x2_t __p0) {
  float32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float32x2_t __ret;
  __ret = (float32x2_t) __builtin_neon_vsqrt_v((int8x8_t)__rev0, 9);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
#define vsrad_n_u64(__p0, __p1, __p2) __extension__ ({ \
  uint64_t __s0 = __p0; \
  uint64_t __s1 = __p1; \
  uint64_t __ret; \
  __ret = (uint64_t) __builtin_neon_vsrad_n_u64(__s0, __s1, __p2); \
  __ret; \
})
#else
#define vsrad_n_u64(__p0, __p1, __p2) __extension__ ({ \
  uint64_t __s0 = __p0; \
  uint64_t __s1 = __p1; \
  uint64_t __ret; \
  __ret = (uint64_t) __builtin_neon_vsrad_n_u64(__s0, __s1, __p2); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vsrad_n_s64(__p0, __p1, __p2) __extension__ ({ \
  int64_t __s0 = __p0; \
  int64_t __s1 = __p1; \
  int64_t __ret; \
  __ret = (int64_t) __builtin_neon_vsrad_n_s64(__s0, __s1, __p2); \
  __ret; \
})
#else
#define vsrad_n_s64(__p0, __p1, __p2) __extension__ ({ \
  int64_t __s0 = __p0; \
  int64_t __s1 = __p1; \
  int64_t __ret; \
  __ret = (int64_t) __builtin_neon_vsrad_n_s64(__s0, __s1, __p2); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vsrid_n_u64(__p0, __p1, __p2) __extension__ ({ \
  uint64_t __s0 = __p0; \
  uint64_t __s1 = __p1; \
  uint64_t __ret; \
  __ret = (uint64_t) __builtin_neon_vsrid_n_u64(__s0, __s1, __p2); \
  __ret; \
})
#else
#define vsrid_n_u64(__p0, __p1, __p2) __extension__ ({ \
  uint64_t __s0 = __p0; \
  uint64_t __s1 = __p1; \
  uint64_t __ret; \
  __ret = (uint64_t) __builtin_neon_vsrid_n_u64(__s0, __s1, __p2); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vsrid_n_s64(__p0, __p1, __p2) __extension__ ({ \
  int64_t __s0 = __p0; \
  int64_t __s1 = __p1; \
  int64_t __ret; \
  __ret = (int64_t) __builtin_neon_vsrid_n_s64(__s0, __s1, __p2); \
  __ret; \
})
#else
#define vsrid_n_s64(__p0, __p1, __p2) __extension__ ({ \
  int64_t __s0 = __p0; \
  int64_t __s1 = __p1; \
  int64_t __ret; \
  __ret = (int64_t) __builtin_neon_vsrid_n_s64(__s0, __s1, __p2); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vsri_n_p64(__p0, __p1, __p2) __extension__ ({ \
  poly64x1_t __s0 = __p0; \
  poly64x1_t __s1 = __p1; \
  poly64x1_t __ret; \
  __ret = (poly64x1_t) __builtin_neon_vsri_n_v((int8x8_t)__s0, (int8x8_t)__s1, __p2, 6); \
  __ret; \
})
#else
#define vsri_n_p64(__p0, __p1, __p2) __extension__ ({ \
  poly64x1_t __s0 = __p0; \
  poly64x1_t __s1 = __p1; \
  poly64x1_t __ret; \
  __ret = (poly64x1_t) __builtin_neon_vsri_n_v((int8x8_t)__s0, (int8x8_t)__s1, __p2, 6); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vsriq_n_p64(__p0, __p1, __p2) __extension__ ({ \
  poly64x2_t __s0 = __p0; \
  poly64x2_t __s1 = __p1; \
  poly64x2_t __ret; \
  __ret = (poly64x2_t) __builtin_neon_vsriq_n_v((int8x16_t)__s0, (int8x16_t)__s1, __p2, 38); \
  __ret; \
})
#else
#define vsriq_n_p64(__p0, __p1, __p2) __extension__ ({ \
  poly64x2_t __s0 = __p0; \
  poly64x2_t __s1 = __p1; \
  poly64x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  poly64x2_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 1, 0); \
  poly64x2_t __ret; \
  __ret = (poly64x2_t) __builtin_neon_vsriq_n_v((int8x16_t)__rev0, (int8x16_t)__rev1, __p2, 38); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1_p64(__p0, __p1) __extension__ ({ \
  poly64x1_t __s1 = __p1; \
  __builtin_neon_vst1_v(__p0, (int8x8_t)__s1, 6); \
})
#else
#define vst1_p64(__p0, __p1) __extension__ ({ \
  poly64x1_t __s1 = __p1; \
  __builtin_neon_vst1_v(__p0, (int8x8_t)__s1, 6); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1q_p64(__p0, __p1) __extension__ ({ \
  poly64x2_t __s1 = __p1; \
  __builtin_neon_vst1q_v(__p0, (int8x16_t)__s1, 38); \
})
#else
#define vst1q_p64(__p0, __p1) __extension__ ({ \
  poly64x2_t __s1 = __p1; \
  poly64x2_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 1, 0); \
  __builtin_neon_vst1q_v(__p0, (int8x16_t)__rev1, 38); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1q_f64(__p0, __p1) __extension__ ({ \
  float64x2_t __s1 = __p1; \
  __builtin_neon_vst1q_v(__p0, (int8x16_t)__s1, 42); \
})
#else
#define vst1q_f64(__p0, __p1) __extension__ ({ \
  float64x2_t __s1 = __p1; \
  float64x2_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 1, 0); \
  __builtin_neon_vst1q_v(__p0, (int8x16_t)__rev1, 42); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1_f64(__p0, __p1) __extension__ ({ \
  float64x1_t __s1 = __p1; \
  __builtin_neon_vst1_v(__p0, (int8x8_t)__s1, 10); \
})
#else
#define vst1_f64(__p0, __p1) __extension__ ({ \
  float64x1_t __s1 = __p1; \
  __builtin_neon_vst1_v(__p0, (int8x8_t)__s1, 10); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1_lane_p64(__p0, __p1, __p2) __extension__ ({ \
  poly64x1_t __s1 = __p1; \
  __builtin_neon_vst1_lane_v(__p0, (int8x8_t)__s1, __p2, 6); \
})
#else
#define vst1_lane_p64(__p0, __p1, __p2) __extension__ ({ \
  poly64x1_t __s1 = __p1; \
  __builtin_neon_vst1_lane_v(__p0, (int8x8_t)__s1, __p2, 6); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1q_lane_p64(__p0, __p1, __p2) __extension__ ({ \
  poly64x2_t __s1 = __p1; \
  __builtin_neon_vst1q_lane_v(__p0, (int8x16_t)__s1, __p2, 38); \
})
#else
#define vst1q_lane_p64(__p0, __p1, __p2) __extension__ ({ \
  poly64x2_t __s1 = __p1; \
  poly64x2_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 1, 0); \
  __builtin_neon_vst1q_lane_v(__p0, (int8x16_t)__rev1, __p2, 38); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1q_lane_f64(__p0, __p1, __p2) __extension__ ({ \
  float64x2_t __s1 = __p1; \
  __builtin_neon_vst1q_lane_v(__p0, (int8x16_t)__s1, __p2, 42); \
})
#else
#define vst1q_lane_f64(__p0, __p1, __p2) __extension__ ({ \
  float64x2_t __s1 = __p1; \
  float64x2_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 1, 0); \
  __builtin_neon_vst1q_lane_v(__p0, (int8x16_t)__rev1, __p2, 42); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1_lane_f64(__p0, __p1, __p2) __extension__ ({ \
  float64x1_t __s1 = __p1; \
  __builtin_neon_vst1_lane_v(__p0, (int8x8_t)__s1, __p2, 10); \
})
#else
#define vst1_lane_f64(__p0, __p1, __p2) __extension__ ({ \
  float64x1_t __s1 = __p1; \
  __builtin_neon_vst1_lane_v(__p0, (int8x8_t)__s1, __p2, 10); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1_p8_x2(__p0, __p1) __extension__ ({ \
  poly8x8x2_t __s1 = __p1; \
  __builtin_neon_vst1_x2_v(__p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], 4); \
})
#else
#define vst1_p8_x2(__p0, __p1) __extension__ ({ \
  poly8x8x2_t __s1 = __p1; \
  poly8x8x2_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst1_x2_v(__p0, (int8x8_t)__rev1.val[0], (int8x8_t)__rev1.val[1], 4); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1_p64_x2(__p0, __p1) __extension__ ({ \
  poly64x1x2_t __s1 = __p1; \
  __builtin_neon_vst1_x2_v(__p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], 6); \
})
#else
#define vst1_p64_x2(__p0, __p1) __extension__ ({ \
  poly64x1x2_t __s1 = __p1; \
  __builtin_neon_vst1_x2_v(__p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], 6); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1_p16_x2(__p0, __p1) __extension__ ({ \
  poly16x4x2_t __s1 = __p1; \
  __builtin_neon_vst1_x2_v(__p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], 5); \
})
#else
#define vst1_p16_x2(__p0, __p1) __extension__ ({ \
  poly16x4x2_t __s1 = __p1; \
  poly16x4x2_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 3, 2, 1, 0); \
  __builtin_neon_vst1_x2_v(__p0, (int8x8_t)__rev1.val[0], (int8x8_t)__rev1.val[1], 5); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1q_p8_x2(__p0, __p1) __extension__ ({ \
  poly8x16x2_t __s1 = __p1; \
  __builtin_neon_vst1q_x2_v(__p0, (int8x16_t)__s1.val[0], (int8x16_t)__s1.val[1], 36); \
})
#else
#define vst1q_p8_x2(__p0, __p1) __extension__ ({ \
  poly8x16x2_t __s1 = __p1; \
  poly8x16x2_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst1q_x2_v(__p0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], 36); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1q_p64_x2(__p0, __p1) __extension__ ({ \
  poly64x2x2_t __s1 = __p1; \
  __builtin_neon_vst1q_x2_v(__p0, (int8x16_t)__s1.val[0], (int8x16_t)__s1.val[1], 38); \
})
#else
#define vst1q_p64_x2(__p0, __p1) __extension__ ({ \
  poly64x2x2_t __s1 = __p1; \
  poly64x2x2_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 1, 0); \
  __builtin_neon_vst1q_x2_v(__p0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], 38); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1q_p16_x2(__p0, __p1) __extension__ ({ \
  poly16x8x2_t __s1 = __p1; \
  __builtin_neon_vst1q_x2_v(__p0, (int8x16_t)__s1.val[0], (int8x16_t)__s1.val[1], 37); \
})
#else
#define vst1q_p16_x2(__p0, __p1) __extension__ ({ \
  poly16x8x2_t __s1 = __p1; \
  poly16x8x2_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst1q_x2_v(__p0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], 37); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1q_u8_x2(__p0, __p1) __extension__ ({ \
  uint8x16x2_t __s1 = __p1; \
  __builtin_neon_vst1q_x2_v(__p0, (int8x16_t)__s1.val[0], (int8x16_t)__s1.val[1], 48); \
})
#else
#define vst1q_u8_x2(__p0, __p1) __extension__ ({ \
  uint8x16x2_t __s1 = __p1; \
  uint8x16x2_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst1q_x2_v(__p0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], 48); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1q_u32_x2(__p0, __p1) __extension__ ({ \
  uint32x4x2_t __s1 = __p1; \
  __builtin_neon_vst1q_x2_v(__p0, (int8x16_t)__s1.val[0], (int8x16_t)__s1.val[1], 50); \
})
#else
#define vst1q_u32_x2(__p0, __p1) __extension__ ({ \
  uint32x4x2_t __s1 = __p1; \
  uint32x4x2_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 3, 2, 1, 0); \
  __builtin_neon_vst1q_x2_v(__p0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], 50); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1q_u64_x2(__p0, __p1) __extension__ ({ \
  uint64x2x2_t __s1 = __p1; \
  __builtin_neon_vst1q_x2_v(__p0, (int8x16_t)__s1.val[0], (int8x16_t)__s1.val[1], 51); \
})
#else
#define vst1q_u64_x2(__p0, __p1) __extension__ ({ \
  uint64x2x2_t __s1 = __p1; \
  uint64x2x2_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 1, 0); \
  __builtin_neon_vst1q_x2_v(__p0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], 51); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1q_u16_x2(__p0, __p1) __extension__ ({ \
  uint16x8x2_t __s1 = __p1; \
  __builtin_neon_vst1q_x2_v(__p0, (int8x16_t)__s1.val[0], (int8x16_t)__s1.val[1], 49); \
})
#else
#define vst1q_u16_x2(__p0, __p1) __extension__ ({ \
  uint16x8x2_t __s1 = __p1; \
  uint16x8x2_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst1q_x2_v(__p0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], 49); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1q_s8_x2(__p0, __p1) __extension__ ({ \
  int8x16x2_t __s1 = __p1; \
  __builtin_neon_vst1q_x2_v(__p0, (int8x16_t)__s1.val[0], (int8x16_t)__s1.val[1], 32); \
})
#else
#define vst1q_s8_x2(__p0, __p1) __extension__ ({ \
  int8x16x2_t __s1 = __p1; \
  int8x16x2_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst1q_x2_v(__p0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], 32); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1q_f64_x2(__p0, __p1) __extension__ ({ \
  float64x2x2_t __s1 = __p1; \
  __builtin_neon_vst1q_x2_v(__p0, __s1.val[0], __s1.val[1], 42); \
})
#else
#define vst1q_f64_x2(__p0, __p1) __extension__ ({ \
  float64x2x2_t __s1 = __p1; \
  float64x2x2_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 1, 0); \
  __builtin_neon_vst1q_x2_v(__p0, __rev1.val[0], __rev1.val[1], 42); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1q_f32_x2(__p0, __p1) __extension__ ({ \
  float32x4x2_t __s1 = __p1; \
  __builtin_neon_vst1q_x2_v(__p0, __s1.val[0], __s1.val[1], 41); \
})
#else
#define vst1q_f32_x2(__p0, __p1) __extension__ ({ \
  float32x4x2_t __s1 = __p1; \
  float32x4x2_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 3, 2, 1, 0); \
  __builtin_neon_vst1q_x2_v(__p0, __rev1.val[0], __rev1.val[1], 41); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1q_f16_x2(__p0, __p1) __extension__ ({ \
  float16x8x2_t __s1 = __p1; \
  __builtin_neon_vst1q_x2_v(__p0, __s1.val[0], __s1.val[1], 40); \
})
#else
#define vst1q_f16_x2(__p0, __p1) __extension__ ({ \
  float16x8x2_t __s1 = __p1; \
  float16x8x2_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst1q_x2_v(__p0, __rev1.val[0], __rev1.val[1], 40); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1q_s32_x2(__p0, __p1) __extension__ ({ \
  int32x4x2_t __s1 = __p1; \
  __builtin_neon_vst1q_x2_v(__p0, __s1.val[0], __s1.val[1], 34); \
})
#else
#define vst1q_s32_x2(__p0, __p1) __extension__ ({ \
  int32x4x2_t __s1 = __p1; \
  int32x4x2_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 3, 2, 1, 0); \
  __builtin_neon_vst1q_x2_v(__p0, __rev1.val[0], __rev1.val[1], 34); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1q_s64_x2(__p0, __p1) __extension__ ({ \
  int64x2x2_t __s1 = __p1; \
  __builtin_neon_vst1q_x2_v(__p0, __s1.val[0], __s1.val[1], 35); \
})
#else
#define vst1q_s64_x2(__p0, __p1) __extension__ ({ \
  int64x2x2_t __s1 = __p1; \
  int64x2x2_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 1, 0); \
  __builtin_neon_vst1q_x2_v(__p0, __rev1.val[0], __rev1.val[1], 35); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1q_s16_x2(__p0, __p1) __extension__ ({ \
  int16x8x2_t __s1 = __p1; \
  __builtin_neon_vst1q_x2_v(__p0, __s1.val[0], __s1.val[1], 33); \
})
#else
#define vst1q_s16_x2(__p0, __p1) __extension__ ({ \
  int16x8x2_t __s1 = __p1; \
  int16x8x2_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst1q_x2_v(__p0, __rev1.val[0], __rev1.val[1], 33); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1_u8_x2(__p0, __p1) __extension__ ({ \
  uint8x8x2_t __s1 = __p1; \
  __builtin_neon_vst1_x2_v(__p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], 16); \
})
#else
#define vst1_u8_x2(__p0, __p1) __extension__ ({ \
  uint8x8x2_t __s1 = __p1; \
  uint8x8x2_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst1_x2_v(__p0, (int8x8_t)__rev1.val[0], (int8x8_t)__rev1.val[1], 16); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1_u32_x2(__p0, __p1) __extension__ ({ \
  uint32x2x2_t __s1 = __p1; \
  __builtin_neon_vst1_x2_v(__p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], 18); \
})
#else
#define vst1_u32_x2(__p0, __p1) __extension__ ({ \
  uint32x2x2_t __s1 = __p1; \
  uint32x2x2_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 1, 0); \
  __builtin_neon_vst1_x2_v(__p0, (int8x8_t)__rev1.val[0], (int8x8_t)__rev1.val[1], 18); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1_u64_x2(__p0, __p1) __extension__ ({ \
  uint64x1x2_t __s1 = __p1; \
  __builtin_neon_vst1_x2_v(__p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], 19); \
})
#else
#define vst1_u64_x2(__p0, __p1) __extension__ ({ \
  uint64x1x2_t __s1 = __p1; \
  __builtin_neon_vst1_x2_v(__p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], 19); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1_u16_x2(__p0, __p1) __extension__ ({ \
  uint16x4x2_t __s1 = __p1; \
  __builtin_neon_vst1_x2_v(__p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], 17); \
})
#else
#define vst1_u16_x2(__p0, __p1) __extension__ ({ \
  uint16x4x2_t __s1 = __p1; \
  uint16x4x2_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 3, 2, 1, 0); \
  __builtin_neon_vst1_x2_v(__p0, (int8x8_t)__rev1.val[0], (int8x8_t)__rev1.val[1], 17); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1_s8_x2(__p0, __p1) __extension__ ({ \
  int8x8x2_t __s1 = __p1; \
  __builtin_neon_vst1_x2_v(__p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], 0); \
})
#else
#define vst1_s8_x2(__p0, __p1) __extension__ ({ \
  int8x8x2_t __s1 = __p1; \
  int8x8x2_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst1_x2_v(__p0, (int8x8_t)__rev1.val[0], (int8x8_t)__rev1.val[1], 0); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1_f64_x2(__p0, __p1) __extension__ ({ \
  float64x1x2_t __s1 = __p1; \
  __builtin_neon_vst1_x2_v(__p0, __s1.val[0], __s1.val[1], 10); \
})
#else
#define vst1_f64_x2(__p0, __p1) __extension__ ({ \
  float64x1x2_t __s1 = __p1; \
  __builtin_neon_vst1_x2_v(__p0, __s1.val[0], __s1.val[1], 10); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1_f32_x2(__p0, __p1) __extension__ ({ \
  float32x2x2_t __s1 = __p1; \
  __builtin_neon_vst1_x2_v(__p0, __s1.val[0], __s1.val[1], 9); \
})
#else
#define vst1_f32_x2(__p0, __p1) __extension__ ({ \
  float32x2x2_t __s1 = __p1; \
  float32x2x2_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 1, 0); \
  __builtin_neon_vst1_x2_v(__p0, __rev1.val[0], __rev1.val[1], 9); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1_f16_x2(__p0, __p1) __extension__ ({ \
  float16x4x2_t __s1 = __p1; \
  __builtin_neon_vst1_x2_v(__p0, __s1.val[0], __s1.val[1], 8); \
})
#else
#define vst1_f16_x2(__p0, __p1) __extension__ ({ \
  float16x4x2_t __s1 = __p1; \
  float16x4x2_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 3, 2, 1, 0); \
  __builtin_neon_vst1_x2_v(__p0, __rev1.val[0], __rev1.val[1], 8); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1_s32_x2(__p0, __p1) __extension__ ({ \
  int32x2x2_t __s1 = __p1; \
  __builtin_neon_vst1_x2_v(__p0, __s1.val[0], __s1.val[1], 2); \
})
#else
#define vst1_s32_x2(__p0, __p1) __extension__ ({ \
  int32x2x2_t __s1 = __p1; \
  int32x2x2_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 1, 0); \
  __builtin_neon_vst1_x2_v(__p0, __rev1.val[0], __rev1.val[1], 2); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1_s64_x2(__p0, __p1) __extension__ ({ \
  int64x1x2_t __s1 = __p1; \
  __builtin_neon_vst1_x2_v(__p0, __s1.val[0], __s1.val[1], 3); \
})
#else
#define vst1_s64_x2(__p0, __p1) __extension__ ({ \
  int64x1x2_t __s1 = __p1; \
  __builtin_neon_vst1_x2_v(__p0, __s1.val[0], __s1.val[1], 3); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1_s16_x2(__p0, __p1) __extension__ ({ \
  int16x4x2_t __s1 = __p1; \
  __builtin_neon_vst1_x2_v(__p0, __s1.val[0], __s1.val[1], 1); \
})
#else
#define vst1_s16_x2(__p0, __p1) __extension__ ({ \
  int16x4x2_t __s1 = __p1; \
  int16x4x2_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 3, 2, 1, 0); \
  __builtin_neon_vst1_x2_v(__p0, __rev1.val[0], __rev1.val[1], 1); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1_p8_x3(__p0, __p1) __extension__ ({ \
  poly8x8x3_t __s1 = __p1; \
  __builtin_neon_vst1_x3_v(__p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], (int8x8_t)__s1.val[2], 4); \
})
#else
#define vst1_p8_x3(__p0, __p1) __extension__ ({ \
  poly8x8x3_t __s1 = __p1; \
  poly8x8x3_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst1_x3_v(__p0, (int8x8_t)__rev1.val[0], (int8x8_t)__rev1.val[1], (int8x8_t)__rev1.val[2], 4); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1_p64_x3(__p0, __p1) __extension__ ({ \
  poly64x1x3_t __s1 = __p1; \
  __builtin_neon_vst1_x3_v(__p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], (int8x8_t)__s1.val[2], 6); \
})
#else
#define vst1_p64_x3(__p0, __p1) __extension__ ({ \
  poly64x1x3_t __s1 = __p1; \
  __builtin_neon_vst1_x3_v(__p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], (int8x8_t)__s1.val[2], 6); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1_p16_x3(__p0, __p1) __extension__ ({ \
  poly16x4x3_t __s1 = __p1; \
  __builtin_neon_vst1_x3_v(__p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], (int8x8_t)__s1.val[2], 5); \
})
#else
#define vst1_p16_x3(__p0, __p1) __extension__ ({ \
  poly16x4x3_t __s1 = __p1; \
  poly16x4x3_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 3, 2, 1, 0); \
  __builtin_neon_vst1_x3_v(__p0, (int8x8_t)__rev1.val[0], (int8x8_t)__rev1.val[1], (int8x8_t)__rev1.val[2], 5); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1q_p8_x3(__p0, __p1) __extension__ ({ \
  poly8x16x3_t __s1 = __p1; \
  __builtin_neon_vst1q_x3_v(__p0, (int8x16_t)__s1.val[0], (int8x16_t)__s1.val[1], (int8x16_t)__s1.val[2], 36); \
})
#else
#define vst1q_p8_x3(__p0, __p1) __extension__ ({ \
  poly8x16x3_t __s1 = __p1; \
  poly8x16x3_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst1q_x3_v(__p0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], (int8x16_t)__rev1.val[2], 36); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1q_p64_x3(__p0, __p1) __extension__ ({ \
  poly64x2x3_t __s1 = __p1; \
  __builtin_neon_vst1q_x3_v(__p0, (int8x16_t)__s1.val[0], (int8x16_t)__s1.val[1], (int8x16_t)__s1.val[2], 38); \
})
#else
#define vst1q_p64_x3(__p0, __p1) __extension__ ({ \
  poly64x2x3_t __s1 = __p1; \
  poly64x2x3_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 1, 0); \
  __builtin_neon_vst1q_x3_v(__p0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], (int8x16_t)__rev1.val[2], 38); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1q_p16_x3(__p0, __p1) __extension__ ({ \
  poly16x8x3_t __s1 = __p1; \
  __builtin_neon_vst1q_x3_v(__p0, (int8x16_t)__s1.val[0], (int8x16_t)__s1.val[1], (int8x16_t)__s1.val[2], 37); \
})
#else
#define vst1q_p16_x3(__p0, __p1) __extension__ ({ \
  poly16x8x3_t __s1 = __p1; \
  poly16x8x3_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst1q_x3_v(__p0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], (int8x16_t)__rev1.val[2], 37); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1q_u8_x3(__p0, __p1) __extension__ ({ \
  uint8x16x3_t __s1 = __p1; \
  __builtin_neon_vst1q_x3_v(__p0, (int8x16_t)__s1.val[0], (int8x16_t)__s1.val[1], (int8x16_t)__s1.val[2], 48); \
})
#else
#define vst1q_u8_x3(__p0, __p1) __extension__ ({ \
  uint8x16x3_t __s1 = __p1; \
  uint8x16x3_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst1q_x3_v(__p0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], (int8x16_t)__rev1.val[2], 48); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1q_u32_x3(__p0, __p1) __extension__ ({ \
  uint32x4x3_t __s1 = __p1; \
  __builtin_neon_vst1q_x3_v(__p0, (int8x16_t)__s1.val[0], (int8x16_t)__s1.val[1], (int8x16_t)__s1.val[2], 50); \
})
#else
#define vst1q_u32_x3(__p0, __p1) __extension__ ({ \
  uint32x4x3_t __s1 = __p1; \
  uint32x4x3_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 3, 2, 1, 0); \
  __builtin_neon_vst1q_x3_v(__p0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], (int8x16_t)__rev1.val[2], 50); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1q_u64_x3(__p0, __p1) __extension__ ({ \
  uint64x2x3_t __s1 = __p1; \
  __builtin_neon_vst1q_x3_v(__p0, (int8x16_t)__s1.val[0], (int8x16_t)__s1.val[1], (int8x16_t)__s1.val[2], 51); \
})
#else
#define vst1q_u64_x3(__p0, __p1) __extension__ ({ \
  uint64x2x3_t __s1 = __p1; \
  uint64x2x3_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 1, 0); \
  __builtin_neon_vst1q_x3_v(__p0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], (int8x16_t)__rev1.val[2], 51); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1q_u16_x3(__p0, __p1) __extension__ ({ \
  uint16x8x3_t __s1 = __p1; \
  __builtin_neon_vst1q_x3_v(__p0, (int8x16_t)__s1.val[0], (int8x16_t)__s1.val[1], (int8x16_t)__s1.val[2], 49); \
})
#else
#define vst1q_u16_x3(__p0, __p1) __extension__ ({ \
  uint16x8x3_t __s1 = __p1; \
  uint16x8x3_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst1q_x3_v(__p0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], (int8x16_t)__rev1.val[2], 49); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1q_s8_x3(__p0, __p1) __extension__ ({ \
  int8x16x3_t __s1 = __p1; \
  __builtin_neon_vst1q_x3_v(__p0, (int8x16_t)__s1.val[0], (int8x16_t)__s1.val[1], (int8x16_t)__s1.val[2], 32); \
})
#else
#define vst1q_s8_x3(__p0, __p1) __extension__ ({ \
  int8x16x3_t __s1 = __p1; \
  int8x16x3_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst1q_x3_v(__p0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], (int8x16_t)__rev1.val[2], 32); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1q_f64_x3(__p0, __p1) __extension__ ({ \
  float64x2x3_t __s1 = __p1; \
  __builtin_neon_vst1q_x3_v(__p0, __s1.val[0], __s1.val[1], __s1.val[2], 42); \
})
#else
#define vst1q_f64_x3(__p0, __p1) __extension__ ({ \
  float64x2x3_t __s1 = __p1; \
  float64x2x3_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 1, 0); \
  __builtin_neon_vst1q_x3_v(__p0, __rev1.val[0], __rev1.val[1], __rev1.val[2], 42); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1q_f32_x3(__p0, __p1) __extension__ ({ \
  float32x4x3_t __s1 = __p1; \
  __builtin_neon_vst1q_x3_v(__p0, __s1.val[0], __s1.val[1], __s1.val[2], 41); \
})
#else
#define vst1q_f32_x3(__p0, __p1) __extension__ ({ \
  float32x4x3_t __s1 = __p1; \
  float32x4x3_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 3, 2, 1, 0); \
  __builtin_neon_vst1q_x3_v(__p0, __rev1.val[0], __rev1.val[1], __rev1.val[2], 41); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1q_f16_x3(__p0, __p1) __extension__ ({ \
  float16x8x3_t __s1 = __p1; \
  __builtin_neon_vst1q_x3_v(__p0, __s1.val[0], __s1.val[1], __s1.val[2], 40); \
})
#else
#define vst1q_f16_x3(__p0, __p1) __extension__ ({ \
  float16x8x3_t __s1 = __p1; \
  float16x8x3_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst1q_x3_v(__p0, __rev1.val[0], __rev1.val[1], __rev1.val[2], 40); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1q_s32_x3(__p0, __p1) __extension__ ({ \
  int32x4x3_t __s1 = __p1; \
  __builtin_neon_vst1q_x3_v(__p0, __s1.val[0], __s1.val[1], __s1.val[2], 34); \
})
#else
#define vst1q_s32_x3(__p0, __p1) __extension__ ({ \
  int32x4x3_t __s1 = __p1; \
  int32x4x3_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 3, 2, 1, 0); \
  __builtin_neon_vst1q_x3_v(__p0, __rev1.val[0], __rev1.val[1], __rev1.val[2], 34); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1q_s64_x3(__p0, __p1) __extension__ ({ \
  int64x2x3_t __s1 = __p1; \
  __builtin_neon_vst1q_x3_v(__p0, __s1.val[0], __s1.val[1], __s1.val[2], 35); \
})
#else
#define vst1q_s64_x3(__p0, __p1) __extension__ ({ \
  int64x2x3_t __s1 = __p1; \
  int64x2x3_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 1, 0); \
  __builtin_neon_vst1q_x3_v(__p0, __rev1.val[0], __rev1.val[1], __rev1.val[2], 35); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1q_s16_x3(__p0, __p1) __extension__ ({ \
  int16x8x3_t __s1 = __p1; \
  __builtin_neon_vst1q_x3_v(__p0, __s1.val[0], __s1.val[1], __s1.val[2], 33); \
})
#else
#define vst1q_s16_x3(__p0, __p1) __extension__ ({ \
  int16x8x3_t __s1 = __p1; \
  int16x8x3_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst1q_x3_v(__p0, __rev1.val[0], __rev1.val[1], __rev1.val[2], 33); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1_u8_x3(__p0, __p1) __extension__ ({ \
  uint8x8x3_t __s1 = __p1; \
  __builtin_neon_vst1_x3_v(__p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], (int8x8_t)__s1.val[2], 16); \
})
#else
#define vst1_u8_x3(__p0, __p1) __extension__ ({ \
  uint8x8x3_t __s1 = __p1; \
  uint8x8x3_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst1_x3_v(__p0, (int8x8_t)__rev1.val[0], (int8x8_t)__rev1.val[1], (int8x8_t)__rev1.val[2], 16); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1_u32_x3(__p0, __p1) __extension__ ({ \
  uint32x2x3_t __s1 = __p1; \
  __builtin_neon_vst1_x3_v(__p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], (int8x8_t)__s1.val[2], 18); \
})
#else
#define vst1_u32_x3(__p0, __p1) __extension__ ({ \
  uint32x2x3_t __s1 = __p1; \
  uint32x2x3_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 1, 0); \
  __builtin_neon_vst1_x3_v(__p0, (int8x8_t)__rev1.val[0], (int8x8_t)__rev1.val[1], (int8x8_t)__rev1.val[2], 18); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1_u64_x3(__p0, __p1) __extension__ ({ \
  uint64x1x3_t __s1 = __p1; \
  __builtin_neon_vst1_x3_v(__p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], (int8x8_t)__s1.val[2], 19); \
})
#else
#define vst1_u64_x3(__p0, __p1) __extension__ ({ \
  uint64x1x3_t __s1 = __p1; \
  __builtin_neon_vst1_x3_v(__p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], (int8x8_t)__s1.val[2], 19); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1_u16_x3(__p0, __p1) __extension__ ({ \
  uint16x4x3_t __s1 = __p1; \
  __builtin_neon_vst1_x3_v(__p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], (int8x8_t)__s1.val[2], 17); \
})
#else
#define vst1_u16_x3(__p0, __p1) __extension__ ({ \
  uint16x4x3_t __s1 = __p1; \
  uint16x4x3_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 3, 2, 1, 0); \
  __builtin_neon_vst1_x3_v(__p0, (int8x8_t)__rev1.val[0], (int8x8_t)__rev1.val[1], (int8x8_t)__rev1.val[2], 17); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1_s8_x3(__p0, __p1) __extension__ ({ \
  int8x8x3_t __s1 = __p1; \
  __builtin_neon_vst1_x3_v(__p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], (int8x8_t)__s1.val[2], 0); \
})
#else
#define vst1_s8_x3(__p0, __p1) __extension__ ({ \
  int8x8x3_t __s1 = __p1; \
  int8x8x3_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst1_x3_v(__p0, (int8x8_t)__rev1.val[0], (int8x8_t)__rev1.val[1], (int8x8_t)__rev1.val[2], 0); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1_f64_x3(__p0, __p1) __extension__ ({ \
  float64x1x3_t __s1 = __p1; \
  __builtin_neon_vst1_x3_v(__p0, __s1.val[0], __s1.val[1], __s1.val[2], 10); \
})
#else
#define vst1_f64_x3(__p0, __p1) __extension__ ({ \
  float64x1x3_t __s1 = __p1; \
  __builtin_neon_vst1_x3_v(__p0, __s1.val[0], __s1.val[1], __s1.val[2], 10); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1_f32_x3(__p0, __p1) __extension__ ({ \
  float32x2x3_t __s1 = __p1; \
  __builtin_neon_vst1_x3_v(__p0, __s1.val[0], __s1.val[1], __s1.val[2], 9); \
})
#else
#define vst1_f32_x3(__p0, __p1) __extension__ ({ \
  float32x2x3_t __s1 = __p1; \
  float32x2x3_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 1, 0); \
  __builtin_neon_vst1_x3_v(__p0, __rev1.val[0], __rev1.val[1], __rev1.val[2], 9); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1_f16_x3(__p0, __p1) __extension__ ({ \
  float16x4x3_t __s1 = __p1; \
  __builtin_neon_vst1_x3_v(__p0, __s1.val[0], __s1.val[1], __s1.val[2], 8); \
})
#else
#define vst1_f16_x3(__p0, __p1) __extension__ ({ \
  float16x4x3_t __s1 = __p1; \
  float16x4x3_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 3, 2, 1, 0); \
  __builtin_neon_vst1_x3_v(__p0, __rev1.val[0], __rev1.val[1], __rev1.val[2], 8); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1_s32_x3(__p0, __p1) __extension__ ({ \
  int32x2x3_t __s1 = __p1; \
  __builtin_neon_vst1_x3_v(__p0, __s1.val[0], __s1.val[1], __s1.val[2], 2); \
})
#else
#define vst1_s32_x3(__p0, __p1) __extension__ ({ \
  int32x2x3_t __s1 = __p1; \
  int32x2x3_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 1, 0); \
  __builtin_neon_vst1_x3_v(__p0, __rev1.val[0], __rev1.val[1], __rev1.val[2], 2); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1_s64_x3(__p0, __p1) __extension__ ({ \
  int64x1x3_t __s1 = __p1; \
  __builtin_neon_vst1_x3_v(__p0, __s1.val[0], __s1.val[1], __s1.val[2], 3); \
})
#else
#define vst1_s64_x3(__p0, __p1) __extension__ ({ \
  int64x1x3_t __s1 = __p1; \
  __builtin_neon_vst1_x3_v(__p0, __s1.val[0], __s1.val[1], __s1.val[2], 3); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1_s16_x3(__p0, __p1) __extension__ ({ \
  int16x4x3_t __s1 = __p1; \
  __builtin_neon_vst1_x3_v(__p0, __s1.val[0], __s1.val[1], __s1.val[2], 1); \
})
#else
#define vst1_s16_x3(__p0, __p1) __extension__ ({ \
  int16x4x3_t __s1 = __p1; \
  int16x4x3_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 3, 2, 1, 0); \
  __builtin_neon_vst1_x3_v(__p0, __rev1.val[0], __rev1.val[1], __rev1.val[2], 1); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1_p8_x4(__p0, __p1) __extension__ ({ \
  poly8x8x4_t __s1 = __p1; \
  __builtin_neon_vst1_x4_v(__p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], (int8x8_t)__s1.val[2], (int8x8_t)__s1.val[3], 4); \
})
#else
#define vst1_p8_x4(__p0, __p1) __extension__ ({ \
  poly8x8x4_t __s1 = __p1; \
  poly8x8x4_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[3] = __builtin_shufflevector(__s1.val[3], __s1.val[3], 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst1_x4_v(__p0, (int8x8_t)__rev1.val[0], (int8x8_t)__rev1.val[1], (int8x8_t)__rev1.val[2], (int8x8_t)__rev1.val[3], 4); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1_p64_x4(__p0, __p1) __extension__ ({ \
  poly64x1x4_t __s1 = __p1; \
  __builtin_neon_vst1_x4_v(__p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], (int8x8_t)__s1.val[2], (int8x8_t)__s1.val[3], 6); \
})
#else
#define vst1_p64_x4(__p0, __p1) __extension__ ({ \
  poly64x1x4_t __s1 = __p1; \
  __builtin_neon_vst1_x4_v(__p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], (int8x8_t)__s1.val[2], (int8x8_t)__s1.val[3], 6); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1_p16_x4(__p0, __p1) __extension__ ({ \
  poly16x4x4_t __s1 = __p1; \
  __builtin_neon_vst1_x4_v(__p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], (int8x8_t)__s1.val[2], (int8x8_t)__s1.val[3], 5); \
})
#else
#define vst1_p16_x4(__p0, __p1) __extension__ ({ \
  poly16x4x4_t __s1 = __p1; \
  poly16x4x4_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 3, 2, 1, 0); \
  __rev1.val[3] = __builtin_shufflevector(__s1.val[3], __s1.val[3], 3, 2, 1, 0); \
  __builtin_neon_vst1_x4_v(__p0, (int8x8_t)__rev1.val[0], (int8x8_t)__rev1.val[1], (int8x8_t)__rev1.val[2], (int8x8_t)__rev1.val[3], 5); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1q_p8_x4(__p0, __p1) __extension__ ({ \
  poly8x16x4_t __s1 = __p1; \
  __builtin_neon_vst1q_x4_v(__p0, (int8x16_t)__s1.val[0], (int8x16_t)__s1.val[1], (int8x16_t)__s1.val[2], (int8x16_t)__s1.val[3], 36); \
})
#else
#define vst1q_p8_x4(__p0, __p1) __extension__ ({ \
  poly8x16x4_t __s1 = __p1; \
  poly8x16x4_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[3] = __builtin_shufflevector(__s1.val[3], __s1.val[3], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst1q_x4_v(__p0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], (int8x16_t)__rev1.val[2], (int8x16_t)__rev1.val[3], 36); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1q_p64_x4(__p0, __p1) __extension__ ({ \
  poly64x2x4_t __s1 = __p1; \
  __builtin_neon_vst1q_x4_v(__p0, (int8x16_t)__s1.val[0], (int8x16_t)__s1.val[1], (int8x16_t)__s1.val[2], (int8x16_t)__s1.val[3], 38); \
})
#else
#define vst1q_p64_x4(__p0, __p1) __extension__ ({ \
  poly64x2x4_t __s1 = __p1; \
  poly64x2x4_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 1, 0); \
  __rev1.val[3] = __builtin_shufflevector(__s1.val[3], __s1.val[3], 1, 0); \
  __builtin_neon_vst1q_x4_v(__p0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], (int8x16_t)__rev1.val[2], (int8x16_t)__rev1.val[3], 38); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1q_p16_x4(__p0, __p1) __extension__ ({ \
  poly16x8x4_t __s1 = __p1; \
  __builtin_neon_vst1q_x4_v(__p0, (int8x16_t)__s1.val[0], (int8x16_t)__s1.val[1], (int8x16_t)__s1.val[2], (int8x16_t)__s1.val[3], 37); \
})
#else
#define vst1q_p16_x4(__p0, __p1) __extension__ ({ \
  poly16x8x4_t __s1 = __p1; \
  poly16x8x4_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[3] = __builtin_shufflevector(__s1.val[3], __s1.val[3], 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst1q_x4_v(__p0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], (int8x16_t)__rev1.val[2], (int8x16_t)__rev1.val[3], 37); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1q_u8_x4(__p0, __p1) __extension__ ({ \
  uint8x16x4_t __s1 = __p1; \
  __builtin_neon_vst1q_x4_v(__p0, (int8x16_t)__s1.val[0], (int8x16_t)__s1.val[1], (int8x16_t)__s1.val[2], (int8x16_t)__s1.val[3], 48); \
})
#else
#define vst1q_u8_x4(__p0, __p1) __extension__ ({ \
  uint8x16x4_t __s1 = __p1; \
  uint8x16x4_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[3] = __builtin_shufflevector(__s1.val[3], __s1.val[3], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst1q_x4_v(__p0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], (int8x16_t)__rev1.val[2], (int8x16_t)__rev1.val[3], 48); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1q_u32_x4(__p0, __p1) __extension__ ({ \
  uint32x4x4_t __s1 = __p1; \
  __builtin_neon_vst1q_x4_v(__p0, (int8x16_t)__s1.val[0], (int8x16_t)__s1.val[1], (int8x16_t)__s1.val[2], (int8x16_t)__s1.val[3], 50); \
})
#else
#define vst1q_u32_x4(__p0, __p1) __extension__ ({ \
  uint32x4x4_t __s1 = __p1; \
  uint32x4x4_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 3, 2, 1, 0); \
  __rev1.val[3] = __builtin_shufflevector(__s1.val[3], __s1.val[3], 3, 2, 1, 0); \
  __builtin_neon_vst1q_x4_v(__p0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], (int8x16_t)__rev1.val[2], (int8x16_t)__rev1.val[3], 50); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1q_u64_x4(__p0, __p1) __extension__ ({ \
  uint64x2x4_t __s1 = __p1; \
  __builtin_neon_vst1q_x4_v(__p0, (int8x16_t)__s1.val[0], (int8x16_t)__s1.val[1], (int8x16_t)__s1.val[2], (int8x16_t)__s1.val[3], 51); \
})
#else
#define vst1q_u64_x4(__p0, __p1) __extension__ ({ \
  uint64x2x4_t __s1 = __p1; \
  uint64x2x4_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 1, 0); \
  __rev1.val[3] = __builtin_shufflevector(__s1.val[3], __s1.val[3], 1, 0); \
  __builtin_neon_vst1q_x4_v(__p0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], (int8x16_t)__rev1.val[2], (int8x16_t)__rev1.val[3], 51); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1q_u16_x4(__p0, __p1) __extension__ ({ \
  uint16x8x4_t __s1 = __p1; \
  __builtin_neon_vst1q_x4_v(__p0, (int8x16_t)__s1.val[0], (int8x16_t)__s1.val[1], (int8x16_t)__s1.val[2], (int8x16_t)__s1.val[3], 49); \
})
#else
#define vst1q_u16_x4(__p0, __p1) __extension__ ({ \
  uint16x8x4_t __s1 = __p1; \
  uint16x8x4_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[3] = __builtin_shufflevector(__s1.val[3], __s1.val[3], 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst1q_x4_v(__p0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], (int8x16_t)__rev1.val[2], (int8x16_t)__rev1.val[3], 49); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1q_s8_x4(__p0, __p1) __extension__ ({ \
  int8x16x4_t __s1 = __p1; \
  __builtin_neon_vst1q_x4_v(__p0, (int8x16_t)__s1.val[0], (int8x16_t)__s1.val[1], (int8x16_t)__s1.val[2], (int8x16_t)__s1.val[3], 32); \
})
#else
#define vst1q_s8_x4(__p0, __p1) __extension__ ({ \
  int8x16x4_t __s1 = __p1; \
  int8x16x4_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[3] = __builtin_shufflevector(__s1.val[3], __s1.val[3], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst1q_x4_v(__p0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], (int8x16_t)__rev1.val[2], (int8x16_t)__rev1.val[3], 32); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1q_f64_x4(__p0, __p1) __extension__ ({ \
  float64x2x4_t __s1 = __p1; \
  __builtin_neon_vst1q_x4_v(__p0, __s1.val[0], __s1.val[1], __s1.val[2], __s1.val[3], 42); \
})
#else
#define vst1q_f64_x4(__p0, __p1) __extension__ ({ \
  float64x2x4_t __s1 = __p1; \
  float64x2x4_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 1, 0); \
  __rev1.val[3] = __builtin_shufflevector(__s1.val[3], __s1.val[3], 1, 0); \
  __builtin_neon_vst1q_x4_v(__p0, __rev1.val[0], __rev1.val[1], __rev1.val[2], __rev1.val[3], 42); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1q_f32_x4(__p0, __p1) __extension__ ({ \
  float32x4x4_t __s1 = __p1; \
  __builtin_neon_vst1q_x4_v(__p0, __s1.val[0], __s1.val[1], __s1.val[2], __s1.val[3], 41); \
})
#else
#define vst1q_f32_x4(__p0, __p1) __extension__ ({ \
  float32x4x4_t __s1 = __p1; \
  float32x4x4_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 3, 2, 1, 0); \
  __rev1.val[3] = __builtin_shufflevector(__s1.val[3], __s1.val[3], 3, 2, 1, 0); \
  __builtin_neon_vst1q_x4_v(__p0, __rev1.val[0], __rev1.val[1], __rev1.val[2], __rev1.val[3], 41); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1q_f16_x4(__p0, __p1) __extension__ ({ \
  float16x8x4_t __s1 = __p1; \
  __builtin_neon_vst1q_x4_v(__p0, __s1.val[0], __s1.val[1], __s1.val[2], __s1.val[3], 40); \
})
#else
#define vst1q_f16_x4(__p0, __p1) __extension__ ({ \
  float16x8x4_t __s1 = __p1; \
  float16x8x4_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[3] = __builtin_shufflevector(__s1.val[3], __s1.val[3], 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst1q_x4_v(__p0, __rev1.val[0], __rev1.val[1], __rev1.val[2], __rev1.val[3], 40); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1q_s32_x4(__p0, __p1) __extension__ ({ \
  int32x4x4_t __s1 = __p1; \
  __builtin_neon_vst1q_x4_v(__p0, __s1.val[0], __s1.val[1], __s1.val[2], __s1.val[3], 34); \
})
#else
#define vst1q_s32_x4(__p0, __p1) __extension__ ({ \
  int32x4x4_t __s1 = __p1; \
  int32x4x4_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 3, 2, 1, 0); \
  __rev1.val[3] = __builtin_shufflevector(__s1.val[3], __s1.val[3], 3, 2, 1, 0); \
  __builtin_neon_vst1q_x4_v(__p0, __rev1.val[0], __rev1.val[1], __rev1.val[2], __rev1.val[3], 34); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1q_s64_x4(__p0, __p1) __extension__ ({ \
  int64x2x4_t __s1 = __p1; \
  __builtin_neon_vst1q_x4_v(__p0, __s1.val[0], __s1.val[1], __s1.val[2], __s1.val[3], 35); \
})
#else
#define vst1q_s64_x4(__p0, __p1) __extension__ ({ \
  int64x2x4_t __s1 = __p1; \
  int64x2x4_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 1, 0); \
  __rev1.val[3] = __builtin_shufflevector(__s1.val[3], __s1.val[3], 1, 0); \
  __builtin_neon_vst1q_x4_v(__p0, __rev1.val[0], __rev1.val[1], __rev1.val[2], __rev1.val[3], 35); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1q_s16_x4(__p0, __p1) __extension__ ({ \
  int16x8x4_t __s1 = __p1; \
  __builtin_neon_vst1q_x4_v(__p0, __s1.val[0], __s1.val[1], __s1.val[2], __s1.val[3], 33); \
})
#else
#define vst1q_s16_x4(__p0, __p1) __extension__ ({ \
  int16x8x4_t __s1 = __p1; \
  int16x8x4_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[3] = __builtin_shufflevector(__s1.val[3], __s1.val[3], 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst1q_x4_v(__p0, __rev1.val[0], __rev1.val[1], __rev1.val[2], __rev1.val[3], 33); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1_u8_x4(__p0, __p1) __extension__ ({ \
  uint8x8x4_t __s1 = __p1; \
  __builtin_neon_vst1_x4_v(__p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], (int8x8_t)__s1.val[2], (int8x8_t)__s1.val[3], 16); \
})
#else
#define vst1_u8_x4(__p0, __p1) __extension__ ({ \
  uint8x8x4_t __s1 = __p1; \
  uint8x8x4_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[3] = __builtin_shufflevector(__s1.val[3], __s1.val[3], 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst1_x4_v(__p0, (int8x8_t)__rev1.val[0], (int8x8_t)__rev1.val[1], (int8x8_t)__rev1.val[2], (int8x8_t)__rev1.val[3], 16); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1_u32_x4(__p0, __p1) __extension__ ({ \
  uint32x2x4_t __s1 = __p1; \
  __builtin_neon_vst1_x4_v(__p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], (int8x8_t)__s1.val[2], (int8x8_t)__s1.val[3], 18); \
})
#else
#define vst1_u32_x4(__p0, __p1) __extension__ ({ \
  uint32x2x4_t __s1 = __p1; \
  uint32x2x4_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 1, 0); \
  __rev1.val[3] = __builtin_shufflevector(__s1.val[3], __s1.val[3], 1, 0); \
  __builtin_neon_vst1_x4_v(__p0, (int8x8_t)__rev1.val[0], (int8x8_t)__rev1.val[1], (int8x8_t)__rev1.val[2], (int8x8_t)__rev1.val[3], 18); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1_u64_x4(__p0, __p1) __extension__ ({ \
  uint64x1x4_t __s1 = __p1; \
  __builtin_neon_vst1_x4_v(__p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], (int8x8_t)__s1.val[2], (int8x8_t)__s1.val[3], 19); \
})
#else
#define vst1_u64_x4(__p0, __p1) __extension__ ({ \
  uint64x1x4_t __s1 = __p1; \
  __builtin_neon_vst1_x4_v(__p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], (int8x8_t)__s1.val[2], (int8x8_t)__s1.val[3], 19); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1_u16_x4(__p0, __p1) __extension__ ({ \
  uint16x4x4_t __s1 = __p1; \
  __builtin_neon_vst1_x4_v(__p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], (int8x8_t)__s1.val[2], (int8x8_t)__s1.val[3], 17); \
})
#else
#define vst1_u16_x4(__p0, __p1) __extension__ ({ \
  uint16x4x4_t __s1 = __p1; \
  uint16x4x4_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 3, 2, 1, 0); \
  __rev1.val[3] = __builtin_shufflevector(__s1.val[3], __s1.val[3], 3, 2, 1, 0); \
  __builtin_neon_vst1_x4_v(__p0, (int8x8_t)__rev1.val[0], (int8x8_t)__rev1.val[1], (int8x8_t)__rev1.val[2], (int8x8_t)__rev1.val[3], 17); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1_s8_x4(__p0, __p1) __extension__ ({ \
  int8x8x4_t __s1 = __p1; \
  __builtin_neon_vst1_x4_v(__p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], (int8x8_t)__s1.val[2], (int8x8_t)__s1.val[3], 0); \
})
#else
#define vst1_s8_x4(__p0, __p1) __extension__ ({ \
  int8x8x4_t __s1 = __p1; \
  int8x8x4_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[3] = __builtin_shufflevector(__s1.val[3], __s1.val[3], 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst1_x4_v(__p0, (int8x8_t)__rev1.val[0], (int8x8_t)__rev1.val[1], (int8x8_t)__rev1.val[2], (int8x8_t)__rev1.val[3], 0); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1_f64_x4(__p0, __p1) __extension__ ({ \
  float64x1x4_t __s1 = __p1; \
  __builtin_neon_vst1_x4_v(__p0, __s1.val[0], __s1.val[1], __s1.val[2], __s1.val[3], 10); \
})
#else
#define vst1_f64_x4(__p0, __p1) __extension__ ({ \
  float64x1x4_t __s1 = __p1; \
  __builtin_neon_vst1_x4_v(__p0, __s1.val[0], __s1.val[1], __s1.val[2], __s1.val[3], 10); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1_f32_x4(__p0, __p1) __extension__ ({ \
  float32x2x4_t __s1 = __p1; \
  __builtin_neon_vst1_x4_v(__p0, __s1.val[0], __s1.val[1], __s1.val[2], __s1.val[3], 9); \
})
#else
#define vst1_f32_x4(__p0, __p1) __extension__ ({ \
  float32x2x4_t __s1 = __p1; \
  float32x2x4_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 1, 0); \
  __rev1.val[3] = __builtin_shufflevector(__s1.val[3], __s1.val[3], 1, 0); \
  __builtin_neon_vst1_x4_v(__p0, __rev1.val[0], __rev1.val[1], __rev1.val[2], __rev1.val[3], 9); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1_f16_x4(__p0, __p1) __extension__ ({ \
  float16x4x4_t __s1 = __p1; \
  __builtin_neon_vst1_x4_v(__p0, __s1.val[0], __s1.val[1], __s1.val[2], __s1.val[3], 8); \
})
#else
#define vst1_f16_x4(__p0, __p1) __extension__ ({ \
  float16x4x4_t __s1 = __p1; \
  float16x4x4_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 3, 2, 1, 0); \
  __rev1.val[3] = __builtin_shufflevector(__s1.val[3], __s1.val[3], 3, 2, 1, 0); \
  __builtin_neon_vst1_x4_v(__p0, __rev1.val[0], __rev1.val[1], __rev1.val[2], __rev1.val[3], 8); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1_s32_x4(__p0, __p1) __extension__ ({ \
  int32x2x4_t __s1 = __p1; \
  __builtin_neon_vst1_x4_v(__p0, __s1.val[0], __s1.val[1], __s1.val[2], __s1.val[3], 2); \
})
#else
#define vst1_s32_x4(__p0, __p1) __extension__ ({ \
  int32x2x4_t __s1 = __p1; \
  int32x2x4_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 1, 0); \
  __rev1.val[3] = __builtin_shufflevector(__s1.val[3], __s1.val[3], 1, 0); \
  __builtin_neon_vst1_x4_v(__p0, __rev1.val[0], __rev1.val[1], __rev1.val[2], __rev1.val[3], 2); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1_s64_x4(__p0, __p1) __extension__ ({ \
  int64x1x4_t __s1 = __p1; \
  __builtin_neon_vst1_x4_v(__p0, __s1.val[0], __s1.val[1], __s1.val[2], __s1.val[3], 3); \
})
#else
#define vst1_s64_x4(__p0, __p1) __extension__ ({ \
  int64x1x4_t __s1 = __p1; \
  __builtin_neon_vst1_x4_v(__p0, __s1.val[0], __s1.val[1], __s1.val[2], __s1.val[3], 3); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1_s16_x4(__p0, __p1) __extension__ ({ \
  int16x4x4_t __s1 = __p1; \
  __builtin_neon_vst1_x4_v(__p0, __s1.val[0], __s1.val[1], __s1.val[2], __s1.val[3], 1); \
})
#else
#define vst1_s16_x4(__p0, __p1) __extension__ ({ \
  int16x4x4_t __s1 = __p1; \
  int16x4x4_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 3, 2, 1, 0); \
  __rev1.val[3] = __builtin_shufflevector(__s1.val[3], __s1.val[3], 3, 2, 1, 0); \
  __builtin_neon_vst1_x4_v(__p0, __rev1.val[0], __rev1.val[1], __rev1.val[2], __rev1.val[3], 1); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst2_p64(__p0, __p1) __extension__ ({ \
  poly64x1x2_t __s1 = __p1; \
  __builtin_neon_vst2_v(__p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], 6); \
})
#else
#define vst2_p64(__p0, __p1) __extension__ ({ \
  poly64x1x2_t __s1 = __p1; \
  __builtin_neon_vst2_v(__p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], 6); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst2q_p64(__p0, __p1) __extension__ ({ \
  poly64x2x2_t __s1 = __p1; \
  __builtin_neon_vst2q_v(__p0, (int8x16_t)__s1.val[0], (int8x16_t)__s1.val[1], 38); \
})
#else
#define vst2q_p64(__p0, __p1) __extension__ ({ \
  poly64x2x2_t __s1 = __p1; \
  poly64x2x2_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 1, 0); \
  __builtin_neon_vst2q_v(__p0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], 38); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst2q_u64(__p0, __p1) __extension__ ({ \
  uint64x2x2_t __s1 = __p1; \
  __builtin_neon_vst2q_v(__p0, (int8x16_t)__s1.val[0], (int8x16_t)__s1.val[1], 51); \
})
#else
#define vst2q_u64(__p0, __p1) __extension__ ({ \
  uint64x2x2_t __s1 = __p1; \
  uint64x2x2_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 1, 0); \
  __builtin_neon_vst2q_v(__p0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], 51); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst2q_f64(__p0, __p1) __extension__ ({ \
  float64x2x2_t __s1 = __p1; \
  __builtin_neon_vst2q_v(__p0, __s1.val[0], __s1.val[1], 42); \
})
#else
#define vst2q_f64(__p0, __p1) __extension__ ({ \
  float64x2x2_t __s1 = __p1; \
  float64x2x2_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 1, 0); \
  __builtin_neon_vst2q_v(__p0, __rev1.val[0], __rev1.val[1], 42); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst2q_s64(__p0, __p1) __extension__ ({ \
  int64x2x2_t __s1 = __p1; \
  __builtin_neon_vst2q_v(__p0, __s1.val[0], __s1.val[1], 35); \
})
#else
#define vst2q_s64(__p0, __p1) __extension__ ({ \
  int64x2x2_t __s1 = __p1; \
  int64x2x2_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 1, 0); \
  __builtin_neon_vst2q_v(__p0, __rev1.val[0], __rev1.val[1], 35); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst2_f64(__p0, __p1) __extension__ ({ \
  float64x1x2_t __s1 = __p1; \
  __builtin_neon_vst2_v(__p0, __s1.val[0], __s1.val[1], 10); \
})
#else
#define vst2_f64(__p0, __p1) __extension__ ({ \
  float64x1x2_t __s1 = __p1; \
  __builtin_neon_vst2_v(__p0, __s1.val[0], __s1.val[1], 10); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst2_lane_p64(__p0, __p1, __p2) __extension__ ({ \
  poly64x1x2_t __s1 = __p1; \
  __builtin_neon_vst2_lane_v(__p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], __p2, 6); \
})
#else
#define vst2_lane_p64(__p0, __p1, __p2) __extension__ ({ \
  poly64x1x2_t __s1 = __p1; \
  __builtin_neon_vst2_lane_v(__p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], __p2, 6); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst2q_lane_p8(__p0, __p1, __p2) __extension__ ({ \
  poly8x16x2_t __s1 = __p1; \
  __builtin_neon_vst2q_lane_v(__p0, (int8x16_t)__s1.val[0], (int8x16_t)__s1.val[1], __p2, 36); \
})
#else
#define vst2q_lane_p8(__p0, __p1, __p2) __extension__ ({ \
  poly8x16x2_t __s1 = __p1; \
  poly8x16x2_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst2q_lane_v(__p0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], __p2, 36); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst2q_lane_p64(__p0, __p1, __p2) __extension__ ({ \
  poly64x2x2_t __s1 = __p1; \
  __builtin_neon_vst2q_lane_v(__p0, (int8x16_t)__s1.val[0], (int8x16_t)__s1.val[1], __p2, 38); \
})
#else
#define vst2q_lane_p64(__p0, __p1, __p2) __extension__ ({ \
  poly64x2x2_t __s1 = __p1; \
  poly64x2x2_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 1, 0); \
  __builtin_neon_vst2q_lane_v(__p0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], __p2, 38); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst2q_lane_u8(__p0, __p1, __p2) __extension__ ({ \
  uint8x16x2_t __s1 = __p1; \
  __builtin_neon_vst2q_lane_v(__p0, (int8x16_t)__s1.val[0], (int8x16_t)__s1.val[1], __p2, 48); \
})
#else
#define vst2q_lane_u8(__p0, __p1, __p2) __extension__ ({ \
  uint8x16x2_t __s1 = __p1; \
  uint8x16x2_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst2q_lane_v(__p0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], __p2, 48); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst2q_lane_u64(__p0, __p1, __p2) __extension__ ({ \
  uint64x2x2_t __s1 = __p1; \
  __builtin_neon_vst2q_lane_v(__p0, (int8x16_t)__s1.val[0], (int8x16_t)__s1.val[1], __p2, 51); \
})
#else
#define vst2q_lane_u64(__p0, __p1, __p2) __extension__ ({ \
  uint64x2x2_t __s1 = __p1; \
  uint64x2x2_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 1, 0); \
  __builtin_neon_vst2q_lane_v(__p0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], __p2, 51); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst2q_lane_s8(__p0, __p1, __p2) __extension__ ({ \
  int8x16x2_t __s1 = __p1; \
  __builtin_neon_vst2q_lane_v(__p0, (int8x16_t)__s1.val[0], (int8x16_t)__s1.val[1], __p2, 32); \
})
#else
#define vst2q_lane_s8(__p0, __p1, __p2) __extension__ ({ \
  int8x16x2_t __s1 = __p1; \
  int8x16x2_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst2q_lane_v(__p0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], __p2, 32); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst2q_lane_f64(__p0, __p1, __p2) __extension__ ({ \
  float64x2x2_t __s1 = __p1; \
  __builtin_neon_vst2q_lane_v(__p0, __s1.val[0], __s1.val[1], __p2, 42); \
})
#else
#define vst2q_lane_f64(__p0, __p1, __p2) __extension__ ({ \
  float64x2x2_t __s1 = __p1; \
  float64x2x2_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 1, 0); \
  __builtin_neon_vst2q_lane_v(__p0, __rev1.val[0], __rev1.val[1], __p2, 42); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst2q_lane_s64(__p0, __p1, __p2) __extension__ ({ \
  int64x2x2_t __s1 = __p1; \
  __builtin_neon_vst2q_lane_v(__p0, __s1.val[0], __s1.val[1], __p2, 35); \
})
#else
#define vst2q_lane_s64(__p0, __p1, __p2) __extension__ ({ \
  int64x2x2_t __s1 = __p1; \
  int64x2x2_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 1, 0); \
  __builtin_neon_vst2q_lane_v(__p0, __rev1.val[0], __rev1.val[1], __p2, 35); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst2_lane_u64(__p0, __p1, __p2) __extension__ ({ \
  uint64x1x2_t __s1 = __p1; \
  __builtin_neon_vst2_lane_v(__p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], __p2, 19); \
})
#else
#define vst2_lane_u64(__p0, __p1, __p2) __extension__ ({ \
  uint64x1x2_t __s1 = __p1; \
  __builtin_neon_vst2_lane_v(__p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], __p2, 19); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst2_lane_f64(__p0, __p1, __p2) __extension__ ({ \
  float64x1x2_t __s1 = __p1; \
  __builtin_neon_vst2_lane_v(__p0, __s1.val[0], __s1.val[1], __p2, 10); \
})
#else
#define vst2_lane_f64(__p0, __p1, __p2) __extension__ ({ \
  float64x1x2_t __s1 = __p1; \
  __builtin_neon_vst2_lane_v(__p0, __s1.val[0], __s1.val[1], __p2, 10); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst2_lane_s64(__p0, __p1, __p2) __extension__ ({ \
  int64x1x2_t __s1 = __p1; \
  __builtin_neon_vst2_lane_v(__p0, __s1.val[0], __s1.val[1], __p2, 3); \
})
#else
#define vst2_lane_s64(__p0, __p1, __p2) __extension__ ({ \
  int64x1x2_t __s1 = __p1; \
  __builtin_neon_vst2_lane_v(__p0, __s1.val[0], __s1.val[1], __p2, 3); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst3_p64(__p0, __p1) __extension__ ({ \
  poly64x1x3_t __s1 = __p1; \
  __builtin_neon_vst3_v(__p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], (int8x8_t)__s1.val[2], 6); \
})
#else
#define vst3_p64(__p0, __p1) __extension__ ({ \
  poly64x1x3_t __s1 = __p1; \
  __builtin_neon_vst3_v(__p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], (int8x8_t)__s1.val[2], 6); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst3q_p64(__p0, __p1) __extension__ ({ \
  poly64x2x3_t __s1 = __p1; \
  __builtin_neon_vst3q_v(__p0, (int8x16_t)__s1.val[0], (int8x16_t)__s1.val[1], (int8x16_t)__s1.val[2], 38); \
})
#else
#define vst3q_p64(__p0, __p1) __extension__ ({ \
  poly64x2x3_t __s1 = __p1; \
  poly64x2x3_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 1, 0); \
  __builtin_neon_vst3q_v(__p0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], (int8x16_t)__rev1.val[2], 38); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst3q_u64(__p0, __p1) __extension__ ({ \
  uint64x2x3_t __s1 = __p1; \
  __builtin_neon_vst3q_v(__p0, (int8x16_t)__s1.val[0], (int8x16_t)__s1.val[1], (int8x16_t)__s1.val[2], 51); \
})
#else
#define vst3q_u64(__p0, __p1) __extension__ ({ \
  uint64x2x3_t __s1 = __p1; \
  uint64x2x3_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 1, 0); \
  __builtin_neon_vst3q_v(__p0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], (int8x16_t)__rev1.val[2], 51); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst3q_f64(__p0, __p1) __extension__ ({ \
  float64x2x3_t __s1 = __p1; \
  __builtin_neon_vst3q_v(__p0, __s1.val[0], __s1.val[1], __s1.val[2], 42); \
})
#else
#define vst3q_f64(__p0, __p1) __extension__ ({ \
  float64x2x3_t __s1 = __p1; \
  float64x2x3_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 1, 0); \
  __builtin_neon_vst3q_v(__p0, __rev1.val[0], __rev1.val[1], __rev1.val[2], 42); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst3q_s64(__p0, __p1) __extension__ ({ \
  int64x2x3_t __s1 = __p1; \
  __builtin_neon_vst3q_v(__p0, __s1.val[0], __s1.val[1], __s1.val[2], 35); \
})
#else
#define vst3q_s64(__p0, __p1) __extension__ ({ \
  int64x2x3_t __s1 = __p1; \
  int64x2x3_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 1, 0); \
  __builtin_neon_vst3q_v(__p0, __rev1.val[0], __rev1.val[1], __rev1.val[2], 35); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst3_f64(__p0, __p1) __extension__ ({ \
  float64x1x3_t __s1 = __p1; \
  __builtin_neon_vst3_v(__p0, __s1.val[0], __s1.val[1], __s1.val[2], 10); \
})
#else
#define vst3_f64(__p0, __p1) __extension__ ({ \
  float64x1x3_t __s1 = __p1; \
  __builtin_neon_vst3_v(__p0, __s1.val[0], __s1.val[1], __s1.val[2], 10); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst3_lane_p64(__p0, __p1, __p2) __extension__ ({ \
  poly64x1x3_t __s1 = __p1; \
  __builtin_neon_vst3_lane_v(__p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], (int8x8_t)__s1.val[2], __p2, 6); \
})
#else
#define vst3_lane_p64(__p0, __p1, __p2) __extension__ ({ \
  poly64x1x3_t __s1 = __p1; \
  __builtin_neon_vst3_lane_v(__p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], (int8x8_t)__s1.val[2], __p2, 6); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst3q_lane_p8(__p0, __p1, __p2) __extension__ ({ \
  poly8x16x3_t __s1 = __p1; \
  __builtin_neon_vst3q_lane_v(__p0, (int8x16_t)__s1.val[0], (int8x16_t)__s1.val[1], (int8x16_t)__s1.val[2], __p2, 36); \
})
#else
#define vst3q_lane_p8(__p0, __p1, __p2) __extension__ ({ \
  poly8x16x3_t __s1 = __p1; \
  poly8x16x3_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst3q_lane_v(__p0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], (int8x16_t)__rev1.val[2], __p2, 36); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst3q_lane_p64(__p0, __p1, __p2) __extension__ ({ \
  poly64x2x3_t __s1 = __p1; \
  __builtin_neon_vst3q_lane_v(__p0, (int8x16_t)__s1.val[0], (int8x16_t)__s1.val[1], (int8x16_t)__s1.val[2], __p2, 38); \
})
#else
#define vst3q_lane_p64(__p0, __p1, __p2) __extension__ ({ \
  poly64x2x3_t __s1 = __p1; \
  poly64x2x3_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 1, 0); \
  __builtin_neon_vst3q_lane_v(__p0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], (int8x16_t)__rev1.val[2], __p2, 38); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst3q_lane_u8(__p0, __p1, __p2) __extension__ ({ \
  uint8x16x3_t __s1 = __p1; \
  __builtin_neon_vst3q_lane_v(__p0, (int8x16_t)__s1.val[0], (int8x16_t)__s1.val[1], (int8x16_t)__s1.val[2], __p2, 48); \
})
#else
#define vst3q_lane_u8(__p0, __p1, __p2) __extension__ ({ \
  uint8x16x3_t __s1 = __p1; \
  uint8x16x3_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst3q_lane_v(__p0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], (int8x16_t)__rev1.val[2], __p2, 48); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst3q_lane_u64(__p0, __p1, __p2) __extension__ ({ \
  uint64x2x3_t __s1 = __p1; \
  __builtin_neon_vst3q_lane_v(__p0, (int8x16_t)__s1.val[0], (int8x16_t)__s1.val[1], (int8x16_t)__s1.val[2], __p2, 51); \
})
#else
#define vst3q_lane_u64(__p0, __p1, __p2) __extension__ ({ \
  uint64x2x3_t __s1 = __p1; \
  uint64x2x3_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 1, 0); \
  __builtin_neon_vst3q_lane_v(__p0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], (int8x16_t)__rev1.val[2], __p2, 51); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst3q_lane_s8(__p0, __p1, __p2) __extension__ ({ \
  int8x16x3_t __s1 = __p1; \
  __builtin_neon_vst3q_lane_v(__p0, (int8x16_t)__s1.val[0], (int8x16_t)__s1.val[1], (int8x16_t)__s1.val[2], __p2, 32); \
})
#else
#define vst3q_lane_s8(__p0, __p1, __p2) __extension__ ({ \
  int8x16x3_t __s1 = __p1; \
  int8x16x3_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst3q_lane_v(__p0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], (int8x16_t)__rev1.val[2], __p2, 32); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst3q_lane_f64(__p0, __p1, __p2) __extension__ ({ \
  float64x2x3_t __s1 = __p1; \
  __builtin_neon_vst3q_lane_v(__p0, __s1.val[0], __s1.val[1], __s1.val[2], __p2, 42); \
})
#else
#define vst3q_lane_f64(__p0, __p1, __p2) __extension__ ({ \
  float64x2x3_t __s1 = __p1; \
  float64x2x3_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 1, 0); \
  __builtin_neon_vst3q_lane_v(__p0, __rev1.val[0], __rev1.val[1], __rev1.val[2], __p2, 42); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst3q_lane_s64(__p0, __p1, __p2) __extension__ ({ \
  int64x2x3_t __s1 = __p1; \
  __builtin_neon_vst3q_lane_v(__p0, __s1.val[0], __s1.val[1], __s1.val[2], __p2, 35); \
})
#else
#define vst3q_lane_s64(__p0, __p1, __p2) __extension__ ({ \
  int64x2x3_t __s1 = __p1; \
  int64x2x3_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 1, 0); \
  __builtin_neon_vst3q_lane_v(__p0, __rev1.val[0], __rev1.val[1], __rev1.val[2], __p2, 35); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst3_lane_u64(__p0, __p1, __p2) __extension__ ({ \
  uint64x1x3_t __s1 = __p1; \
  __builtin_neon_vst3_lane_v(__p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], (int8x8_t)__s1.val[2], __p2, 19); \
})
#else
#define vst3_lane_u64(__p0, __p1, __p2) __extension__ ({ \
  uint64x1x3_t __s1 = __p1; \
  __builtin_neon_vst3_lane_v(__p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], (int8x8_t)__s1.val[2], __p2, 19); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst3_lane_f64(__p0, __p1, __p2) __extension__ ({ \
  float64x1x3_t __s1 = __p1; \
  __builtin_neon_vst3_lane_v(__p0, __s1.val[0], __s1.val[1], __s1.val[2], __p2, 10); \
})
#else
#define vst3_lane_f64(__p0, __p1, __p2) __extension__ ({ \
  float64x1x3_t __s1 = __p1; \
  __builtin_neon_vst3_lane_v(__p0, __s1.val[0], __s1.val[1], __s1.val[2], __p2, 10); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst3_lane_s64(__p0, __p1, __p2) __extension__ ({ \
  int64x1x3_t __s1 = __p1; \
  __builtin_neon_vst3_lane_v(__p0, __s1.val[0], __s1.val[1], __s1.val[2], __p2, 3); \
})
#else
#define vst3_lane_s64(__p0, __p1, __p2) __extension__ ({ \
  int64x1x3_t __s1 = __p1; \
  __builtin_neon_vst3_lane_v(__p0, __s1.val[0], __s1.val[1], __s1.val[2], __p2, 3); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst4_p64(__p0, __p1) __extension__ ({ \
  poly64x1x4_t __s1 = __p1; \
  __builtin_neon_vst4_v(__p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], (int8x8_t)__s1.val[2], (int8x8_t)__s1.val[3], 6); \
})
#else
#define vst4_p64(__p0, __p1) __extension__ ({ \
  poly64x1x4_t __s1 = __p1; \
  __builtin_neon_vst4_v(__p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], (int8x8_t)__s1.val[2], (int8x8_t)__s1.val[3], 6); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst4q_p64(__p0, __p1) __extension__ ({ \
  poly64x2x4_t __s1 = __p1; \
  __builtin_neon_vst4q_v(__p0, (int8x16_t)__s1.val[0], (int8x16_t)__s1.val[1], (int8x16_t)__s1.val[2], (int8x16_t)__s1.val[3], 38); \
})
#else
#define vst4q_p64(__p0, __p1) __extension__ ({ \
  poly64x2x4_t __s1 = __p1; \
  poly64x2x4_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 1, 0); \
  __rev1.val[3] = __builtin_shufflevector(__s1.val[3], __s1.val[3], 1, 0); \
  __builtin_neon_vst4q_v(__p0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], (int8x16_t)__rev1.val[2], (int8x16_t)__rev1.val[3], 38); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst4q_u64(__p0, __p1) __extension__ ({ \
  uint64x2x4_t __s1 = __p1; \
  __builtin_neon_vst4q_v(__p0, (int8x16_t)__s1.val[0], (int8x16_t)__s1.val[1], (int8x16_t)__s1.val[2], (int8x16_t)__s1.val[3], 51); \
})
#else
#define vst4q_u64(__p0, __p1) __extension__ ({ \
  uint64x2x4_t __s1 = __p1; \
  uint64x2x4_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 1, 0); \
  __rev1.val[3] = __builtin_shufflevector(__s1.val[3], __s1.val[3], 1, 0); \
  __builtin_neon_vst4q_v(__p0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], (int8x16_t)__rev1.val[2], (int8x16_t)__rev1.val[3], 51); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst4q_f64(__p0, __p1) __extension__ ({ \
  float64x2x4_t __s1 = __p1; \
  __builtin_neon_vst4q_v(__p0, __s1.val[0], __s1.val[1], __s1.val[2], __s1.val[3], 42); \
})
#else
#define vst4q_f64(__p0, __p1) __extension__ ({ \
  float64x2x4_t __s1 = __p1; \
  float64x2x4_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 1, 0); \
  __rev1.val[3] = __builtin_shufflevector(__s1.val[3], __s1.val[3], 1, 0); \
  __builtin_neon_vst4q_v(__p0, __rev1.val[0], __rev1.val[1], __rev1.val[2], __rev1.val[3], 42); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst4q_s64(__p0, __p1) __extension__ ({ \
  int64x2x4_t __s1 = __p1; \
  __builtin_neon_vst4q_v(__p0, __s1.val[0], __s1.val[1], __s1.val[2], __s1.val[3], 35); \
})
#else
#define vst4q_s64(__p0, __p1) __extension__ ({ \
  int64x2x4_t __s1 = __p1; \
  int64x2x4_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 1, 0); \
  __rev1.val[3] = __builtin_shufflevector(__s1.val[3], __s1.val[3], 1, 0); \
  __builtin_neon_vst4q_v(__p0, __rev1.val[0], __rev1.val[1], __rev1.val[2], __rev1.val[3], 35); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst4_f64(__p0, __p1) __extension__ ({ \
  float64x1x4_t __s1 = __p1; \
  __builtin_neon_vst4_v(__p0, __s1.val[0], __s1.val[1], __s1.val[2], __s1.val[3], 10); \
})
#else
#define vst4_f64(__p0, __p1) __extension__ ({ \
  float64x1x4_t __s1 = __p1; \
  __builtin_neon_vst4_v(__p0, __s1.val[0], __s1.val[1], __s1.val[2], __s1.val[3], 10); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst4_lane_p64(__p0, __p1, __p2) __extension__ ({ \
  poly64x1x4_t __s1 = __p1; \
  __builtin_neon_vst4_lane_v(__p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], (int8x8_t)__s1.val[2], (int8x8_t)__s1.val[3], __p2, 6); \
})
#else
#define vst4_lane_p64(__p0, __p1, __p2) __extension__ ({ \
  poly64x1x4_t __s1 = __p1; \
  __builtin_neon_vst4_lane_v(__p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], (int8x8_t)__s1.val[2], (int8x8_t)__s1.val[3], __p2, 6); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst4q_lane_p8(__p0, __p1, __p2) __extension__ ({ \
  poly8x16x4_t __s1 = __p1; \
  __builtin_neon_vst4q_lane_v(__p0, (int8x16_t)__s1.val[0], (int8x16_t)__s1.val[1], (int8x16_t)__s1.val[2], (int8x16_t)__s1.val[3], __p2, 36); \
})
#else
#define vst4q_lane_p8(__p0, __p1, __p2) __extension__ ({ \
  poly8x16x4_t __s1 = __p1; \
  poly8x16x4_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[3] = __builtin_shufflevector(__s1.val[3], __s1.val[3], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst4q_lane_v(__p0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], (int8x16_t)__rev1.val[2], (int8x16_t)__rev1.val[3], __p2, 36); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst4q_lane_p64(__p0, __p1, __p2) __extension__ ({ \
  poly64x2x4_t __s1 = __p1; \
  __builtin_neon_vst4q_lane_v(__p0, (int8x16_t)__s1.val[0], (int8x16_t)__s1.val[1], (int8x16_t)__s1.val[2], (int8x16_t)__s1.val[3], __p2, 38); \
})
#else
#define vst4q_lane_p64(__p0, __p1, __p2) __extension__ ({ \
  poly64x2x4_t __s1 = __p1; \
  poly64x2x4_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 1, 0); \
  __rev1.val[3] = __builtin_shufflevector(__s1.val[3], __s1.val[3], 1, 0); \
  __builtin_neon_vst4q_lane_v(__p0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], (int8x16_t)__rev1.val[2], (int8x16_t)__rev1.val[3], __p2, 38); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst4q_lane_u8(__p0, __p1, __p2) __extension__ ({ \
  uint8x16x4_t __s1 = __p1; \
  __builtin_neon_vst4q_lane_v(__p0, (int8x16_t)__s1.val[0], (int8x16_t)__s1.val[1], (int8x16_t)__s1.val[2], (int8x16_t)__s1.val[3], __p2, 48); \
})
#else
#define vst4q_lane_u8(__p0, __p1, __p2) __extension__ ({ \
  uint8x16x4_t __s1 = __p1; \
  uint8x16x4_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[3] = __builtin_shufflevector(__s1.val[3], __s1.val[3], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst4q_lane_v(__p0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], (int8x16_t)__rev1.val[2], (int8x16_t)__rev1.val[3], __p2, 48); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst4q_lane_u64(__p0, __p1, __p2) __extension__ ({ \
  uint64x2x4_t __s1 = __p1; \
  __builtin_neon_vst4q_lane_v(__p0, (int8x16_t)__s1.val[0], (int8x16_t)__s1.val[1], (int8x16_t)__s1.val[2], (int8x16_t)__s1.val[3], __p2, 51); \
})
#else
#define vst4q_lane_u64(__p0, __p1, __p2) __extension__ ({ \
  uint64x2x4_t __s1 = __p1; \
  uint64x2x4_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 1, 0); \
  __rev1.val[3] = __builtin_shufflevector(__s1.val[3], __s1.val[3], 1, 0); \
  __builtin_neon_vst4q_lane_v(__p0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], (int8x16_t)__rev1.val[2], (int8x16_t)__rev1.val[3], __p2, 51); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst4q_lane_s8(__p0, __p1, __p2) __extension__ ({ \
  int8x16x4_t __s1 = __p1; \
  __builtin_neon_vst4q_lane_v(__p0, (int8x16_t)__s1.val[0], (int8x16_t)__s1.val[1], (int8x16_t)__s1.val[2], (int8x16_t)__s1.val[3], __p2, 32); \
})
#else
#define vst4q_lane_s8(__p0, __p1, __p2) __extension__ ({ \
  int8x16x4_t __s1 = __p1; \
  int8x16x4_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[3] = __builtin_shufflevector(__s1.val[3], __s1.val[3], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst4q_lane_v(__p0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], (int8x16_t)__rev1.val[2], (int8x16_t)__rev1.val[3], __p2, 32); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst4q_lane_f64(__p0, __p1, __p2) __extension__ ({ \
  float64x2x4_t __s1 = __p1; \
  __builtin_neon_vst4q_lane_v(__p0, __s1.val[0], __s1.val[1], __s1.val[2], __s1.val[3], __p2, 42); \
})
#else
#define vst4q_lane_f64(__p0, __p1, __p2) __extension__ ({ \
  float64x2x4_t __s1 = __p1; \
  float64x2x4_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 1, 0); \
  __rev1.val[3] = __builtin_shufflevector(__s1.val[3], __s1.val[3], 1, 0); \
  __builtin_neon_vst4q_lane_v(__p0, __rev1.val[0], __rev1.val[1], __rev1.val[2], __rev1.val[3], __p2, 42); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst4q_lane_s64(__p0, __p1, __p2) __extension__ ({ \
  int64x2x4_t __s1 = __p1; \
  __builtin_neon_vst4q_lane_v(__p0, __s1.val[0], __s1.val[1], __s1.val[2], __s1.val[3], __p2, 35); \
})
#else
#define vst4q_lane_s64(__p0, __p1, __p2) __extension__ ({ \
  int64x2x4_t __s1 = __p1; \
  int64x2x4_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 1, 0); \
  __rev1.val[3] = __builtin_shufflevector(__s1.val[3], __s1.val[3], 1, 0); \
  __builtin_neon_vst4q_lane_v(__p0, __rev1.val[0], __rev1.val[1], __rev1.val[2], __rev1.val[3], __p2, 35); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst4_lane_u64(__p0, __p1, __p2) __extension__ ({ \
  uint64x1x4_t __s1 = __p1; \
  __builtin_neon_vst4_lane_v(__p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], (int8x8_t)__s1.val[2], (int8x8_t)__s1.val[3], __p2, 19); \
})
#else
#define vst4_lane_u64(__p0, __p1, __p2) __extension__ ({ \
  uint64x1x4_t __s1 = __p1; \
  __builtin_neon_vst4_lane_v(__p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], (int8x8_t)__s1.val[2], (int8x8_t)__s1.val[3], __p2, 19); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst4_lane_f64(__p0, __p1, __p2) __extension__ ({ \
  float64x1x4_t __s1 = __p1; \
  __builtin_neon_vst4_lane_v(__p0, __s1.val[0], __s1.val[1], __s1.val[2], __s1.val[3], __p2, 10); \
})
#else
#define vst4_lane_f64(__p0, __p1, __p2) __extension__ ({ \
  float64x1x4_t __s1 = __p1; \
  __builtin_neon_vst4_lane_v(__p0, __s1.val[0], __s1.val[1], __s1.val[2], __s1.val[3], __p2, 10); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst4_lane_s64(__p0, __p1, __p2) __extension__ ({ \
  int64x1x4_t __s1 = __p1; \
  __builtin_neon_vst4_lane_v(__p0, __s1.val[0], __s1.val[1], __s1.val[2], __s1.val[3], __p2, 3); \
})
#else
#define vst4_lane_s64(__p0, __p1, __p2) __extension__ ({ \
  int64x1x4_t __s1 = __p1; \
  __builtin_neon_vst4_lane_v(__p0, __s1.val[0], __s1.val[1], __s1.val[2], __s1.val[3], __p2, 3); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vstrq_p128(__p0, __p1) __extension__ ({ \
  poly128_t __s1 = __p1; \
  __builtin_neon_vstrq_p128(__p0, __s1); \
})
#else
#define vstrq_p128(__p0, __p1) __extension__ ({ \
  poly128_t __s1 = __p1; \
  __builtin_neon_vstrq_p128(__p0, __s1); \
})
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64_t vsubd_u64(uint64_t __p0, uint64_t __p1) {
  uint64_t __ret;
  __ret = (uint64_t) __builtin_neon_vsubd_u64(__p0, __p1);
  return __ret;
}
#else
__ai uint64_t vsubd_u64(uint64_t __p0, uint64_t __p1) {
  uint64_t __ret;
  __ret = (uint64_t) __builtin_neon_vsubd_u64(__p0, __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64_t vsubd_s64(int64_t __p0, int64_t __p1) {
  int64_t __ret;
  __ret = (int64_t) __builtin_neon_vsubd_s64(__p0, __p1);
  return __ret;
}
#else
__ai int64_t vsubd_s64(int64_t __p0, int64_t __p1) {
  int64_t __ret;
  __ret = (int64_t) __builtin_neon_vsubd_s64(__p0, __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x2_t vsubq_f64(float64x2_t __p0, float64x2_t __p1) {
  float64x2_t __ret;
  __ret = __p0 - __p1;
  return __ret;
}
#else
__ai float64x2_t vsubq_f64(float64x2_t __p0, float64x2_t __p1) {
  float64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  float64x2_t __ret;
  __ret = __rev0 - __rev1;
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x1_t vsub_f64(float64x1_t __p0, float64x1_t __p1) {
  float64x1_t __ret;
  __ret = __p0 - __p1;
  return __ret;
}
#else
__ai float64x1_t vsub_f64(float64x1_t __p0, float64x1_t __p1) {
  float64x1_t __ret;
  __ret = __p0 - __p1;
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x8_t vsubhn_high_u32(uint16x4_t __p0, uint32x4_t __p1, uint32x4_t __p2) {
  uint16x8_t __ret;
  __ret = vcombine_u16(__p0, vsubhn_u32(__p1, __p2));
  return __ret;
}
#else
__ai uint16x8_t vsubhn_high_u32(uint16x4_t __p0, uint32x4_t __p1, uint32x4_t __p2) {
  uint16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint32x4_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 3, 2, 1, 0);
  uint16x8_t __ret;
  __ret = __noswap_vcombine_u16(__rev0, __noswap_vsubhn_u32(__rev1, __rev2));
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vsubhn_high_u64(uint32x2_t __p0, uint64x2_t __p1, uint64x2_t __p2) {
  uint32x4_t __ret;
  __ret = vcombine_u32(__p0, vsubhn_u64(__p1, __p2));
  return __ret;
}
#else
__ai uint32x4_t vsubhn_high_u64(uint32x2_t __p0, uint64x2_t __p1, uint64x2_t __p2) {
  uint32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  uint64x2_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 1, 0);
  uint32x4_t __ret;
  __ret = __noswap_vcombine_u32(__rev0, __noswap_vsubhn_u64(__rev1, __rev2));
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x16_t vsubhn_high_u16(uint8x8_t __p0, uint16x8_t __p1, uint16x8_t __p2) {
  uint8x16_t __ret;
  __ret = vcombine_u8(__p0, vsubhn_u16(__p1, __p2));
  return __ret;
}
#else
__ai uint8x16_t vsubhn_high_u16(uint8x8_t __p0, uint16x8_t __p1, uint16x8_t __p2) {
  uint8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __ret;
  __ret = __noswap_vcombine_u8(__rev0, __noswap_vsubhn_u16(__rev1, __rev2));
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x8_t vsubhn_high_s32(int16x4_t __p0, int32x4_t __p1, int32x4_t __p2) {
  int16x8_t __ret;
  __ret = vcombine_s16(__p0, vsubhn_s32(__p1, __p2));
  return __ret;
}
#else
__ai int16x8_t vsubhn_high_s32(int16x4_t __p0, int32x4_t __p1, int32x4_t __p2) {
  int16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int32x4_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 3, 2, 1, 0);
  int16x8_t __ret;
  __ret = __noswap_vcombine_s16(__rev0, __noswap_vsubhn_s32(__rev1, __rev2));
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vsubhn_high_s64(int32x2_t __p0, int64x2_t __p1, int64x2_t __p2) {
  int32x4_t __ret;
  __ret = vcombine_s32(__p0, vsubhn_s64(__p1, __p2));
  return __ret;
}
#else
__ai int32x4_t vsubhn_high_s64(int32x2_t __p0, int64x2_t __p1, int64x2_t __p2) {
  int32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  int64x2_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 1, 0);
  int32x4_t __ret;
  __ret = __noswap_vcombine_s32(__rev0, __noswap_vsubhn_s64(__rev1, __rev2));
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x16_t vsubhn_high_s16(int8x8_t __p0, int16x8_t __p1, int16x8_t __p2) {
  int8x16_t __ret;
  __ret = vcombine_s8(__p0, vsubhn_s16(__p1, __p2));
  return __ret;
}
#else
__ai int8x16_t vsubhn_high_s16(int8x8_t __p0, int16x8_t __p1, int16x8_t __p2) {
  int8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __ret;
  __ret = __noswap_vcombine_s8(__rev0, __noswap_vsubhn_s16(__rev1, __rev2));
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x8_t vsubl_high_u8(uint8x16_t __p0, uint8x16_t __p1) {
  uint16x8_t __ret;
  __ret = vmovl_high_u8(__p0) - vmovl_high_u8(__p1);
  return __ret;
}
#else
__ai uint16x8_t vsubl_high_u8(uint8x16_t __p0, uint8x16_t __p1) {
  uint8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __ret;
  __ret = __noswap_vmovl_high_u8(__rev0) - __noswap_vmovl_high_u8(__rev1);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vsubl_high_u32(uint32x4_t __p0, uint32x4_t __p1) {
  uint64x2_t __ret;
  __ret = vmovl_high_u32(__p0) - vmovl_high_u32(__p1);
  return __ret;
}
#else
__ai uint64x2_t vsubl_high_u32(uint32x4_t __p0, uint32x4_t __p1) {
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint64x2_t __ret;
  __ret = __noswap_vmovl_high_u32(__rev0) - __noswap_vmovl_high_u32(__rev1);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vsubl_high_u16(uint16x8_t __p0, uint16x8_t __p1) {
  uint32x4_t __ret;
  __ret = vmovl_high_u16(__p0) - vmovl_high_u16(__p1);
  return __ret;
}
#else
__ai uint32x4_t vsubl_high_u16(uint16x8_t __p0, uint16x8_t __p1) {
  uint16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint32x4_t __ret;
  __ret = __noswap_vmovl_high_u16(__rev0) - __noswap_vmovl_high_u16(__rev1);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x8_t vsubl_high_s8(int8x16_t __p0, int8x16_t __p1) {
  int16x8_t __ret;
  __ret = vmovl_high_s8(__p0) - vmovl_high_s8(__p1);
  return __ret;
}
#else
__ai int16x8_t vsubl_high_s8(int8x16_t __p0, int8x16_t __p1) {
  int8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __ret;
  __ret = __noswap_vmovl_high_s8(__rev0) - __noswap_vmovl_high_s8(__rev1);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x2_t vsubl_high_s32(int32x4_t __p0, int32x4_t __p1) {
  int64x2_t __ret;
  __ret = vmovl_high_s32(__p0) - vmovl_high_s32(__p1);
  return __ret;
}
#else
__ai int64x2_t vsubl_high_s32(int32x4_t __p0, int32x4_t __p1) {
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int64x2_t __ret;
  __ret = __noswap_vmovl_high_s32(__rev0) - __noswap_vmovl_high_s32(__rev1);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vsubl_high_s16(int16x8_t __p0, int16x8_t __p1) {
  int32x4_t __ret;
  __ret = vmovl_high_s16(__p0) - vmovl_high_s16(__p1);
  return __ret;
}
#else
__ai int32x4_t vsubl_high_s16(int16x8_t __p0, int16x8_t __p1) {
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int32x4_t __ret;
  __ret = __noswap_vmovl_high_s16(__rev0) - __noswap_vmovl_high_s16(__rev1);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x8_t vsubw_high_u8(uint16x8_t __p0, uint8x16_t __p1) {
  uint16x8_t __ret;
  __ret = __p0 - vmovl_high_u8(__p1);
  return __ret;
}
#else
__ai uint16x8_t vsubw_high_u8(uint16x8_t __p0, uint8x16_t __p1) {
  uint16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __ret;
  __ret = __rev0 - __noswap_vmovl_high_u8(__rev1);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vsubw_high_u32(uint64x2_t __p0, uint32x4_t __p1) {
  uint64x2_t __ret;
  __ret = __p0 - vmovl_high_u32(__p1);
  return __ret;
}
#else
__ai uint64x2_t vsubw_high_u32(uint64x2_t __p0, uint32x4_t __p1) {
  uint64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint64x2_t __ret;
  __ret = __rev0 - __noswap_vmovl_high_u32(__rev1);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vsubw_high_u16(uint32x4_t __p0, uint16x8_t __p1) {
  uint32x4_t __ret;
  __ret = __p0 - vmovl_high_u16(__p1);
  return __ret;
}
#else
__ai uint32x4_t vsubw_high_u16(uint32x4_t __p0, uint16x8_t __p1) {
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint32x4_t __ret;
  __ret = __rev0 - __noswap_vmovl_high_u16(__rev1);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x8_t vsubw_high_s8(int16x8_t __p0, int8x16_t __p1) {
  int16x8_t __ret;
  __ret = __p0 - vmovl_high_s8(__p1);
  return __ret;
}
#else
__ai int16x8_t vsubw_high_s8(int16x8_t __p0, int8x16_t __p1) {
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __ret;
  __ret = __rev0 - __noswap_vmovl_high_s8(__rev1);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x2_t vsubw_high_s32(int64x2_t __p0, int32x4_t __p1) {
  int64x2_t __ret;
  __ret = __p0 - vmovl_high_s32(__p1);
  return __ret;
}
#else
__ai int64x2_t vsubw_high_s32(int64x2_t __p0, int32x4_t __p1) {
  int64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int64x2_t __ret;
  __ret = __rev0 - __noswap_vmovl_high_s32(__rev1);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vsubw_high_s16(int32x4_t __p0, int16x8_t __p1) {
  int32x4_t __ret;
  __ret = __p0 - vmovl_high_s16(__p1);
  return __ret;
}
#else
__ai int32x4_t vsubw_high_s16(int32x4_t __p0, int16x8_t __p1) {
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int32x4_t __ret;
  __ret = __rev0 - __noswap_vmovl_high_s16(__rev1);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x8_t vtrn1_p8(poly8x8_t __p0, poly8x8_t __p1) {
  poly8x8_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 0, 8, 2, 10, 4, 12, 6, 14);
  return __ret;
}
#else
__ai poly8x8_t vtrn1_p8(poly8x8_t __p0, poly8x8_t __p1) {
  poly8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  poly8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  poly8x8_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 0, 8, 2, 10, 4, 12, 6, 14);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly16x4_t vtrn1_p16(poly16x4_t __p0, poly16x4_t __p1) {
  poly16x4_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 0, 4, 2, 6);
  return __ret;
}
#else
__ai poly16x4_t vtrn1_p16(poly16x4_t __p0, poly16x4_t __p1) {
  poly16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  poly16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  poly16x4_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 0, 4, 2, 6);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x16_t vtrn1q_p8(poly8x16_t __p0, poly8x16_t __p1) {
  poly8x16_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 0, 16, 2, 18, 4, 20, 6, 22, 8, 24, 10, 26, 12, 28, 14, 30);
  return __ret;
}
#else
__ai poly8x16_t vtrn1q_p8(poly8x16_t __p0, poly8x16_t __p1) {
  poly8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  poly8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  poly8x16_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 0, 16, 2, 18, 4, 20, 6, 22, 8, 24, 10, 26, 12, 28, 14, 30);
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly64x2_t vtrn1q_p64(poly64x2_t __p0, poly64x2_t __p1) {
  poly64x2_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 0, 2);
  return __ret;
}
#else
__ai poly64x2_t vtrn1q_p64(poly64x2_t __p0, poly64x2_t __p1) {
  poly64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  poly64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  poly64x2_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 0, 2);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly16x8_t vtrn1q_p16(poly16x8_t __p0, poly16x8_t __p1) {
  poly16x8_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 0, 8, 2, 10, 4, 12, 6, 14);
  return __ret;
}
#else
__ai poly16x8_t vtrn1q_p16(poly16x8_t __p0, poly16x8_t __p1) {
  poly16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  poly16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  poly16x8_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 0, 8, 2, 10, 4, 12, 6, 14);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x16_t vtrn1q_u8(uint8x16_t __p0, uint8x16_t __p1) {
  uint8x16_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 0, 16, 2, 18, 4, 20, 6, 22, 8, 24, 10, 26, 12, 28, 14, 30);
  return __ret;
}
#else
__ai uint8x16_t vtrn1q_u8(uint8x16_t __p0, uint8x16_t __p1) {
  uint8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 0, 16, 2, 18, 4, 20, 6, 22, 8, 24, 10, 26, 12, 28, 14, 30);
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vtrn1q_u32(uint32x4_t __p0, uint32x4_t __p1) {
  uint32x4_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 0, 4, 2, 6);
  return __ret;
}
#else
__ai uint32x4_t vtrn1q_u32(uint32x4_t __p0, uint32x4_t __p1) {
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint32x4_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 0, 4, 2, 6);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vtrn1q_u64(uint64x2_t __p0, uint64x2_t __p1) {
  uint64x2_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 0, 2);
  return __ret;
}
#else
__ai uint64x2_t vtrn1q_u64(uint64x2_t __p0, uint64x2_t __p1) {
  uint64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  uint64x2_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 0, 2);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x8_t vtrn1q_u16(uint16x8_t __p0, uint16x8_t __p1) {
  uint16x8_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 0, 8, 2, 10, 4, 12, 6, 14);
  return __ret;
}
#else
__ai uint16x8_t vtrn1q_u16(uint16x8_t __p0, uint16x8_t __p1) {
  uint16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 0, 8, 2, 10, 4, 12, 6, 14);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x16_t vtrn1q_s8(int8x16_t __p0, int8x16_t __p1) {
  int8x16_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 0, 16, 2, 18, 4, 20, 6, 22, 8, 24, 10, 26, 12, 28, 14, 30);
  return __ret;
}
#else
__ai int8x16_t vtrn1q_s8(int8x16_t __p0, int8x16_t __p1) {
  int8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 0, 16, 2, 18, 4, 20, 6, 22, 8, 24, 10, 26, 12, 28, 14, 30);
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x2_t vtrn1q_f64(float64x2_t __p0, float64x2_t __p1) {
  float64x2_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 0, 2);
  return __ret;
}
#else
__ai float64x2_t vtrn1q_f64(float64x2_t __p0, float64x2_t __p1) {
  float64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  float64x2_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 0, 2);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x4_t vtrn1q_f32(float32x4_t __p0, float32x4_t __p1) {
  float32x4_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 0, 4, 2, 6);
  return __ret;
}
#else
__ai float32x4_t vtrn1q_f32(float32x4_t __p0, float32x4_t __p1) {
  float32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  float32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  float32x4_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 0, 4, 2, 6);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vtrn1q_s32(int32x4_t __p0, int32x4_t __p1) {
  int32x4_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 0, 4, 2, 6);
  return __ret;
}
#else
__ai int32x4_t vtrn1q_s32(int32x4_t __p0, int32x4_t __p1) {
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int32x4_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 0, 4, 2, 6);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x2_t vtrn1q_s64(int64x2_t __p0, int64x2_t __p1) {
  int64x2_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 0, 2);
  return __ret;
}
#else
__ai int64x2_t vtrn1q_s64(int64x2_t __p0, int64x2_t __p1) {
  int64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  int64x2_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 0, 2);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x8_t vtrn1q_s16(int16x8_t __p0, int16x8_t __p1) {
  int16x8_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 0, 8, 2, 10, 4, 12, 6, 14);
  return __ret;
}
#else
__ai int16x8_t vtrn1q_s16(int16x8_t __p0, int16x8_t __p1) {
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 0, 8, 2, 10, 4, 12, 6, 14);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x8_t vtrn1_u8(uint8x8_t __p0, uint8x8_t __p1) {
  uint8x8_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 0, 8, 2, 10, 4, 12, 6, 14);
  return __ret;
}
#else
__ai uint8x8_t vtrn1_u8(uint8x8_t __p0, uint8x8_t __p1) {
  uint8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 0, 8, 2, 10, 4, 12, 6, 14);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x2_t vtrn1_u32(uint32x2_t __p0, uint32x2_t __p1) {
  uint32x2_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 0, 2);
  return __ret;
}
#else
__ai uint32x2_t vtrn1_u32(uint32x2_t __p0, uint32x2_t __p1) {
  uint32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  uint32x2_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 0, 2);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x4_t vtrn1_u16(uint16x4_t __p0, uint16x4_t __p1) {
  uint16x4_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 0, 4, 2, 6);
  return __ret;
}
#else
__ai uint16x4_t vtrn1_u16(uint16x4_t __p0, uint16x4_t __p1) {
  uint16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint16x4_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 0, 4, 2, 6);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x8_t vtrn1_s8(int8x8_t __p0, int8x8_t __p1) {
  int8x8_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 0, 8, 2, 10, 4, 12, 6, 14);
  return __ret;
}
#else
__ai int8x8_t vtrn1_s8(int8x8_t __p0, int8x8_t __p1) {
  int8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 0, 8, 2, 10, 4, 12, 6, 14);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x2_t vtrn1_f32(float32x2_t __p0, float32x2_t __p1) {
  float32x2_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 0, 2);
  return __ret;
}
#else
__ai float32x2_t vtrn1_f32(float32x2_t __p0, float32x2_t __p1) {
  float32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  float32x2_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 0, 2);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x2_t vtrn1_s32(int32x2_t __p0, int32x2_t __p1) {
  int32x2_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 0, 2);
  return __ret;
}
#else
__ai int32x2_t vtrn1_s32(int32x2_t __p0, int32x2_t __p1) {
  int32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  int32x2_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 0, 2);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x4_t vtrn1_s16(int16x4_t __p0, int16x4_t __p1) {
  int16x4_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 0, 4, 2, 6);
  return __ret;
}
#else
__ai int16x4_t vtrn1_s16(int16x4_t __p0, int16x4_t __p1) {
  int16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int16x4_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 0, 4, 2, 6);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x8_t vtrn2_p8(poly8x8_t __p0, poly8x8_t __p1) {
  poly8x8_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 1, 9, 3, 11, 5, 13, 7, 15);
  return __ret;
}
#else
__ai poly8x8_t vtrn2_p8(poly8x8_t __p0, poly8x8_t __p1) {
  poly8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  poly8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  poly8x8_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 1, 9, 3, 11, 5, 13, 7, 15);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly16x4_t vtrn2_p16(poly16x4_t __p0, poly16x4_t __p1) {
  poly16x4_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 1, 5, 3, 7);
  return __ret;
}
#else
__ai poly16x4_t vtrn2_p16(poly16x4_t __p0, poly16x4_t __p1) {
  poly16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  poly16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  poly16x4_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 1, 5, 3, 7);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x16_t vtrn2q_p8(poly8x16_t __p0, poly8x16_t __p1) {
  poly8x16_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 1, 17, 3, 19, 5, 21, 7, 23, 9, 25, 11, 27, 13, 29, 15, 31);
  return __ret;
}
#else
__ai poly8x16_t vtrn2q_p8(poly8x16_t __p0, poly8x16_t __p1) {
  poly8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  poly8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  poly8x16_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 1, 17, 3, 19, 5, 21, 7, 23, 9, 25, 11, 27, 13, 29, 15, 31);
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly64x2_t vtrn2q_p64(poly64x2_t __p0, poly64x2_t __p1) {
  poly64x2_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 1, 3);
  return __ret;
}
#else
__ai poly64x2_t vtrn2q_p64(poly64x2_t __p0, poly64x2_t __p1) {
  poly64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  poly64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  poly64x2_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 1, 3);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly16x8_t vtrn2q_p16(poly16x8_t __p0, poly16x8_t __p1) {
  poly16x8_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 1, 9, 3, 11, 5, 13, 7, 15);
  return __ret;
}
#else
__ai poly16x8_t vtrn2q_p16(poly16x8_t __p0, poly16x8_t __p1) {
  poly16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  poly16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  poly16x8_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 1, 9, 3, 11, 5, 13, 7, 15);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x16_t vtrn2q_u8(uint8x16_t __p0, uint8x16_t __p1) {
  uint8x16_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 1, 17, 3, 19, 5, 21, 7, 23, 9, 25, 11, 27, 13, 29, 15, 31);
  return __ret;
}
#else
__ai uint8x16_t vtrn2q_u8(uint8x16_t __p0, uint8x16_t __p1) {
  uint8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 1, 17, 3, 19, 5, 21, 7, 23, 9, 25, 11, 27, 13, 29, 15, 31);
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vtrn2q_u32(uint32x4_t __p0, uint32x4_t __p1) {
  uint32x4_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 1, 5, 3, 7);
  return __ret;
}
#else
__ai uint32x4_t vtrn2q_u32(uint32x4_t __p0, uint32x4_t __p1) {
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint32x4_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 1, 5, 3, 7);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vtrn2q_u64(uint64x2_t __p0, uint64x2_t __p1) {
  uint64x2_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 1, 3);
  return __ret;
}
#else
__ai uint64x2_t vtrn2q_u64(uint64x2_t __p0, uint64x2_t __p1) {
  uint64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  uint64x2_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 1, 3);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x8_t vtrn2q_u16(uint16x8_t __p0, uint16x8_t __p1) {
  uint16x8_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 1, 9, 3, 11, 5, 13, 7, 15);
  return __ret;
}
#else
__ai uint16x8_t vtrn2q_u16(uint16x8_t __p0, uint16x8_t __p1) {
  uint16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 1, 9, 3, 11, 5, 13, 7, 15);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x16_t vtrn2q_s8(int8x16_t __p0, int8x16_t __p1) {
  int8x16_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 1, 17, 3, 19, 5, 21, 7, 23, 9, 25, 11, 27, 13, 29, 15, 31);
  return __ret;
}
#else
__ai int8x16_t vtrn2q_s8(int8x16_t __p0, int8x16_t __p1) {
  int8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 1, 17, 3, 19, 5, 21, 7, 23, 9, 25, 11, 27, 13, 29, 15, 31);
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x2_t vtrn2q_f64(float64x2_t __p0, float64x2_t __p1) {
  float64x2_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 1, 3);
  return __ret;
}
#else
__ai float64x2_t vtrn2q_f64(float64x2_t __p0, float64x2_t __p1) {
  float64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  float64x2_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 1, 3);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x4_t vtrn2q_f32(float32x4_t __p0, float32x4_t __p1) {
  float32x4_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 1, 5, 3, 7);
  return __ret;
}
#else
__ai float32x4_t vtrn2q_f32(float32x4_t __p0, float32x4_t __p1) {
  float32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  float32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  float32x4_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 1, 5, 3, 7);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vtrn2q_s32(int32x4_t __p0, int32x4_t __p1) {
  int32x4_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 1, 5, 3, 7);
  return __ret;
}
#else
__ai int32x4_t vtrn2q_s32(int32x4_t __p0, int32x4_t __p1) {
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int32x4_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 1, 5, 3, 7);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x2_t vtrn2q_s64(int64x2_t __p0, int64x2_t __p1) {
  int64x2_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 1, 3);
  return __ret;
}
#else
__ai int64x2_t vtrn2q_s64(int64x2_t __p0, int64x2_t __p1) {
  int64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  int64x2_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 1, 3);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x8_t vtrn2q_s16(int16x8_t __p0, int16x8_t __p1) {
  int16x8_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 1, 9, 3, 11, 5, 13, 7, 15);
  return __ret;
}
#else
__ai int16x8_t vtrn2q_s16(int16x8_t __p0, int16x8_t __p1) {
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 1, 9, 3, 11, 5, 13, 7, 15);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x8_t vtrn2_u8(uint8x8_t __p0, uint8x8_t __p1) {
  uint8x8_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 1, 9, 3, 11, 5, 13, 7, 15);
  return __ret;
}
#else
__ai uint8x8_t vtrn2_u8(uint8x8_t __p0, uint8x8_t __p1) {
  uint8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 1, 9, 3, 11, 5, 13, 7, 15);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x2_t vtrn2_u32(uint32x2_t __p0, uint32x2_t __p1) {
  uint32x2_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 1, 3);
  return __ret;
}
#else
__ai uint32x2_t vtrn2_u32(uint32x2_t __p0, uint32x2_t __p1) {
  uint32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  uint32x2_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 1, 3);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x4_t vtrn2_u16(uint16x4_t __p0, uint16x4_t __p1) {
  uint16x4_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 1, 5, 3, 7);
  return __ret;
}
#else
__ai uint16x4_t vtrn2_u16(uint16x4_t __p0, uint16x4_t __p1) {
  uint16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint16x4_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 1, 5, 3, 7);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x8_t vtrn2_s8(int8x8_t __p0, int8x8_t __p1) {
  int8x8_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 1, 9, 3, 11, 5, 13, 7, 15);
  return __ret;
}
#else
__ai int8x8_t vtrn2_s8(int8x8_t __p0, int8x8_t __p1) {
  int8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 1, 9, 3, 11, 5, 13, 7, 15);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x2_t vtrn2_f32(float32x2_t __p0, float32x2_t __p1) {
  float32x2_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 1, 3);
  return __ret;
}
#else
__ai float32x2_t vtrn2_f32(float32x2_t __p0, float32x2_t __p1) {
  float32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  float32x2_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 1, 3);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x2_t vtrn2_s32(int32x2_t __p0, int32x2_t __p1) {
  int32x2_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 1, 3);
  return __ret;
}
#else
__ai int32x2_t vtrn2_s32(int32x2_t __p0, int32x2_t __p1) {
  int32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  int32x2_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 1, 3);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x4_t vtrn2_s16(int16x4_t __p0, int16x4_t __p1) {
  int16x4_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 1, 5, 3, 7);
  return __ret;
}
#else
__ai int16x4_t vtrn2_s16(int16x4_t __p0, int16x4_t __p1) {
  int16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int16x4_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 1, 5, 3, 7);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x1_t vtst_p64(poly64x1_t __p0, poly64x1_t __p1) {
  uint64x1_t __ret;
  __ret = (uint64x1_t) __builtin_neon_vtst_v((int8x8_t)__p0, (int8x8_t)__p1, 19);
  return __ret;
}
#else
__ai uint64x1_t vtst_p64(poly64x1_t __p0, poly64x1_t __p1) {
  uint64x1_t __ret;
  __ret = (uint64x1_t) __builtin_neon_vtst_v((int8x8_t)__p0, (int8x8_t)__p1, 19);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vtstq_p64(poly64x2_t __p0, poly64x2_t __p1) {
  uint64x2_t __ret;
  __ret = (uint64x2_t) __builtin_neon_vtstq_v((int8x16_t)__p0, (int8x16_t)__p1, 51);
  return __ret;
}
#else
__ai uint64x2_t vtstq_p64(poly64x2_t __p0, poly64x2_t __p1) {
  poly64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  poly64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  uint64x2_t __ret;
  __ret = (uint64x2_t) __builtin_neon_vtstq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 51);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vtstq_u64(uint64x2_t __p0, uint64x2_t __p1) {
  uint64x2_t __ret;
  __ret = (uint64x2_t) __builtin_neon_vtstq_v((int8x16_t)__p0, (int8x16_t)__p1, 51);
  return __ret;
}
#else
__ai uint64x2_t vtstq_u64(uint64x2_t __p0, uint64x2_t __p1) {
  uint64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  uint64x2_t __ret;
  __ret = (uint64x2_t) __builtin_neon_vtstq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 51);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vtstq_s64(int64x2_t __p0, int64x2_t __p1) {
  uint64x2_t __ret;
  __ret = (uint64x2_t) __builtin_neon_vtstq_v((int8x16_t)__p0, (int8x16_t)__p1, 51);
  return __ret;
}
#else
__ai uint64x2_t vtstq_s64(int64x2_t __p0, int64x2_t __p1) {
  int64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  uint64x2_t __ret;
  __ret = (uint64x2_t) __builtin_neon_vtstq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 51);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x1_t vtst_u64(uint64x1_t __p0, uint64x1_t __p1) {
  uint64x1_t __ret;
  __ret = (uint64x1_t) __builtin_neon_vtst_v((int8x8_t)__p0, (int8x8_t)__p1, 19);
  return __ret;
}
#else
__ai uint64x1_t vtst_u64(uint64x1_t __p0, uint64x1_t __p1) {
  uint64x1_t __ret;
  __ret = (uint64x1_t) __builtin_neon_vtst_v((int8x8_t)__p0, (int8x8_t)__p1, 19);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x1_t vtst_s64(int64x1_t __p0, int64x1_t __p1) {
  uint64x1_t __ret;
  __ret = (uint64x1_t) __builtin_neon_vtst_v((int8x8_t)__p0, (int8x8_t)__p1, 19);
  return __ret;
}
#else
__ai uint64x1_t vtst_s64(int64x1_t __p0, int64x1_t __p1) {
  uint64x1_t __ret;
  __ret = (uint64x1_t) __builtin_neon_vtst_v((int8x8_t)__p0, (int8x8_t)__p1, 19);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64_t vtstd_u64(uint64_t __p0, uint64_t __p1) {
  uint64_t __ret;
  __ret = (uint64_t) __builtin_neon_vtstd_u64(__p0, __p1);
  return __ret;
}
#else
__ai uint64_t vtstd_u64(uint64_t __p0, uint64_t __p1) {
  uint64_t __ret;
  __ret = (uint64_t) __builtin_neon_vtstd_u64(__p0, __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64_t vtstd_s64(int64_t __p0, int64_t __p1) {
  int64_t __ret;
  __ret = (int64_t) __builtin_neon_vtstd_s64(__p0, __p1);
  return __ret;
}
#else
__ai int64_t vtstd_s64(int64_t __p0, int64_t __p1) {
  int64_t __ret;
  __ret = (int64_t) __builtin_neon_vtstd_s64(__p0, __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8_t vuqaddb_s8(int8_t __p0, int8_t __p1) {
  int8_t __ret;
  __ret = (int8_t) __builtin_neon_vuqaddb_s8(__p0, __p1);
  return __ret;
}
#else
__ai int8_t vuqaddb_s8(int8_t __p0, int8_t __p1) {
  int8_t __ret;
  __ret = (int8_t) __builtin_neon_vuqaddb_s8(__p0, __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32_t vuqadds_s32(int32_t __p0, int32_t __p1) {
  int32_t __ret;
  __ret = (int32_t) __builtin_neon_vuqadds_s32(__p0, __p1);
  return __ret;
}
#else
__ai int32_t vuqadds_s32(int32_t __p0, int32_t __p1) {
  int32_t __ret;
  __ret = (int32_t) __builtin_neon_vuqadds_s32(__p0, __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64_t vuqaddd_s64(int64_t __p0, int64_t __p1) {
  int64_t __ret;
  __ret = (int64_t) __builtin_neon_vuqaddd_s64(__p0, __p1);
  return __ret;
}
#else
__ai int64_t vuqaddd_s64(int64_t __p0, int64_t __p1) {
  int64_t __ret;
  __ret = (int64_t) __builtin_neon_vuqaddd_s64(__p0, __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16_t vuqaddh_s16(int16_t __p0, int16_t __p1) {
  int16_t __ret;
  __ret = (int16_t) __builtin_neon_vuqaddh_s16(__p0, __p1);
  return __ret;
}
#else
__ai int16_t vuqaddh_s16(int16_t __p0, int16_t __p1) {
  int16_t __ret;
  __ret = (int16_t) __builtin_neon_vuqaddh_s16(__p0, __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x16_t vuqaddq_s8(int8x16_t __p0, int8x16_t __p1) {
  int8x16_t __ret;
  __ret = (int8x16_t) __builtin_neon_vuqaddq_v((int8x16_t)__p0, (int8x16_t)__p1, 32);
  return __ret;
}
#else
__ai int8x16_t vuqaddq_s8(int8x16_t __p0, int8x16_t __p1) {
  int8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __ret;
  __ret = (int8x16_t) __builtin_neon_vuqaddq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 32);
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vuqaddq_s32(int32x4_t __p0, int32x4_t __p1) {
  int32x4_t __ret;
  __ret = (int32x4_t) __builtin_neon_vuqaddq_v((int8x16_t)__p0, (int8x16_t)__p1, 34);
  return __ret;
}
#else
__ai int32x4_t vuqaddq_s32(int32x4_t __p0, int32x4_t __p1) {
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int32x4_t __ret;
  __ret = (int32x4_t) __builtin_neon_vuqaddq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 34);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x2_t vuqaddq_s64(int64x2_t __p0, int64x2_t __p1) {
  int64x2_t __ret;
  __ret = (int64x2_t) __builtin_neon_vuqaddq_v((int8x16_t)__p0, (int8x16_t)__p1, 35);
  return __ret;
}
#else
__ai int64x2_t vuqaddq_s64(int64x2_t __p0, int64x2_t __p1) {
  int64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  int64x2_t __ret;
  __ret = (int64x2_t) __builtin_neon_vuqaddq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 35);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x8_t vuqaddq_s16(int16x8_t __p0, int16x8_t __p1) {
  int16x8_t __ret;
  __ret = (int16x8_t) __builtin_neon_vuqaddq_v((int8x16_t)__p0, (int8x16_t)__p1, 33);
  return __ret;
}
#else
__ai int16x8_t vuqaddq_s16(int16x8_t __p0, int16x8_t __p1) {
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __ret;
  __ret = (int16x8_t) __builtin_neon_vuqaddq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 33);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x8_t vuqadd_s8(int8x8_t __p0, int8x8_t __p1) {
  int8x8_t __ret;
  __ret = (int8x8_t) __builtin_neon_vuqadd_v((int8x8_t)__p0, (int8x8_t)__p1, 0);
  return __ret;
}
#else
__ai int8x8_t vuqadd_s8(int8x8_t __p0, int8x8_t __p1) {
  int8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __ret;
  __ret = (int8x8_t) __builtin_neon_vuqadd_v((int8x8_t)__rev0, (int8x8_t)__rev1, 0);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x2_t vuqadd_s32(int32x2_t __p0, int32x2_t __p1) {
  int32x2_t __ret;
  __ret = (int32x2_t) __builtin_neon_vuqadd_v((int8x8_t)__p0, (int8x8_t)__p1, 2);
  return __ret;
}
#else
__ai int32x2_t vuqadd_s32(int32x2_t __p0, int32x2_t __p1) {
  int32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  int32x2_t __ret;
  __ret = (int32x2_t) __builtin_neon_vuqadd_v((int8x8_t)__rev0, (int8x8_t)__rev1, 2);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x1_t vuqadd_s64(int64x1_t __p0, int64x1_t __p1) {
  int64x1_t __ret;
  __ret = (int64x1_t) __builtin_neon_vuqadd_v((int8x8_t)__p0, (int8x8_t)__p1, 3);
  return __ret;
}
#else
__ai int64x1_t vuqadd_s64(int64x1_t __p0, int64x1_t __p1) {
  int64x1_t __ret;
  __ret = (int64x1_t) __builtin_neon_vuqadd_v((int8x8_t)__p0, (int8x8_t)__p1, 3);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x4_t vuqadd_s16(int16x4_t __p0, int16x4_t __p1) {
  int16x4_t __ret;
  __ret = (int16x4_t) __builtin_neon_vuqadd_v((int8x8_t)__p0, (int8x8_t)__p1, 1);
  return __ret;
}
#else
__ai int16x4_t vuqadd_s16(int16x4_t __p0, int16x4_t __p1) {
  int16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int16x4_t __ret;
  __ret = (int16x4_t) __builtin_neon_vuqadd_v((int8x8_t)__rev0, (int8x8_t)__rev1, 1);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x8_t vuzp1_p8(poly8x8_t __p0, poly8x8_t __p1) {
  poly8x8_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 0, 2, 4, 6, 8, 10, 12, 14);
  return __ret;
}
#else
__ai poly8x8_t vuzp1_p8(poly8x8_t __p0, poly8x8_t __p1) {
  poly8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  poly8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  poly8x8_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 0, 2, 4, 6, 8, 10, 12, 14);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly16x4_t vuzp1_p16(poly16x4_t __p0, poly16x4_t __p1) {
  poly16x4_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 0, 2, 4, 6);
  return __ret;
}
#else
__ai poly16x4_t vuzp1_p16(poly16x4_t __p0, poly16x4_t __p1) {
  poly16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  poly16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  poly16x4_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 0, 2, 4, 6);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x16_t vuzp1q_p8(poly8x16_t __p0, poly8x16_t __p1) {
  poly8x16_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30);
  return __ret;
}
#else
__ai poly8x16_t vuzp1q_p8(poly8x16_t __p0, poly8x16_t __p1) {
  poly8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  poly8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  poly8x16_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30);
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly64x2_t vuzp1q_p64(poly64x2_t __p0, poly64x2_t __p1) {
  poly64x2_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 0, 2);
  return __ret;
}
#else
__ai poly64x2_t vuzp1q_p64(poly64x2_t __p0, poly64x2_t __p1) {
  poly64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  poly64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  poly64x2_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 0, 2);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly16x8_t vuzp1q_p16(poly16x8_t __p0, poly16x8_t __p1) {
  poly16x8_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 0, 2, 4, 6, 8, 10, 12, 14);
  return __ret;
}
#else
__ai poly16x8_t vuzp1q_p16(poly16x8_t __p0, poly16x8_t __p1) {
  poly16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  poly16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  poly16x8_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 0, 2, 4, 6, 8, 10, 12, 14);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x16_t vuzp1q_u8(uint8x16_t __p0, uint8x16_t __p1) {
  uint8x16_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30);
  return __ret;
}
#else
__ai uint8x16_t vuzp1q_u8(uint8x16_t __p0, uint8x16_t __p1) {
  uint8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30);
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vuzp1q_u32(uint32x4_t __p0, uint32x4_t __p1) {
  uint32x4_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 0, 2, 4, 6);
  return __ret;
}
#else
__ai uint32x4_t vuzp1q_u32(uint32x4_t __p0, uint32x4_t __p1) {
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint32x4_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 0, 2, 4, 6);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vuzp1q_u64(uint64x2_t __p0, uint64x2_t __p1) {
  uint64x2_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 0, 2);
  return __ret;
}
#else
__ai uint64x2_t vuzp1q_u64(uint64x2_t __p0, uint64x2_t __p1) {
  uint64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  uint64x2_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 0, 2);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x8_t vuzp1q_u16(uint16x8_t __p0, uint16x8_t __p1) {
  uint16x8_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 0, 2, 4, 6, 8, 10, 12, 14);
  return __ret;
}
#else
__ai uint16x8_t vuzp1q_u16(uint16x8_t __p0, uint16x8_t __p1) {
  uint16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 0, 2, 4, 6, 8, 10, 12, 14);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x16_t vuzp1q_s8(int8x16_t __p0, int8x16_t __p1) {
  int8x16_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30);
  return __ret;
}
#else
__ai int8x16_t vuzp1q_s8(int8x16_t __p0, int8x16_t __p1) {
  int8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30);
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x2_t vuzp1q_f64(float64x2_t __p0, float64x2_t __p1) {
  float64x2_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 0, 2);
  return __ret;
}
#else
__ai float64x2_t vuzp1q_f64(float64x2_t __p0, float64x2_t __p1) {
  float64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  float64x2_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 0, 2);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x4_t vuzp1q_f32(float32x4_t __p0, float32x4_t __p1) {
  float32x4_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 0, 2, 4, 6);
  return __ret;
}
#else
__ai float32x4_t vuzp1q_f32(float32x4_t __p0, float32x4_t __p1) {
  float32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  float32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  float32x4_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 0, 2, 4, 6);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vuzp1q_s32(int32x4_t __p0, int32x4_t __p1) {
  int32x4_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 0, 2, 4, 6);
  return __ret;
}
#else
__ai int32x4_t vuzp1q_s32(int32x4_t __p0, int32x4_t __p1) {
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int32x4_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 0, 2, 4, 6);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x2_t vuzp1q_s64(int64x2_t __p0, int64x2_t __p1) {
  int64x2_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 0, 2);
  return __ret;
}
#else
__ai int64x2_t vuzp1q_s64(int64x2_t __p0, int64x2_t __p1) {
  int64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  int64x2_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 0, 2);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x8_t vuzp1q_s16(int16x8_t __p0, int16x8_t __p1) {
  int16x8_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 0, 2, 4, 6, 8, 10, 12, 14);
  return __ret;
}
#else
__ai int16x8_t vuzp1q_s16(int16x8_t __p0, int16x8_t __p1) {
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 0, 2, 4, 6, 8, 10, 12, 14);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x8_t vuzp1_u8(uint8x8_t __p0, uint8x8_t __p1) {
  uint8x8_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 0, 2, 4, 6, 8, 10, 12, 14);
  return __ret;
}
#else
__ai uint8x8_t vuzp1_u8(uint8x8_t __p0, uint8x8_t __p1) {
  uint8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 0, 2, 4, 6, 8, 10, 12, 14);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x2_t vuzp1_u32(uint32x2_t __p0, uint32x2_t __p1) {
  uint32x2_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 0, 2);
  return __ret;
}
#else
__ai uint32x2_t vuzp1_u32(uint32x2_t __p0, uint32x2_t __p1) {
  uint32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  uint32x2_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 0, 2);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x4_t vuzp1_u16(uint16x4_t __p0, uint16x4_t __p1) {
  uint16x4_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 0, 2, 4, 6);
  return __ret;
}
#else
__ai uint16x4_t vuzp1_u16(uint16x4_t __p0, uint16x4_t __p1) {
  uint16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint16x4_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 0, 2, 4, 6);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x8_t vuzp1_s8(int8x8_t __p0, int8x8_t __p1) {
  int8x8_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 0, 2, 4, 6, 8, 10, 12, 14);
  return __ret;
}
#else
__ai int8x8_t vuzp1_s8(int8x8_t __p0, int8x8_t __p1) {
  int8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 0, 2, 4, 6, 8, 10, 12, 14);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x2_t vuzp1_f32(float32x2_t __p0, float32x2_t __p1) {
  float32x2_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 0, 2);
  return __ret;
}
#else
__ai float32x2_t vuzp1_f32(float32x2_t __p0, float32x2_t __p1) {
  float32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  float32x2_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 0, 2);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x2_t vuzp1_s32(int32x2_t __p0, int32x2_t __p1) {
  int32x2_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 0, 2);
  return __ret;
}
#else
__ai int32x2_t vuzp1_s32(int32x2_t __p0, int32x2_t __p1) {
  int32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  int32x2_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 0, 2);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x4_t vuzp1_s16(int16x4_t __p0, int16x4_t __p1) {
  int16x4_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 0, 2, 4, 6);
  return __ret;
}
#else
__ai int16x4_t vuzp1_s16(int16x4_t __p0, int16x4_t __p1) {
  int16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int16x4_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 0, 2, 4, 6);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x8_t vuzp2_p8(poly8x8_t __p0, poly8x8_t __p1) {
  poly8x8_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 1, 3, 5, 7, 9, 11, 13, 15);
  return __ret;
}
#else
__ai poly8x8_t vuzp2_p8(poly8x8_t __p0, poly8x8_t __p1) {
  poly8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  poly8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  poly8x8_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 1, 3, 5, 7, 9, 11, 13, 15);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly16x4_t vuzp2_p16(poly16x4_t __p0, poly16x4_t __p1) {
  poly16x4_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 1, 3, 5, 7);
  return __ret;
}
#else
__ai poly16x4_t vuzp2_p16(poly16x4_t __p0, poly16x4_t __p1) {
  poly16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  poly16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  poly16x4_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 1, 3, 5, 7);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x16_t vuzp2q_p8(poly8x16_t __p0, poly8x16_t __p1) {
  poly8x16_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31);
  return __ret;
}
#else
__ai poly8x16_t vuzp2q_p8(poly8x16_t __p0, poly8x16_t __p1) {
  poly8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  poly8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  poly8x16_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31);
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly64x2_t vuzp2q_p64(poly64x2_t __p0, poly64x2_t __p1) {
  poly64x2_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 1, 3);
  return __ret;
}
#else
__ai poly64x2_t vuzp2q_p64(poly64x2_t __p0, poly64x2_t __p1) {
  poly64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  poly64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  poly64x2_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 1, 3);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly16x8_t vuzp2q_p16(poly16x8_t __p0, poly16x8_t __p1) {
  poly16x8_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 1, 3, 5, 7, 9, 11, 13, 15);
  return __ret;
}
#else
__ai poly16x8_t vuzp2q_p16(poly16x8_t __p0, poly16x8_t __p1) {
  poly16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  poly16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  poly16x8_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 1, 3, 5, 7, 9, 11, 13, 15);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x16_t vuzp2q_u8(uint8x16_t __p0, uint8x16_t __p1) {
  uint8x16_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31);
  return __ret;
}
#else
__ai uint8x16_t vuzp2q_u8(uint8x16_t __p0, uint8x16_t __p1) {
  uint8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31);
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vuzp2q_u32(uint32x4_t __p0, uint32x4_t __p1) {
  uint32x4_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 1, 3, 5, 7);
  return __ret;
}
#else
__ai uint32x4_t vuzp2q_u32(uint32x4_t __p0, uint32x4_t __p1) {
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint32x4_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 1, 3, 5, 7);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vuzp2q_u64(uint64x2_t __p0, uint64x2_t __p1) {
  uint64x2_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 1, 3);
  return __ret;
}
#else
__ai uint64x2_t vuzp2q_u64(uint64x2_t __p0, uint64x2_t __p1) {
  uint64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  uint64x2_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 1, 3);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x8_t vuzp2q_u16(uint16x8_t __p0, uint16x8_t __p1) {
  uint16x8_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 1, 3, 5, 7, 9, 11, 13, 15);
  return __ret;
}
#else
__ai uint16x8_t vuzp2q_u16(uint16x8_t __p0, uint16x8_t __p1) {
  uint16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 1, 3, 5, 7, 9, 11, 13, 15);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x16_t vuzp2q_s8(int8x16_t __p0, int8x16_t __p1) {
  int8x16_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31);
  return __ret;
}
#else
__ai int8x16_t vuzp2q_s8(int8x16_t __p0, int8x16_t __p1) {
  int8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31);
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x2_t vuzp2q_f64(float64x2_t __p0, float64x2_t __p1) {
  float64x2_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 1, 3);
  return __ret;
}
#else
__ai float64x2_t vuzp2q_f64(float64x2_t __p0, float64x2_t __p1) {
  float64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  float64x2_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 1, 3);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x4_t vuzp2q_f32(float32x4_t __p0, float32x4_t __p1) {
  float32x4_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 1, 3, 5, 7);
  return __ret;
}
#else
__ai float32x4_t vuzp2q_f32(float32x4_t __p0, float32x4_t __p1) {
  float32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  float32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  float32x4_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 1, 3, 5, 7);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vuzp2q_s32(int32x4_t __p0, int32x4_t __p1) {
  int32x4_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 1, 3, 5, 7);
  return __ret;
}
#else
__ai int32x4_t vuzp2q_s32(int32x4_t __p0, int32x4_t __p1) {
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int32x4_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 1, 3, 5, 7);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x2_t vuzp2q_s64(int64x2_t __p0, int64x2_t __p1) {
  int64x2_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 1, 3);
  return __ret;
}
#else
__ai int64x2_t vuzp2q_s64(int64x2_t __p0, int64x2_t __p1) {
  int64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  int64x2_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 1, 3);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x8_t vuzp2q_s16(int16x8_t __p0, int16x8_t __p1) {
  int16x8_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 1, 3, 5, 7, 9, 11, 13, 15);
  return __ret;
}
#else
__ai int16x8_t vuzp2q_s16(int16x8_t __p0, int16x8_t __p1) {
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 1, 3, 5, 7, 9, 11, 13, 15);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x8_t vuzp2_u8(uint8x8_t __p0, uint8x8_t __p1) {
  uint8x8_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 1, 3, 5, 7, 9, 11, 13, 15);
  return __ret;
}
#else
__ai uint8x8_t vuzp2_u8(uint8x8_t __p0, uint8x8_t __p1) {
  uint8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 1, 3, 5, 7, 9, 11, 13, 15);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x2_t vuzp2_u32(uint32x2_t __p0, uint32x2_t __p1) {
  uint32x2_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 1, 3);
  return __ret;
}
#else
__ai uint32x2_t vuzp2_u32(uint32x2_t __p0, uint32x2_t __p1) {
  uint32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  uint32x2_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 1, 3);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x4_t vuzp2_u16(uint16x4_t __p0, uint16x4_t __p1) {
  uint16x4_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 1, 3, 5, 7);
  return __ret;
}
#else
__ai uint16x4_t vuzp2_u16(uint16x4_t __p0, uint16x4_t __p1) {
  uint16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint16x4_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 1, 3, 5, 7);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x8_t vuzp2_s8(int8x8_t __p0, int8x8_t __p1) {
  int8x8_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 1, 3, 5, 7, 9, 11, 13, 15);
  return __ret;
}
#else
__ai int8x8_t vuzp2_s8(int8x8_t __p0, int8x8_t __p1) {
  int8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 1, 3, 5, 7, 9, 11, 13, 15);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x2_t vuzp2_f32(float32x2_t __p0, float32x2_t __p1) {
  float32x2_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 1, 3);
  return __ret;
}
#else
__ai float32x2_t vuzp2_f32(float32x2_t __p0, float32x2_t __p1) {
  float32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  float32x2_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 1, 3);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x2_t vuzp2_s32(int32x2_t __p0, int32x2_t __p1) {
  int32x2_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 1, 3);
  return __ret;
}
#else
__ai int32x2_t vuzp2_s32(int32x2_t __p0, int32x2_t __p1) {
  int32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  int32x2_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 1, 3);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x4_t vuzp2_s16(int16x4_t __p0, int16x4_t __p1) {
  int16x4_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 1, 3, 5, 7);
  return __ret;
}
#else
__ai int16x4_t vuzp2_s16(int16x4_t __p0, int16x4_t __p1) {
  int16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int16x4_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 1, 3, 5, 7);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x8_t vzip1_p8(poly8x8_t __p0, poly8x8_t __p1) {
  poly8x8_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 0, 8, 1, 9, 2, 10, 3, 11);
  return __ret;
}
#else
__ai poly8x8_t vzip1_p8(poly8x8_t __p0, poly8x8_t __p1) {
  poly8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  poly8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  poly8x8_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 0, 8, 1, 9, 2, 10, 3, 11);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly16x4_t vzip1_p16(poly16x4_t __p0, poly16x4_t __p1) {
  poly16x4_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 0, 4, 1, 5);
  return __ret;
}
#else
__ai poly16x4_t vzip1_p16(poly16x4_t __p0, poly16x4_t __p1) {
  poly16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  poly16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  poly16x4_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 0, 4, 1, 5);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x16_t vzip1q_p8(poly8x16_t __p0, poly8x16_t __p1) {
  poly8x16_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 0, 16, 1, 17, 2, 18, 3, 19, 4, 20, 5, 21, 6, 22, 7, 23);
  return __ret;
}
#else
__ai poly8x16_t vzip1q_p8(poly8x16_t __p0, poly8x16_t __p1) {
  poly8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  poly8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  poly8x16_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 0, 16, 1, 17, 2, 18, 3, 19, 4, 20, 5, 21, 6, 22, 7, 23);
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly64x2_t vzip1q_p64(poly64x2_t __p0, poly64x2_t __p1) {
  poly64x2_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 0, 2);
  return __ret;
}
#else
__ai poly64x2_t vzip1q_p64(poly64x2_t __p0, poly64x2_t __p1) {
  poly64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  poly64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  poly64x2_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 0, 2);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly16x8_t vzip1q_p16(poly16x8_t __p0, poly16x8_t __p1) {
  poly16x8_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 0, 8, 1, 9, 2, 10, 3, 11);
  return __ret;
}
#else
__ai poly16x8_t vzip1q_p16(poly16x8_t __p0, poly16x8_t __p1) {
  poly16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  poly16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  poly16x8_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 0, 8, 1, 9, 2, 10, 3, 11);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x16_t vzip1q_u8(uint8x16_t __p0, uint8x16_t __p1) {
  uint8x16_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 0, 16, 1, 17, 2, 18, 3, 19, 4, 20, 5, 21, 6, 22, 7, 23);
  return __ret;
}
#else
__ai uint8x16_t vzip1q_u8(uint8x16_t __p0, uint8x16_t __p1) {
  uint8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 0, 16, 1, 17, 2, 18, 3, 19, 4, 20, 5, 21, 6, 22, 7, 23);
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vzip1q_u32(uint32x4_t __p0, uint32x4_t __p1) {
  uint32x4_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 0, 4, 1, 5);
  return __ret;
}
#else
__ai uint32x4_t vzip1q_u32(uint32x4_t __p0, uint32x4_t __p1) {
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint32x4_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 0, 4, 1, 5);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vzip1q_u64(uint64x2_t __p0, uint64x2_t __p1) {
  uint64x2_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 0, 2);
  return __ret;
}
#else
__ai uint64x2_t vzip1q_u64(uint64x2_t __p0, uint64x2_t __p1) {
  uint64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  uint64x2_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 0, 2);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x8_t vzip1q_u16(uint16x8_t __p0, uint16x8_t __p1) {
  uint16x8_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 0, 8, 1, 9, 2, 10, 3, 11);
  return __ret;
}
#else
__ai uint16x8_t vzip1q_u16(uint16x8_t __p0, uint16x8_t __p1) {
  uint16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 0, 8, 1, 9, 2, 10, 3, 11);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x16_t vzip1q_s8(int8x16_t __p0, int8x16_t __p1) {
  int8x16_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 0, 16, 1, 17, 2, 18, 3, 19, 4, 20, 5, 21, 6, 22, 7, 23);
  return __ret;
}
#else
__ai int8x16_t vzip1q_s8(int8x16_t __p0, int8x16_t __p1) {
  int8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 0, 16, 1, 17, 2, 18, 3, 19, 4, 20, 5, 21, 6, 22, 7, 23);
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x2_t vzip1q_f64(float64x2_t __p0, float64x2_t __p1) {
  float64x2_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 0, 2);
  return __ret;
}
#else
__ai float64x2_t vzip1q_f64(float64x2_t __p0, float64x2_t __p1) {
  float64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  float64x2_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 0, 2);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x4_t vzip1q_f32(float32x4_t __p0, float32x4_t __p1) {
  float32x4_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 0, 4, 1, 5);
  return __ret;
}
#else
__ai float32x4_t vzip1q_f32(float32x4_t __p0, float32x4_t __p1) {
  float32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  float32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  float32x4_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 0, 4, 1, 5);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vzip1q_s32(int32x4_t __p0, int32x4_t __p1) {
  int32x4_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 0, 4, 1, 5);
  return __ret;
}
#else
__ai int32x4_t vzip1q_s32(int32x4_t __p0, int32x4_t __p1) {
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int32x4_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 0, 4, 1, 5);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x2_t vzip1q_s64(int64x2_t __p0, int64x2_t __p1) {
  int64x2_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 0, 2);
  return __ret;
}
#else
__ai int64x2_t vzip1q_s64(int64x2_t __p0, int64x2_t __p1) {
  int64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  int64x2_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 0, 2);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x8_t vzip1q_s16(int16x8_t __p0, int16x8_t __p1) {
  int16x8_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 0, 8, 1, 9, 2, 10, 3, 11);
  return __ret;
}
#else
__ai int16x8_t vzip1q_s16(int16x8_t __p0, int16x8_t __p1) {
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 0, 8, 1, 9, 2, 10, 3, 11);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x8_t vzip1_u8(uint8x8_t __p0, uint8x8_t __p1) {
  uint8x8_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 0, 8, 1, 9, 2, 10, 3, 11);
  return __ret;
}
#else
__ai uint8x8_t vzip1_u8(uint8x8_t __p0, uint8x8_t __p1) {
  uint8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 0, 8, 1, 9, 2, 10, 3, 11);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x2_t vzip1_u32(uint32x2_t __p0, uint32x2_t __p1) {
  uint32x2_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 0, 2);
  return __ret;
}
#else
__ai uint32x2_t vzip1_u32(uint32x2_t __p0, uint32x2_t __p1) {
  uint32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  uint32x2_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 0, 2);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x4_t vzip1_u16(uint16x4_t __p0, uint16x4_t __p1) {
  uint16x4_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 0, 4, 1, 5);
  return __ret;
}
#else
__ai uint16x4_t vzip1_u16(uint16x4_t __p0, uint16x4_t __p1) {
  uint16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint16x4_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 0, 4, 1, 5);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x8_t vzip1_s8(int8x8_t __p0, int8x8_t __p1) {
  int8x8_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 0, 8, 1, 9, 2, 10, 3, 11);
  return __ret;
}
#else
__ai int8x8_t vzip1_s8(int8x8_t __p0, int8x8_t __p1) {
  int8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 0, 8, 1, 9, 2, 10, 3, 11);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x2_t vzip1_f32(float32x2_t __p0, float32x2_t __p1) {
  float32x2_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 0, 2);
  return __ret;
}
#else
__ai float32x2_t vzip1_f32(float32x2_t __p0, float32x2_t __p1) {
  float32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  float32x2_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 0, 2);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x2_t vzip1_s32(int32x2_t __p0, int32x2_t __p1) {
  int32x2_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 0, 2);
  return __ret;
}
#else
__ai int32x2_t vzip1_s32(int32x2_t __p0, int32x2_t __p1) {
  int32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  int32x2_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 0, 2);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x4_t vzip1_s16(int16x4_t __p0, int16x4_t __p1) {
  int16x4_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 0, 4, 1, 5);
  return __ret;
}
#else
__ai int16x4_t vzip1_s16(int16x4_t __p0, int16x4_t __p1) {
  int16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int16x4_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 0, 4, 1, 5);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x8_t vzip2_p8(poly8x8_t __p0, poly8x8_t __p1) {
  poly8x8_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 4, 12, 5, 13, 6, 14, 7, 15);
  return __ret;
}
#else
__ai poly8x8_t vzip2_p8(poly8x8_t __p0, poly8x8_t __p1) {
  poly8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  poly8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  poly8x8_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 4, 12, 5, 13, 6, 14, 7, 15);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly16x4_t vzip2_p16(poly16x4_t __p0, poly16x4_t __p1) {
  poly16x4_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 2, 6, 3, 7);
  return __ret;
}
#else
__ai poly16x4_t vzip2_p16(poly16x4_t __p0, poly16x4_t __p1) {
  poly16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  poly16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  poly16x4_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 2, 6, 3, 7);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x16_t vzip2q_p8(poly8x16_t __p0, poly8x16_t __p1) {
  poly8x16_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 8, 24, 9, 25, 10, 26, 11, 27, 12, 28, 13, 29, 14, 30, 15, 31);
  return __ret;
}
#else
__ai poly8x16_t vzip2q_p8(poly8x16_t __p0, poly8x16_t __p1) {
  poly8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  poly8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  poly8x16_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 8, 24, 9, 25, 10, 26, 11, 27, 12, 28, 13, 29, 14, 30, 15, 31);
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly64x2_t vzip2q_p64(poly64x2_t __p0, poly64x2_t __p1) {
  poly64x2_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 1, 3);
  return __ret;
}
#else
__ai poly64x2_t vzip2q_p64(poly64x2_t __p0, poly64x2_t __p1) {
  poly64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  poly64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  poly64x2_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 1, 3);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly16x8_t vzip2q_p16(poly16x8_t __p0, poly16x8_t __p1) {
  poly16x8_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 4, 12, 5, 13, 6, 14, 7, 15);
  return __ret;
}
#else
__ai poly16x8_t vzip2q_p16(poly16x8_t __p0, poly16x8_t __p1) {
  poly16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  poly16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  poly16x8_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 4, 12, 5, 13, 6, 14, 7, 15);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x16_t vzip2q_u8(uint8x16_t __p0, uint8x16_t __p1) {
  uint8x16_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 8, 24, 9, 25, 10, 26, 11, 27, 12, 28, 13, 29, 14, 30, 15, 31);
  return __ret;
}
#else
__ai uint8x16_t vzip2q_u8(uint8x16_t __p0, uint8x16_t __p1) {
  uint8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 8, 24, 9, 25, 10, 26, 11, 27, 12, 28, 13, 29, 14, 30, 15, 31);
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vzip2q_u32(uint32x4_t __p0, uint32x4_t __p1) {
  uint32x4_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 2, 6, 3, 7);
  return __ret;
}
#else
__ai uint32x4_t vzip2q_u32(uint32x4_t __p0, uint32x4_t __p1) {
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint32x4_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 2, 6, 3, 7);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vzip2q_u64(uint64x2_t __p0, uint64x2_t __p1) {
  uint64x2_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 1, 3);
  return __ret;
}
#else
__ai uint64x2_t vzip2q_u64(uint64x2_t __p0, uint64x2_t __p1) {
  uint64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  uint64x2_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 1, 3);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x8_t vzip2q_u16(uint16x8_t __p0, uint16x8_t __p1) {
  uint16x8_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 4, 12, 5, 13, 6, 14, 7, 15);
  return __ret;
}
#else
__ai uint16x8_t vzip2q_u16(uint16x8_t __p0, uint16x8_t __p1) {
  uint16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 4, 12, 5, 13, 6, 14, 7, 15);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x16_t vzip2q_s8(int8x16_t __p0, int8x16_t __p1) {
  int8x16_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 8, 24, 9, 25, 10, 26, 11, 27, 12, 28, 13, 29, 14, 30, 15, 31);
  return __ret;
}
#else
__ai int8x16_t vzip2q_s8(int8x16_t __p0, int8x16_t __p1) {
  int8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 8, 24, 9, 25, 10, 26, 11, 27, 12, 28, 13, 29, 14, 30, 15, 31);
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x2_t vzip2q_f64(float64x2_t __p0, float64x2_t __p1) {
  float64x2_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 1, 3);
  return __ret;
}
#else
__ai float64x2_t vzip2q_f64(float64x2_t __p0, float64x2_t __p1) {
  float64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  float64x2_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 1, 3);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x4_t vzip2q_f32(float32x4_t __p0, float32x4_t __p1) {
  float32x4_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 2, 6, 3, 7);
  return __ret;
}
#else
__ai float32x4_t vzip2q_f32(float32x4_t __p0, float32x4_t __p1) {
  float32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  float32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  float32x4_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 2, 6, 3, 7);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vzip2q_s32(int32x4_t __p0, int32x4_t __p1) {
  int32x4_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 2, 6, 3, 7);
  return __ret;
}
#else
__ai int32x4_t vzip2q_s32(int32x4_t __p0, int32x4_t __p1) {
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int32x4_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 2, 6, 3, 7);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x2_t vzip2q_s64(int64x2_t __p0, int64x2_t __p1) {
  int64x2_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 1, 3);
  return __ret;
}
#else
__ai int64x2_t vzip2q_s64(int64x2_t __p0, int64x2_t __p1) {
  int64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  int64x2_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 1, 3);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x8_t vzip2q_s16(int16x8_t __p0, int16x8_t __p1) {
  int16x8_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 4, 12, 5, 13, 6, 14, 7, 15);
  return __ret;
}
#else
__ai int16x8_t vzip2q_s16(int16x8_t __p0, int16x8_t __p1) {
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 4, 12, 5, 13, 6, 14, 7, 15);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x8_t vzip2_u8(uint8x8_t __p0, uint8x8_t __p1) {
  uint8x8_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 4, 12, 5, 13, 6, 14, 7, 15);
  return __ret;
}
#else
__ai uint8x8_t vzip2_u8(uint8x8_t __p0, uint8x8_t __p1) {
  uint8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 4, 12, 5, 13, 6, 14, 7, 15);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x2_t vzip2_u32(uint32x2_t __p0, uint32x2_t __p1) {
  uint32x2_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 1, 3);
  return __ret;
}
#else
__ai uint32x2_t vzip2_u32(uint32x2_t __p0, uint32x2_t __p1) {
  uint32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  uint32x2_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 1, 3);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x4_t vzip2_u16(uint16x4_t __p0, uint16x4_t __p1) {
  uint16x4_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 2, 6, 3, 7);
  return __ret;
}
#else
__ai uint16x4_t vzip2_u16(uint16x4_t __p0, uint16x4_t __p1) {
  uint16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint16x4_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 2, 6, 3, 7);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x8_t vzip2_s8(int8x8_t __p0, int8x8_t __p1) {
  int8x8_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 4, 12, 5, 13, 6, 14, 7, 15);
  return __ret;
}
#else
__ai int8x8_t vzip2_s8(int8x8_t __p0, int8x8_t __p1) {
  int8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 4, 12, 5, 13, 6, 14, 7, 15);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x2_t vzip2_f32(float32x2_t __p0, float32x2_t __p1) {
  float32x2_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 1, 3);
  return __ret;
}
#else
__ai float32x2_t vzip2_f32(float32x2_t __p0, float32x2_t __p1) {
  float32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  float32x2_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 1, 3);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x2_t vzip2_s32(int32x2_t __p0, int32x2_t __p1) {
  int32x2_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 1, 3);
  return __ret;
}
#else
__ai int32x2_t vzip2_s32(int32x2_t __p0, int32x2_t __p1) {
  int32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  int32x2_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 1, 3);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x4_t vzip2_s16(int16x4_t __p0, int16x4_t __p1) {
  int16x4_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 2, 6, 3, 7);
  return __ret;
}
#else
__ai int16x4_t vzip2_s16(int16x4_t __p0, int16x4_t __p1) {
  int16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int16x4_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev1, 2, 6, 3, 7);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#endif
#ifdef __LITTLE_ENDIAN__
__ai uint8x16_t vabaq_u8(uint8x16_t __p0, uint8x16_t __p1, uint8x16_t __p2) {
  uint8x16_t __ret;
  __ret = __p0 + vabdq_u8(__p1, __p2);
  return __ret;
}
#else
__ai uint8x16_t vabaq_u8(uint8x16_t __p0, uint8x16_t __p1, uint8x16_t __p2) {
  uint8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __ret;
  __ret = __rev0 + __noswap_vabdq_u8(__rev1, __rev2);
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vabaq_u32(uint32x4_t __p0, uint32x4_t __p1, uint32x4_t __p2) {
  uint32x4_t __ret;
  __ret = __p0 + vabdq_u32(__p1, __p2);
  return __ret;
}
#else
__ai uint32x4_t vabaq_u32(uint32x4_t __p0, uint32x4_t __p1, uint32x4_t __p2) {
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint32x4_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 3, 2, 1, 0);
  uint32x4_t __ret;
  __ret = __rev0 + __noswap_vabdq_u32(__rev1, __rev2);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x8_t vabaq_u16(uint16x8_t __p0, uint16x8_t __p1, uint16x8_t __p2) {
  uint16x8_t __ret;
  __ret = __p0 + vabdq_u16(__p1, __p2);
  return __ret;
}
#else
__ai uint16x8_t vabaq_u16(uint16x8_t __p0, uint16x8_t __p1, uint16x8_t __p2) {
  uint16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __ret;
  __ret = __rev0 + __noswap_vabdq_u16(__rev1, __rev2);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x16_t vabaq_s8(int8x16_t __p0, int8x16_t __p1, int8x16_t __p2) {
  int8x16_t __ret;
  __ret = __p0 + vabdq_s8(__p1, __p2);
  return __ret;
}
#else
__ai int8x16_t vabaq_s8(int8x16_t __p0, int8x16_t __p1, int8x16_t __p2) {
  int8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __ret;
  __ret = __rev0 + __noswap_vabdq_s8(__rev1, __rev2);
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vabaq_s32(int32x4_t __p0, int32x4_t __p1, int32x4_t __p2) {
  int32x4_t __ret;
  __ret = __p0 + vabdq_s32(__p1, __p2);
  return __ret;
}
#else
__ai int32x4_t vabaq_s32(int32x4_t __p0, int32x4_t __p1, int32x4_t __p2) {
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int32x4_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 3, 2, 1, 0);
  int32x4_t __ret;
  __ret = __rev0 + __noswap_vabdq_s32(__rev1, __rev2);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x8_t vabaq_s16(int16x8_t __p0, int16x8_t __p1, int16x8_t __p2) {
  int16x8_t __ret;
  __ret = __p0 + vabdq_s16(__p1, __p2);
  return __ret;
}
#else
__ai int16x8_t vabaq_s16(int16x8_t __p0, int16x8_t __p1, int16x8_t __p2) {
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __ret;
  __ret = __rev0 + __noswap_vabdq_s16(__rev1, __rev2);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x8_t vaba_u8(uint8x8_t __p0, uint8x8_t __p1, uint8x8_t __p2) {
  uint8x8_t __ret;
  __ret = __p0 + vabd_u8(__p1, __p2);
  return __ret;
}
#else
__ai uint8x8_t vaba_u8(uint8x8_t __p0, uint8x8_t __p1, uint8x8_t __p2) {
  uint8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __ret;
  __ret = __rev0 + __noswap_vabd_u8(__rev1, __rev2);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x2_t vaba_u32(uint32x2_t __p0, uint32x2_t __p1, uint32x2_t __p2) {
  uint32x2_t __ret;
  __ret = __p0 + vabd_u32(__p1, __p2);
  return __ret;
}
#else
__ai uint32x2_t vaba_u32(uint32x2_t __p0, uint32x2_t __p1, uint32x2_t __p2) {
  uint32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  uint32x2_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 1, 0);
  uint32x2_t __ret;
  __ret = __rev0 + __noswap_vabd_u32(__rev1, __rev2);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x4_t vaba_u16(uint16x4_t __p0, uint16x4_t __p1, uint16x4_t __p2) {
  uint16x4_t __ret;
  __ret = __p0 + vabd_u16(__p1, __p2);
  return __ret;
}
#else
__ai uint16x4_t vaba_u16(uint16x4_t __p0, uint16x4_t __p1, uint16x4_t __p2) {
  uint16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint16x4_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 3, 2, 1, 0);
  uint16x4_t __ret;
  __ret = __rev0 + __noswap_vabd_u16(__rev1, __rev2);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x8_t vaba_s8(int8x8_t __p0, int8x8_t __p1, int8x8_t __p2) {
  int8x8_t __ret;
  __ret = __p0 + vabd_s8(__p1, __p2);
  return __ret;
}
#else
__ai int8x8_t vaba_s8(int8x8_t __p0, int8x8_t __p1, int8x8_t __p2) {
  int8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __ret;
  __ret = __rev0 + __noswap_vabd_s8(__rev1, __rev2);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x2_t vaba_s32(int32x2_t __p0, int32x2_t __p1, int32x2_t __p2) {
  int32x2_t __ret;
  __ret = __p0 + vabd_s32(__p1, __p2);
  return __ret;
}
#else
__ai int32x2_t vaba_s32(int32x2_t __p0, int32x2_t __p1, int32x2_t __p2) {
  int32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  int32x2_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 1, 0);
  int32x2_t __ret;
  __ret = __rev0 + __noswap_vabd_s32(__rev1, __rev2);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x4_t vaba_s16(int16x4_t __p0, int16x4_t __p1, int16x4_t __p2) {
  int16x4_t __ret;
  __ret = __p0 + vabd_s16(__p1, __p2);
  return __ret;
}
#else
__ai int16x4_t vaba_s16(int16x4_t __p0, int16x4_t __p1, int16x4_t __p2) {
  int16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int16x4_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 3, 2, 1, 0);
  int16x4_t __ret;
  __ret = __rev0 + __noswap_vabd_s16(__rev1, __rev2);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x8_t vabdl_u8(uint8x8_t __p0, uint8x8_t __p1) {
  uint16x8_t __ret;
  __ret = (uint16x8_t)(vmovl_u8((uint8x8_t)(vabd_u8(__p0, __p1))));
  return __ret;
}
#else
__ai uint16x8_t vabdl_u8(uint8x8_t __p0, uint8x8_t __p1) {
  uint8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __ret;
  __ret = (uint16x8_t)(__noswap_vmovl_u8((uint8x8_t)(__noswap_vabd_u8(__rev0, __rev1))));
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
__ai uint16x8_t __noswap_vabdl_u8(uint8x8_t __p0, uint8x8_t __p1) {
  uint16x8_t __ret;
  __ret = (uint16x8_t)(__noswap_vmovl_u8((uint8x8_t)(__noswap_vabd_u8(__p0, __p1))));
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vabdl_u32(uint32x2_t __p0, uint32x2_t __p1) {
  uint64x2_t __ret;
  __ret = (uint64x2_t)(vmovl_u32((uint32x2_t)(vabd_u32(__p0, __p1))));
  return __ret;
}
#else
__ai uint64x2_t vabdl_u32(uint32x2_t __p0, uint32x2_t __p1) {
  uint32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  uint64x2_t __ret;
  __ret = (uint64x2_t)(__noswap_vmovl_u32((uint32x2_t)(__noswap_vabd_u32(__rev0, __rev1))));
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
__ai uint64x2_t __noswap_vabdl_u32(uint32x2_t __p0, uint32x2_t __p1) {
  uint64x2_t __ret;
  __ret = (uint64x2_t)(__noswap_vmovl_u32((uint32x2_t)(__noswap_vabd_u32(__p0, __p1))));
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vabdl_u16(uint16x4_t __p0, uint16x4_t __p1) {
  uint32x4_t __ret;
  __ret = (uint32x4_t)(vmovl_u16((uint16x4_t)(vabd_u16(__p0, __p1))));
  return __ret;
}
#else
__ai uint32x4_t vabdl_u16(uint16x4_t __p0, uint16x4_t __p1) {
  uint16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint32x4_t __ret;
  __ret = (uint32x4_t)(__noswap_vmovl_u16((uint16x4_t)(__noswap_vabd_u16(__rev0, __rev1))));
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
__ai uint32x4_t __noswap_vabdl_u16(uint16x4_t __p0, uint16x4_t __p1) {
  uint32x4_t __ret;
  __ret = (uint32x4_t)(__noswap_vmovl_u16((uint16x4_t)(__noswap_vabd_u16(__p0, __p1))));
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x8_t vabdl_s8(int8x8_t __p0, int8x8_t __p1) {
  int16x8_t __ret;
  __ret = (int16x8_t)(vmovl_u8((uint8x8_t)(vabd_s8(__p0, __p1))));
  return __ret;
}
#else
__ai int16x8_t vabdl_s8(int8x8_t __p0, int8x8_t __p1) {
  int8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __ret;
  __ret = (int16x8_t)(__noswap_vmovl_u8((uint8x8_t)(__noswap_vabd_s8(__rev0, __rev1))));
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
__ai int16x8_t __noswap_vabdl_s8(int8x8_t __p0, int8x8_t __p1) {
  int16x8_t __ret;
  __ret = (int16x8_t)(__noswap_vmovl_u8((uint8x8_t)(__noswap_vabd_s8(__p0, __p1))));
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x2_t vabdl_s32(int32x2_t __p0, int32x2_t __p1) {
  int64x2_t __ret;
  __ret = (int64x2_t)(vmovl_u32((uint32x2_t)(vabd_s32(__p0, __p1))));
  return __ret;
}
#else
__ai int64x2_t vabdl_s32(int32x2_t __p0, int32x2_t __p1) {
  int32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  int64x2_t __ret;
  __ret = (int64x2_t)(__noswap_vmovl_u32((uint32x2_t)(__noswap_vabd_s32(__rev0, __rev1))));
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
__ai int64x2_t __noswap_vabdl_s32(int32x2_t __p0, int32x2_t __p1) {
  int64x2_t __ret;
  __ret = (int64x2_t)(__noswap_vmovl_u32((uint32x2_t)(__noswap_vabd_s32(__p0, __p1))));
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vabdl_s16(int16x4_t __p0, int16x4_t __p1) {
  int32x4_t __ret;
  __ret = (int32x4_t)(vmovl_u16((uint16x4_t)(vabd_s16(__p0, __p1))));
  return __ret;
}
#else
__ai int32x4_t vabdl_s16(int16x4_t __p0, int16x4_t __p1) {
  int16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int32x4_t __ret;
  __ret = (int32x4_t)(__noswap_vmovl_u16((uint16x4_t)(__noswap_vabd_s16(__rev0, __rev1))));
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
__ai int32x4_t __noswap_vabdl_s16(int16x4_t __p0, int16x4_t __p1) {
  int32x4_t __ret;
  __ret = (int32x4_t)(__noswap_vmovl_u16((uint16x4_t)(__noswap_vabd_s16(__p0, __p1))));
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x8_t vaddl_u8(uint8x8_t __p0, uint8x8_t __p1) {
  uint16x8_t __ret;
  __ret = vmovl_u8(__p0) + vmovl_u8(__p1);
  return __ret;
}
#else
__ai uint16x8_t vaddl_u8(uint8x8_t __p0, uint8x8_t __p1) {
  uint8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __ret;
  __ret = __noswap_vmovl_u8(__rev0) + __noswap_vmovl_u8(__rev1);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vaddl_u32(uint32x2_t __p0, uint32x2_t __p1) {
  uint64x2_t __ret;
  __ret = vmovl_u32(__p0) + vmovl_u32(__p1);
  return __ret;
}
#else
__ai uint64x2_t vaddl_u32(uint32x2_t __p0, uint32x2_t __p1) {
  uint32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  uint64x2_t __ret;
  __ret = __noswap_vmovl_u32(__rev0) + __noswap_vmovl_u32(__rev1);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vaddl_u16(uint16x4_t __p0, uint16x4_t __p1) {
  uint32x4_t __ret;
  __ret = vmovl_u16(__p0) + vmovl_u16(__p1);
  return __ret;
}
#else
__ai uint32x4_t vaddl_u16(uint16x4_t __p0, uint16x4_t __p1) {
  uint16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint32x4_t __ret;
  __ret = __noswap_vmovl_u16(__rev0) + __noswap_vmovl_u16(__rev1);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x8_t vaddl_s8(int8x8_t __p0, int8x8_t __p1) {
  int16x8_t __ret;
  __ret = vmovl_s8(__p0) + vmovl_s8(__p1);
  return __ret;
}
#else
__ai int16x8_t vaddl_s8(int8x8_t __p0, int8x8_t __p1) {
  int8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __ret;
  __ret = __noswap_vmovl_s8(__rev0) + __noswap_vmovl_s8(__rev1);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x2_t vaddl_s32(int32x2_t __p0, int32x2_t __p1) {
  int64x2_t __ret;
  __ret = vmovl_s32(__p0) + vmovl_s32(__p1);
  return __ret;
}
#else
__ai int64x2_t vaddl_s32(int32x2_t __p0, int32x2_t __p1) {
  int32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  int64x2_t __ret;
  __ret = __noswap_vmovl_s32(__rev0) + __noswap_vmovl_s32(__rev1);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vaddl_s16(int16x4_t __p0, int16x4_t __p1) {
  int32x4_t __ret;
  __ret = vmovl_s16(__p0) + vmovl_s16(__p1);
  return __ret;
}
#else
__ai int32x4_t vaddl_s16(int16x4_t __p0, int16x4_t __p1) {
  int16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int32x4_t __ret;
  __ret = __noswap_vmovl_s16(__rev0) + __noswap_vmovl_s16(__rev1);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x8_t vaddw_u8(uint16x8_t __p0, uint8x8_t __p1) {
  uint16x8_t __ret;
  __ret = __p0 + vmovl_u8(__p1);
  return __ret;
}
#else
__ai uint16x8_t vaddw_u8(uint16x8_t __p0, uint8x8_t __p1) {
  uint16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __ret;
  __ret = __rev0 + __noswap_vmovl_u8(__rev1);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vaddw_u32(uint64x2_t __p0, uint32x2_t __p1) {
  uint64x2_t __ret;
  __ret = __p0 + vmovl_u32(__p1);
  return __ret;
}
#else
__ai uint64x2_t vaddw_u32(uint64x2_t __p0, uint32x2_t __p1) {
  uint64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  uint64x2_t __ret;
  __ret = __rev0 + __noswap_vmovl_u32(__rev1);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vaddw_u16(uint32x4_t __p0, uint16x4_t __p1) {
  uint32x4_t __ret;
  __ret = __p0 + vmovl_u16(__p1);
  return __ret;
}
#else
__ai uint32x4_t vaddw_u16(uint32x4_t __p0, uint16x4_t __p1) {
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint32x4_t __ret;
  __ret = __rev0 + __noswap_vmovl_u16(__rev1);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x8_t vaddw_s8(int16x8_t __p0, int8x8_t __p1) {
  int16x8_t __ret;
  __ret = __p0 + vmovl_s8(__p1);
  return __ret;
}
#else
__ai int16x8_t vaddw_s8(int16x8_t __p0, int8x8_t __p1) {
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __ret;
  __ret = __rev0 + __noswap_vmovl_s8(__rev1);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x2_t vaddw_s32(int64x2_t __p0, int32x2_t __p1) {
  int64x2_t __ret;
  __ret = __p0 + vmovl_s32(__p1);
  return __ret;
}
#else
__ai int64x2_t vaddw_s32(int64x2_t __p0, int32x2_t __p1) {
  int64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  int64x2_t __ret;
  __ret = __rev0 + __noswap_vmovl_s32(__rev1);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vaddw_s16(int32x4_t __p0, int16x4_t __p1) {
  int32x4_t __ret;
  __ret = __p0 + vmovl_s16(__p1);
  return __ret;
}
#else
__ai int32x4_t vaddw_s16(int32x4_t __p0, int16x4_t __p1) {
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int32x4_t __ret;
  __ret = __rev0 + __noswap_vmovl_s16(__rev1);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x8_t vmlal_u8(uint16x8_t __p0, uint8x8_t __p1, uint8x8_t __p2) {
  uint16x8_t __ret;
  __ret = __p0 + vmull_u8(__p1, __p2);
  return __ret;
}
#else
__ai uint16x8_t vmlal_u8(uint16x8_t __p0, uint8x8_t __p1, uint8x8_t __p2) {
  uint16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __ret;
  __ret = __rev0 + __noswap_vmull_u8(__rev1, __rev2);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
__ai uint16x8_t __noswap_vmlal_u8(uint16x8_t __p0, uint8x8_t __p1, uint8x8_t __p2) {
  uint16x8_t __ret;
  __ret = __p0 + __noswap_vmull_u8(__p1, __p2);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vmlal_u32(uint64x2_t __p0, uint32x2_t __p1, uint32x2_t __p2) {
  uint64x2_t __ret;
  __ret = __p0 + vmull_u32(__p1, __p2);
  return __ret;
}
#else
__ai uint64x2_t vmlal_u32(uint64x2_t __p0, uint32x2_t __p1, uint32x2_t __p2) {
  uint64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  uint32x2_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 1, 0);
  uint64x2_t __ret;
  __ret = __rev0 + __noswap_vmull_u32(__rev1, __rev2);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
__ai uint64x2_t __noswap_vmlal_u32(uint64x2_t __p0, uint32x2_t __p1, uint32x2_t __p2) {
  uint64x2_t __ret;
  __ret = __p0 + __noswap_vmull_u32(__p1, __p2);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vmlal_u16(uint32x4_t __p0, uint16x4_t __p1, uint16x4_t __p2) {
  uint32x4_t __ret;
  __ret = __p0 + vmull_u16(__p1, __p2);
  return __ret;
}
#else
__ai uint32x4_t vmlal_u16(uint32x4_t __p0, uint16x4_t __p1, uint16x4_t __p2) {
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint16x4_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 3, 2, 1, 0);
  uint32x4_t __ret;
  __ret = __rev0 + __noswap_vmull_u16(__rev1, __rev2);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
__ai uint32x4_t __noswap_vmlal_u16(uint32x4_t __p0, uint16x4_t __p1, uint16x4_t __p2) {
  uint32x4_t __ret;
  __ret = __p0 + __noswap_vmull_u16(__p1, __p2);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x8_t vmlal_s8(int16x8_t __p0, int8x8_t __p1, int8x8_t __p2) {
  int16x8_t __ret;
  __ret = __p0 + vmull_s8(__p1, __p2);
  return __ret;
}
#else
__ai int16x8_t vmlal_s8(int16x8_t __p0, int8x8_t __p1, int8x8_t __p2) {
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __ret;
  __ret = __rev0 + __noswap_vmull_s8(__rev1, __rev2);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
__ai int16x8_t __noswap_vmlal_s8(int16x8_t __p0, int8x8_t __p1, int8x8_t __p2) {
  int16x8_t __ret;
  __ret = __p0 + __noswap_vmull_s8(__p1, __p2);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x2_t vmlal_s32(int64x2_t __p0, int32x2_t __p1, int32x2_t __p2) {
  int64x2_t __ret;
  __ret = __p0 + vmull_s32(__p1, __p2);
  return __ret;
}
#else
__ai int64x2_t vmlal_s32(int64x2_t __p0, int32x2_t __p1, int32x2_t __p2) {
  int64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  int32x2_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 1, 0);
  int64x2_t __ret;
  __ret = __rev0 + __noswap_vmull_s32(__rev1, __rev2);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
__ai int64x2_t __noswap_vmlal_s32(int64x2_t __p0, int32x2_t __p1, int32x2_t __p2) {
  int64x2_t __ret;
  __ret = __p0 + __noswap_vmull_s32(__p1, __p2);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vmlal_s16(int32x4_t __p0, int16x4_t __p1, int16x4_t __p2) {
  int32x4_t __ret;
  __ret = __p0 + vmull_s16(__p1, __p2);
  return __ret;
}
#else
__ai int32x4_t vmlal_s16(int32x4_t __p0, int16x4_t __p1, int16x4_t __p2) {
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int16x4_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 3, 2, 1, 0);
  int32x4_t __ret;
  __ret = __rev0 + __noswap_vmull_s16(__rev1, __rev2);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
__ai int32x4_t __noswap_vmlal_s16(int32x4_t __p0, int16x4_t __p1, int16x4_t __p2) {
  int32x4_t __ret;
  __ret = __p0 + __noswap_vmull_s16(__p1, __p2);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
#define vmlal_lane_u32(__p0, __p1, __p2, __p3) __extension__ ({ \
  uint64x2_t __s0 = __p0; \
  uint32x2_t __s1 = __p1; \
  uint32x2_t __s2 = __p2; \
  uint64x2_t __ret; \
  __ret = __s0 + vmull_u32(__s1, __builtin_shufflevector(__s2, __s2, __p3, __p3)); \
  __ret; \
})
#else
#define vmlal_lane_u32(__p0, __p1, __p2, __p3) __extension__ ({ \
  uint64x2_t __s0 = __p0; \
  uint32x2_t __s1 = __p1; \
  uint32x2_t __s2 = __p2; \
  uint64x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  uint32x2_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 1, 0); \
  uint32x2_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 1, 0); \
  uint64x2_t __ret; \
  __ret = __rev0 + __noswap_vmull_u32(__rev1, __builtin_shufflevector(__rev2, __rev2, __p3, __p3)); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmlal_lane_u16(__p0, __p1, __p2, __p3) __extension__ ({ \
  uint32x4_t __s0 = __p0; \
  uint16x4_t __s1 = __p1; \
  uint16x4_t __s2 = __p2; \
  uint32x4_t __ret; \
  __ret = __s0 + vmull_u16(__s1, __builtin_shufflevector(__s2, __s2, __p3, __p3, __p3, __p3)); \
  __ret; \
})
#else
#define vmlal_lane_u16(__p0, __p1, __p2, __p3) __extension__ ({ \
  uint32x4_t __s0 = __p0; \
  uint16x4_t __s1 = __p1; \
  uint16x4_t __s2 = __p2; \
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  uint16x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  uint16x4_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 3, 2, 1, 0); \
  uint32x4_t __ret; \
  __ret = __rev0 + __noswap_vmull_u16(__rev1, __builtin_shufflevector(__rev2, __rev2, __p3, __p3, __p3, __p3)); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmlal_lane_s32(__p0, __p1, __p2, __p3) __extension__ ({ \
  int64x2_t __s0 = __p0; \
  int32x2_t __s1 = __p1; \
  int32x2_t __s2 = __p2; \
  int64x2_t __ret; \
  __ret = __s0 + vmull_s32(__s1, __builtin_shufflevector(__s2, __s2, __p3, __p3)); \
  __ret; \
})
#else
#define vmlal_lane_s32(__p0, __p1, __p2, __p3) __extension__ ({ \
  int64x2_t __s0 = __p0; \
  int32x2_t __s1 = __p1; \
  int32x2_t __s2 = __p2; \
  int64x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  int32x2_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 1, 0); \
  int32x2_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 1, 0); \
  int64x2_t __ret; \
  __ret = __rev0 + __noswap_vmull_s32(__rev1, __builtin_shufflevector(__rev2, __rev2, __p3, __p3)); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmlal_lane_s16(__p0, __p1, __p2, __p3) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  int16x4_t __s1 = __p1; \
  int16x4_t __s2 = __p2; \
  int32x4_t __ret; \
  __ret = __s0 + vmull_s16(__s1, __builtin_shufflevector(__s2, __s2, __p3, __p3, __p3, __p3)); \
  __ret; \
})
#else
#define vmlal_lane_s16(__p0, __p1, __p2, __p3) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  int16x4_t __s1 = __p1; \
  int16x4_t __s2 = __p2; \
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  int16x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  int16x4_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 3, 2, 1, 0); \
  int32x4_t __ret; \
  __ret = __rev0 + __noswap_vmull_s16(__rev1, __builtin_shufflevector(__rev2, __rev2, __p3, __p3, __p3, __p3)); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vmlal_n_u32(uint64x2_t __p0, uint32x2_t __p1, uint32_t __p2) {
  uint64x2_t __ret;
  __ret = __p0 + vmull_u32(__p1, (uint32x2_t) {__p2, __p2});
  return __ret;
}
#else
__ai uint64x2_t vmlal_n_u32(uint64x2_t __p0, uint32x2_t __p1, uint32_t __p2) {
  uint64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  uint64x2_t __ret;
  __ret = __rev0 + __noswap_vmull_u32(__rev1, (uint32x2_t) {__p2, __p2});
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
__ai uint64x2_t __noswap_vmlal_n_u32(uint64x2_t __p0, uint32x2_t __p1, uint32_t __p2) {
  uint64x2_t __ret;
  __ret = __p0 + __noswap_vmull_u32(__p1, (uint32x2_t) {__p2, __p2});
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vmlal_n_u16(uint32x4_t __p0, uint16x4_t __p1, uint16_t __p2) {
  uint32x4_t __ret;
  __ret = __p0 + vmull_u16(__p1, (uint16x4_t) {__p2, __p2, __p2, __p2});
  return __ret;
}
#else
__ai uint32x4_t vmlal_n_u16(uint32x4_t __p0, uint16x4_t __p1, uint16_t __p2) {
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint32x4_t __ret;
  __ret = __rev0 + __noswap_vmull_u16(__rev1, (uint16x4_t) {__p2, __p2, __p2, __p2});
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
__ai uint32x4_t __noswap_vmlal_n_u16(uint32x4_t __p0, uint16x4_t __p1, uint16_t __p2) {
  uint32x4_t __ret;
  __ret = __p0 + __noswap_vmull_u16(__p1, (uint16x4_t) {__p2, __p2, __p2, __p2});
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x2_t vmlal_n_s32(int64x2_t __p0, int32x2_t __p1, int32_t __p2) {
  int64x2_t __ret;
  __ret = __p0 + vmull_s32(__p1, (int32x2_t) {__p2, __p2});
  return __ret;
}
#else
__ai int64x2_t vmlal_n_s32(int64x2_t __p0, int32x2_t __p1, int32_t __p2) {
  int64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  int64x2_t __ret;
  __ret = __rev0 + __noswap_vmull_s32(__rev1, (int32x2_t) {__p2, __p2});
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
__ai int64x2_t __noswap_vmlal_n_s32(int64x2_t __p0, int32x2_t __p1, int32_t __p2) {
  int64x2_t __ret;
  __ret = __p0 + __noswap_vmull_s32(__p1, (int32x2_t) {__p2, __p2});
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vmlal_n_s16(int32x4_t __p0, int16x4_t __p1, int16_t __p2) {
  int32x4_t __ret;
  __ret = __p0 + vmull_s16(__p1, (int16x4_t) {__p2, __p2, __p2, __p2});
  return __ret;
}
#else
__ai int32x4_t vmlal_n_s16(int32x4_t __p0, int16x4_t __p1, int16_t __p2) {
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int32x4_t __ret;
  __ret = __rev0 + __noswap_vmull_s16(__rev1, (int16x4_t) {__p2, __p2, __p2, __p2});
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
__ai int32x4_t __noswap_vmlal_n_s16(int32x4_t __p0, int16x4_t __p1, int16_t __p2) {
  int32x4_t __ret;
  __ret = __p0 + __noswap_vmull_s16(__p1, (int16x4_t) {__p2, __p2, __p2, __p2});
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x8_t vmlsl_u8(uint16x8_t __p0, uint8x8_t __p1, uint8x8_t __p2) {
  uint16x8_t __ret;
  __ret = __p0 - vmull_u8(__p1, __p2);
  return __ret;
}
#else
__ai uint16x8_t vmlsl_u8(uint16x8_t __p0, uint8x8_t __p1, uint8x8_t __p2) {
  uint16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __ret;
  __ret = __rev0 - __noswap_vmull_u8(__rev1, __rev2);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
__ai uint16x8_t __noswap_vmlsl_u8(uint16x8_t __p0, uint8x8_t __p1, uint8x8_t __p2) {
  uint16x8_t __ret;
  __ret = __p0 - __noswap_vmull_u8(__p1, __p2);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vmlsl_u32(uint64x2_t __p0, uint32x2_t __p1, uint32x2_t __p2) {
  uint64x2_t __ret;
  __ret = __p0 - vmull_u32(__p1, __p2);
  return __ret;
}
#else
__ai uint64x2_t vmlsl_u32(uint64x2_t __p0, uint32x2_t __p1, uint32x2_t __p2) {
  uint64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  uint32x2_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 1, 0);
  uint64x2_t __ret;
  __ret = __rev0 - __noswap_vmull_u32(__rev1, __rev2);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
__ai uint64x2_t __noswap_vmlsl_u32(uint64x2_t __p0, uint32x2_t __p1, uint32x2_t __p2) {
  uint64x2_t __ret;
  __ret = __p0 - __noswap_vmull_u32(__p1, __p2);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vmlsl_u16(uint32x4_t __p0, uint16x4_t __p1, uint16x4_t __p2) {
  uint32x4_t __ret;
  __ret = __p0 - vmull_u16(__p1, __p2);
  return __ret;
}
#else
__ai uint32x4_t vmlsl_u16(uint32x4_t __p0, uint16x4_t __p1, uint16x4_t __p2) {
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint16x4_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 3, 2, 1, 0);
  uint32x4_t __ret;
  __ret = __rev0 - __noswap_vmull_u16(__rev1, __rev2);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
__ai uint32x4_t __noswap_vmlsl_u16(uint32x4_t __p0, uint16x4_t __p1, uint16x4_t __p2) {
  uint32x4_t __ret;
  __ret = __p0 - __noswap_vmull_u16(__p1, __p2);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x8_t vmlsl_s8(int16x8_t __p0, int8x8_t __p1, int8x8_t __p2) {
  int16x8_t __ret;
  __ret = __p0 - vmull_s8(__p1, __p2);
  return __ret;
}
#else
__ai int16x8_t vmlsl_s8(int16x8_t __p0, int8x8_t __p1, int8x8_t __p2) {
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __ret;
  __ret = __rev0 - __noswap_vmull_s8(__rev1, __rev2);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
__ai int16x8_t __noswap_vmlsl_s8(int16x8_t __p0, int8x8_t __p1, int8x8_t __p2) {
  int16x8_t __ret;
  __ret = __p0 - __noswap_vmull_s8(__p1, __p2);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x2_t vmlsl_s32(int64x2_t __p0, int32x2_t __p1, int32x2_t __p2) {
  int64x2_t __ret;
  __ret = __p0 - vmull_s32(__p1, __p2);
  return __ret;
}
#else
__ai int64x2_t vmlsl_s32(int64x2_t __p0, int32x2_t __p1, int32x2_t __p2) {
  int64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  int32x2_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 1, 0);
  int64x2_t __ret;
  __ret = __rev0 - __noswap_vmull_s32(__rev1, __rev2);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
__ai int64x2_t __noswap_vmlsl_s32(int64x2_t __p0, int32x2_t __p1, int32x2_t __p2) {
  int64x2_t __ret;
  __ret = __p0 - __noswap_vmull_s32(__p1, __p2);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vmlsl_s16(int32x4_t __p0, int16x4_t __p1, int16x4_t __p2) {
  int32x4_t __ret;
  __ret = __p0 - vmull_s16(__p1, __p2);
  return __ret;
}
#else
__ai int32x4_t vmlsl_s16(int32x4_t __p0, int16x4_t __p1, int16x4_t __p2) {
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int16x4_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 3, 2, 1, 0);
  int32x4_t __ret;
  __ret = __rev0 - __noswap_vmull_s16(__rev1, __rev2);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
__ai int32x4_t __noswap_vmlsl_s16(int32x4_t __p0, int16x4_t __p1, int16x4_t __p2) {
  int32x4_t __ret;
  __ret = __p0 - __noswap_vmull_s16(__p1, __p2);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
#define vmlsl_lane_u32(__p0, __p1, __p2, __p3) __extension__ ({ \
  uint64x2_t __s0 = __p0; \
  uint32x2_t __s1 = __p1; \
  uint32x2_t __s2 = __p2; \
  uint64x2_t __ret; \
  __ret = __s0 - vmull_u32(__s1, __builtin_shufflevector(__s2, __s2, __p3, __p3)); \
  __ret; \
})
#else
#define vmlsl_lane_u32(__p0, __p1, __p2, __p3) __extension__ ({ \
  uint64x2_t __s0 = __p0; \
  uint32x2_t __s1 = __p1; \
  uint32x2_t __s2 = __p2; \
  uint64x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  uint32x2_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 1, 0); \
  uint32x2_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 1, 0); \
  uint64x2_t __ret; \
  __ret = __rev0 - __noswap_vmull_u32(__rev1, __builtin_shufflevector(__rev2, __rev2, __p3, __p3)); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmlsl_lane_u16(__p0, __p1, __p2, __p3) __extension__ ({ \
  uint32x4_t __s0 = __p0; \
  uint16x4_t __s1 = __p1; \
  uint16x4_t __s2 = __p2; \
  uint32x4_t __ret; \
  __ret = __s0 - vmull_u16(__s1, __builtin_shufflevector(__s2, __s2, __p3, __p3, __p3, __p3)); \
  __ret; \
})
#else
#define vmlsl_lane_u16(__p0, __p1, __p2, __p3) __extension__ ({ \
  uint32x4_t __s0 = __p0; \
  uint16x4_t __s1 = __p1; \
  uint16x4_t __s2 = __p2; \
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  uint16x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  uint16x4_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 3, 2, 1, 0); \
  uint32x4_t __ret; \
  __ret = __rev0 - __noswap_vmull_u16(__rev1, __builtin_shufflevector(__rev2, __rev2, __p3, __p3, __p3, __p3)); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmlsl_lane_s32(__p0, __p1, __p2, __p3) __extension__ ({ \
  int64x2_t __s0 = __p0; \
  int32x2_t __s1 = __p1; \
  int32x2_t __s2 = __p2; \
  int64x2_t __ret; \
  __ret = __s0 - vmull_s32(__s1, __builtin_shufflevector(__s2, __s2, __p3, __p3)); \
  __ret; \
})
#else
#define vmlsl_lane_s32(__p0, __p1, __p2, __p3) __extension__ ({ \
  int64x2_t __s0 = __p0; \
  int32x2_t __s1 = __p1; \
  int32x2_t __s2 = __p2; \
  int64x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  int32x2_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 1, 0); \
  int32x2_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 1, 0); \
  int64x2_t __ret; \
  __ret = __rev0 - __noswap_vmull_s32(__rev1, __builtin_shufflevector(__rev2, __rev2, __p3, __p3)); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmlsl_lane_s16(__p0, __p1, __p2, __p3) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  int16x4_t __s1 = __p1; \
  int16x4_t __s2 = __p2; \
  int32x4_t __ret; \
  __ret = __s0 - vmull_s16(__s1, __builtin_shufflevector(__s2, __s2, __p3, __p3, __p3, __p3)); \
  __ret; \
})
#else
#define vmlsl_lane_s16(__p0, __p1, __p2, __p3) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  int16x4_t __s1 = __p1; \
  int16x4_t __s2 = __p2; \
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  int16x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  int16x4_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 3, 2, 1, 0); \
  int32x4_t __ret; \
  __ret = __rev0 - __noswap_vmull_s16(__rev1, __builtin_shufflevector(__rev2, __rev2, __p3, __p3, __p3, __p3)); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vmlsl_n_u32(uint64x2_t __p0, uint32x2_t __p1, uint32_t __p2) {
  uint64x2_t __ret;
  __ret = __p0 - vmull_u32(__p1, (uint32x2_t) {__p2, __p2});
  return __ret;
}
#else
__ai uint64x2_t vmlsl_n_u32(uint64x2_t __p0, uint32x2_t __p1, uint32_t __p2) {
  uint64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  uint64x2_t __ret;
  __ret = __rev0 - __noswap_vmull_u32(__rev1, (uint32x2_t) {__p2, __p2});
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
__ai uint64x2_t __noswap_vmlsl_n_u32(uint64x2_t __p0, uint32x2_t __p1, uint32_t __p2) {
  uint64x2_t __ret;
  __ret = __p0 - __noswap_vmull_u32(__p1, (uint32x2_t) {__p2, __p2});
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vmlsl_n_u16(uint32x4_t __p0, uint16x4_t __p1, uint16_t __p2) {
  uint32x4_t __ret;
  __ret = __p0 - vmull_u16(__p1, (uint16x4_t) {__p2, __p2, __p2, __p2});
  return __ret;
}
#else
__ai uint32x4_t vmlsl_n_u16(uint32x4_t __p0, uint16x4_t __p1, uint16_t __p2) {
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint32x4_t __ret;
  __ret = __rev0 - __noswap_vmull_u16(__rev1, (uint16x4_t) {__p2, __p2, __p2, __p2});
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
__ai uint32x4_t __noswap_vmlsl_n_u16(uint32x4_t __p0, uint16x4_t __p1, uint16_t __p2) {
  uint32x4_t __ret;
  __ret = __p0 - __noswap_vmull_u16(__p1, (uint16x4_t) {__p2, __p2, __p2, __p2});
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x2_t vmlsl_n_s32(int64x2_t __p0, int32x2_t __p1, int32_t __p2) {
  int64x2_t __ret;
  __ret = __p0 - vmull_s32(__p1, (int32x2_t) {__p2, __p2});
  return __ret;
}
#else
__ai int64x2_t vmlsl_n_s32(int64x2_t __p0, int32x2_t __p1, int32_t __p2) {
  int64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  int64x2_t __ret;
  __ret = __rev0 - __noswap_vmull_s32(__rev1, (int32x2_t) {__p2, __p2});
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
__ai int64x2_t __noswap_vmlsl_n_s32(int64x2_t __p0, int32x2_t __p1, int32_t __p2) {
  int64x2_t __ret;
  __ret = __p0 - __noswap_vmull_s32(__p1, (int32x2_t) {__p2, __p2});
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vmlsl_n_s16(int32x4_t __p0, int16x4_t __p1, int16_t __p2) {
  int32x4_t __ret;
  __ret = __p0 - vmull_s16(__p1, (int16x4_t) {__p2, __p2, __p2, __p2});
  return __ret;
}
#else
__ai int32x4_t vmlsl_n_s16(int32x4_t __p0, int16x4_t __p1, int16_t __p2) {
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int32x4_t __ret;
  __ret = __rev0 - __noswap_vmull_s16(__rev1, (int16x4_t) {__p2, __p2, __p2, __p2});
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
__ai int32x4_t __noswap_vmlsl_n_s16(int32x4_t __p0, int16x4_t __p1, int16_t __p2) {
  int32x4_t __ret;
  __ret = __p0 - __noswap_vmull_s16(__p1, (int16x4_t) {__p2, __p2, __p2, __p2});
  return __ret;
}
#endif

#if defined(__aarch64__)
#ifdef __LITTLE_ENDIAN__
__ai uint16x8_t vabdl_high_u8(uint8x16_t __p0, uint8x16_t __p1) {
  uint16x8_t __ret;
  __ret = vabdl_u8(vget_high_u8(__p0), vget_high_u8(__p1));
  return __ret;
}
#else
__ai uint16x8_t vabdl_high_u8(uint8x16_t __p0, uint8x16_t __p1) {
  uint8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __ret;
  __ret = __noswap_vabdl_u8(__noswap_vget_high_u8(__rev0), __noswap_vget_high_u8(__rev1));
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vabdl_high_u32(uint32x4_t __p0, uint32x4_t __p1) {
  uint64x2_t __ret;
  __ret = vabdl_u32(vget_high_u32(__p0), vget_high_u32(__p1));
  return __ret;
}
#else
__ai uint64x2_t vabdl_high_u32(uint32x4_t __p0, uint32x4_t __p1) {
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint64x2_t __ret;
  __ret = __noswap_vabdl_u32(__noswap_vget_high_u32(__rev0), __noswap_vget_high_u32(__rev1));
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vabdl_high_u16(uint16x8_t __p0, uint16x8_t __p1) {
  uint32x4_t __ret;
  __ret = vabdl_u16(vget_high_u16(__p0), vget_high_u16(__p1));
  return __ret;
}
#else
__ai uint32x4_t vabdl_high_u16(uint16x8_t __p0, uint16x8_t __p1) {
  uint16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint32x4_t __ret;
  __ret = __noswap_vabdl_u16(__noswap_vget_high_u16(__rev0), __noswap_vget_high_u16(__rev1));
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x8_t vabdl_high_s8(int8x16_t __p0, int8x16_t __p1) {
  int16x8_t __ret;
  __ret = vabdl_s8(vget_high_s8(__p0), vget_high_s8(__p1));
  return __ret;
}
#else
__ai int16x8_t vabdl_high_s8(int8x16_t __p0, int8x16_t __p1) {
  int8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __ret;
  __ret = __noswap_vabdl_s8(__noswap_vget_high_s8(__rev0), __noswap_vget_high_s8(__rev1));
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x2_t vabdl_high_s32(int32x4_t __p0, int32x4_t __p1) {
  int64x2_t __ret;
  __ret = vabdl_s32(vget_high_s32(__p0), vget_high_s32(__p1));
  return __ret;
}
#else
__ai int64x2_t vabdl_high_s32(int32x4_t __p0, int32x4_t __p1) {
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int64x2_t __ret;
  __ret = __noswap_vabdl_s32(__noswap_vget_high_s32(__rev0), __noswap_vget_high_s32(__rev1));
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vabdl_high_s16(int16x8_t __p0, int16x8_t __p1) {
  int32x4_t __ret;
  __ret = vabdl_s16(vget_high_s16(__p0), vget_high_s16(__p1));
  return __ret;
}
#else
__ai int32x4_t vabdl_high_s16(int16x8_t __p0, int16x8_t __p1) {
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int32x4_t __ret;
  __ret = __noswap_vabdl_s16(__noswap_vget_high_s16(__rev0), __noswap_vget_high_s16(__rev1));
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x8_t vaddl_high_u8(uint8x16_t __p0, uint8x16_t __p1) {
  uint16x8_t __ret;
  __ret = vmovl_high_u8(__p0) + vmovl_high_u8(__p1);
  return __ret;
}
#else
__ai uint16x8_t vaddl_high_u8(uint8x16_t __p0, uint8x16_t __p1) {
  uint8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __ret;
  __ret = __noswap_vmovl_high_u8(__rev0) + __noswap_vmovl_high_u8(__rev1);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vaddl_high_u32(uint32x4_t __p0, uint32x4_t __p1) {
  uint64x2_t __ret;
  __ret = vmovl_high_u32(__p0) + vmovl_high_u32(__p1);
  return __ret;
}
#else
__ai uint64x2_t vaddl_high_u32(uint32x4_t __p0, uint32x4_t __p1) {
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint64x2_t __ret;
  __ret = __noswap_vmovl_high_u32(__rev0) + __noswap_vmovl_high_u32(__rev1);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vaddl_high_u16(uint16x8_t __p0, uint16x8_t __p1) {
  uint32x4_t __ret;
  __ret = vmovl_high_u16(__p0) + vmovl_high_u16(__p1);
  return __ret;
}
#else
__ai uint32x4_t vaddl_high_u16(uint16x8_t __p0, uint16x8_t __p1) {
  uint16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint32x4_t __ret;
  __ret = __noswap_vmovl_high_u16(__rev0) + __noswap_vmovl_high_u16(__rev1);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x8_t vaddl_high_s8(int8x16_t __p0, int8x16_t __p1) {
  int16x8_t __ret;
  __ret = vmovl_high_s8(__p0) + vmovl_high_s8(__p1);
  return __ret;
}
#else
__ai int16x8_t vaddl_high_s8(int8x16_t __p0, int8x16_t __p1) {
  int8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __ret;
  __ret = __noswap_vmovl_high_s8(__rev0) + __noswap_vmovl_high_s8(__rev1);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x2_t vaddl_high_s32(int32x4_t __p0, int32x4_t __p1) {
  int64x2_t __ret;
  __ret = vmovl_high_s32(__p0) + vmovl_high_s32(__p1);
  return __ret;
}
#else
__ai int64x2_t vaddl_high_s32(int32x4_t __p0, int32x4_t __p1) {
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int64x2_t __ret;
  __ret = __noswap_vmovl_high_s32(__rev0) + __noswap_vmovl_high_s32(__rev1);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vaddl_high_s16(int16x8_t __p0, int16x8_t __p1) {
  int32x4_t __ret;
  __ret = vmovl_high_s16(__p0) + vmovl_high_s16(__p1);
  return __ret;
}
#else
__ai int32x4_t vaddl_high_s16(int16x8_t __p0, int16x8_t __p1) {
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int32x4_t __ret;
  __ret = __noswap_vmovl_high_s16(__rev0) + __noswap_vmovl_high_s16(__rev1);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x8_t vaddw_high_u8(uint16x8_t __p0, uint8x16_t __p1) {
  uint16x8_t __ret;
  __ret = __p0 + vmovl_high_u8(__p1);
  return __ret;
}
#else
__ai uint16x8_t vaddw_high_u8(uint16x8_t __p0, uint8x16_t __p1) {
  uint16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __ret;
  __ret = __rev0 + __noswap_vmovl_high_u8(__rev1);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vaddw_high_u32(uint64x2_t __p0, uint32x4_t __p1) {
  uint64x2_t __ret;
  __ret = __p0 + vmovl_high_u32(__p1);
  return __ret;
}
#else
__ai uint64x2_t vaddw_high_u32(uint64x2_t __p0, uint32x4_t __p1) {
  uint64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint64x2_t __ret;
  __ret = __rev0 + __noswap_vmovl_high_u32(__rev1);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vaddw_high_u16(uint32x4_t __p0, uint16x8_t __p1) {
  uint32x4_t __ret;
  __ret = __p0 + vmovl_high_u16(__p1);
  return __ret;
}
#else
__ai uint32x4_t vaddw_high_u16(uint32x4_t __p0, uint16x8_t __p1) {
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint32x4_t __ret;
  __ret = __rev0 + __noswap_vmovl_high_u16(__rev1);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x8_t vaddw_high_s8(int16x8_t __p0, int8x16_t __p1) {
  int16x8_t __ret;
  __ret = __p0 + vmovl_high_s8(__p1);
  return __ret;
}
#else
__ai int16x8_t vaddw_high_s8(int16x8_t __p0, int8x16_t __p1) {
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __ret;
  __ret = __rev0 + __noswap_vmovl_high_s8(__rev1);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x2_t vaddw_high_s32(int64x2_t __p0, int32x4_t __p1) {
  int64x2_t __ret;
  __ret = __p0 + vmovl_high_s32(__p1);
  return __ret;
}
#else
__ai int64x2_t vaddw_high_s32(int64x2_t __p0, int32x4_t __p1) {
  int64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int64x2_t __ret;
  __ret = __rev0 + __noswap_vmovl_high_s32(__rev1);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vaddw_high_s16(int32x4_t __p0, int16x8_t __p1) {
  int32x4_t __ret;
  __ret = __p0 + vmovl_high_s16(__p1);
  return __ret;
}
#else
__ai int32x4_t vaddw_high_s16(int32x4_t __p0, int16x8_t __p1) {
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int32x4_t __ret;
  __ret = __rev0 + __noswap_vmovl_high_s16(__rev1);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
#define vcopyq_lane_p64(__p0_250, __p1_250, __p2_250, __p3_250) __extension__ ({ \
  poly64x2_t __s0_250 = __p0_250; \
  poly64x1_t __s2_250 = __p2_250; \
  poly64x2_t __ret_250; \
  __ret_250 = vsetq_lane_p64(vget_lane_p64(__s2_250, __p3_250), __s0_250, __p1_250); \
  __ret_250; \
})
#else
#define vcopyq_lane_p64(__p0_251, __p1_251, __p2_251, __p3_251) __extension__ ({ \
  poly64x2_t __s0_251 = __p0_251; \
  poly64x1_t __s2_251 = __p2_251; \
  poly64x2_t __rev0_251;  __rev0_251 = __builtin_shufflevector(__s0_251, __s0_251, 1, 0); \
  poly64x2_t __ret_251; \
  __ret_251 = __noswap_vsetq_lane_p64(__noswap_vget_lane_p64(__s2_251, __p3_251), __rev0_251, __p1_251); \
  __ret_251 = __builtin_shufflevector(__ret_251, __ret_251, 1, 0); \
  __ret_251; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vcopyq_lane_f64(__p0_252, __p1_252, __p2_252, __p3_252) __extension__ ({ \
  float64x2_t __s0_252 = __p0_252; \
  float64x1_t __s2_252 = __p2_252; \
  float64x2_t __ret_252; \
  __ret_252 = vsetq_lane_f64(vget_lane_f64(__s2_252, __p3_252), __s0_252, __p1_252); \
  __ret_252; \
})
#else
#define vcopyq_lane_f64(__p0_253, __p1_253, __p2_253, __p3_253) __extension__ ({ \
  float64x2_t __s0_253 = __p0_253; \
  float64x1_t __s2_253 = __p2_253; \
  float64x2_t __rev0_253;  __rev0_253 = __builtin_shufflevector(__s0_253, __s0_253, 1, 0); \
  float64x2_t __ret_253; \
  __ret_253 = __noswap_vsetq_lane_f64(__noswap_vget_lane_f64(__s2_253, __p3_253), __rev0_253, __p1_253); \
  __ret_253 = __builtin_shufflevector(__ret_253, __ret_253, 1, 0); \
  __ret_253; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vcopy_lane_p64(__p0_254, __p1_254, __p2_254, __p3_254) __extension__ ({ \
  poly64x1_t __s0_254 = __p0_254; \
  poly64x1_t __s2_254 = __p2_254; \
  poly64x1_t __ret_254; \
  __ret_254 = vset_lane_p64(vget_lane_p64(__s2_254, __p3_254), __s0_254, __p1_254); \
  __ret_254; \
})
#else
#define vcopy_lane_p64(__p0_255, __p1_255, __p2_255, __p3_255) __extension__ ({ \
  poly64x1_t __s0_255 = __p0_255; \
  poly64x1_t __s2_255 = __p2_255; \
  poly64x1_t __ret_255; \
  __ret_255 = __noswap_vset_lane_p64(__noswap_vget_lane_p64(__s2_255, __p3_255), __s0_255, __p1_255); \
  __ret_255; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vcopy_lane_f64(__p0_256, __p1_256, __p2_256, __p3_256) __extension__ ({ \
  float64x1_t __s0_256 = __p0_256; \
  float64x1_t __s2_256 = __p2_256; \
  float64x1_t __ret_256; \
  __ret_256 = vset_lane_f64(vget_lane_f64(__s2_256, __p3_256), __s0_256, __p1_256); \
  __ret_256; \
})
#else
#define vcopy_lane_f64(__p0_257, __p1_257, __p2_257, __p3_257) __extension__ ({ \
  float64x1_t __s0_257 = __p0_257; \
  float64x1_t __s2_257 = __p2_257; \
  float64x1_t __ret_257; \
  __ret_257 = __noswap_vset_lane_f64(__noswap_vget_lane_f64(__s2_257, __p3_257), __s0_257, __p1_257); \
  __ret_257; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vcopyq_laneq_p64(__p0_258, __p1_258, __p2_258, __p3_258) __extension__ ({ \
  poly64x2_t __s0_258 = __p0_258; \
  poly64x2_t __s2_258 = __p2_258; \
  poly64x2_t __ret_258; \
  __ret_258 = vsetq_lane_p64(vgetq_lane_p64(__s2_258, __p3_258), __s0_258, __p1_258); \
  __ret_258; \
})
#else
#define vcopyq_laneq_p64(__p0_259, __p1_259, __p2_259, __p3_259) __extension__ ({ \
  poly64x2_t __s0_259 = __p0_259; \
  poly64x2_t __s2_259 = __p2_259; \
  poly64x2_t __rev0_259;  __rev0_259 = __builtin_shufflevector(__s0_259, __s0_259, 1, 0); \
  poly64x2_t __rev2_259;  __rev2_259 = __builtin_shufflevector(__s2_259, __s2_259, 1, 0); \
  poly64x2_t __ret_259; \
  __ret_259 = __noswap_vsetq_lane_p64(__noswap_vgetq_lane_p64(__rev2_259, __p3_259), __rev0_259, __p1_259); \
  __ret_259 = __builtin_shufflevector(__ret_259, __ret_259, 1, 0); \
  __ret_259; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vcopyq_laneq_f64(__p0_260, __p1_260, __p2_260, __p3_260) __extension__ ({ \
  float64x2_t __s0_260 = __p0_260; \
  float64x2_t __s2_260 = __p2_260; \
  float64x2_t __ret_260; \
  __ret_260 = vsetq_lane_f64(vgetq_lane_f64(__s2_260, __p3_260), __s0_260, __p1_260); \
  __ret_260; \
})
#else
#define vcopyq_laneq_f64(__p0_261, __p1_261, __p2_261, __p3_261) __extension__ ({ \
  float64x2_t __s0_261 = __p0_261; \
  float64x2_t __s2_261 = __p2_261; \
  float64x2_t __rev0_261;  __rev0_261 = __builtin_shufflevector(__s0_261, __s0_261, 1, 0); \
  float64x2_t __rev2_261;  __rev2_261 = __builtin_shufflevector(__s2_261, __s2_261, 1, 0); \
  float64x2_t __ret_261; \
  __ret_261 = __noswap_vsetq_lane_f64(__noswap_vgetq_lane_f64(__rev2_261, __p3_261), __rev0_261, __p1_261); \
  __ret_261 = __builtin_shufflevector(__ret_261, __ret_261, 1, 0); \
  __ret_261; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vcopy_laneq_p64(__p0_262, __p1_262, __p2_262, __p3_262) __extension__ ({ \
  poly64x1_t __s0_262 = __p0_262; \
  poly64x2_t __s2_262 = __p2_262; \
  poly64x1_t __ret_262; \
  __ret_262 = vset_lane_p64(vgetq_lane_p64(__s2_262, __p3_262), __s0_262, __p1_262); \
  __ret_262; \
})
#else
#define vcopy_laneq_p64(__p0_263, __p1_263, __p2_263, __p3_263) __extension__ ({ \
  poly64x1_t __s0_263 = __p0_263; \
  poly64x2_t __s2_263 = __p2_263; \
  poly64x2_t __rev2_263;  __rev2_263 = __builtin_shufflevector(__s2_263, __s2_263, 1, 0); \
  poly64x1_t __ret_263; \
  __ret_263 = __noswap_vset_lane_p64(__noswap_vgetq_lane_p64(__rev2_263, __p3_263), __s0_263, __p1_263); \
  __ret_263; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vcopy_laneq_f64(__p0_264, __p1_264, __p2_264, __p3_264) __extension__ ({ \
  float64x1_t __s0_264 = __p0_264; \
  float64x2_t __s2_264 = __p2_264; \
  float64x1_t __ret_264; \
  __ret_264 = vset_lane_f64(vgetq_lane_f64(__s2_264, __p3_264), __s0_264, __p1_264); \
  __ret_264; \
})
#else
#define vcopy_laneq_f64(__p0_265, __p1_265, __p2_265, __p3_265) __extension__ ({ \
  float64x1_t __s0_265 = __p0_265; \
  float64x2_t __s2_265 = __p2_265; \
  float64x2_t __rev2_265;  __rev2_265 = __builtin_shufflevector(__s2_265, __s2_265, 1, 0); \
  float64x1_t __ret_265; \
  __ret_265 = __noswap_vset_lane_f64(__noswap_vgetq_lane_f64(__rev2_265, __p3_265), __s0_265, __p1_265); \
  __ret_265; \
})
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x8_t vmlal_high_u8(uint16x8_t __p0, uint8x16_t __p1, uint8x16_t __p2) {
  uint16x8_t __ret;
  __ret = vmlal_u8(__p0, vget_high_u8(__p1), vget_high_u8(__p2));
  return __ret;
}
#else
__ai uint16x8_t vmlal_high_u8(uint16x8_t __p0, uint8x16_t __p1, uint8x16_t __p2) {
  uint16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __ret;
  __ret = __noswap_vmlal_u8(__rev0, __noswap_vget_high_u8(__rev1), __noswap_vget_high_u8(__rev2));
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vmlal_high_u32(uint64x2_t __p0, uint32x4_t __p1, uint32x4_t __p2) {
  uint64x2_t __ret;
  __ret = vmlal_u32(__p0, vget_high_u32(__p1), vget_high_u32(__p2));
  return __ret;
}
#else
__ai uint64x2_t vmlal_high_u32(uint64x2_t __p0, uint32x4_t __p1, uint32x4_t __p2) {
  uint64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint32x4_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 3, 2, 1, 0);
  uint64x2_t __ret;
  __ret = __noswap_vmlal_u32(__rev0, __noswap_vget_high_u32(__rev1), __noswap_vget_high_u32(__rev2));
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vmlal_high_u16(uint32x4_t __p0, uint16x8_t __p1, uint16x8_t __p2) {
  uint32x4_t __ret;
  __ret = vmlal_u16(__p0, vget_high_u16(__p1), vget_high_u16(__p2));
  return __ret;
}
#else
__ai uint32x4_t vmlal_high_u16(uint32x4_t __p0, uint16x8_t __p1, uint16x8_t __p2) {
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 7, 6, 5, 4, 3, 2, 1, 0);
  uint32x4_t __ret;
  __ret = __noswap_vmlal_u16(__rev0, __noswap_vget_high_u16(__rev1), __noswap_vget_high_u16(__rev2));
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x8_t vmlal_high_s8(int16x8_t __p0, int8x16_t __p1, int8x16_t __p2) {
  int16x8_t __ret;
  __ret = vmlal_s8(__p0, vget_high_s8(__p1), vget_high_s8(__p2));
  return __ret;
}
#else
__ai int16x8_t vmlal_high_s8(int16x8_t __p0, int8x16_t __p1, int8x16_t __p2) {
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __ret;
  __ret = __noswap_vmlal_s8(__rev0, __noswap_vget_high_s8(__rev1), __noswap_vget_high_s8(__rev2));
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x2_t vmlal_high_s32(int64x2_t __p0, int32x4_t __p1, int32x4_t __p2) {
  int64x2_t __ret;
  __ret = vmlal_s32(__p0, vget_high_s32(__p1), vget_high_s32(__p2));
  return __ret;
}
#else
__ai int64x2_t vmlal_high_s32(int64x2_t __p0, int32x4_t __p1, int32x4_t __p2) {
  int64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int32x4_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 3, 2, 1, 0);
  int64x2_t __ret;
  __ret = __noswap_vmlal_s32(__rev0, __noswap_vget_high_s32(__rev1), __noswap_vget_high_s32(__rev2));
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vmlal_high_s16(int32x4_t __p0, int16x8_t __p1, int16x8_t __p2) {
  int32x4_t __ret;
  __ret = vmlal_s16(__p0, vget_high_s16(__p1), vget_high_s16(__p2));
  return __ret;
}
#else
__ai int32x4_t vmlal_high_s16(int32x4_t __p0, int16x8_t __p1, int16x8_t __p2) {
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 7, 6, 5, 4, 3, 2, 1, 0);
  int32x4_t __ret;
  __ret = __noswap_vmlal_s16(__rev0, __noswap_vget_high_s16(__rev1), __noswap_vget_high_s16(__rev2));
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vmlal_high_n_u32(uint64x2_t __p0, uint32x4_t __p1, uint32_t __p2) {
  uint64x2_t __ret;
  __ret = vmlal_n_u32(__p0, vget_high_u32(__p1), __p2);
  return __ret;
}
#else
__ai uint64x2_t vmlal_high_n_u32(uint64x2_t __p0, uint32x4_t __p1, uint32_t __p2) {
  uint64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint64x2_t __ret;
  __ret = __noswap_vmlal_n_u32(__rev0, __noswap_vget_high_u32(__rev1), __p2);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vmlal_high_n_u16(uint32x4_t __p0, uint16x8_t __p1, uint16_t __p2) {
  uint32x4_t __ret;
  __ret = vmlal_n_u16(__p0, vget_high_u16(__p1), __p2);
  return __ret;
}
#else
__ai uint32x4_t vmlal_high_n_u16(uint32x4_t __p0, uint16x8_t __p1, uint16_t __p2) {
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint32x4_t __ret;
  __ret = __noswap_vmlal_n_u16(__rev0, __noswap_vget_high_u16(__rev1), __p2);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x2_t vmlal_high_n_s32(int64x2_t __p0, int32x4_t __p1, int32_t __p2) {
  int64x2_t __ret;
  __ret = vmlal_n_s32(__p0, vget_high_s32(__p1), __p2);
  return __ret;
}
#else
__ai int64x2_t vmlal_high_n_s32(int64x2_t __p0, int32x4_t __p1, int32_t __p2) {
  int64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int64x2_t __ret;
  __ret = __noswap_vmlal_n_s32(__rev0, __noswap_vget_high_s32(__rev1), __p2);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vmlal_high_n_s16(int32x4_t __p0, int16x8_t __p1, int16_t __p2) {
  int32x4_t __ret;
  __ret = vmlal_n_s16(__p0, vget_high_s16(__p1), __p2);
  return __ret;
}
#else
__ai int32x4_t vmlal_high_n_s16(int32x4_t __p0, int16x8_t __p1, int16_t __p2) {
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int32x4_t __ret;
  __ret = __noswap_vmlal_n_s16(__rev0, __noswap_vget_high_s16(__rev1), __p2);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x8_t vmlsl_high_u8(uint16x8_t __p0, uint8x16_t __p1, uint8x16_t __p2) {
  uint16x8_t __ret;
  __ret = vmlsl_u8(__p0, vget_high_u8(__p1), vget_high_u8(__p2));
  return __ret;
}
#else
__ai uint16x8_t vmlsl_high_u8(uint16x8_t __p0, uint8x16_t __p1, uint8x16_t __p2) {
  uint16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __ret;
  __ret = __noswap_vmlsl_u8(__rev0, __noswap_vget_high_u8(__rev1), __noswap_vget_high_u8(__rev2));
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vmlsl_high_u32(uint64x2_t __p0, uint32x4_t __p1, uint32x4_t __p2) {
  uint64x2_t __ret;
  __ret = vmlsl_u32(__p0, vget_high_u32(__p1), vget_high_u32(__p2));
  return __ret;
}
#else
__ai uint64x2_t vmlsl_high_u32(uint64x2_t __p0, uint32x4_t __p1, uint32x4_t __p2) {
  uint64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint32x4_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 3, 2, 1, 0);
  uint64x2_t __ret;
  __ret = __noswap_vmlsl_u32(__rev0, __noswap_vget_high_u32(__rev1), __noswap_vget_high_u32(__rev2));
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vmlsl_high_u16(uint32x4_t __p0, uint16x8_t __p1, uint16x8_t __p2) {
  uint32x4_t __ret;
  __ret = vmlsl_u16(__p0, vget_high_u16(__p1), vget_high_u16(__p2));
  return __ret;
}
#else
__ai uint32x4_t vmlsl_high_u16(uint32x4_t __p0, uint16x8_t __p1, uint16x8_t __p2) {
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 7, 6, 5, 4, 3, 2, 1, 0);
  uint32x4_t __ret;
  __ret = __noswap_vmlsl_u16(__rev0, __noswap_vget_high_u16(__rev1), __noswap_vget_high_u16(__rev2));
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x8_t vmlsl_high_s8(int16x8_t __p0, int8x16_t __p1, int8x16_t __p2) {
  int16x8_t __ret;
  __ret = vmlsl_s8(__p0, vget_high_s8(__p1), vget_high_s8(__p2));
  return __ret;
}
#else
__ai int16x8_t vmlsl_high_s8(int16x8_t __p0, int8x16_t __p1, int8x16_t __p2) {
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __ret;
  __ret = __noswap_vmlsl_s8(__rev0, __noswap_vget_high_s8(__rev1), __noswap_vget_high_s8(__rev2));
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x2_t vmlsl_high_s32(int64x2_t __p0, int32x4_t __p1, int32x4_t __p2) {
  int64x2_t __ret;
  __ret = vmlsl_s32(__p0, vget_high_s32(__p1), vget_high_s32(__p2));
  return __ret;
}
#else
__ai int64x2_t vmlsl_high_s32(int64x2_t __p0, int32x4_t __p1, int32x4_t __p2) {
  int64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int32x4_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 3, 2, 1, 0);
  int64x2_t __ret;
  __ret = __noswap_vmlsl_s32(__rev0, __noswap_vget_high_s32(__rev1), __noswap_vget_high_s32(__rev2));
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vmlsl_high_s16(int32x4_t __p0, int16x8_t __p1, int16x8_t __p2) {
  int32x4_t __ret;
  __ret = vmlsl_s16(__p0, vget_high_s16(__p1), vget_high_s16(__p2));
  return __ret;
}
#else
__ai int32x4_t vmlsl_high_s16(int32x4_t __p0, int16x8_t __p1, int16x8_t __p2) {
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 7, 6, 5, 4, 3, 2, 1, 0);
  int32x4_t __ret;
  __ret = __noswap_vmlsl_s16(__rev0, __noswap_vget_high_s16(__rev1), __noswap_vget_high_s16(__rev2));
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vmlsl_high_n_u32(uint64x2_t __p0, uint32x4_t __p1, uint32_t __p2) {
  uint64x2_t __ret;
  __ret = vmlsl_n_u32(__p0, vget_high_u32(__p1), __p2);
  return __ret;
}
#else
__ai uint64x2_t vmlsl_high_n_u32(uint64x2_t __p0, uint32x4_t __p1, uint32_t __p2) {
  uint64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint64x2_t __ret;
  __ret = __noswap_vmlsl_n_u32(__rev0, __noswap_vget_high_u32(__rev1), __p2);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vmlsl_high_n_u16(uint32x4_t __p0, uint16x8_t __p1, uint16_t __p2) {
  uint32x4_t __ret;
  __ret = vmlsl_n_u16(__p0, vget_high_u16(__p1), __p2);
  return __ret;
}
#else
__ai uint32x4_t vmlsl_high_n_u16(uint32x4_t __p0, uint16x8_t __p1, uint16_t __p2) {
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint32x4_t __ret;
  __ret = __noswap_vmlsl_n_u16(__rev0, __noswap_vget_high_u16(__rev1), __p2);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x2_t vmlsl_high_n_s32(int64x2_t __p0, int32x4_t __p1, int32_t __p2) {
  int64x2_t __ret;
  __ret = vmlsl_n_s32(__p0, vget_high_s32(__p1), __p2);
  return __ret;
}
#else
__ai int64x2_t vmlsl_high_n_s32(int64x2_t __p0, int32x4_t __p1, int32_t __p2) {
  int64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int64x2_t __ret;
  __ret = __noswap_vmlsl_n_s32(__rev0, __noswap_vget_high_s32(__rev1), __p2);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vmlsl_high_n_s16(int32x4_t __p0, int16x8_t __p1, int16_t __p2) {
  int32x4_t __ret;
  __ret = vmlsl_n_s16(__p0, vget_high_s16(__p1), __p2);
  return __ret;
}
#else
__ai int32x4_t vmlsl_high_n_s16(int32x4_t __p0, int16x8_t __p1, int16_t __p2) {
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int32x4_t __ret;
  __ret = __noswap_vmlsl_n_s16(__rev0, __noswap_vget_high_s16(__rev1), __p2);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
#define vmulx_lane_f64(__p0_266, __p1_266, __p2_266) __extension__ ({ \
  float64x1_t __s0_266 = __p0_266; \
  float64x1_t __s1_266 = __p1_266; \
  float64x1_t __ret_266; \
  float64_t __x_266 = vget_lane_f64(__s0_266, 0); \
  float64_t __y_266 = vget_lane_f64(__s1_266, __p2_266); \
  float64_t __z_266 = vmulxd_f64(__x_266, __y_266); \
  __ret_266 = vset_lane_f64(__z_266, __s0_266, __p2_266); \
  __ret_266; \
})
#else
#define vmulx_lane_f64(__p0_267, __p1_267, __p2_267) __extension__ ({ \
  float64x1_t __s0_267 = __p0_267; \
  float64x1_t __s1_267 = __p1_267; \
  float64x1_t __ret_267; \
  float64_t __x_267 = __noswap_vget_lane_f64(__s0_267, 0); \
  float64_t __y_267 = __noswap_vget_lane_f64(__s1_267, __p2_267); \
  float64_t __z_267 = __noswap_vmulxd_f64(__x_267, __y_267); \
  __ret_267 = __noswap_vset_lane_f64(__z_267, __s0_267, __p2_267); \
  __ret_267; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmulx_laneq_f64(__p0_268, __p1_268, __p2_268) __extension__ ({ \
  float64x1_t __s0_268 = __p0_268; \
  float64x2_t __s1_268 = __p1_268; \
  float64x1_t __ret_268; \
  float64_t __x_268 = vget_lane_f64(__s0_268, 0); \
  float64_t __y_268 = vgetq_lane_f64(__s1_268, __p2_268); \
  float64_t __z_268 = vmulxd_f64(__x_268, __y_268); \
  __ret_268 = vset_lane_f64(__z_268, __s0_268, 0); \
  __ret_268; \
})
#else
#define vmulx_laneq_f64(__p0_269, __p1_269, __p2_269) __extension__ ({ \
  float64x1_t __s0_269 = __p0_269; \
  float64x2_t __s1_269 = __p1_269; \
  float64x2_t __rev1_269;  __rev1_269 = __builtin_shufflevector(__s1_269, __s1_269, 1, 0); \
  float64x1_t __ret_269; \
  float64_t __x_269 = __noswap_vget_lane_f64(__s0_269, 0); \
  float64_t __y_269 = __noswap_vgetq_lane_f64(__rev1_269, __p2_269); \
  float64_t __z_269 = __noswap_vmulxd_f64(__x_269, __y_269); \
  __ret_269 = __noswap_vset_lane_f64(__z_269, __s0_269, 0); \
  __ret_269; \
})
#endif

#endif
#ifdef __LITTLE_ENDIAN__
__ai uint16x8_t vabal_u8(uint16x8_t __p0, uint8x8_t __p1, uint8x8_t __p2) {
  uint16x8_t __ret;
  __ret = __p0 + vabdl_u8(__p1, __p2);
  return __ret;
}
#else
__ai uint16x8_t vabal_u8(uint16x8_t __p0, uint8x8_t __p1, uint8x8_t __p2) {
  uint16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __ret;
  __ret = __rev0 + __noswap_vabdl_u8(__rev1, __rev2);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
__ai uint16x8_t __noswap_vabal_u8(uint16x8_t __p0, uint8x8_t __p1, uint8x8_t __p2) {
  uint16x8_t __ret;
  __ret = __p0 + __noswap_vabdl_u8(__p1, __p2);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vabal_u32(uint64x2_t __p0, uint32x2_t __p1, uint32x2_t __p2) {
  uint64x2_t __ret;
  __ret = __p0 + vabdl_u32(__p1, __p2);
  return __ret;
}
#else
__ai uint64x2_t vabal_u32(uint64x2_t __p0, uint32x2_t __p1, uint32x2_t __p2) {
  uint64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  uint32x2_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 1, 0);
  uint64x2_t __ret;
  __ret = __rev0 + __noswap_vabdl_u32(__rev1, __rev2);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
__ai uint64x2_t __noswap_vabal_u32(uint64x2_t __p0, uint32x2_t __p1, uint32x2_t __p2) {
  uint64x2_t __ret;
  __ret = __p0 + __noswap_vabdl_u32(__p1, __p2);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vabal_u16(uint32x4_t __p0, uint16x4_t __p1, uint16x4_t __p2) {
  uint32x4_t __ret;
  __ret = __p0 + vabdl_u16(__p1, __p2);
  return __ret;
}
#else
__ai uint32x4_t vabal_u16(uint32x4_t __p0, uint16x4_t __p1, uint16x4_t __p2) {
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint16x4_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 3, 2, 1, 0);
  uint32x4_t __ret;
  __ret = __rev0 + __noswap_vabdl_u16(__rev1, __rev2);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
__ai uint32x4_t __noswap_vabal_u16(uint32x4_t __p0, uint16x4_t __p1, uint16x4_t __p2) {
  uint32x4_t __ret;
  __ret = __p0 + __noswap_vabdl_u16(__p1, __p2);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x8_t vabal_s8(int16x8_t __p0, int8x8_t __p1, int8x8_t __p2) {
  int16x8_t __ret;
  __ret = __p0 + vabdl_s8(__p1, __p2);
  return __ret;
}
#else
__ai int16x8_t vabal_s8(int16x8_t __p0, int8x8_t __p1, int8x8_t __p2) {
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __ret;
  __ret = __rev0 + __noswap_vabdl_s8(__rev1, __rev2);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
__ai int16x8_t __noswap_vabal_s8(int16x8_t __p0, int8x8_t __p1, int8x8_t __p2) {
  int16x8_t __ret;
  __ret = __p0 + __noswap_vabdl_s8(__p1, __p2);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x2_t vabal_s32(int64x2_t __p0, int32x2_t __p1, int32x2_t __p2) {
  int64x2_t __ret;
  __ret = __p0 + vabdl_s32(__p1, __p2);
  return __ret;
}
#else
__ai int64x2_t vabal_s32(int64x2_t __p0, int32x2_t __p1, int32x2_t __p2) {
  int64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  int32x2_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 1, 0);
  int64x2_t __ret;
  __ret = __rev0 + __noswap_vabdl_s32(__rev1, __rev2);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
__ai int64x2_t __noswap_vabal_s32(int64x2_t __p0, int32x2_t __p1, int32x2_t __p2) {
  int64x2_t __ret;
  __ret = __p0 + __noswap_vabdl_s32(__p1, __p2);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vabal_s16(int32x4_t __p0, int16x4_t __p1, int16x4_t __p2) {
  int32x4_t __ret;
  __ret = __p0 + vabdl_s16(__p1, __p2);
  return __ret;
}
#else
__ai int32x4_t vabal_s16(int32x4_t __p0, int16x4_t __p1, int16x4_t __p2) {
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int16x4_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 3, 2, 1, 0);
  int32x4_t __ret;
  __ret = __rev0 + __noswap_vabdl_s16(__rev1, __rev2);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
__ai int32x4_t __noswap_vabal_s16(int32x4_t __p0, int16x4_t __p1, int16x4_t __p2) {
  int32x4_t __ret;
  __ret = __p0 + __noswap_vabdl_s16(__p1, __p2);
  return __ret;
}
#endif

#if defined(__aarch64__)
#ifdef __LITTLE_ENDIAN__
__ai uint16x8_t vabal_high_u8(uint16x8_t __p0, uint8x16_t __p1, uint8x16_t __p2) {
  uint16x8_t __ret;
  __ret = vabal_u8(__p0, vget_high_u8(__p1), vget_high_u8(__p2));
  return __ret;
}
#else
__ai uint16x8_t vabal_high_u8(uint16x8_t __p0, uint8x16_t __p1, uint8x16_t __p2) {
  uint16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __ret;
  __ret = __noswap_vabal_u8(__rev0, __noswap_vget_high_u8(__rev1), __noswap_vget_high_u8(__rev2));
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vabal_high_u32(uint64x2_t __p0, uint32x4_t __p1, uint32x4_t __p2) {
  uint64x2_t __ret;
  __ret = vabal_u32(__p0, vget_high_u32(__p1), vget_high_u32(__p2));
  return __ret;
}
#else
__ai uint64x2_t vabal_high_u32(uint64x2_t __p0, uint32x4_t __p1, uint32x4_t __p2) {
  uint64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint32x4_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 3, 2, 1, 0);
  uint64x2_t __ret;
  __ret = __noswap_vabal_u32(__rev0, __noswap_vget_high_u32(__rev1), __noswap_vget_high_u32(__rev2));
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vabal_high_u16(uint32x4_t __p0, uint16x8_t __p1, uint16x8_t __p2) {
  uint32x4_t __ret;
  __ret = vabal_u16(__p0, vget_high_u16(__p1), vget_high_u16(__p2));
  return __ret;
}
#else
__ai uint32x4_t vabal_high_u16(uint32x4_t __p0, uint16x8_t __p1, uint16x8_t __p2) {
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 7, 6, 5, 4, 3, 2, 1, 0);
  uint32x4_t __ret;
  __ret = __noswap_vabal_u16(__rev0, __noswap_vget_high_u16(__rev1), __noswap_vget_high_u16(__rev2));
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x8_t vabal_high_s8(int16x8_t __p0, int8x16_t __p1, int8x16_t __p2) {
  int16x8_t __ret;
  __ret = vabal_s8(__p0, vget_high_s8(__p1), vget_high_s8(__p2));
  return __ret;
}
#else
__ai int16x8_t vabal_high_s8(int16x8_t __p0, int8x16_t __p1, int8x16_t __p2) {
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __ret;
  __ret = __noswap_vabal_s8(__rev0, __noswap_vget_high_s8(__rev1), __noswap_vget_high_s8(__rev2));
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x2_t vabal_high_s32(int64x2_t __p0, int32x4_t __p1, int32x4_t __p2) {
  int64x2_t __ret;
  __ret = vabal_s32(__p0, vget_high_s32(__p1), vget_high_s32(__p2));
  return __ret;
}
#else
__ai int64x2_t vabal_high_s32(int64x2_t __p0, int32x4_t __p1, int32x4_t __p2) {
  int64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int32x4_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 3, 2, 1, 0);
  int64x2_t __ret;
  __ret = __noswap_vabal_s32(__rev0, __noswap_vget_high_s32(__rev1), __noswap_vget_high_s32(__rev2));
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vabal_high_s16(int32x4_t __p0, int16x8_t __p1, int16x8_t __p2) {
  int32x4_t __ret;
  __ret = vabal_s16(__p0, vget_high_s16(__p1), vget_high_s16(__p2));
  return __ret;
}
#else
__ai int32x4_t vabal_high_s16(int32x4_t __p0, int16x8_t __p1, int16x8_t __p2) {
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 7, 6, 5, 4, 3, 2, 1, 0);
  int32x4_t __ret;
  __ret = __noswap_vabal_s16(__rev0, __noswap_vget_high_s16(__rev1), __noswap_vget_high_s16(__rev2));
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#endif

#undef __ai

#endif /* __ARM_NEON_H */
                                                                                                                                                                                                                                                                                                                                                               usr/lib/llvm-3.5/lib/clang/3.5.0/include/avx2intrin.h                                               0100644 0000000 0000000 00000124251 12474130047 020305  0                                                                                                    ustar 00                                                                0000000 0000000                                                                                                                                                                        /*===---- avx2intrin.h - AVX2 intrinsics -----------------------------------===
 *
 * Permission is hereby granted, free of charge, to any person obtaining a copy
 * of this software and associated documentation files (the "Software"), to deal
 * in the Software without restriction, including without limitation the rights
 * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 * copies of the Software, and to permit persons to whom the Software is
 * furnished to do so, subject to the following conditions:
 *
 * The above copyright notice and this permission notice shall be included in
 * all copies or substantial portions of the Software.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
 * THE SOFTWARE.
 *
 *===-----------------------------------------------------------------------===
 */

#ifndef __IMMINTRIN_H
#error "Never use <avx2intrin.h> directly; include <immintrin.h> instead."
#endif

#ifndef __AVX2INTRIN_H
#define __AVX2INTRIN_H

/* SSE4 Multiple Packed Sums of Absolute Difference.  */
#define _mm256_mpsadbw_epu8(X, Y, M) __builtin_ia32_mpsadbw256((X), (Y), (M))

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_abs_epi8(__m256i __a)
{
    return (__m256i)__builtin_ia32_pabsb256((__v32qi)__a);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_abs_epi16(__m256i __a)
{
    return (__m256i)__builtin_ia32_pabsw256((__v16hi)__a);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_abs_epi32(__m256i __a)
{
    return (__m256i)__builtin_ia32_pabsd256((__v8si)__a);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_packs_epi16(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_ia32_packsswb256((__v16hi)__a, (__v16hi)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_packs_epi32(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_ia32_packssdw256((__v8si)__a, (__v8si)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_packus_epi16(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_ia32_packuswb256((__v16hi)__a, (__v16hi)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_packus_epi32(__m256i __V1, __m256i __V2)
{
  return (__m256i) __builtin_ia32_packusdw256((__v8si)__V1, (__v8si)__V2);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_add_epi8(__m256i __a, __m256i __b)
{
  return (__m256i)((__v32qi)__a + (__v32qi)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_add_epi16(__m256i __a, __m256i __b)
{
  return (__m256i)((__v16hi)__a + (__v16hi)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_add_epi32(__m256i __a, __m256i __b)
{
  return (__m256i)((__v8si)__a + (__v8si)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_add_epi64(__m256i __a, __m256i __b)
{
  return __a + __b;
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_adds_epi8(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_ia32_paddsb256((__v32qi)__a, (__v32qi)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_adds_epi16(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_ia32_paddsw256((__v16hi)__a, (__v16hi)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_adds_epu8(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_ia32_paddusb256((__v32qi)__a, (__v32qi)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_adds_epu16(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_ia32_paddusw256((__v16hi)__a, (__v16hi)__b);
}

#define _mm256_alignr_epi8(a, b, n) __extension__ ({ \
  __m256i __a = (a); \
  __m256i __b = (b); \
  (__m256i)__builtin_ia32_palignr256((__v32qi)__a, (__v32qi)__b, (n)); })

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_and_si256(__m256i __a, __m256i __b)
{
  return __a & __b;
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_andnot_si256(__m256i __a, __m256i __b)
{
  return ~__a & __b;
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_avg_epu8(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_ia32_pavgb256((__v32qi)__a, (__v32qi)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_avg_epu16(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_ia32_pavgw256((__v16hi)__a, (__v16hi)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_blendv_epi8(__m256i __V1, __m256i __V2, __m256i __M)
{
  return (__m256i)__builtin_ia32_pblendvb256((__v32qi)__V1, (__v32qi)__V2,
                                              (__v32qi)__M);
}

#define _mm256_blend_epi16(V1, V2, M) __extension__ ({ \
  __m256i __V1 = (V1); \
  __m256i __V2 = (V2); \
  (__m256d)__builtin_shufflevector((__v16hi)__V1, (__v16hi)__V2, \
                                   (((M) & 0x01) ? 16 : 0), \
                                   (((M) & 0x02) ? 17 : 1), \
                                   (((M) & 0x04) ? 18 : 2), \
                                   (((M) & 0x08) ? 19 : 3), \
                                   (((M) & 0x10) ? 20 : 4), \
                                   (((M) & 0x20) ? 21 : 5), \
                                   (((M) & 0x40) ? 22 : 6), \
                                   (((M) & 0x80) ? 23 : 7), \
                                   (((M) & 0x01) ? 24 : 8), \
                                   (((M) & 0x02) ? 25 : 9), \
                                   (((M) & 0x04) ? 26 : 10), \
                                   (((M) & 0x08) ? 27 : 11), \
                                   (((M) & 0x10) ? 28 : 12), \
                                   (((M) & 0x20) ? 29 : 13), \
                                   (((M) & 0x40) ? 30 : 14), \
                                   (((M) & 0x80) ? 31 : 15)); })

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_cmpeq_epi8(__m256i __a, __m256i __b)
{
  return (__m256i)((__v32qi)__a == (__v32qi)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_cmpeq_epi16(__m256i __a, __m256i __b)
{
  return (__m256i)((__v16hi)__a == (__v16hi)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_cmpeq_epi32(__m256i __a, __m256i __b)
{
  return (__m256i)((__v8si)__a == (__v8si)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_cmpeq_epi64(__m256i __a, __m256i __b)
{
  return (__m256i)(__a == __b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_cmpgt_epi8(__m256i __a, __m256i __b)
{
  return (__m256i)((__v32qi)__a > (__v32qi)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_cmpgt_epi16(__m256i __a, __m256i __b)
{
  return (__m256i)((__v16hi)__a > (__v16hi)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_cmpgt_epi32(__m256i __a, __m256i __b)
{
  return (__m256i)((__v8si)__a > (__v8si)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_cmpgt_epi64(__m256i __a, __m256i __b)
{
  return (__m256i)(__a > __b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_hadd_epi16(__m256i __a, __m256i __b)
{
    return (__m256i)__builtin_ia32_phaddw256((__v16hi)__a, (__v16hi)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_hadd_epi32(__m256i __a, __m256i __b)
{
    return (__m256i)__builtin_ia32_phaddd256((__v8si)__a, (__v8si)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_hadds_epi16(__m256i __a, __m256i __b)
{
    return (__m256i)__builtin_ia32_phaddsw256((__v16hi)__a, (__v16hi)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_hsub_epi16(__m256i __a, __m256i __b)
{
    return (__m256i)__builtin_ia32_phsubw256((__v16hi)__a, (__v16hi)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_hsub_epi32(__m256i __a, __m256i __b)
{
    return (__m256i)__builtin_ia32_phsubd256((__v8si)__a, (__v8si)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_hsubs_epi16(__m256i __a, __m256i __b)
{
    return (__m256i)__builtin_ia32_phsubsw256((__v16hi)__a, (__v16hi)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_maddubs_epi16(__m256i __a, __m256i __b)
{
    return (__m256i)__builtin_ia32_pmaddubsw256((__v32qi)__a, (__v32qi)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_madd_epi16(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_ia32_pmaddwd256((__v16hi)__a, (__v16hi)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_max_epi8(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_ia32_pmaxsb256((__v32qi)__a, (__v32qi)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_max_epi16(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_ia32_pmaxsw256((__v16hi)__a, (__v16hi)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_max_epi32(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_ia32_pmaxsd256((__v8si)__a, (__v8si)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_max_epu8(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_ia32_pmaxub256((__v32qi)__a, (__v32qi)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_max_epu16(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_ia32_pmaxuw256((__v16hi)__a, (__v16hi)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_max_epu32(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_ia32_pmaxud256((__v8si)__a, (__v8si)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_min_epi8(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_ia32_pminsb256((__v32qi)__a, (__v32qi)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_min_epi16(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_ia32_pminsw256((__v16hi)__a, (__v16hi)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_min_epi32(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_ia32_pminsd256((__v8si)__a, (__v8si)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_min_epu8(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_ia32_pminub256((__v32qi)__a, (__v32qi)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_min_epu16(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_ia32_pminuw256 ((__v16hi)__a, (__v16hi)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_min_epu32(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_ia32_pminud256((__v8si)__a, (__v8si)__b);
}

static __inline__ int __attribute__((__always_inline__, __nodebug__))
_mm256_movemask_epi8(__m256i __a)
{
  return __builtin_ia32_pmovmskb256((__v32qi)__a);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_cvtepi8_epi16(__m128i __V)
{
  return (__m256i)__builtin_ia32_pmovsxbw256((__v16qi)__V);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_cvtepi8_epi32(__m128i __V)
{
  return (__m256i)__builtin_ia32_pmovsxbd256((__v16qi)__V);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_cvtepi8_epi64(__m128i __V)
{
  return (__m256i)__builtin_ia32_pmovsxbq256((__v16qi)__V);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_cvtepi16_epi32(__m128i __V)
{
  return (__m256i)__builtin_ia32_pmovsxwd256((__v8hi)__V);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_cvtepi16_epi64(__m128i __V)
{
  return (__m256i)__builtin_ia32_pmovsxwq256((__v8hi)__V);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_cvtepi32_epi64(__m128i __V)
{
  return (__m256i)__builtin_ia32_pmovsxdq256((__v4si)__V);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_cvtepu8_epi16(__m128i __V)
{
  return (__m256i)__builtin_ia32_pmovzxbw256((__v16qi)__V);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_cvtepu8_epi32(__m128i __V)
{
  return (__m256i)__builtin_ia32_pmovzxbd256((__v16qi)__V);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_cvtepu8_epi64(__m128i __V)
{
  return (__m256i)__builtin_ia32_pmovzxbq256((__v16qi)__V);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_cvtepu16_epi32(__m128i __V)
{
  return (__m256i)__builtin_ia32_pmovzxwd256((__v8hi)__V);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_cvtepu16_epi64(__m128i __V)
{
  return (__m256i)__builtin_ia32_pmovzxwq256((__v8hi)__V);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_cvtepu32_epi64(__m128i __V)
{
  return (__m256i)__builtin_ia32_pmovzxdq256((__v4si)__V);
}

static __inline__  __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_mul_epi32(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_ia32_pmuldq256((__v8si)__a, (__v8si)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_mulhrs_epi16(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_ia32_pmulhrsw256((__v16hi)__a, (__v16hi)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_mulhi_epu16(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_ia32_pmulhuw256((__v16hi)__a, (__v16hi)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_mulhi_epi16(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_ia32_pmulhw256((__v16hi)__a, (__v16hi)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_mullo_epi16(__m256i __a, __m256i __b)
{
  return (__m256i)((__v16hi)__a * (__v16hi)__b);
}

static __inline__  __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_mullo_epi32 (__m256i __a, __m256i __b)
{
  return (__m256i)((__v8si)__a * (__v8si)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_mul_epu32(__m256i __a, __m256i __b)
{
  return __builtin_ia32_pmuludq256((__v8si)__a, (__v8si)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_or_si256(__m256i __a, __m256i __b)
{
  return __a | __b;
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_sad_epu8(__m256i __a, __m256i __b)
{
  return __builtin_ia32_psadbw256((__v32qi)__a, (__v32qi)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_shuffle_epi8(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_ia32_pshufb256((__v32qi)__a, (__v32qi)__b);
}

#define _mm256_shuffle_epi32(a, imm) __extension__ ({ \
  __m256i __a = (a); \
  (__m256i)__builtin_shufflevector((__v8si)__a, (__v8si)_mm256_set1_epi32(0), \
                                   (imm) & 0x3, ((imm) & 0xc) >> 2, \
                                   ((imm) & 0x30) >> 4, ((imm) & 0xc0) >> 6, \
                                   4 + (((imm) & 0x03) >> 0), \
                                   4 + (((imm) & 0x0c) >> 2), \
                                   4 + (((imm) & 0x30) >> 4), \
                                   4 + (((imm) & 0xc0) >> 6)); })

#define _mm256_shufflehi_epi16(a, imm) __extension__ ({ \
  __m256i __a = (a); \
  (__m256i)__builtin_shufflevector((__v16hi)__a, (__v16hi)_mm256_set1_epi16(0), \
                                   0, 1, 2, 3, \
                                   4 + (((imm) & 0x03) >> 0), \
                                   4 + (((imm) & 0x0c) >> 2), \
                                   4 + (((imm) & 0x30) >> 4), \
                                   4 + (((imm) & 0xc0) >> 6), \
                                   8, 9, 10, 11, \
                                   12 + (((imm) & 0x03) >> 0), \
                                   12 + (((imm) & 0x0c) >> 2), \
                                   12 + (((imm) & 0x30) >> 4), \
                                   12 + (((imm) & 0xc0) >> 6)); })

#define _mm256_shufflelo_epi16(a, imm) __extension__ ({ \
  __m256i __a = (a); \
  (__m256i)__builtin_shufflevector((__v16hi)__a, (__v16hi)_mm256_set1_epi16(0), \
                                   (imm) & 0x3,((imm) & 0xc) >> 2, \
                                   ((imm) & 0x30) >> 4, ((imm) & 0xc0) >> 6, \
                                   4, 5, 6, 7, \
                                   8 + (((imm) & 0x03) >> 0), \
                                   8 + (((imm) & 0x0c) >> 2), \
                                   8 + (((imm) & 0x30) >> 4), \
                                   8 + (((imm) & 0xc0) >> 6), \
                                   12, 13, 14, 15); })

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_sign_epi8(__m256i __a, __m256i __b)
{
    return (__m256i)__builtin_ia32_psignb256((__v32qi)__a, (__v32qi)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_sign_epi16(__m256i __a, __m256i __b)
{
    return (__m256i)__builtin_ia32_psignw256((__v16hi)__a, (__v16hi)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_sign_epi32(__m256i __a, __m256i __b)
{
    return (__m256i)__builtin_ia32_psignd256((__v8si)__a, (__v8si)__b);
}

#define _mm256_slli_si256(a, count) __extension__ ({ \
  __m256i __a = (a); \
  (__m256i)__builtin_ia32_pslldqi256(__a, (count)*8); })

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_slli_epi16(__m256i __a, int __count)
{
  return (__m256i)__builtin_ia32_psllwi256((__v16hi)__a, __count);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_sll_epi16(__m256i __a, __m128i __count)
{
  return (__m256i)__builtin_ia32_psllw256((__v16hi)__a, (__v8hi)__count);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_slli_epi32(__m256i __a, int __count)
{
  return (__m256i)__builtin_ia32_pslldi256((__v8si)__a, __count);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_sll_epi32(__m256i __a, __m128i __count)
{
  return (__m256i)__builtin_ia32_pslld256((__v8si)__a, (__v4si)__count);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_slli_epi64(__m256i __a, int __count)
{
  return __builtin_ia32_psllqi256(__a, __count);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_sll_epi64(__m256i __a, __m128i __count)
{
  return __builtin_ia32_psllq256(__a, __count);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_srai_epi16(__m256i __a, int __count)
{
  return (__m256i)__builtin_ia32_psrawi256((__v16hi)__a, __count);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_sra_epi16(__m256i __a, __m128i __count)
{
  return (__m256i)__builtin_ia32_psraw256((__v16hi)__a, (__v8hi)__count);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_srai_epi32(__m256i __a, int __count)
{
  return (__m256i)__builtin_ia32_psradi256((__v8si)__a, __count);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_sra_epi32(__m256i __a, __m128i __count)
{
  return (__m256i)__builtin_ia32_psrad256((__v8si)__a, (__v4si)__count);
}

#define _mm256_srli_si256(a, count) __extension__ ({ \
  __m256i __a = (a); \
  (__m256i)__builtin_ia32_psrldqi256(__a, (count)*8); })

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_srli_epi16(__m256i __a, int __count)
{
  return (__m256i)__builtin_ia32_psrlwi256((__v16hi)__a, __count);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_srl_epi16(__m256i __a, __m128i __count)
{
  return (__m256i)__builtin_ia32_psrlw256((__v16hi)__a, (__v8hi)__count);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_srli_epi32(__m256i __a, int __count)
{
  return (__m256i)__builtin_ia32_psrldi256((__v8si)__a, __count);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_srl_epi32(__m256i __a, __m128i __count)
{
  return (__m256i)__builtin_ia32_psrld256((__v8si)__a, (__v4si)__count);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_srli_epi64(__m256i __a, int __count)
{
  return __builtin_ia32_psrlqi256(__a, __count);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_srl_epi64(__m256i __a, __m128i __count)
{
  return __builtin_ia32_psrlq256(__a, __count);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_sub_epi8(__m256i __a, __m256i __b)
{
  return (__m256i)((__v32qi)__a - (__v32qi)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_sub_epi16(__m256i __a, __m256i __b)
{
  return (__m256i)((__v16hi)__a - (__v16hi)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_sub_epi32(__m256i __a, __m256i __b)
{
  return (__m256i)((__v8si)__a - (__v8si)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_sub_epi64(__m256i __a, __m256i __b)
{
  return __a - __b;
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_subs_epi8(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_ia32_psubsb256((__v32qi)__a, (__v32qi)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_subs_epi16(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_ia32_psubsw256((__v16hi)__a, (__v16hi)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_subs_epu8(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_ia32_psubusb256((__v32qi)__a, (__v32qi)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_subs_epu16(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_ia32_psubusw256((__v16hi)__a, (__v16hi)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_unpackhi_epi8(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_shufflevector((__v32qi)__a, (__v32qi)__b, 8, 32+8, 9, 32+9, 10, 32+10, 11, 32+11, 12, 32+12, 13, 32+13, 14, 32+14, 15, 32+15, 24, 32+24, 25, 32+25, 26, 32+26, 27, 32+27, 28, 32+28, 29, 32+29, 30, 32+30, 31, 32+31);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_unpackhi_epi16(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_shufflevector((__v16hi)__a, (__v16hi)__b, 4, 16+4, 5, 16+5, 6, 16+6, 7, 16+7, 12, 16+12, 13, 16+13, 14, 16+14, 15, 16+15);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_unpackhi_epi32(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_shufflevector((__v8si)__a, (__v8si)__b, 2, 8+2, 3, 8+3, 6, 8+6, 7, 8+7);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_unpackhi_epi64(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_shufflevector(__a, __b, 1, 4+1, 3, 4+3);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_unpacklo_epi8(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_shufflevector((__v32qi)__a, (__v32qi)__b, 0, 32+0, 1, 32+1, 2, 32+2, 3, 32+3, 4, 32+4, 5, 32+5, 6, 32+6, 7, 32+7, 16, 32+16, 17, 32+17, 18, 32+18, 19, 32+19, 20, 32+20, 21, 32+21, 22, 32+22, 23, 32+23);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_unpacklo_epi16(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_shufflevector((__v16hi)__a, (__v16hi)__b, 0, 16+0, 1, 16+1, 2, 16+2, 3, 16+3, 8, 16+8, 9, 16+9, 10, 16+10, 11, 16+11);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_unpacklo_epi32(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_shufflevector((__v8si)__a, (__v8si)__b, 0, 8+0, 1, 8+1, 4, 8+4, 5, 8+5);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_unpacklo_epi64(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_shufflevector(__a, __b, 0, 4+0, 2, 4+2);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_xor_si256(__m256i __a, __m256i __b)
{
  return __a ^ __b;
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_stream_load_si256(__m256i *__V)
{
  return (__m256i)__builtin_ia32_movntdqa256((__v4di *)__V);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
_mm_broadcastss_ps(__m128 __X)
{
  return (__m128)__builtin_ia32_vbroadcastss_ps((__v4sf)__X);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__))
_mm256_broadcastss_ps(__m128 __X)
{
  return (__m256)__builtin_ia32_vbroadcastss_ps256((__v4sf)__X);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__))
_mm256_broadcastsd_pd(__m128d __X)
{
  return (__m256d)__builtin_ia32_vbroadcastsd_pd256((__v2df)__X);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_broadcastsi128_si256(__m128i __X)
{
  return (__m256i)__builtin_ia32_vbroadcastsi256(__X);
}

#define _mm_blend_epi32(V1, V2, M) __extension__ ({ \
  __m128i __V1 = (V1); \
  __m128i __V2 = (V2); \
  (__m128i)__builtin_shufflevector((__v4si)__V1, (__v4si)__V2, \
                                   (((M) & 0x01) ? 4 : 0), \
                                   (((M) & 0x02) ? 5 : 1), \
                                   (((M) & 0x04) ? 6 : 2), \
                                   (((M) & 0x08) ? 7 : 3)); })

#define _mm256_blend_epi32(V1, V2, M) __extension__ ({ \
  __m256i __V1 = (V1); \
  __m256i __V2 = (V2); \
  (__m256i)__builtin_shufflevector((__v8si)__V1, (__v8si)__V2, \
                                   (((M) & 0x01) ?  8 : 0), \
                                   (((M) & 0x02) ?  9 : 1), \
                                   (((M) & 0x04) ? 10 : 2), \
                                   (((M) & 0x08) ? 11 : 3), \
                                   (((M) & 0x10) ? 12 : 4), \
                                   (((M) & 0x20) ? 13 : 5), \
                                   (((M) & 0x40) ? 14 : 6), \
                                   (((M) & 0x80) ? 15 : 7)); })

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_broadcastb_epi8(__m128i __X)
{
  return (__m256i)__builtin_ia32_pbroadcastb256((__v16qi)__X);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_broadcastw_epi16(__m128i __X)
{
  return (__m256i)__builtin_ia32_pbroadcastw256((__v8hi)__X);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_broadcastd_epi32(__m128i __X)
{
  return (__m256i)__builtin_ia32_pbroadcastd256((__v4si)__X);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_broadcastq_epi64(__m128i __X)
{
  return (__m256i)__builtin_ia32_pbroadcastq256(__X);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_broadcastb_epi8(__m128i __X)
{
  return (__m128i)__builtin_ia32_pbroadcastb128((__v16qi)__X);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_broadcastw_epi16(__m128i __X)
{
  return (__m128i)__builtin_ia32_pbroadcastw128((__v8hi)__X);
}


static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_broadcastd_epi32(__m128i __X)
{
  return (__m128i)__builtin_ia32_pbroadcastd128((__v4si)__X);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_broadcastq_epi64(__m128i __X)
{
  return (__m128i)__builtin_ia32_pbroadcastq128(__X);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_permutevar8x32_epi32(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_ia32_permvarsi256((__v8si)__a, (__v8si)__b);
}

#define _mm256_permute4x64_pd(V, M) __extension__ ({ \
  __m256d __V = (V); \
  (__m256d)__builtin_shufflevector((__v4df)__V, (__v4df) _mm256_setzero_pd(), \
                                   (M) & 0x3, ((M) & 0xc) >> 2, \
                                   ((M) & 0x30) >> 4, ((M) & 0xc0) >> 6); })

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__))
_mm256_permutevar8x32_ps(__m256 __a, __m256 __b)
{
  return (__m256)__builtin_ia32_permvarsf256((__v8sf)__a, (__v8sf)__b);
}

#define _mm256_permute4x64_epi64(V, M) __extension__ ({ \
  __m256i __V = (V); \
  (__m256i)__builtin_shufflevector((__v4di)__V, (__v4di) _mm256_setzero_si256(), \
                                   (M) & 0x3, ((M) & 0xc) >> 2, \
                                   ((M) & 0x30) >> 4, ((M) & 0xc0) >> 6); })

#define _mm256_permute2x128_si256(V1, V2, M) __extension__ ({ \
  __m256i __V1 = (V1); \
  __m256i __V2 = (V2); \
  (__m256i)__builtin_ia32_permti256(__V1, __V2, (M)); })

#define _mm256_extracti128_si256(A, O) __extension__ ({ \
  __m256i __A = (A); \
  (__m128i)__builtin_ia32_extract128i256(__A, (O)); })

#define _mm256_inserti128_si256(V1, V2, O) __extension__ ({ \
  __m256i __V1 = (V1); \
  __m128i __V2 = (V2); \
  (__m256i)__builtin_ia32_insert128i256(__V1, __V2, (O)); })

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_maskload_epi32(int const *__X, __m256i __M)
{
  return (__m256i)__builtin_ia32_maskloadd256((const __v8si *)__X, (__v8si)__M);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_maskload_epi64(long long const *__X, __m256i __M)
{
  return (__m256i)__builtin_ia32_maskloadq256((const __v4di *)__X, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_maskload_epi32(int const *__X, __m128i __M)
{
  return (__m128i)__builtin_ia32_maskloadd((const __v4si *)__X, (__v4si)__M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_maskload_epi64(long long const *__X, __m128i __M)
{
  return (__m128i)__builtin_ia32_maskloadq((const __v2di *)__X, (__v2di)__M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__))
_mm256_maskstore_epi32(int *__X, __m256i __M, __m256i __Y)
{
  __builtin_ia32_maskstored256((__v8si *)__X, (__v8si)__M, (__v8si)__Y);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__))
_mm256_maskstore_epi64(long long *__X, __m256i __M, __m256i __Y)
{
  __builtin_ia32_maskstoreq256((__v4di *)__X, __M, __Y);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__))
_mm_maskstore_epi32(int *__X, __m128i __M, __m128i __Y)
{
  __builtin_ia32_maskstored((__v4si *)__X, (__v4si)__M, (__v4si)__Y);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__))
_mm_maskstore_epi64(long long *__X, __m128i __M, __m128i __Y)
{
  __builtin_ia32_maskstoreq(( __v2di *)__X, __M, __Y);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_sllv_epi32(__m256i __X, __m256i __Y)
{
  return (__m256i)__builtin_ia32_psllv8si((__v8si)__X, (__v8si)__Y);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_sllv_epi32(__m128i __X, __m128i __Y)
{
  return (__m128i)__builtin_ia32_psllv4si((__v4si)__X, (__v4si)__Y);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_sllv_epi64(__m256i __X, __m256i __Y)
{
  return (__m256i)__builtin_ia32_psllv4di(__X, __Y);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_sllv_epi64(__m128i __X, __m128i __Y)
{
  return (__m128i)__builtin_ia32_psllv2di(__X, __Y);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_srav_epi32(__m256i __X, __m256i __Y)
{
  return (__m256i)__builtin_ia32_psrav8si((__v8si)__X, (__v8si)__Y);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_srav_epi32(__m128i __X, __m128i __Y)
{
  return (__m128i)__builtin_ia32_psrav4si((__v4si)__X, (__v4si)__Y);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_srlv_epi32(__m256i __X, __m256i __Y)
{
  return (__m256i)__builtin_ia32_psrlv8si((__v8si)__X, (__v8si)__Y);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_srlv_epi32(__m128i __X, __m128i __Y)
{
  return (__m128i)__builtin_ia32_psrlv4si((__v4si)__X, (__v4si)__Y);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_srlv_epi64(__m256i __X, __m256i __Y)
{
  return (__m256i)__builtin_ia32_psrlv4di(__X, __Y);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_srlv_epi64(__m128i __X, __m128i __Y)
{
  return (__m128i)__builtin_ia32_psrlv2di(__X, __Y);
}

#define _mm_mask_i32gather_pd(a, m, i, mask, s) __extension__ ({ \
  __m128d __a = (a); \
  double const *__m = (m); \
  __m128i __i = (i); \
  __m128d __mask = (mask); \
  (__m128d)__builtin_ia32_gatherd_pd((__v2df)__a, (const __v2df *)__m, \
             (__v4si)__i, (__v2df)__mask, (s)); })

#define _mm256_mask_i32gather_pd(a, m, i, mask, s) __extension__ ({ \
  __m256d __a = (a); \
  double const *__m = (m); \
  __m128i __i = (i); \
  __m256d __mask = (mask); \
  (__m256d)__builtin_ia32_gatherd_pd256((__v4df)__a, (const __v4df *)__m, \
             (__v4si)__i, (__v4df)__mask, (s)); })

#define _mm_mask_i64gather_pd(a, m, i, mask, s) __extension__ ({ \
  __m128d __a = (a); \
  double const *__m = (m); \
  __m128i __i = (i); \
  __m128d __mask = (mask); \
  (__m128d)__builtin_ia32_gatherq_pd((__v2df)__a, (const __v2df *)__m, \
             (__v2di)__i, (__v2df)__mask, (s)); })

#define _mm256_mask_i64gather_pd(a, m, i, mask, s) __extension__ ({ \
  __m256d __a = (a); \
  double const *__m = (m); \
  __m256i __i = (i); \
  __m256d __mask = (mask); \
  (__m256d)__builtin_ia32_gatherq_pd256((__v4df)__a, (const __v4df *)__m, \
             (__v4di)__i, (__v4df)__mask, (s)); })

#define _mm_mask_i32gather_ps(a, m, i, mask, s) __extension__ ({ \
  __m128 __a = (a); \
  float const *__m = (m); \
  __m128i __i = (i); \
  __m128 __mask = (mask); \
  (__m128)__builtin_ia32_gatherd_ps((__v4sf)__a, (const __v4sf *)__m, \
            (__v4si)__i, (__v4sf)__mask, (s)); })

#define _mm256_mask_i32gather_ps(a, m, i, mask, s) __extension__ ({ \
  __m256 __a = (a); \
  float const *__m = (m); \
  __m256i __i = (i); \
  __m256 __mask = (mask); \
  (__m256)__builtin_ia32_gatherd_ps256((__v8sf)__a, (const __v8sf *)__m, \
            (__v8si)__i, (__v8sf)__mask, (s)); })

#define _mm_mask_i64gather_ps(a, m, i, mask, s) __extension__ ({ \
  __m128 __a = (a); \
  float const *__m = (m); \
  __m128i __i = (i); \
  __m128 __mask = (mask); \
  (__m128)__builtin_ia32_gatherq_ps((__v4sf)__a, (const __v4sf *)__m, \
            (__v2di)__i, (__v4sf)__mask, (s)); })

#define _mm256_mask_i64gather_ps(a, m, i, mask, s) __extension__ ({ \
  __m128 __a = (a); \
  float const *__m = (m); \
  __m256i __i = (i); \
  __m128 __mask = (mask); \
  (__m128)__builtin_ia32_gatherq_ps256((__v4sf)__a, (const __v4sf *)__m, \
            (__v4di)__i, (__v4sf)__mask, (s)); })

#define _mm_mask_i32gather_epi32(a, m, i, mask, s) __extension__ ({ \
  __m128i __a = (a); \
  int const *__m = (m); \
  __m128i __i = (i); \
  __m128i __mask = (mask); \
  (__m128i)__builtin_ia32_gatherd_d((__v4si)__a, (const __v4si *)__m, \
            (__v4si)__i, (__v4si)__mask, (s)); })

#define _mm256_mask_i32gather_epi32(a, m, i, mask, s) __extension__ ({ \
  __m256i __a = (a); \
  int const *__m = (m); \
  __m256i __i = (i); \
  __m256i __mask = (mask); \
  (__m256i)__builtin_ia32_gatherd_d256((__v8si)__a, (const __v8si *)__m, \
            (__v8si)__i, (__v8si)__mask, (s)); })

#define _mm_mask_i64gather_epi32(a, m, i, mask, s) __extension__ ({ \
  __m128i __a = (a); \
  int const *__m = (m); \
  __m128i __i = (i); \
  __m128i __mask = (mask); \
  (__m128i)__builtin_ia32_gatherq_d((__v4si)__a, (const __v4si *)__m, \
            (__v2di)__i, (__v4si)__mask, (s)); })

#define _mm256_mask_i64gather_epi32(a, m, i, mask, s) __extension__ ({ \
  __m128i __a = (a); \
  int const *__m = (m); \
  __m256i __i = (i); \
  __m128i __mask = (mask); \
  (__m128i)__builtin_ia32_gatherq_d256((__v4si)__a, (const __v4si *)__m, \
            (__v4di)__i, (__v4si)__mask, (s)); })

#define _mm_mask_i32gather_epi64(a, m, i, mask, s) __extension__ ({ \
  __m128i __a = (a); \
  long long const *__m = (m); \
  __m128i __i = (i); \
  __m128i __mask = (mask); \
  (__m128i)__builtin_ia32_gatherd_q((__v2di)__a, (const __v2di *)__m, \
             (__v4si)__i, (__v2di)__mask, (s)); })

#define _mm256_mask_i32gather_epi64(a, m, i, mask, s) __extension__ ({ \
  __m256i __a = (a); \
  long long const *__m = (m); \
  __m128i __i = (i); \
  __m256i __mask = (mask); \
  (__m256i)__builtin_ia32_gatherd_q256((__v4di)__a, (const __v4di *)__m, \
             (__v4si)__i, (__v4di)__mask, (s)); })

#define _mm_mask_i64gather_epi64(a, m, i, mask, s) __extension__ ({ \
  __m128i __a = (a); \
  long long const *__m = (m); \
  __m128i __i = (i); \
  __m128i __mask = (mask); \
  (__m128i)__builtin_ia32_gatherq_q((__v2di)__a, (const __v2di *)__m, \
             (__v2di)__i, (__v2di)__mask, (s)); })

#define _mm256_mask_i64gather_epi64(a, m, i, mask, s) __extension__ ({ \
  __m256i __a = (a); \
  long long const *__m = (m); \
  __m256i __i = (i); \
  __m256i __mask = (mask); \
  (__m256i)__builtin_ia32_gatherq_q256((__v4di)__a, (const __v4di *)__m, \
             (__v4di)__i, (__v4di)__mask, (s)); })

#define _mm_i32gather_pd(m, i, s) __extension__ ({ \
  double const *__m = (m); \
  __m128i __i = (i); \
  (__m128d)__builtin_ia32_gatherd_pd((__v2df)_mm_setzero_pd(), \
             (const __v2df *)__m, (__v4si)__i, \
             (__v2df)_mm_set1_pd((double)(long long int)-1), (s)); })

#define _mm256_i32gather_pd(m, i, s) __extension__ ({ \
  double const *__m = (m); \
  __m128i __i = (i); \
  (__m256d)__builtin_ia32_gatherd_pd256((__v4df)_mm256_setzero_pd(), \
             (const __v4df *)__m, (__v4si)__i, \
             (__v4df)_mm256_set1_pd((double)(long long int)-1), (s)); })

#define _mm_i64gather_pd(m, i, s) __extension__ ({ \
  double const *__m = (m); \
  __m128i __i = (i); \
  (__m128d)__builtin_ia32_gatherq_pd((__v2df)_mm_setzero_pd(), \
             (const __v2df *)__m, (__v2di)__i, \
             (__v2df)_mm_set1_pd((double)(long long int)-1), (s)); })

#define _mm256_i64gather_pd(m, i, s) __extension__ ({ \
  double const *__m = (m); \
  __m256i __i = (i); \
  (__m256d)__builtin_ia32_gatherq_pd256((__v4df)_mm256_setzero_pd(), \
             (const __v4df *)__m, (__v4di)__i, \
             (__v4df)_mm256_set1_pd((double)(long long int)-1), (s)); })

#define _mm_i32gather_ps(m, i, s) __extension__ ({ \
  float const *__m = (m); \
  __m128i __i = (i); \
  (__m128)__builtin_ia32_gatherd_ps((__v4sf)_mm_setzero_ps(), \
             (const __v4sf *)__m, (__v4si)__i, \
             (__v4sf)_mm_set1_ps((float)(int)-1), (s)); })

#define _mm256_i32gather_ps(m, i, s) __extension__ ({ \
  float const *__m = (m); \
  __m256i __i = (i); \
  (__m256)__builtin_ia32_gatherd_ps256((__v8sf)_mm256_setzero_ps(), \
             (const __v8sf *)__m, (__v8si)__i, \
             (__v8sf)_mm256_set1_ps((float)(int)-1), (s)); })

#define _mm_i64gather_ps(m, i, s) __extension__ ({ \
  float const *__m = (m); \
  __m128i __i = (i); \
  (__m128)__builtin_ia32_gatherq_ps((__v4sf)_mm_setzero_ps(), \
             (const __v4sf *)__m, (__v2di)__i, \
             (__v4sf)_mm_set1_ps((float)(int)-1), (s)); })

#define _mm256_i64gather_ps(m, i, s) __extension__ ({ \
  float const *__m = (m); \
  __m256i __i = (i); \
  (__m128)__builtin_ia32_gatherq_ps256((__v4sf)_mm_setzero_ps(), \
             (const __v4sf *)__m, (__v4di)__i, \
             (__v4sf)_mm_set1_ps((float)(int)-1), (s)); })

#define _mm_i32gather_epi32(m, i, s) __extension__ ({ \
  int const *__m = (m); \
  __m128i __i = (i); \
  (__m128i)__builtin_ia32_gatherd_d((__v4si)_mm_setzero_si128(), \
            (const __v4si *)__m, (__v4si)__i, \
            (__v4si)_mm_set1_epi32(-1), (s)); })

#define _mm256_i32gather_epi32(m, i, s) __extension__ ({ \
  int const *__m = (m); \
  __m256i __i = (i); \
  (__m256i)__builtin_ia32_gatherd_d256((__v8si)_mm256_setzero_si256(), \
            (const __v8si *)__m, (__v8si)__i, \
            (__v8si)_mm256_set1_epi32(-1), (s)); })

#define _mm_i64gather_epi32(m, i, s) __extension__ ({ \
  int const *__m = (m); \
  __m128i __i = (i); \
  (__m128i)__builtin_ia32_gatherq_d((__v4si)_mm_setzero_si128(), \
            (const __v4si *)__m, (__v2di)__i, \
            (__v4si)_mm_set1_epi32(-1), (s)); })

#define _mm256_i64gather_epi32(m, i, s) __extension__ ({ \
  int const *__m = (m); \
  __m256i __i = (i); \
  (__m128i)__builtin_ia32_gatherq_d256((__v4si)_mm_setzero_si128(), \
            (const __v4si *)__m, (__v4di)__i, \
            (__v4si)_mm_set1_epi32(-1), (s)); })

#define _mm_i32gather_epi64(m, i, s) __extension__ ({ \
  long long const *__m = (m); \
  __m128i __i = (i); \
  (__m128i)__builtin_ia32_gatherd_q((__v2di)_mm_setzero_si128(), \
             (const __v2di *)__m, (__v4si)__i, \
             (__v2di)_mm_set1_epi64x(-1), (s)); })

#define _mm256_i32gather_epi64(m, i, s) __extension__ ({ \
  long long const *__m = (m); \
  __m128i __i = (i); \
  (__m256i)__builtin_ia32_gatherd_q256((__v4di)_mm256_setzero_si256(), \
             (const __v4di *)__m, (__v4si)__i, \
             (__v4di)_mm256_set1_epi64x(-1), (s)); })

#define _mm_i64gather_epi64(m, i, s) __extension__ ({ \
  long long const *__m = (m); \
  __m128i __i = (i); \
  (__m128i)__builtin_ia32_gatherq_q((__v2di)_mm_setzero_si128(), \
             (const __v2di *)__m, (__v2di)__i, \
             (__v2di)_mm_set1_epi64x(-1), (s)); })

#define _mm256_i64gather_epi64(m, i, s) __extension__ ({ \
  long long const *__m = (m); \
  __m256i __i = (i); \
  (__m256i)__builtin_ia32_gatherq_q256((__v4di)_mm256_setzero_si256(), \
             (const __v4di *)__m, (__v4di)__i, \
             (__v4di)_mm256_set1_epi64x(-1), (s)); })

#endif /* __AVX2INTRIN_H */
                                                                                                                                                                                                                                                                                                                                                       usr/lib/llvm-3.5/lib/clang/3.5.0/include/avxintrin.h                                                0100644 0000000 0000000 00000116737 12474130047 020235  0                                                                                                    ustar 00                                                                0000000 0000000                                                                                                                                                                        /*===---- avxintrin.h - AVX intrinsics -------------------------------------===
 *
 * Permission is hereby granted, free of charge, to any person obtaining a copy
 * of this software and associated documentation files (the "Software"), to deal
 * in the Software without restriction, including without limitation the rights
 * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 * copies of the Software, and to permit persons to whom the Software is
 * furnished to do so, subject to the following conditions:
 *
 * The above copyright notice and this permission notice shall be included in
 * all copies or substantial portions of the Software.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
 * THE SOFTWARE.
 *
 *===-----------------------------------------------------------------------===
 */

#ifndef __IMMINTRIN_H
#error "Never use <avxintrin.h> directly; include <immintrin.h> instead."
#endif

#ifndef __AVXINTRIN_H
#define __AVXINTRIN_H

typedef double __v4df __attribute__ ((__vector_size__ (32)));
typedef float __v8sf __attribute__ ((__vector_size__ (32)));
typedef long long __v4di __attribute__ ((__vector_size__ (32)));
typedef int __v8si __attribute__ ((__vector_size__ (32)));
typedef short __v16hi __attribute__ ((__vector_size__ (32)));
typedef char __v32qi __attribute__ ((__vector_size__ (32)));

typedef float __m256 __attribute__ ((__vector_size__ (32)));
typedef double __m256d __attribute__((__vector_size__(32)));
typedef long long __m256i __attribute__((__vector_size__(32)));

/* Arithmetic */
static __inline __m256d __attribute__((__always_inline__, __nodebug__))
_mm256_add_pd(__m256d __a, __m256d __b)
{
  return __a+__b;
}

static __inline __m256 __attribute__((__always_inline__, __nodebug__))
_mm256_add_ps(__m256 __a, __m256 __b)
{
  return __a+__b;
}

static __inline __m256d __attribute__((__always_inline__, __nodebug__))
_mm256_sub_pd(__m256d __a, __m256d __b)
{
  return __a-__b;
}

static __inline __m256 __attribute__((__always_inline__, __nodebug__))
_mm256_sub_ps(__m256 __a, __m256 __b)
{
  return __a-__b;
}

static __inline __m256d __attribute__((__always_inline__, __nodebug__))
_mm256_addsub_pd(__m256d __a, __m256d __b)
{
  return (__m256d)__builtin_ia32_addsubpd256((__v4df)__a, (__v4df)__b);
}

static __inline __m256 __attribute__((__always_inline__, __nodebug__))
_mm256_addsub_ps(__m256 __a, __m256 __b)
{
  return (__m256)__builtin_ia32_addsubps256((__v8sf)__a, (__v8sf)__b);
}

static __inline __m256d __attribute__((__always_inline__, __nodebug__))
_mm256_div_pd(__m256d __a, __m256d __b)
{
  return __a / __b;
}

static __inline __m256 __attribute__((__always_inline__, __nodebug__))
_mm256_div_ps(__m256 __a, __m256 __b)
{
  return __a / __b;
}

static __inline __m256d __attribute__((__always_inline__, __nodebug__))
_mm256_max_pd(__m256d __a, __m256d __b)
{
  return (__m256d)__builtin_ia32_maxpd256((__v4df)__a, (__v4df)__b);
}

static __inline __m256 __attribute__((__always_inline__, __nodebug__))
_mm256_max_ps(__m256 __a, __m256 __b)
{
  return (__m256)__builtin_ia32_maxps256((__v8sf)__a, (__v8sf)__b);
}

static __inline __m256d __attribute__((__always_inline__, __nodebug__))
_mm256_min_pd(__m256d __a, __m256d __b)
{
  return (__m256d)__builtin_ia32_minpd256((__v4df)__a, (__v4df)__b);
}

static __inline __m256 __attribute__((__always_inline__, __nodebug__))
_mm256_min_ps(__m256 __a, __m256 __b)
{
  return (__m256)__builtin_ia32_minps256((__v8sf)__a, (__v8sf)__b);
}

static __inline __m256d __attribute__((__always_inline__, __nodebug__))
_mm256_mul_pd(__m256d __a, __m256d __b)
{
  return __a * __b;
}

static __inline __m256 __attribute__((__always_inline__, __nodebug__))
_mm256_mul_ps(__m256 __a, __m256 __b)
{
  return __a * __b;
}

static __inline __m256d __attribute__((__always_inline__, __nodebug__))
_mm256_sqrt_pd(__m256d __a)
{
  return (__m256d)__builtin_ia32_sqrtpd256((__v4df)__a);
}

static __inline __m256 __attribute__((__always_inline__, __nodebug__))
_mm256_sqrt_ps(__m256 __a)
{
  return (__m256)__builtin_ia32_sqrtps256((__v8sf)__a);
}

static __inline __m256 __attribute__((__always_inline__, __nodebug__))
_mm256_rsqrt_ps(__m256 __a)
{
  return (__m256)__builtin_ia32_rsqrtps256((__v8sf)__a);
}

static __inline __m256 __attribute__((__always_inline__, __nodebug__))
_mm256_rcp_ps(__m256 __a)
{
  return (__m256)__builtin_ia32_rcpps256((__v8sf)__a);
}

#define _mm256_round_pd(V, M) __extension__ ({ \
    __m256d __V = (V); \
    (__m256d)__builtin_ia32_roundpd256((__v4df)__V, (M)); })

#define _mm256_round_ps(V, M) __extension__ ({ \
  __m256 __V = (V); \
  (__m256)__builtin_ia32_roundps256((__v8sf)__V, (M)); })

#define _mm256_ceil_pd(V)  _mm256_round_pd((V), _MM_FROUND_CEIL)
#define _mm256_floor_pd(V) _mm256_round_pd((V), _MM_FROUND_FLOOR)
#define _mm256_ceil_ps(V)  _mm256_round_ps((V), _MM_FROUND_CEIL)
#define _mm256_floor_ps(V) _mm256_round_ps((V), _MM_FROUND_FLOOR)

/* Logical */
static __inline __m256d __attribute__((__always_inline__, __nodebug__))
_mm256_and_pd(__m256d __a, __m256d __b)
{
  return (__m256d)((__v4di)__a & (__v4di)__b);
}

static __inline __m256 __attribute__((__always_inline__, __nodebug__))
_mm256_and_ps(__m256 __a, __m256 __b)
{
  return (__m256)((__v8si)__a & (__v8si)__b);
}

static __inline __m256d __attribute__((__always_inline__, __nodebug__))
_mm256_andnot_pd(__m256d __a, __m256d __b)
{
  return (__m256d)(~(__v4di)__a & (__v4di)__b);
}

static __inline __m256 __attribute__((__always_inline__, __nodebug__))
_mm256_andnot_ps(__m256 __a, __m256 __b)
{
  return (__m256)(~(__v8si)__a & (__v8si)__b);
}

static __inline __m256d __attribute__((__always_inline__, __nodebug__))
_mm256_or_pd(__m256d __a, __m256d __b)
{
  return (__m256d)((__v4di)__a | (__v4di)__b);
}

static __inline __m256 __attribute__((__always_inline__, __nodebug__))
_mm256_or_ps(__m256 __a, __m256 __b)
{
  return (__m256)((__v8si)__a | (__v8si)__b);
}

static __inline __m256d __attribute__((__always_inline__, __nodebug__))
_mm256_xor_pd(__m256d __a, __m256d __b)
{
  return (__m256d)((__v4di)__a ^ (__v4di)__b);
}

static __inline __m256 __attribute__((__always_inline__, __nodebug__))
_mm256_xor_ps(__m256 __a, __m256 __b)
{
  return (__m256)((__v8si)__a ^ (__v8si)__b);
}

/* Horizontal arithmetic */
static __inline __m256d __attribute__((__always_inline__, __nodebug__))
_mm256_hadd_pd(__m256d __a, __m256d __b)
{
  return (__m256d)__builtin_ia32_haddpd256((__v4df)__a, (__v4df)__b);
}

static __inline __m256 __attribute__((__always_inline__, __nodebug__))
_mm256_hadd_ps(__m256 __a, __m256 __b)
{
  return (__m256)__builtin_ia32_haddps256((__v8sf)__a, (__v8sf)__b);
}

static __inline __m256d __attribute__((__always_inline__, __nodebug__))
_mm256_hsub_pd(__m256d __a, __m256d __b)
{
  return (__m256d)__builtin_ia32_hsubpd256((__v4df)__a, (__v4df)__b);
}

static __inline __m256 __attribute__((__always_inline__, __nodebug__))
_mm256_hsub_ps(__m256 __a, __m256 __b)
{
  return (__m256)__builtin_ia32_hsubps256((__v8sf)__a, (__v8sf)__b);
}

/* Vector permutations */
static __inline __m128d __attribute__((__always_inline__, __nodebug__))
_mm_permutevar_pd(__m128d __a, __m128i __c)
{
  return (__m128d)__builtin_ia32_vpermilvarpd((__v2df)__a, (__v2di)__c);
}

static __inline __m256d __attribute__((__always_inline__, __nodebug__))
_mm256_permutevar_pd(__m256d __a, __m256i __c)
{
  return (__m256d)__builtin_ia32_vpermilvarpd256((__v4df)__a, (__v4di)__c);
}

static __inline __m128 __attribute__((__always_inline__, __nodebug__))
_mm_permutevar_ps(__m128 __a, __m128i __c)
{
  return (__m128)__builtin_ia32_vpermilvarps((__v4sf)__a, (__v4si)__c);
}

static __inline __m256 __attribute__((__always_inline__, __nodebug__))
_mm256_permutevar_ps(__m256 __a, __m256i __c)
{
  return (__m256)__builtin_ia32_vpermilvarps256((__v8sf)__a,
						  (__v8si)__c);
}

#define _mm_permute_pd(A, C) __extension__ ({ \
  __m128d __A = (A); \
  (__m128d)__builtin_shufflevector((__v2df)__A, (__v2df) _mm_setzero_pd(), \
                                   (C) & 0x1, ((C) & 0x2) >> 1); })

#define _mm256_permute_pd(A, C) __extension__ ({ \
  __m256d __A = (A); \
  (__m256d)__builtin_shufflevector((__v4df)__A, (__v4df) _mm256_setzero_pd(), \
                                   (C) & 0x1, ((C) & 0x2) >> 1, \
                                   2 + (((C) & 0x4) >> 2), \
                                   2 + (((C) & 0x8) >> 3)); })

#define _mm_permute_ps(A, C) __extension__ ({ \
  __m128 __A = (A); \
  (__m128)__builtin_shufflevector((__v4sf)__A, (__v4sf) _mm_setzero_ps(), \
                                   (C) & 0x3, ((C) & 0xc) >> 2, \
                                   ((C) & 0x30) >> 4, ((C) & 0xc0) >> 6); })

#define _mm256_permute_ps(A, C) __extension__ ({ \
  __m256 __A = (A); \
  (__m256)__builtin_shufflevector((__v8sf)__A, (__v8sf) _mm256_setzero_ps(), \
                                  (C) & 0x3, ((C) & 0xc) >> 2, \
                                  ((C) & 0x30) >> 4, ((C) & 0xc0) >> 6, \
                                  4 + (((C) & 0x03) >> 0), \
                                  4 + (((C) & 0x0c) >> 2), \
                                  4 + (((C) & 0x30) >> 4), \
                                  4 + (((C) & 0xc0) >> 6)); })

#define _mm256_permute2f128_pd(V1, V2, M) __extension__ ({ \
  __m256d __V1 = (V1); \
  __m256d __V2 = (V2); \
  (__m256d)__builtin_ia32_vperm2f128_pd256((__v4df)__V1, (__v4df)__V2, (M)); })

#define _mm256_permute2f128_ps(V1, V2, M) __extension__ ({ \
  __m256 __V1 = (V1); \
  __m256 __V2 = (V2); \
  (__m256)__builtin_ia32_vperm2f128_ps256((__v8sf)__V1, (__v8sf)__V2, (M)); })

#define _mm256_permute2f128_si256(V1, V2, M) __extension__ ({ \
  __m256i __V1 = (V1); \
  __m256i __V2 = (V2); \
  (__m256i)__builtin_ia32_vperm2f128_si256((__v8si)__V1, (__v8si)__V2, (M)); })

/* Vector Blend */
#define _mm256_blend_pd(V1, V2, M) __extension__ ({ \
  __m256d __V1 = (V1); \
  __m256d __V2 = (V2); \
  (__m256d)__builtin_shufflevector((__v4df)__V1, (__v4df)__V2, \
                                   (((M) & 0x01) ? 4 : 0), \
                                   (((M) & 0x02) ? 5 : 1), \
                                   (((M) & 0x04) ? 6 : 2), \
                                   (((M) & 0x08) ? 7 : 3)); })

#define _mm256_blend_ps(V1, V2, M) __extension__ ({ \
  __m256 __V1 = (V1); \
  __m256 __V2 = (V2); \
  (__m256)__builtin_shufflevector((__v8sf)__V1, (__v8sf)__V2, \
                                  (((M) & 0x01) ?  8 : 0), \
                                  (((M) & 0x02) ?  9 : 1), \
                                  (((M) & 0x04) ? 10 : 2), \
                                  (((M) & 0x08) ? 11 : 3), \
                                  (((M) & 0x10) ? 12 : 4), \
                                  (((M) & 0x20) ? 13 : 5), \
                                  (((M) & 0x40) ? 14 : 6), \
                                  (((M) & 0x80) ? 15 : 7)); })

static __inline __m256d __attribute__((__always_inline__, __nodebug__))
_mm256_blendv_pd(__m256d __a, __m256d __b, __m256d __c)
{
  return (__m256d)__builtin_ia32_blendvpd256(
    (__v4df)__a, (__v4df)__b, (__v4df)__c);
}

static __inline __m256 __attribute__((__always_inline__, __nodebug__))
_mm256_blendv_ps(__m256 __a, __m256 __b, __m256 __c)
{
  return (__m256)__builtin_ia32_blendvps256(
    (__v8sf)__a, (__v8sf)__b, (__v8sf)__c);
}

/* Vector Dot Product */
#define _mm256_dp_ps(V1, V2, M) __extension__ ({ \
  __m256 __V1 = (V1); \
  __m256 __V2 = (V2); \
  (__m256)__builtin_ia32_dpps256((__v8sf)__V1, (__v8sf)__V2, (M)); })

/* Vector shuffle */
#define _mm256_shuffle_ps(a, b, mask) __extension__ ({ \
        __m256 __a = (a); \
        __m256 __b = (b); \
        (__m256)__builtin_shufflevector((__v8sf)__a, (__v8sf)__b, \
        (mask) & 0x3,                ((mask) & 0xc) >> 2, \
        (((mask) & 0x30) >> 4) + 8,  (((mask) & 0xc0) >> 6) + 8, \
        ((mask) & 0x3) + 4,          (((mask) & 0xc) >> 2) + 4, \
        (((mask) & 0x30) >> 4) + 12, (((mask) & 0xc0) >> 6) + 12); })

#define _mm256_shuffle_pd(a, b, mask) __extension__ ({ \
        __m256d __a = (a); \
        __m256d __b = (b); \
        (__m256d)__builtin_shufflevector((__v4df)__a, (__v4df)__b, \
        (mask) & 0x1, \
        (((mask) & 0x2) >> 1) + 4, \
        (((mask) & 0x4) >> 2) + 2, \
        (((mask) & 0x8) >> 3) + 6); })

/* Compare */
#define _CMP_EQ_OQ    0x00 /* Equal (ordered, non-signaling)  */
#define _CMP_LT_OS    0x01 /* Less-than (ordered, signaling)  */
#define _CMP_LE_OS    0x02 /* Less-than-or-equal (ordered, signaling)  */
#define _CMP_UNORD_Q  0x03 /* Unordered (non-signaling)  */
#define _CMP_NEQ_UQ   0x04 /* Not-equal (unordered, non-signaling)  */
#define _CMP_NLT_US   0x05 /* Not-less-than (unordered, signaling)  */
#define _CMP_NLE_US   0x06 /* Not-less-than-or-equal (unordered, signaling)  */
#define _CMP_ORD_Q    0x07 /* Ordered (nonsignaling)   */
#define _CMP_EQ_UQ    0x08 /* Equal (unordered, non-signaling)  */
#define _CMP_NGE_US   0x09 /* Not-greater-than-or-equal (unord, signaling)  */
#define _CMP_NGT_US   0x0a /* Not-greater-than (unordered, signaling)  */
#define _CMP_FALSE_OQ 0x0b /* False (ordered, non-signaling)  */
#define _CMP_NEQ_OQ   0x0c /* Not-equal (ordered, non-signaling)  */
#define _CMP_GE_OS    0x0d /* Greater-than-or-equal (ordered, signaling)  */
#define _CMP_GT_OS    0x0e /* Greater-than (ordered, signaling)  */
#define _CMP_TRUE_UQ  0x0f /* True (unordered, non-signaling)  */
#define _CMP_EQ_OS    0x10 /* Equal (ordered, signaling)  */
#define _CMP_LT_OQ    0x11 /* Less-than (ordered, non-signaling)  */
#define _CMP_LE_OQ    0x12 /* Less-than-or-equal (ordered, non-signaling)  */
#define _CMP_UNORD_S  0x13 /* Unordered (signaling)  */
#define _CMP_NEQ_US   0x14 /* Not-equal (unordered, signaling)  */
#define _CMP_NLT_UQ   0x15 /* Not-less-than (unordered, non-signaling)  */
#define _CMP_NLE_UQ   0x16 /* Not-less-than-or-equal (unord, non-signaling)  */
#define _CMP_ORD_S    0x17 /* Ordered (signaling)  */
#define _CMP_EQ_US    0x18 /* Equal (unordered, signaling)  */
#define _CMP_NGE_UQ   0x19 /* Not-greater-than-or-equal (unord, non-sign)  */
#define _CMP_NGT_UQ   0x1a /* Not-greater-than (unordered, non-signaling)  */
#define _CMP_FALSE_OS 0x1b /* False (ordered, signaling)  */
#define _CMP_NEQ_OS   0x1c /* Not-equal (ordered, signaling)  */
#define _CMP_GE_OQ    0x1d /* Greater-than-or-equal (ordered, non-signaling)  */
#define _CMP_GT_OQ    0x1e /* Greater-than (ordered, non-signaling)  */
#define _CMP_TRUE_US  0x1f /* True (unordered, signaling)  */

#define _mm_cmp_pd(a, b, c) __extension__ ({ \
  __m128d __a = (a); \
  __m128d __b = (b); \
  (__m128d)__builtin_ia32_cmppd((__v2df)__a, (__v2df)__b, (c)); })

#define _mm_cmp_ps(a, b, c) __extension__ ({ \
  __m128 __a = (a); \
  __m128 __b = (b); \
  (__m128)__builtin_ia32_cmpps((__v4sf)__a, (__v4sf)__b, (c)); })

#define _mm256_cmp_pd(a, b, c) __extension__ ({ \
  __m256d __a = (a); \
  __m256d __b = (b); \
  (__m256d)__builtin_ia32_cmppd256((__v4df)__a, (__v4df)__b, (c)); })

#define _mm256_cmp_ps(a, b, c) __extension__ ({ \
  __m256 __a = (a); \
  __m256 __b = (b); \
  (__m256)__builtin_ia32_cmpps256((__v8sf)__a, (__v8sf)__b, (c)); })

#define _mm_cmp_sd(a, b, c) __extension__ ({ \
  __m128d __a = (a); \
  __m128d __b = (b); \
  (__m128d)__builtin_ia32_cmpsd((__v2df)__a, (__v2df)__b, (c)); })

#define _mm_cmp_ss(a, b, c) __extension__ ({ \
  __m128 __a = (a); \
  __m128 __b = (b); \
  (__m128)__builtin_ia32_cmpss((__v4sf)__a, (__v4sf)__b, (c)); })

/* Vector extract */
#define _mm256_extractf128_pd(A, O) __extension__ ({ \
  __m256d __A = (A); \
  (__m128d)__builtin_ia32_vextractf128_pd256((__v4df)__A, (O)); })

#define _mm256_extractf128_ps(A, O) __extension__ ({ \
  __m256 __A = (A); \
  (__m128)__builtin_ia32_vextractf128_ps256((__v8sf)__A, (O)); })

#define _mm256_extractf128_si256(A, O) __extension__ ({ \
  __m256i __A = (A); \
  (__m128i)__builtin_ia32_vextractf128_si256((__v8si)__A, (O)); })

static __inline int __attribute__((__always_inline__, __nodebug__))
_mm256_extract_epi32(__m256i __a, int const __imm)
{
  __v8si __b = (__v8si)__a;
  return __b[__imm & 7];
}

static __inline int __attribute__((__always_inline__, __nodebug__))
_mm256_extract_epi16(__m256i __a, int const __imm)
{
  __v16hi __b = (__v16hi)__a;
  return __b[__imm & 15];
}

static __inline int __attribute__((__always_inline__, __nodebug__))
_mm256_extract_epi8(__m256i __a, int const __imm)
{
  __v32qi __b = (__v32qi)__a;
  return __b[__imm & 31];
}

#ifdef __x86_64__
static __inline long long  __attribute__((__always_inline__, __nodebug__))
_mm256_extract_epi64(__m256i __a, const int __imm)
{
  __v4di __b = (__v4di)__a;
  return __b[__imm & 3];
}
#endif

/* Vector insert */
#define _mm256_insertf128_pd(V1, V2, O) __extension__ ({ \
  __m256d __V1 = (V1); \
  __m128d __V2 = (V2); \
  (__m256d)__builtin_ia32_vinsertf128_pd256((__v4df)__V1, (__v2df)__V2, (O)); })

#define _mm256_insertf128_ps(V1, V2, O) __extension__ ({ \
  __m256 __V1 = (V1); \
  __m128 __V2 = (V2); \
  (__m256)__builtin_ia32_vinsertf128_ps256((__v8sf)__V1, (__v4sf)__V2, (O)); })

#define _mm256_insertf128_si256(V1, V2, O) __extension__ ({ \
  __m256i __V1 = (V1); \
  __m128i __V2 = (V2); \
  (__m256i)__builtin_ia32_vinsertf128_si256((__v8si)__V1, (__v4si)__V2, (O)); })

static __inline __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_insert_epi32(__m256i __a, int __b, int const __imm)
{
  __v8si __c = (__v8si)__a;
  __c[__imm & 7] = __b;
  return (__m256i)__c;
}

static __inline __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_insert_epi16(__m256i __a, int __b, int const __imm)
{
  __v16hi __c = (__v16hi)__a;
  __c[__imm & 15] = __b;
  return (__m256i)__c;
}

static __inline __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_insert_epi8(__m256i __a, int __b, int const __imm)
{
  __v32qi __c = (__v32qi)__a;
  __c[__imm & 31] = __b;
  return (__m256i)__c;
}

#ifdef __x86_64__
static __inline __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_insert_epi64(__m256i __a, int __b, int const __imm)
{
  __v4di __c = (__v4di)__a;
  __c[__imm & 3] = __b;
  return (__m256i)__c;
}
#endif

/* Conversion */
static __inline __m256d __attribute__((__always_inline__, __nodebug__))
_mm256_cvtepi32_pd(__m128i __a)
{
  return (__m256d)__builtin_ia32_cvtdq2pd256((__v4si) __a);
}

static __inline __m256 __attribute__((__always_inline__, __nodebug__))
_mm256_cvtepi32_ps(__m256i __a)
{
  return (__m256)__builtin_ia32_cvtdq2ps256((__v8si) __a);
}

static __inline __m128 __attribute__((__always_inline__, __nodebug__))
_mm256_cvtpd_ps(__m256d __a)
{
  return (__m128)__builtin_ia32_cvtpd2ps256((__v4df) __a);
}

static __inline __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_cvtps_epi32(__m256 __a)
{
  return (__m256i)__builtin_ia32_cvtps2dq256((__v8sf) __a);
}

static __inline __m256d __attribute__((__always_inline__, __nodebug__))
_mm256_cvtps_pd(__m128 __a)
{
  return (__m256d)__builtin_ia32_cvtps2pd256((__v4sf) __a);
}

static __inline __m128i __attribute__((__always_inline__, __nodebug__))
_mm256_cvttpd_epi32(__m256d __a)
{
  return (__m128i)__builtin_ia32_cvttpd2dq256((__v4df) __a);
}

static __inline __m128i __attribute__((__always_inline__, __nodebug__))
_mm256_cvtpd_epi32(__m256d __a)
{
  return (__m128i)__builtin_ia32_cvtpd2dq256((__v4df) __a);
}

static __inline __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_cvttps_epi32(__m256 __a)
{
  return (__m256i)__builtin_ia32_cvttps2dq256((__v8sf) __a);
}

/* Vector replicate */
static __inline __m256 __attribute__((__always_inline__, __nodebug__))
_mm256_movehdup_ps(__m256 __a)
{
  return __builtin_shufflevector(__a, __a, 1, 1, 3, 3, 5, 5, 7, 7);
}

static __inline __m256 __attribute__((__always_inline__, __nodebug__))
_mm256_moveldup_ps(__m256 __a)
{
  return __builtin_shufflevector(__a, __a, 0, 0, 2, 2, 4, 4, 6, 6);
}

static __inline __m256d __attribute__((__always_inline__, __nodebug__))
_mm256_movedup_pd(__m256d __a)
{
  return __builtin_shufflevector(__a, __a, 0, 0, 2, 2);
}

/* Unpack and Interleave */
static __inline __m256d __attribute__((__always_inline__, __nodebug__))
_mm256_unpackhi_pd(__m256d __a, __m256d __b)
{
  return __builtin_shufflevector(__a, __b, 1, 5, 1+2, 5+2);
}

static __inline __m256d __attribute__((__always_inline__, __nodebug__))
_mm256_unpacklo_pd(__m256d __a, __m256d __b)
{
  return __builtin_shufflevector(__a, __b, 0, 4, 0+2, 4+2);
}

static __inline __m256 __attribute__((__always_inline__, __nodebug__))
_mm256_unpackhi_ps(__m256 __a, __m256 __b)
{
  return __builtin_shufflevector(__a, __b, 2, 10, 2+1, 10+1, 6, 14, 6+1, 14+1);
}

static __inline __m256 __attribute__((__always_inline__, __nodebug__))
_mm256_unpacklo_ps(__m256 __a, __m256 __b)
{
  return __builtin_shufflevector(__a, __b, 0, 8, 0+1, 8+1, 4, 12, 4+1, 12+1);
}

/* Bit Test */
static __inline int __attribute__((__always_inline__, __nodebug__))
_mm_testz_pd(__m128d __a, __m128d __b)
{
  return __builtin_ia32_vtestzpd((__v2df)__a, (__v2df)__b);
}

static __inline int __attribute__((__always_inline__, __nodebug__))
_mm_testc_pd(__m128d __a, __m128d __b)
{
  return __builtin_ia32_vtestcpd((__v2df)__a, (__v2df)__b);
}

static __inline int __attribute__((__always_inline__, __nodebug__))
_mm_testnzc_pd(__m128d __a, __m128d __b)
{
  return __builtin_ia32_vtestnzcpd((__v2df)__a, (__v2df)__b);
}

static __inline int __attribute__((__always_inline__, __nodebug__))
_mm_testz_ps(__m128 __a, __m128 __b)
{
  return __builtin_ia32_vtestzps((__v4sf)__a, (__v4sf)__b);
}

static __inline int __attribute__((__always_inline__, __nodebug__))
_mm_testc_ps(__m128 __a, __m128 __b)
{
  return __builtin_ia32_vtestcps((__v4sf)__a, (__v4sf)__b);
}

static __inline int __attribute__((__always_inline__, __nodebug__))
_mm_testnzc_ps(__m128 __a, __m128 __b)
{
  return __builtin_ia32_vtestnzcps((__v4sf)__a, (__v4sf)__b);
}

static __inline int __attribute__((__always_inline__, __nodebug__))
_mm256_testz_pd(__m256d __a, __m256d __b)
{
  return __builtin_ia32_vtestzpd256((__v4df)__a, (__v4df)__b);
}

static __inline int __attribute__((__always_inline__, __nodebug__))
_mm256_testc_pd(__m256d __a, __m256d __b)
{
  return __builtin_ia32_vtestcpd256((__v4df)__a, (__v4df)__b);
}

static __inline int __attribute__((__always_inline__, __nodebug__))
_mm256_testnzc_pd(__m256d __a, __m256d __b)
{
  return __builtin_ia32_vtestnzcpd256((__v4df)__a, (__v4df)__b);
}

static __inline int __attribute__((__always_inline__, __nodebug__))
_mm256_testz_ps(__m256 __a, __m256 __b)
{
  return __builtin_ia32_vtestzps256((__v8sf)__a, (__v8sf)__b);
}

static __inline int __attribute__((__always_inline__, __nodebug__))
_mm256_testc_ps(__m256 __a, __m256 __b)
{
  return __builtin_ia32_vtestcps256((__v8sf)__a, (__v8sf)__b);
}

static __inline int __attribute__((__always_inline__, __nodebug__))
_mm256_testnzc_ps(__m256 __a, __m256 __b)
{
  return __builtin_ia32_vtestnzcps256((__v8sf)__a, (__v8sf)__b);
}

static __inline int __attribute__((__always_inline__, __nodebug__))
_mm256_testz_si256(__m256i __a, __m256i __b)
{
  return __builtin_ia32_ptestz256((__v4di)__a, (__v4di)__b);
}

static __inline int __attribute__((__always_inline__, __nodebug__))
_mm256_testc_si256(__m256i __a, __m256i __b)
{
  return __builtin_ia32_ptestc256((__v4di)__a, (__v4di)__b);
}

static __inline int __attribute__((__always_inline__, __nodebug__))
_mm256_testnzc_si256(__m256i __a, __m256i __b)
{
  return __builtin_ia32_ptestnzc256((__v4di)__a, (__v4di)__b);
}

/* Vector extract sign mask */
static __inline int __attribute__((__always_inline__, __nodebug__))
_mm256_movemask_pd(__m256d __a)
{
  return __builtin_ia32_movmskpd256((__v4df)__a);
}

static __inline int __attribute__((__always_inline__, __nodebug__))
_mm256_movemask_ps(__m256 __a)
{
  return __builtin_ia32_movmskps256((__v8sf)__a);
}

/* Vector __zero */
static __inline void __attribute__((__always_inline__, __nodebug__))
_mm256_zeroall(void)
{
  __builtin_ia32_vzeroall();
}

static __inline void __attribute__((__always_inline__, __nodebug__))
_mm256_zeroupper(void)
{
  __builtin_ia32_vzeroupper();
}

/* Vector load with broadcast */
static __inline __m128 __attribute__((__always_inline__, __nodebug__))
_mm_broadcast_ss(float const *__a)
{
  float __f = *__a;
  return (__m128)(__v4sf){ __f, __f, __f, __f };
}

static __inline __m256d __attribute__((__always_inline__, __nodebug__))
_mm256_broadcast_sd(double const *__a)
{
  double __d = *__a;
  return (__m256d)(__v4df){ __d, __d, __d, __d };
}

static __inline __m256 __attribute__((__always_inline__, __nodebug__))
_mm256_broadcast_ss(float const *__a)
{
  float __f = *__a;
  return (__m256)(__v8sf){ __f, __f, __f, __f, __f, __f, __f, __f };
}

static __inline __m256d __attribute__((__always_inline__, __nodebug__))
_mm256_broadcast_pd(__m128d const *__a)
{
  return (__m256d)__builtin_ia32_vbroadcastf128_pd256(__a);
}

static __inline __m256 __attribute__((__always_inline__, __nodebug__))
_mm256_broadcast_ps(__m128 const *__a)
{
  return (__m256)__builtin_ia32_vbroadcastf128_ps256(__a);
}

/* SIMD load ops */
static __inline __m256d __attribute__((__always_inline__, __nodebug__))
_mm256_load_pd(double const *__p)
{
  return *(__m256d *)__p;
}

static __inline __m256 __attribute__((__always_inline__, __nodebug__))
_mm256_load_ps(float const *__p)
{
  return *(__m256 *)__p;
}

static __inline __m256d __attribute__((__always_inline__, __nodebug__))
_mm256_loadu_pd(double const *__p)
{
  struct __loadu_pd {
    __m256d __v;
  } __attribute__((packed, may_alias));
  return ((struct __loadu_pd*)__p)->__v;
}

static __inline __m256 __attribute__((__always_inline__, __nodebug__))
_mm256_loadu_ps(float const *__p)
{
  struct __loadu_ps {
    __m256 __v;
  } __attribute__((packed, may_alias));
  return ((struct __loadu_ps*)__p)->__v;
}

static __inline __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_load_si256(__m256i const *__p)
{
  return *__p;
}

static __inline __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_loadu_si256(__m256i const *__p)
{
  struct __loadu_si256 {
    __m256i __v;
  } __attribute__((packed, may_alias));
  return ((struct __loadu_si256*)__p)->__v;
}

static __inline __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_lddqu_si256(__m256i const *__p)
{
  return (__m256i)__builtin_ia32_lddqu256((char const *)__p);
}

/* SIMD store ops */
static __inline void __attribute__((__always_inline__, __nodebug__))
_mm256_store_pd(double *__p, __m256d __a)
{
  *(__m256d *)__p = __a;
}

static __inline void __attribute__((__always_inline__, __nodebug__))
_mm256_store_ps(float *__p, __m256 __a)
{
  *(__m256 *)__p = __a;
}

static __inline void __attribute__((__always_inline__, __nodebug__))
_mm256_storeu_pd(double *__p, __m256d __a)
{
  __builtin_ia32_storeupd256(__p, (__v4df)__a);
}

static __inline void __attribute__((__always_inline__, __nodebug__))
_mm256_storeu_ps(float *__p, __m256 __a)
{
  __builtin_ia32_storeups256(__p, (__v8sf)__a);
}

static __inline void __attribute__((__always_inline__, __nodebug__))
_mm256_store_si256(__m256i *__p, __m256i __a)
{
  *__p = __a;
}

static __inline void __attribute__((__always_inline__, __nodebug__))
_mm256_storeu_si256(__m256i *__p, __m256i __a)
{
  __builtin_ia32_storedqu256((char *)__p, (__v32qi)__a);
}

/* Conditional load ops */
static __inline __m128d __attribute__((__always_inline__, __nodebug__))
_mm_maskload_pd(double const *__p, __m128d __m)
{
  return (__m128d)__builtin_ia32_maskloadpd((const __v2df *)__p, (__v2df)__m);
}

static __inline __m256d __attribute__((__always_inline__, __nodebug__))
_mm256_maskload_pd(double const *__p, __m256d __m)
{
  return (__m256d)__builtin_ia32_maskloadpd256((const __v4df *)__p,
                                               (__v4df)__m);
}

static __inline __m128 __attribute__((__always_inline__, __nodebug__))
_mm_maskload_ps(float const *__p, __m128 __m)
{
  return (__m128)__builtin_ia32_maskloadps((const __v4sf *)__p, (__v4sf)__m);
}

static __inline __m256 __attribute__((__always_inline__, __nodebug__))
_mm256_maskload_ps(float const *__p, __m256 __m)
{
  return (__m256)__builtin_ia32_maskloadps256((const __v8sf *)__p, (__v8sf)__m);
}

/* Conditional store ops */
static __inline void __attribute__((__always_inline__, __nodebug__))
_mm256_maskstore_ps(float *__p, __m256 __m, __m256 __a)
{
  __builtin_ia32_maskstoreps256((__v8sf *)__p, (__v8sf)__m, (__v8sf)__a);
}

static __inline void __attribute__((__always_inline__, __nodebug__))
_mm_maskstore_pd(double *__p, __m128d __m, __m128d __a)
{
  __builtin_ia32_maskstorepd((__v2df *)__p, (__v2df)__m, (__v2df)__a);
}

static __inline void __attribute__((__always_inline__, __nodebug__))
_mm256_maskstore_pd(double *__p, __m256d __m, __m256d __a)
{
  __builtin_ia32_maskstorepd256((__v4df *)__p, (__v4df)__m, (__v4df)__a);
}

static __inline void __attribute__((__always_inline__, __nodebug__))
_mm_maskstore_ps(float *__p, __m128 __m, __m128 __a)
{
  __builtin_ia32_maskstoreps((__v4sf *)__p, (__v4sf)__m, (__v4sf)__a);
}

/* Cacheability support ops */
static __inline void __attribute__((__always_inline__, __nodebug__))
_mm256_stream_si256(__m256i *__a, __m256i __b)
{
  __builtin_ia32_movntdq256((__v4di *)__a, (__v4di)__b);
}

static __inline void __attribute__((__always_inline__, __nodebug__))
_mm256_stream_pd(double *__a, __m256d __b)
{
  __builtin_ia32_movntpd256(__a, (__v4df)__b);
}

static __inline void __attribute__((__always_inline__, __nodebug__))
_mm256_stream_ps(float *__p, __m256 __a)
{
  __builtin_ia32_movntps256(__p, (__v8sf)__a);
}

/* Create vectors */
static __inline __m256d __attribute__((__always_inline__, __nodebug__))
_mm256_set_pd(double __a, double __b, double __c, double __d)
{
  return (__m256d){ __d, __c, __b, __a };
}

static __inline __m256 __attribute__((__always_inline__, __nodebug__))
_mm256_set_ps(float __a, float __b, float __c, float __d,
	            float __e, float __f, float __g, float __h)
{
  return (__m256){ __h, __g, __f, __e, __d, __c, __b, __a };
}

static __inline __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_set_epi32(int __i0, int __i1, int __i2, int __i3,
		             int __i4, int __i5, int __i6, int __i7)
{
  return (__m256i)(__v8si){ __i7, __i6, __i5, __i4, __i3, __i2, __i1, __i0 };
}

static __inline __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_set_epi16(short __w15, short __w14, short __w13, short __w12,
		             short __w11, short __w10, short __w09, short __w08,
		             short __w07, short __w06, short __w05, short __w04,
		             short __w03, short __w02, short __w01, short __w00)
{
  return (__m256i)(__v16hi){ __w00, __w01, __w02, __w03, __w04, __w05, __w06,
    __w07, __w08, __w09, __w10, __w11, __w12, __w13, __w14, __w15 };
}

static __inline __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_set_epi8(char __b31, char __b30, char __b29, char __b28,
		            char __b27, char __b26, char __b25, char __b24,
		            char __b23, char __b22, char __b21, char __b20,
		            char __b19, char __b18, char __b17, char __b16,
		            char __b15, char __b14, char __b13, char __b12,
		            char __b11, char __b10, char __b09, char __b08,
		            char __b07, char __b06, char __b05, char __b04,
		            char __b03, char __b02, char __b01, char __b00)
{
  return (__m256i)(__v32qi){
    __b00, __b01, __b02, __b03, __b04, __b05, __b06, __b07,
    __b08, __b09, __b10, __b11, __b12, __b13, __b14, __b15,
    __b16, __b17, __b18, __b19, __b20, __b21, __b22, __b23,
    __b24, __b25, __b26, __b27, __b28, __b29, __b30, __b31
  };
}

static __inline __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_set_epi64x(long long __a, long long __b, long long __c, long long __d)
{
  return (__m256i)(__v4di){ __d, __c, __b, __a };
}

/* Create vectors with elements in reverse order */
static __inline __m256d __attribute__((__always_inline__, __nodebug__))
_mm256_setr_pd(double __a, double __b, double __c, double __d)
{
  return (__m256d){ __a, __b, __c, __d };
}

static __inline __m256 __attribute__((__always_inline__, __nodebug__))
_mm256_setr_ps(float __a, float __b, float __c, float __d,
		           float __e, float __f, float __g, float __h)
{
  return (__m256){ __a, __b, __c, __d, __e, __f, __g, __h };
}

static __inline __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_setr_epi32(int __i0, int __i1, int __i2, int __i3,
		              int __i4, int __i5, int __i6, int __i7)
{
  return (__m256i)(__v8si){ __i0, __i1, __i2, __i3, __i4, __i5, __i6, __i7 };
}

static __inline __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_setr_epi16(short __w15, short __w14, short __w13, short __w12,
		   short __w11, short __w10, short __w09, short __w08,
		   short __w07, short __w06, short __w05, short __w04,
		   short __w03, short __w02, short __w01, short __w00)
{
  return (__m256i)(__v16hi){ __w15, __w14, __w13, __w12, __w11, __w10, __w09,
    __w08, __w07, __w06, __w05, __w04, __w03, __w02, __w01, __w00 };
}

static __inline __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_setr_epi8(char __b31, char __b30, char __b29, char __b28,
		             char __b27, char __b26, char __b25, char __b24,
		             char __b23, char __b22, char __b21, char __b20,
		             char __b19, char __b18, char __b17, char __b16,
		             char __b15, char __b14, char __b13, char __b12,
		             char __b11, char __b10, char __b09, char __b08,
		             char __b07, char __b06, char __b05, char __b04,
		             char __b03, char __b02, char __b01, char __b00)
{
  return (__m256i)(__v32qi){
    __b31, __b30, __b29, __b28, __b27, __b26, __b25, __b24,
		__b23, __b22, __b21, __b20, __b19, __b18, __b17, __b16,
		__b15, __b14, __b13, __b12, __b11, __b10, __b09, __b08,
		__b07, __b06, __b05, __b04, __b03, __b02, __b01, __b00 };
}

static __inline __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_setr_epi64x(long long __a, long long __b, long long __c, long long __d)
{
  return (__m256i)(__v4di){ __a, __b, __c, __d };
}

/* Create vectors with repeated elements */
static __inline __m256d __attribute__((__always_inline__, __nodebug__))
_mm256_set1_pd(double __w)
{
  return (__m256d){ __w, __w, __w, __w };
}

static __inline __m256 __attribute__((__always_inline__, __nodebug__))
_mm256_set1_ps(float __w)
{
  return (__m256){ __w, __w, __w, __w, __w, __w, __w, __w };
}

static __inline __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_set1_epi32(int __i)
{
  return (__m256i)(__v8si){ __i, __i, __i, __i, __i, __i, __i, __i };
}

static __inline __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_set1_epi16(short __w)
{
  return (__m256i)(__v16hi){ __w, __w, __w, __w, __w, __w, __w, __w, __w, __w,
    __w, __w, __w, __w, __w, __w };
}

static __inline __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_set1_epi8(char __b)
{
  return (__m256i)(__v32qi){ __b, __b, __b, __b, __b, __b, __b, __b, __b, __b,
    __b, __b, __b, __b, __b, __b, __b, __b, __b, __b, __b, __b, __b, __b, __b,
    __b, __b, __b, __b, __b, __b, __b };
}

static __inline __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_set1_epi64x(long long __q)
{
  return (__m256i)(__v4di){ __q, __q, __q, __q };
}

/* Create __zeroed vectors */
static __inline __m256d __attribute__((__always_inline__, __nodebug__))
_mm256_setzero_pd(void)
{
  return (__m256d){ 0, 0, 0, 0 };
}

static __inline __m256 __attribute__((__always_inline__, __nodebug__))
_mm256_setzero_ps(void)
{
  return (__m256){ 0, 0, 0, 0, 0, 0, 0, 0 };
}

static __inline __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_setzero_si256(void)
{
  return (__m256i){ 0LL, 0LL, 0LL, 0LL };
}

/* Cast between vector types */
static __inline __m256 __attribute__((__always_inline__, __nodebug__))
_mm256_castpd_ps(__m256d __a)
{
  return (__m256)__a;
}

static __inline __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_castpd_si256(__m256d __a)
{
  return (__m256i)__a;
}

static __inline __m256d __attribute__((__always_inline__, __nodebug__))
_mm256_castps_pd(__m256 __a)
{
  return (__m256d)__a;
}

static __inline __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_castps_si256(__m256 __a)
{
  return (__m256i)__a;
}

static __inline __m256 __attribute__((__always_inline__, __nodebug__))
_mm256_castsi256_ps(__m256i __a)
{
  return (__m256)__a;
}

static __inline __m256d __attribute__((__always_inline__, __nodebug__))
_mm256_castsi256_pd(__m256i __a)
{
  return (__m256d)__a;
}

static __inline __m128d __attribute__((__always_inline__, __nodebug__))
_mm256_castpd256_pd128(__m256d __a)
{
  return __builtin_shufflevector(__a, __a, 0, 1);
}

static __inline __m128 __attribute__((__always_inline__, __nodebug__))
_mm256_castps256_ps128(__m256 __a)
{
  return __builtin_shufflevector(__a, __a, 0, 1, 2, 3);
}

static __inline __m128i __attribute__((__always_inline__, __nodebug__))
_mm256_castsi256_si128(__m256i __a)
{
  return __builtin_shufflevector(__a, __a, 0, 1);
}

static __inline __m256d __attribute__((__always_inline__, __nodebug__))
_mm256_castpd128_pd256(__m128d __a)
{
  return __builtin_shufflevector(__a, __a, 0, 1, -1, -1);
}

static __inline __m256 __attribute__((__always_inline__, __nodebug__))
_mm256_castps128_ps256(__m128 __a)
{
  return __builtin_shufflevector(__a, __a, 0, 1, 2, 3, -1, -1, -1, -1);
}

static __inline __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_castsi128_si256(__m128i __a)
{
  return __builtin_shufflevector(__a, __a, 0, 1, -1, -1);
}

/* SIMD load ops (unaligned) */
static __inline __m256 __attribute__((__always_inline__, __nodebug__))
_mm256_loadu2_m128(float const *__addr_hi, float const *__addr_lo)
{
  struct __loadu_ps {
    __m128 __v;
  } __attribute__((__packed__, __may_alias__));

  __m256 __v256 = _mm256_castps128_ps256(((struct __loadu_ps*)__addr_lo)->__v);
  return _mm256_insertf128_ps(__v256, ((struct __loadu_ps*)__addr_hi)->__v, 1);
}

static __inline __m256d __attribute__((__always_inline__, __nodebug__))
_mm256_loadu2_m128d(double const *__addr_hi, double const *__addr_lo)
{
  struct __loadu_pd {
    __m128d __v;
  } __attribute__((__packed__, __may_alias__));
  
  __m256d __v256 = _mm256_castpd128_pd256(((struct __loadu_pd*)__addr_lo)->__v);
  return _mm256_insertf128_pd(__v256, ((struct __loadu_pd*)__addr_hi)->__v, 1);
}

static __inline __m256i __attribute__((__always_inline__, __nodebug__))
_mm256_loadu2_m128i(__m128i const *__addr_hi, __m128i const *__addr_lo)
{
  struct __loadu_si128 {
    __m128i __v;
  } __attribute__((packed, may_alias));
  __m256i __v256 = _mm256_castsi128_si256(
    ((struct __loadu_si128*)__addr_lo)->__v);
  return _mm256_insertf128_si256(__v256,
                                 ((struct __loadu_si128*)__addr_hi)->__v, 1);
}

/* SIMD store ops (unaligned) */
static __inline void __attribute__((__always_inline__, __nodebug__))
_mm256_storeu2_m128(float *__addr_hi, float *__addr_lo, __m256 __a)
{
  __m128 __v128;

  __v128 = _mm256_castps256_ps128(__a);
  __builtin_ia32_storeups(__addr_lo, __v128);
  __v128 = _mm256_extractf128_ps(__a, 1);
  __builtin_ia32_storeups(__addr_hi, __v128);
}

static __inline void __attribute__((__always_inline__, __nodebug__))
_mm256_storeu2_m128d(double *__addr_hi, double *__addr_lo, __m256d __a)
{
  __m128d __v128;

  __v128 = _mm256_castpd256_pd128(__a);
  __builtin_ia32_storeupd(__addr_lo, __v128);
  __v128 = _mm256_extractf128_pd(__a, 1);
  __builtin_ia32_storeupd(__addr_hi, __v128);
}

static __inline void __attribute__((__always_inline__, __nodebug__))
_mm256_storeu2_m128i(__m128i *__addr_hi, __m128i *__addr_lo, __m256i __a)
{
  __m128i __v128;

  __v128 = _mm256_castsi256_si128(__a);
  __builtin_ia32_storedqu((char *)__addr_lo, (__v16qi)__v128);
  __v128 = _mm256_extractf128_si256(__a, 1);
  __builtin_ia32_storedqu((char *)__addr_hi, (__v16qi)__v128);
}

#endif /* __AVXINTRIN_H */
                                 usr/lib/llvm-3.5/lib/clang/3.5.0/include/bmi2intrin.h                                               0100644 0000000 0000000 00000006345 12474130047 020261  0                                                                                                    ustar 00                                                                0000000 0000000                                                                                                                                                                        /*===---- bmi2intrin.h - BMI2 intrinsics -----------------------------------===
 *
 * Permission is hereby granted, free of charge, to any person obtaining a copy
 * of this software and associated documentation files (the "Software"), to deal
 * in the Software without restriction, including without limitation the rights
 * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 * copies of the Software, and to permit persons to whom the Software is
 * furnished to do so, subject to the following conditions:
 *
 * The above copyright notice and this permission notice shall be included in
 * all copies or substantial portions of the Software.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
 * THE SOFTWARE.
 *
 *===-----------------------------------------------------------------------===
 */

#if !defined __X86INTRIN_H && !defined __IMMINTRIN_H
#error "Never use <bmi2intrin.h> directly; include <x86intrin.h> instead."
#endif

#ifndef __BMI2__
# error "BMI2 instruction set not enabled"
#endif /* __BMI2__ */

#ifndef __BMI2INTRIN_H
#define __BMI2INTRIN_H

static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__))
_bzhi_u32(unsigned int __X, unsigned int __Y)
{
  return __builtin_ia32_bzhi_si(__X, __Y);
}

static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__))
_pdep_u32(unsigned int __X, unsigned int __Y)
{
  return __builtin_ia32_pdep_si(__X, __Y);
}

static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__))
_pext_u32(unsigned int __X, unsigned int __Y)
{
  return __builtin_ia32_pext_si(__X, __Y);
}

#ifdef  __x86_64__

static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__))
_bzhi_u64(unsigned long long __X, unsigned long long __Y)
{
  return __builtin_ia32_bzhi_di(__X, __Y);
}

static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__))
_pdep_u64(unsigned long long __X, unsigned long long __Y)
{
  return __builtin_ia32_pdep_di(__X, __Y);
}

static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__))
_pext_u64(unsigned long long __X, unsigned long long __Y)
{
  return __builtin_ia32_pext_di(__X, __Y);
}

static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__))
_mulx_u64 (unsigned long long __X, unsigned long long __Y,
	   unsigned long long *__P)
{
  unsigned __int128 __res = (unsigned __int128) __X * __Y;
  *__P = (unsigned long long) (__res >> 64);
  return (unsigned long long) __res;
}

#else /* !__x86_64__ */

static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__))
_mulx_u32 (unsigned int __X, unsigned int __Y, unsigned int *__P)
{
  unsigned long long __res = (unsigned long long) __X * __Y;
  *__P = (unsigned int) (__res >> 32);
  return (unsigned int) __res;
}

#endif /* !__x86_64__  */

#endif /* __BMI2INTRIN_H */
                                                                                                                                                                                                                                                                                           usr/lib/llvm-3.5/lib/clang/3.5.0/include/bmiintrin.h                                                0100644 0000000 0000000 00000011414 12474130047 020170  0                                                                                                    ustar 00                                                                0000000 0000000                                                                                                                                                                        /*===---- bmiintrin.h - BMI intrinsics -------------------------------------===
 *
 * Permission is hereby granted, free of charge, to any person obtaining a copy
 * of this software and associated documentation files (the "Software"), to deal
 * in the Software without restriction, including without limitation the rights
 * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 * copies of the Software, and to permit persons to whom the Software is
 * furnished to do so, subject to the following conditions:
 *
 * The above copyright notice and this permission notice shall be included in
 * all copies or substantial portions of the Software.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
 * THE SOFTWARE.
 *
 *===-----------------------------------------------------------------------===
 */

#if !defined __X86INTRIN_H && !defined __IMMINTRIN_H
#error "Never use <bmiintrin.h> directly; include <x86intrin.h> instead."
#endif

#ifndef __BMI__
# error "BMI instruction set not enabled"
#endif /* __BMI__ */

#ifndef __BMIINTRIN_H
#define __BMIINTRIN_H

#define _tzcnt_u16(a)     (__tzcnt_u16((a)))
#define _andn_u32(a, b)   (__andn_u32((a), (b)))
/* _bextr_u32 != __bextr_u32 */
#define _blsi_u32(a)      (__blsi_u32((a)))
#define _blsmsk_u32(a)    (__blsmsk_u32((a)))
#define _blsr_u32(a)      (__blsr_u32((a)))
#define _tzcnt_u32(a)     (__tzcnt_u32((a)))

static __inline__ unsigned short __attribute__((__always_inline__, __nodebug__))
__tzcnt_u16(unsigned short __X)
{
  return __builtin_ctzs(__X);
}

static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__))
__andn_u32(unsigned int __X, unsigned int __Y)
{
  return ~__X & __Y;
}

/* AMD-specified, double-leading-underscore version of BEXTR */
static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__))
__bextr_u32(unsigned int __X, unsigned int __Y)
{
  return __builtin_ia32_bextr_u32(__X, __Y);
}

/* Intel-specified, single-leading-underscore version of BEXTR */
static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__))
_bextr_u32(unsigned int __X, unsigned int __Y, unsigned int __Z)
{
  return __builtin_ia32_bextr_u32 (__X, ((__Y & 0xff) | ((__Z & 0xff) << 8)));
}

static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__))
__blsi_u32(unsigned int __X)
{
  return __X & -__X;
}

static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__))
__blsmsk_u32(unsigned int __X)
{
  return __X ^ (__X - 1);
}

static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__))
__blsr_u32(unsigned int __X)
{
  return __X & (__X - 1);
}

static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__))
__tzcnt_u32(unsigned int __X)
{
  return __builtin_ctz(__X);
}

#ifdef __x86_64__

#define _andn_u64(a, b)   (__andn_u64((a), (b)))
/* _bextr_u64 != __bextr_u64 */
#define _blsi_u64(a)      (__blsi_u64((a)))
#define _blsmsk_u64(a)    (__blsmsk_u64((a)))
#define _blsr_u64(a)      (__blsr_u64((a)))
#define _tzcnt_u64(a)     (__tzcnt_u64((a)))

static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__))
__andn_u64 (unsigned long long __X, unsigned long long __Y)
{
  return ~__X & __Y;
}

/* AMD-specified, double-leading-underscore version of BEXTR */
static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__))
__bextr_u64(unsigned long long __X, unsigned long long __Y)
{
  return __builtin_ia32_bextr_u64(__X, __Y);
}

/* Intel-specified, single-leading-underscore version of BEXTR */
static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__))
_bextr_u64(unsigned long long __X, unsigned int __Y, unsigned int __Z)
{
  return __builtin_ia32_bextr_u64 (__X, ((__Y & 0xff) | ((__Z & 0xff) << 8)));
}

static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__))
__blsi_u64(unsigned long long __X)
{
  return __X & -__X;
}

static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__))
__blsmsk_u64(unsigned long long __X)
{
  return __X ^ (__X - 1);
}

static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__))
__blsr_u64(unsigned long long __X)
{
  return __X & (__X - 1);
}

static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__))
__tzcnt_u64(unsigned long long __X)
{
  return __builtin_ctzll(__X);
}

#endif /* __x86_64__ */

#endif /* __BMIINTRIN_H */
                                                                                                                                                                                                                                                    usr/lib/llvm-3.5/lib/clang/3.5.0/include/cpuid.h                                                    0100644 0000000 0000000 00000012740 12474130047 017304  0                                                                                                    ustar 00                                                                0000000 0000000                                                                                                                                                                        /*===---- cpuid.h - X86 cpu model detection --------------------------------===
 *
 * Permission is hereby granted, free of charge, to any person obtaining a copy
 * of this software and associated documentation files (the "Software"), to deal
 * in the Software without restriction, including without limitation the rights
 * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 * copies of the Software, and to permit persons to whom the Software is
 * furnished to do so, subject to the following conditions:
 *
 * The above copyright notice and this permission notice shall be included in
 * all copies or substantial portions of the Software.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
 * THE SOFTWARE.
 *
 *===-----------------------------------------------------------------------===
 */

#if !(__x86_64__ || __i386__)
#error this header is for x86 only
#endif

/* Features in %ecx for level 1 */
#define bit_SSE3        0x00000001
#define bit_PCLMULQDQ   0x00000002
#define bit_DTES64      0x00000004
#define bit_MONITOR     0x00000008
#define bit_DSCPL       0x00000010
#define bit_VMX         0x00000020
#define bit_SMX         0x00000040
#define bit_EIST        0x00000080
#define bit_TM2         0x00000100
#define bit_SSSE3       0x00000200
#define bit_CNXTID      0x00000400
#define bit_FMA         0x00001000
#define bit_CMPXCHG16B  0x00002000
#define bit_xTPR        0x00004000
#define bit_PDCM        0x00008000
#define bit_PCID        0x00020000
#define bit_DCA         0x00040000
#define bit_SSE41       0x00080000
#define bit_SSE42       0x00100000
#define bit_x2APIC      0x00200000
#define bit_MOVBE       0x00400000
#define bit_POPCNT      0x00800000
#define bit_TSCDeadline 0x01000000
#define bit_AESNI       0x02000000
#define bit_XSAVE       0x04000000
#define bit_OSXSAVE     0x08000000
#define bit_AVX         0x10000000
#define bit_RDRAND      0x40000000

/* Features in %edx for level 1 */
#define bit_FPU         0x00000001
#define bit_VME         0x00000002
#define bit_DE          0x00000004
#define bit_PSE         0x00000008
#define bit_TSC         0x00000010
#define bit_MSR         0x00000020
#define bit_PAE         0x00000040
#define bit_MCE         0x00000080
#define bit_CX8         0x00000100
#define bit_APIC        0x00000200
#define bit_SEP         0x00000800
#define bit_MTRR        0x00001000
#define bit_PGE         0x00002000
#define bit_MCA         0x00004000
#define bit_CMOV        0x00008000
#define bit_PAT         0x00010000
#define bit_PSE36       0x00020000
#define bit_PSN         0x00040000
#define bit_CLFSH       0x00080000
#define bit_DS          0x00200000
#define bit_ACPI        0x00400000
#define bit_MMX         0x00800000
#define bit_FXSR        0x01000000
#define bit_FXSAVE      bit_FXSR    /* for gcc compat */
#define bit_SSE         0x02000000
#define bit_SSE2        0x04000000
#define bit_SS          0x08000000
#define bit_HTT         0x10000000
#define bit_TM          0x20000000
#define bit_PBE         0x80000000

/* Features in %ebx for level 7 sub-leaf 0 */
#define bit_FSGSBASE    0x00000001
#define bit_SMEP        0x00000080
#define bit_ENH_MOVSB   0x00000200

/* PIC on i386 uses %ebx, so preserve it. */
#if __i386__
#define __cpuid(__level, __eax, __ebx, __ecx, __edx) \
    __asm("  pushl  %%ebx\n" \
          "  cpuid\n" \
          "  mov    %%ebx,%1\n" \
          "  popl   %%ebx" \
        : "=a"(__eax), "=r" (__ebx), "=c"(__ecx), "=d"(__edx) \
        : "0"(__level))

#define __cpuid_count(__level, __count, __eax, __ebx, __ecx, __edx) \
    __asm("  pushl  %%ebx\n" \
          "  cpuid\n" \
          "  mov    %%ebx,%1\n" \
          "  popl   %%ebx" \
        : "=a"(__eax), "=r" (__ebx), "=c"(__ecx), "=d"(__edx) \
        : "0"(__level), "2"(__count))
#else
#define __cpuid(__level, __eax, __ebx, __ecx, __edx) \
    __asm("cpuid" : "=a"(__eax), "=b" (__ebx), "=c"(__ecx), "=d"(__edx) \
                  : "0"(__level))

#define __cpuid_count(__level, __count, __eax, __ebx, __ecx, __edx) \
    __asm("cpuid" : "=a"(__eax), "=b" (__ebx), "=c"(__ecx), "=d"(__edx) \
                  : "0"(__level), "2"(__count))
#endif

static __inline int __get_cpuid (unsigned int __level, unsigned int *__eax,
                                 unsigned int *__ebx, unsigned int *__ecx,
                                 unsigned int *__edx) {
    __cpuid(__level, *__eax, *__ebx, *__ecx, *__edx);
    return 1;
}

static __inline int __get_cpuid_max (unsigned int __level, unsigned int *__sig)
{
    unsigned int __eax, __ebx, __ecx, __edx;
#if __i386__
    int __cpuid_supported;

    __asm("  pushfl\n"
          "  popl   %%eax\n"
          "  movl   %%eax,%%ecx\n"
          "  xorl   $0x00200000,%%eax\n"
          "  pushl  %%eax\n"
          "  popfl\n"
          "  pushfl\n"
          "  popl   %%eax\n"
          "  movl   $0,%0\n"
          "  cmpl   %%eax,%%ecx\n"
          "  je     1f\n"
          "  movl   $1,%0\n"
          "1:"
        : "=r" (__cpuid_supported) : : "eax", "ecx");
    if (!__cpuid_supported)
        return 0;
#endif

    __cpuid(__level, __eax, __ebx, __ecx, __edx);
    if (__sig)
        *__sig = __ebx;
    return __eax;
}
                                usr/lib/llvm-3.5/lib/clang/3.5.0/include/emmintrin.h                                                0100644 0000000 0000000 00000121607 12474130047 020205  0                                                                                                    ustar 00                                                                0000000 0000000                                                                                                                                                                        /*===---- emmintrin.h - SSE2 intrinsics ------------------------------------===
 *
 * Permission is hereby granted, free of charge, to any person obtaining a copy
 * of this software and associated documentation files (the "Software"), to deal
 * in the Software without restriction, including without limitation the rights
 * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 * copies of the Software, and to permit persons to whom the Software is
 * furnished to do so, subject to the following conditions:
 *
 * The above copyright notice and this permission notice shall be included in
 * all copies or substantial portions of the Software.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
 * THE SOFTWARE.
 *
 *===-----------------------------------------------------------------------===
 */

#ifndef __EMMINTRIN_H
#define __EMMINTRIN_H

#ifndef __SSE2__
#error "SSE2 instruction set not enabled"
#else

#include <xmmintrin.h>

typedef double __m128d __attribute__((__vector_size__(16)));
typedef long long __m128i __attribute__((__vector_size__(16)));

/* Type defines.  */
typedef double __v2df __attribute__ ((__vector_size__ (16)));
typedef long long __v2di __attribute__ ((__vector_size__ (16)));
typedef short __v8hi __attribute__((__vector_size__(16)));
typedef char __v16qi __attribute__((__vector_size__(16)));

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
_mm_add_sd(__m128d __a, __m128d __b)
{
  __a[0] += __b[0];
  return __a;
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
_mm_add_pd(__m128d __a, __m128d __b)
{
  return __a + __b;
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
_mm_sub_sd(__m128d __a, __m128d __b)
{
  __a[0] -= __b[0];
  return __a;
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
_mm_sub_pd(__m128d __a, __m128d __b)
{
  return __a - __b;
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
_mm_mul_sd(__m128d __a, __m128d __b)
{
  __a[0] *= __b[0];
  return __a;
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
_mm_mul_pd(__m128d __a, __m128d __b)
{
  return __a * __b;
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
_mm_div_sd(__m128d __a, __m128d __b)
{
  __a[0] /= __b[0];
  return __a;
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
_mm_div_pd(__m128d __a, __m128d __b)
{
  return __a / __b;
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
_mm_sqrt_sd(__m128d __a, __m128d __b)
{
  __m128d __c = __builtin_ia32_sqrtsd(__b);
  return (__m128d) { __c[0], __a[1] };
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
_mm_sqrt_pd(__m128d __a)
{
  return __builtin_ia32_sqrtpd(__a);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
_mm_min_sd(__m128d __a, __m128d __b)
{
  return __builtin_ia32_minsd(__a, __b);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
_mm_min_pd(__m128d __a, __m128d __b)
{
  return __builtin_ia32_minpd(__a, __b);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
_mm_max_sd(__m128d __a, __m128d __b)
{
  return __builtin_ia32_maxsd(__a, __b);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
_mm_max_pd(__m128d __a, __m128d __b)
{
  return __builtin_ia32_maxpd(__a, __b);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
_mm_and_pd(__m128d __a, __m128d __b)
{
  return (__m128d)((__v4si)__a & (__v4si)__b);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
_mm_andnot_pd(__m128d __a, __m128d __b)
{
  return (__m128d)(~(__v4si)__a & (__v4si)__b);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
_mm_or_pd(__m128d __a, __m128d __b)
{
  return (__m128d)((__v4si)__a | (__v4si)__b);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
_mm_xor_pd(__m128d __a, __m128d __b)
{
  return (__m128d)((__v4si)__a ^ (__v4si)__b);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
_mm_cmpeq_pd(__m128d __a, __m128d __b)
{
  return (__m128d)__builtin_ia32_cmppd(__a, __b, 0);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
_mm_cmplt_pd(__m128d __a, __m128d __b)
{
  return (__m128d)__builtin_ia32_cmppd(__a, __b, 1);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
_mm_cmple_pd(__m128d __a, __m128d __b)
{
  return (__m128d)__builtin_ia32_cmppd(__a, __b, 2);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
_mm_cmpgt_pd(__m128d __a, __m128d __b)
{
  return (__m128d)__builtin_ia32_cmppd(__b, __a, 1);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
_mm_cmpge_pd(__m128d __a, __m128d __b)
{
  return (__m128d)__builtin_ia32_cmppd(__b, __a, 2);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
_mm_cmpord_pd(__m128d __a, __m128d __b)
{
  return (__m128d)__builtin_ia32_cmppd(__a, __b, 7);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
_mm_cmpunord_pd(__m128d __a, __m128d __b)
{
  return (__m128d)__builtin_ia32_cmppd(__a, __b, 3);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
_mm_cmpneq_pd(__m128d __a, __m128d __b)
{
  return (__m128d)__builtin_ia32_cmppd(__a, __b, 4);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
_mm_cmpnlt_pd(__m128d __a, __m128d __b)
{
  return (__m128d)__builtin_ia32_cmppd(__a, __b, 5);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
_mm_cmpnle_pd(__m128d __a, __m128d __b)
{
  return (__m128d)__builtin_ia32_cmppd(__a, __b, 6);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
_mm_cmpngt_pd(__m128d __a, __m128d __b)
{
  return (__m128d)__builtin_ia32_cmppd(__b, __a, 5);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
_mm_cmpnge_pd(__m128d __a, __m128d __b)
{
  return (__m128d)__builtin_ia32_cmppd(__b, __a, 6);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
_mm_cmpeq_sd(__m128d __a, __m128d __b)
{
  return (__m128d)__builtin_ia32_cmpsd(__a, __b, 0);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
_mm_cmplt_sd(__m128d __a, __m128d __b)
{
  return (__m128d)__builtin_ia32_cmpsd(__a, __b, 1);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
_mm_cmple_sd(__m128d __a, __m128d __b)
{
  return (__m128d)__builtin_ia32_cmpsd(__a, __b, 2);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
_mm_cmpgt_sd(__m128d __a, __m128d __b)
{
  __m128d __c = __builtin_ia32_cmpsd(__b, __a, 1);
  return (__m128d) { __c[0], __a[1] };
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
_mm_cmpge_sd(__m128d __a, __m128d __b)
{
  __m128d __c = __builtin_ia32_cmpsd(__b, __a, 2);
  return (__m128d) { __c[0], __a[1] };
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
_mm_cmpord_sd(__m128d __a, __m128d __b)
{
  return (__m128d)__builtin_ia32_cmpsd(__a, __b, 7);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
_mm_cmpunord_sd(__m128d __a, __m128d __b)
{
  return (__m128d)__builtin_ia32_cmpsd(__a, __b, 3);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
_mm_cmpneq_sd(__m128d __a, __m128d __b)
{
  return (__m128d)__builtin_ia32_cmpsd(__a, __b, 4);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
_mm_cmpnlt_sd(__m128d __a, __m128d __b)
{
  return (__m128d)__builtin_ia32_cmpsd(__a, __b, 5);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
_mm_cmpnle_sd(__m128d __a, __m128d __b)
{
  return (__m128d)__builtin_ia32_cmpsd(__a, __b, 6);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
_mm_cmpngt_sd(__m128d __a, __m128d __b)
{
  __m128d __c = __builtin_ia32_cmpsd(__b, __a, 5);
  return (__m128d) { __c[0], __a[1] };
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
_mm_cmpnge_sd(__m128d __a, __m128d __b)
{
  __m128d __c = __builtin_ia32_cmpsd(__b, __a, 6);
  return (__m128d) { __c[0], __a[1] };
}

static __inline__ int __attribute__((__always_inline__, __nodebug__))
_mm_comieq_sd(__m128d __a, __m128d __b)
{
  return __builtin_ia32_comisdeq(__a, __b);
}

static __inline__ int __attribute__((__always_inline__, __nodebug__))
_mm_comilt_sd(__m128d __a, __m128d __b)
{
  return __builtin_ia32_comisdlt(__a, __b);
}

static __inline__ int __attribute__((__always_inline__, __nodebug__))
_mm_comile_sd(__m128d __a, __m128d __b)
{
  return __builtin_ia32_comisdle(__a, __b);
}

static __inline__ int __attribute__((__always_inline__, __nodebug__))
_mm_comigt_sd(__m128d __a, __m128d __b)
{
  return __builtin_ia32_comisdgt(__a, __b);
}

static __inline__ int __attribute__((__always_inline__, __nodebug__))
_mm_comige_sd(__m128d __a, __m128d __b)
{
  return __builtin_ia32_comisdge(__a, __b);
}

static __inline__ int __attribute__((__always_inline__, __nodebug__))
_mm_comineq_sd(__m128d __a, __m128d __b)
{
  return __builtin_ia32_comisdneq(__a, __b);
}

static __inline__ int __attribute__((__always_inline__, __nodebug__))
_mm_ucomieq_sd(__m128d __a, __m128d __b)
{
  return __builtin_ia32_ucomisdeq(__a, __b);
}

static __inline__ int __attribute__((__always_inline__, __nodebug__))
_mm_ucomilt_sd(__m128d __a, __m128d __b)
{
  return __builtin_ia32_ucomisdlt(__a, __b);
}

static __inline__ int __attribute__((__always_inline__, __nodebug__))
_mm_ucomile_sd(__m128d __a, __m128d __b)
{
  return __builtin_ia32_ucomisdle(__a, __b);
}

static __inline__ int __attribute__((__always_inline__, __nodebug__))
_mm_ucomigt_sd(__m128d __a, __m128d __b)
{
  return __builtin_ia32_ucomisdgt(__a, __b);
}

static __inline__ int __attribute__((__always_inline__, __nodebug__))
_mm_ucomige_sd(__m128d __a, __m128d __b)
{
  return __builtin_ia32_ucomisdge(__a, __b);
}

static __inline__ int __attribute__((__always_inline__, __nodebug__))
_mm_ucomineq_sd(__m128d __a, __m128d __b)
{
  return __builtin_ia32_ucomisdneq(__a, __b);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
_mm_cvtpd_ps(__m128d __a)
{
  return __builtin_ia32_cvtpd2ps(__a);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
_mm_cvtps_pd(__m128 __a)
{
  return __builtin_ia32_cvtps2pd(__a);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
_mm_cvtepi32_pd(__m128i __a)
{
  return __builtin_ia32_cvtdq2pd((__v4si)__a);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_cvtpd_epi32(__m128d __a)
{
  return __builtin_ia32_cvtpd2dq(__a);
}

static __inline__ int __attribute__((__always_inline__, __nodebug__))
_mm_cvtsd_si32(__m128d __a)
{
  return __builtin_ia32_cvtsd2si(__a);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
_mm_cvtsd_ss(__m128 __a, __m128d __b)
{
  __a[0] = __b[0];
  return __a;
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
_mm_cvtsi32_sd(__m128d __a, int __b)
{
  __a[0] = __b;
  return __a;
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
_mm_cvtss_sd(__m128d __a, __m128 __b)
{
  __a[0] = __b[0];
  return __a;
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_cvttpd_epi32(__m128d __a)
{
  return (__m128i)__builtin_ia32_cvttpd2dq(__a);
}

static __inline__ int __attribute__((__always_inline__, __nodebug__))
_mm_cvttsd_si32(__m128d __a)
{
  return __a[0];
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
_mm_cvtpd_pi32(__m128d __a)
{
  return (__m64)__builtin_ia32_cvtpd2pi(__a);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
_mm_cvttpd_pi32(__m128d __a)
{
  return (__m64)__builtin_ia32_cvttpd2pi(__a);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
_mm_cvtpi32_pd(__m64 __a)
{
  return __builtin_ia32_cvtpi2pd((__v2si)__a);
}

static __inline__ double __attribute__((__always_inline__, __nodebug__))
_mm_cvtsd_f64(__m128d __a)
{
  return __a[0];
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
_mm_load_pd(double const *__dp)
{
  return *(__m128d*)__dp;
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
_mm_load1_pd(double const *__dp)
{
  struct __mm_load1_pd_struct {
    double __u;
  } __attribute__((__packed__, __may_alias__));
  double __u = ((struct __mm_load1_pd_struct*)__dp)->__u;
  return (__m128d){ __u, __u };
}

#define        _mm_load_pd1(dp)        _mm_load1_pd(dp)

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
_mm_loadr_pd(double const *__dp)
{
  __m128d __u = *(__m128d*)__dp;
  return __builtin_shufflevector(__u, __u, 1, 0);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
_mm_loadu_pd(double const *__dp)
{
  struct __loadu_pd {
    __m128d __v;
  } __attribute__((packed, may_alias));
  return ((struct __loadu_pd*)__dp)->__v;
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
_mm_load_sd(double const *__dp)
{
  struct __mm_load_sd_struct {
    double __u;
  } __attribute__((__packed__, __may_alias__));
  double __u = ((struct __mm_load_sd_struct*)__dp)->__u;
  return (__m128d){ __u, 0 };
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
_mm_loadh_pd(__m128d __a, double const *__dp)
{
  struct __mm_loadh_pd_struct {
    double __u;
  } __attribute__((__packed__, __may_alias__));
  double __u = ((struct __mm_loadh_pd_struct*)__dp)->__u;
  return (__m128d){ __a[0], __u };
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
_mm_loadl_pd(__m128d __a, double const *__dp)
{
  struct __mm_loadl_pd_struct {
    double __u;
  } __attribute__((__packed__, __may_alias__));
  double __u = ((struct __mm_loadl_pd_struct*)__dp)->__u;
  return (__m128d){ __u, __a[1] };
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
_mm_set_sd(double __w)
{
  return (__m128d){ __w, 0 };
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
_mm_set1_pd(double __w)
{
  return (__m128d){ __w, __w };
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
_mm_set_pd(double __w, double __x)
{
  return (__m128d){ __x, __w };
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
_mm_setr_pd(double __w, double __x)
{
  return (__m128d){ __w, __x };
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
_mm_setzero_pd(void)
{
  return (__m128d){ 0, 0 };
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
_mm_move_sd(__m128d __a, __m128d __b)
{
  return (__m128d){ __b[0], __a[1] };
}

static __inline__ void __attribute__((__always_inline__, __nodebug__))
_mm_store_sd(double *__dp, __m128d __a)
{
  struct __mm_store_sd_struct {
    double __u;
  } __attribute__((__packed__, __may_alias__));
  ((struct __mm_store_sd_struct*)__dp)->__u = __a[0];
}

static __inline__ void __attribute__((__always_inline__, __nodebug__))
_mm_store1_pd(double *__dp, __m128d __a)
{
  struct __mm_store1_pd_struct {
    double __u[2];
  } __attribute__((__packed__, __may_alias__));
  ((struct __mm_store1_pd_struct*)__dp)->__u[0] = __a[0];
  ((struct __mm_store1_pd_struct*)__dp)->__u[1] = __a[0];
}

static __inline__ void __attribute__((__always_inline__, __nodebug__))
_mm_store_pd(double *__dp, __m128d __a)
{
  *(__m128d *)__dp = __a;
}

static __inline__ void __attribute__((__always_inline__, __nodebug__))
_mm_storeu_pd(double *__dp, __m128d __a)
{
  __builtin_ia32_storeupd(__dp, __a);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__))
_mm_storer_pd(double *__dp, __m128d __a)
{
  __a = __builtin_shufflevector(__a, __a, 1, 0);
  *(__m128d *)__dp = __a;
}

static __inline__ void __attribute__((__always_inline__, __nodebug__))
_mm_storeh_pd(double *__dp, __m128d __a)
{
  struct __mm_storeh_pd_struct {
    double __u;
  } __attribute__((__packed__, __may_alias__));
  ((struct __mm_storeh_pd_struct*)__dp)->__u = __a[1];
}

static __inline__ void __attribute__((__always_inline__, __nodebug__))
_mm_storel_pd(double *__dp, __m128d __a)
{
  struct __mm_storeh_pd_struct {
    double __u;
  } __attribute__((__packed__, __may_alias__));
  ((struct __mm_storeh_pd_struct*)__dp)->__u = __a[0];
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_add_epi8(__m128i __a, __m128i __b)
{
  return (__m128i)((__v16qi)__a + (__v16qi)__b);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_add_epi16(__m128i __a, __m128i __b)
{
  return (__m128i)((__v8hi)__a + (__v8hi)__b);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_add_epi32(__m128i __a, __m128i __b)
{
  return (__m128i)((__v4si)__a + (__v4si)__b);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
_mm_add_si64(__m64 __a, __m64 __b)
{
  return __a + __b;
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_add_epi64(__m128i __a, __m128i __b)
{
  return __a + __b;
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_adds_epi8(__m128i __a, __m128i __b)
{
  return (__m128i)__builtin_ia32_paddsb128((__v16qi)__a, (__v16qi)__b);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_adds_epi16(__m128i __a, __m128i __b)
{
  return (__m128i)__builtin_ia32_paddsw128((__v8hi)__a, (__v8hi)__b);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_adds_epu8(__m128i __a, __m128i __b)
{
  return (__m128i)__builtin_ia32_paddusb128((__v16qi)__a, (__v16qi)__b);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_adds_epu16(__m128i __a, __m128i __b)
{
  return (__m128i)__builtin_ia32_paddusw128((__v8hi)__a, (__v8hi)__b);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_avg_epu8(__m128i __a, __m128i __b)
{
  return (__m128i)__builtin_ia32_pavgb128((__v16qi)__a, (__v16qi)__b);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_avg_epu16(__m128i __a, __m128i __b)
{
  return (__m128i)__builtin_ia32_pavgw128((__v8hi)__a, (__v8hi)__b);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_madd_epi16(__m128i __a, __m128i __b)
{
  return (__m128i)__builtin_ia32_pmaddwd128((__v8hi)__a, (__v8hi)__b);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_max_epi16(__m128i __a, __m128i __b)
{
  return (__m128i)__builtin_ia32_pmaxsw128((__v8hi)__a, (__v8hi)__b);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_max_epu8(__m128i __a, __m128i __b)
{
  return (__m128i)__builtin_ia32_pmaxub128((__v16qi)__a, (__v16qi)__b);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_min_epi16(__m128i __a, __m128i __b)
{
  return (__m128i)__builtin_ia32_pminsw128((__v8hi)__a, (__v8hi)__b);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_min_epu8(__m128i __a, __m128i __b)
{
  return (__m128i)__builtin_ia32_pminub128((__v16qi)__a, (__v16qi)__b);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_mulhi_epi16(__m128i __a, __m128i __b)
{
  return (__m128i)__builtin_ia32_pmulhw128((__v8hi)__a, (__v8hi)__b);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_mulhi_epu16(__m128i __a, __m128i __b)
{
  return (__m128i)__builtin_ia32_pmulhuw128((__v8hi)__a, (__v8hi)__b);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_mullo_epi16(__m128i __a, __m128i __b)
{
  return (__m128i)((__v8hi)__a * (__v8hi)__b);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
_mm_mul_su32(__m64 __a, __m64 __b)
{
  return __builtin_ia32_pmuludq((__v2si)__a, (__v2si)__b);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_mul_epu32(__m128i __a, __m128i __b)
{
  return __builtin_ia32_pmuludq128((__v4si)__a, (__v4si)__b);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_sad_epu8(__m128i __a, __m128i __b)
{
  return __builtin_ia32_psadbw128((__v16qi)__a, (__v16qi)__b);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_sub_epi8(__m128i __a, __m128i __b)
{
  return (__m128i)((__v16qi)__a - (__v16qi)__b);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_sub_epi16(__m128i __a, __m128i __b)
{
  return (__m128i)((__v8hi)__a - (__v8hi)__b);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_sub_epi32(__m128i __a, __m128i __b)
{
  return (__m128i)((__v4si)__a - (__v4si)__b);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
_mm_sub_si64(__m64 __a, __m64 __b)
{
  return __a - __b;
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_sub_epi64(__m128i __a, __m128i __b)
{
  return __a - __b;
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_subs_epi8(__m128i __a, __m128i __b)
{
  return (__m128i)__builtin_ia32_psubsb128((__v16qi)__a, (__v16qi)__b);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_subs_epi16(__m128i __a, __m128i __b)
{
  return (__m128i)__builtin_ia32_psubsw128((__v8hi)__a, (__v8hi)__b);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_subs_epu8(__m128i __a, __m128i __b)
{
  return (__m128i)__builtin_ia32_psubusb128((__v16qi)__a, (__v16qi)__b);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_subs_epu16(__m128i __a, __m128i __b)
{
  return (__m128i)__builtin_ia32_psubusw128((__v8hi)__a, (__v8hi)__b);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_and_si128(__m128i __a, __m128i __b)
{
  return __a & __b;
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_andnot_si128(__m128i __a, __m128i __b)
{
  return ~__a & __b;
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_or_si128(__m128i __a, __m128i __b)
{
  return __a | __b;
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_xor_si128(__m128i __a, __m128i __b)
{
  return __a ^ __b;
}

#define _mm_slli_si128(a, count) __extension__ ({ \
  _Pragma("clang diagnostic push") _Pragma("clang diagnostic ignored \"-Wshadow\""); \
  __m128i __a = (a); \
   _Pragma("clang diagnostic pop"); \
  (__m128i)__builtin_ia32_pslldqi128(__a, (count)*8); })

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_slli_epi16(__m128i __a, int __count)
{
  return (__m128i)__builtin_ia32_psllwi128((__v8hi)__a, __count);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_sll_epi16(__m128i __a, __m128i __count)
{
  return (__m128i)__builtin_ia32_psllw128((__v8hi)__a, (__v8hi)__count);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_slli_epi32(__m128i __a, int __count)
{
  return (__m128i)__builtin_ia32_pslldi128((__v4si)__a, __count);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_sll_epi32(__m128i __a, __m128i __count)
{
  return (__m128i)__builtin_ia32_pslld128((__v4si)__a, (__v4si)__count);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_slli_epi64(__m128i __a, int __count)
{
  return __builtin_ia32_psllqi128(__a, __count);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_sll_epi64(__m128i __a, __m128i __count)
{
  return __builtin_ia32_psllq128(__a, __count);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_srai_epi16(__m128i __a, int __count)
{
  return (__m128i)__builtin_ia32_psrawi128((__v8hi)__a, __count);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_sra_epi16(__m128i __a, __m128i __count)
{
  return (__m128i)__builtin_ia32_psraw128((__v8hi)__a, (__v8hi)__count);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_srai_epi32(__m128i __a, int __count)
{
  return (__m128i)__builtin_ia32_psradi128((__v4si)__a, __count);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_sra_epi32(__m128i __a, __m128i __count)
{
  return (__m128i)__builtin_ia32_psrad128((__v4si)__a, (__v4si)__count);
}


#define _mm_srli_si128(a, count) __extension__ ({ \
  _Pragma("clang diagnostic push") _Pragma("clang diagnostic ignored \"-Wshadow\""); \
  __m128i __a = (a); \
  _Pragma("clang diagnostic pop"); \
  (__m128i)__builtin_ia32_psrldqi128(__a, (count)*8); })

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_srli_epi16(__m128i __a, int __count)
{
  return (__m128i)__builtin_ia32_psrlwi128((__v8hi)__a, __count);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_srl_epi16(__m128i __a, __m128i __count)
{
  return (__m128i)__builtin_ia32_psrlw128((__v8hi)__a, (__v8hi)__count);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_srli_epi32(__m128i __a, int __count)
{
  return (__m128i)__builtin_ia32_psrldi128((__v4si)__a, __count);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_srl_epi32(__m128i __a, __m128i __count)
{
  return (__m128i)__builtin_ia32_psrld128((__v4si)__a, (__v4si)__count);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_srli_epi64(__m128i __a, int __count)
{
  return __builtin_ia32_psrlqi128(__a, __count);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_srl_epi64(__m128i __a, __m128i __count)
{
  return __builtin_ia32_psrlq128(__a, __count);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_cmpeq_epi8(__m128i __a, __m128i __b)
{
  return (__m128i)((__v16qi)__a == (__v16qi)__b);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_cmpeq_epi16(__m128i __a, __m128i __b)
{
  return (__m128i)((__v8hi)__a == (__v8hi)__b);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_cmpeq_epi32(__m128i __a, __m128i __b)
{
  return (__m128i)((__v4si)__a == (__v4si)__b);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_cmpgt_epi8(__m128i __a, __m128i __b)
{
  /* This function always performs a signed comparison, but __v16qi is a char
     which may be signed or unsigned. */
  typedef signed char __v16qs __attribute__((__vector_size__(16)));
  return (__m128i)((__v16qs)__a > (__v16qs)__b);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_cmpgt_epi16(__m128i __a, __m128i __b)
{
  return (__m128i)((__v8hi)__a > (__v8hi)__b);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_cmpgt_epi32(__m128i __a, __m128i __b)
{
  return (__m128i)((__v4si)__a > (__v4si)__b);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_cmplt_epi8(__m128i __a, __m128i __b)
{
  return _mm_cmpgt_epi8(__b, __a);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_cmplt_epi16(__m128i __a, __m128i __b)
{
  return _mm_cmpgt_epi16(__b, __a);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_cmplt_epi32(__m128i __a, __m128i __b)
{
  return _mm_cmpgt_epi32(__b, __a);
}

#ifdef __x86_64__
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
_mm_cvtsi64_sd(__m128d __a, long long __b)
{
  __a[0] = __b;
  return __a;
}

static __inline__ long long __attribute__((__always_inline__, __nodebug__))
_mm_cvtsd_si64(__m128d __a)
{
  return __builtin_ia32_cvtsd2si64(__a);
}

static __inline__ long long __attribute__((__always_inline__, __nodebug__))
_mm_cvttsd_si64(__m128d __a)
{
  return __a[0];
}
#endif

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
_mm_cvtepi32_ps(__m128i __a)
{
  return __builtin_ia32_cvtdq2ps((__v4si)__a);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_cvtps_epi32(__m128 __a)
{
  return (__m128i)__builtin_ia32_cvtps2dq(__a);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_cvttps_epi32(__m128 __a)
{
  return (__m128i)__builtin_ia32_cvttps2dq(__a);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_cvtsi32_si128(int __a)
{
  return (__m128i)(__v4si){ __a, 0, 0, 0 };
}

#ifdef __x86_64__
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_cvtsi64_si128(long long __a)
{
  return (__m128i){ __a, 0 };
}
#endif

static __inline__ int __attribute__((__always_inline__, __nodebug__))
_mm_cvtsi128_si32(__m128i __a)
{
  __v4si __b = (__v4si)__a;
  return __b[0];
}

#ifdef __x86_64__
static __inline__ long long __attribute__((__always_inline__, __nodebug__))
_mm_cvtsi128_si64(__m128i __a)
{
  return __a[0];
}
#endif

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_load_si128(__m128i const *__p)
{
  return *__p;
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_loadu_si128(__m128i const *__p)
{
  struct __loadu_si128 {
    __m128i __v;
  } __attribute__((packed, may_alias));
  return ((struct __loadu_si128*)__p)->__v;
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_loadl_epi64(__m128i const *__p)
{
  struct __mm_loadl_epi64_struct {
    long long __u;
  } __attribute__((__packed__, __may_alias__));
  return (__m128i) { ((struct __mm_loadl_epi64_struct*)__p)->__u, 0};
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_set_epi64x(long long q1, long long q0)
{
  return (__m128i){ q0, q1 };
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_set_epi64(__m64 q1, __m64 q0)
{
  return (__m128i){ (long long)q0, (long long)q1 };
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_set_epi32(int i3, int i2, int i1, int i0)
{
  return (__m128i)(__v4si){ i0, i1, i2, i3};
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_set_epi16(short w7, short w6, short w5, short w4, short w3, short w2, short w1, short w0)
{
  return (__m128i)(__v8hi){ w0, w1, w2, w3, w4, w5, w6, w7 };
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_set_epi8(char b15, char b14, char b13, char b12, char b11, char b10, char b9, char b8, char b7, char b6, char b5, char b4, char b3, char b2, char b1, char b0)
{
  return (__m128i)(__v16qi){ b0, b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b12, b13, b14, b15 };
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_set1_epi64x(long long __q)
{
  return (__m128i){ __q, __q };
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_set1_epi64(__m64 __q)
{
  return (__m128i){ (long long)__q, (long long)__q };
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_set1_epi32(int __i)
{
  return (__m128i)(__v4si){ __i, __i, __i, __i };
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_set1_epi16(short __w)
{
  return (__m128i)(__v8hi){ __w, __w, __w, __w, __w, __w, __w, __w };
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_set1_epi8(char __b)
{
  return (__m128i)(__v16qi){ __b, __b, __b, __b, __b, __b, __b, __b, __b, __b, __b, __b, __b, __b, __b, __b };
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_setr_epi64(__m64 q0, __m64 q1)
{
  return (__m128i){ (long long)q0, (long long)q1 };
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_setr_epi32(int i0, int i1, int i2, int i3)
{
  return (__m128i)(__v4si){ i0, i1, i2, i3};
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_setr_epi16(short w0, short w1, short w2, short w3, short w4, short w5, short w6, short w7)
{
  return (__m128i)(__v8hi){ w0, w1, w2, w3, w4, w5, w6, w7 };
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_setr_epi8(char b0, char b1, char b2, char b3, char b4, char b5, char b6, char b7, char b8, char b9, char b10, char b11, char b12, char b13, char b14, char b15)
{
  return (__m128i)(__v16qi){ b0, b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b12, b13, b14, b15 };
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_setzero_si128(void)
{
  return (__m128i){ 0LL, 0LL };
}

static __inline__ void __attribute__((__always_inline__, __nodebug__))
_mm_store_si128(__m128i *__p, __m128i __b)
{
  *__p = __b;
}

static __inline__ void __attribute__((__always_inline__, __nodebug__))
_mm_storeu_si128(__m128i *__p, __m128i __b)
{
  __builtin_ia32_storedqu((char *)__p, (__v16qi)__b);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__))
_mm_maskmoveu_si128(__m128i __d, __m128i __n, char *__p)
{
  __builtin_ia32_maskmovdqu((__v16qi)__d, (__v16qi)__n, __p);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__))
_mm_storel_epi64(__m128i *__p, __m128i __a)
{
  struct __mm_storel_epi64_struct {
    long long __u;
  } __attribute__((__packed__, __may_alias__));
  ((struct __mm_storel_epi64_struct*)__p)->__u = __a[0];
}

static __inline__ void __attribute__((__always_inline__, __nodebug__))
_mm_stream_pd(double *__p, __m128d __a)
{
  __builtin_ia32_movntpd(__p, __a);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__))
_mm_stream_si128(__m128i *__p, __m128i __a)
{
  __builtin_ia32_movntdq(__p, __a);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__))
_mm_stream_si32(int *__p, int __a)
{
  __builtin_ia32_movnti(__p, __a);
}

#ifdef __x86_64__
static __inline__ void __attribute__((__always_inline__, __nodebug__))
_mm_stream_si64(long long *__p, long long __a)
{
  __builtin_ia32_movnti64(__p, __a);
}
#endif

static __inline__ void __attribute__((__always_inline__, __nodebug__))
_mm_clflush(void const *__p)
{
  __builtin_ia32_clflush(__p);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__))
_mm_lfence(void)
{
  __builtin_ia32_lfence();
}

static __inline__ void __attribute__((__always_inline__, __nodebug__))
_mm_mfence(void)
{
  __builtin_ia32_mfence();
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_packs_epi16(__m128i __a, __m128i __b)
{
  return (__m128i)__builtin_ia32_packsswb128((__v8hi)__a, (__v8hi)__b);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_packs_epi32(__m128i __a, __m128i __b)
{
  return (__m128i)__builtin_ia32_packssdw128((__v4si)__a, (__v4si)__b);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_packus_epi16(__m128i __a, __m128i __b)
{
  return (__m128i)__builtin_ia32_packuswb128((__v8hi)__a, (__v8hi)__b);
}

static __inline__ int __attribute__((__always_inline__, __nodebug__))
_mm_extract_epi16(__m128i __a, int __imm)
{
  __v8hi __b = (__v8hi)__a;
  return (unsigned short)__b[__imm & 7];
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_insert_epi16(__m128i __a, int __b, int __imm)
{
  __v8hi __c = (__v8hi)__a;
  __c[__imm & 7] = __b;
  return (__m128i)__c;
}

static __inline__ int __attribute__((__always_inline__, __nodebug__))
_mm_movemask_epi8(__m128i __a)
{
  return __builtin_ia32_pmovmskb128((__v16qi)__a);
}

#define _mm_shuffle_epi32(a, imm) __extension__ ({ \
  _Pragma("clang diagnostic push") _Pragma("clang diagnostic ignored \"-Wshadow\""); \
  __m128i __a = (a); \
  _Pragma("clang diagnostic pop"); \
  (__m128i)__builtin_shufflevector((__v4si)__a, (__v4si) _mm_set1_epi32(0), \
                                   (imm) & 0x3, ((imm) & 0xc) >> 2, \
                                   ((imm) & 0x30) >> 4, ((imm) & 0xc0) >> 6); })

#define _mm_shufflelo_epi16(a, imm) __extension__ ({ \
  _Pragma("clang diagnostic push") _Pragma("clang diagnostic ignored \"-Wshadow\""); \
  __m128i __a = (a); \
  _Pragma("clang diagnostic pop"); \
  (__m128i)__builtin_shufflevector((__v8hi)__a, (__v8hi) _mm_set1_epi16(0), \
                                   (imm) & 0x3, ((imm) & 0xc) >> 2, \
                                   ((imm) & 0x30) >> 4, ((imm) & 0xc0) >> 6, \
                                   4, 5, 6, 7); })

#define _mm_shufflehi_epi16(a, imm) __extension__ ({ \
  _Pragma("clang diagnostic push") _Pragma("clang diagnostic ignored \"-Wshadow\""); \
  __m128i __a = (a); \
  _Pragma("clang diagnostic pop"); \
  (__m128i)__builtin_shufflevector((__v8hi)__a, (__v8hi) _mm_set1_epi16(0), \
                                   0, 1, 2, 3, \
                                   4 + (((imm) & 0x03) >> 0), \
                                   4 + (((imm) & 0x0c) >> 2), \
                                   4 + (((imm) & 0x30) >> 4), \
                                   4 + (((imm) & 0xc0) >> 6)); })

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_unpackhi_epi8(__m128i __a, __m128i __b)
{
  return (__m128i)__builtin_shufflevector((__v16qi)__a, (__v16qi)__b, 8, 16+8, 9, 16+9, 10, 16+10, 11, 16+11, 12, 16+12, 13, 16+13, 14, 16+14, 15, 16+15);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_unpackhi_epi16(__m128i __a, __m128i __b)
{
  return (__m128i)__builtin_shufflevector((__v8hi)__a, (__v8hi)__b, 4, 8+4, 5, 8+5, 6, 8+6, 7, 8+7);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_unpackhi_epi32(__m128i __a, __m128i __b)
{
  return (__m128i)__builtin_shufflevector((__v4si)__a, (__v4si)__b, 2, 4+2, 3, 4+3);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_unpackhi_epi64(__m128i __a, __m128i __b)
{
  return (__m128i)__builtin_shufflevector(__a, __b, 1, 2+1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_unpacklo_epi8(__m128i __a, __m128i __b)
{
  return (__m128i)__builtin_shufflevector((__v16qi)__a, (__v16qi)__b, 0, 16+0, 1, 16+1, 2, 16+2, 3, 16+3, 4, 16+4, 5, 16+5, 6, 16+6, 7, 16+7);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_unpacklo_epi16(__m128i __a, __m128i __b)
{
  return (__m128i)__builtin_shufflevector((__v8hi)__a, (__v8hi)__b, 0, 8+0, 1, 8+1, 2, 8+2, 3, 8+3);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_unpacklo_epi32(__m128i __a, __m128i __b)
{
  return (__m128i)__builtin_shufflevector((__v4si)__a, (__v4si)__b, 0, 4+0, 1, 4+1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_unpacklo_epi64(__m128i __a, __m128i __b)
{
  return (__m128i)__builtin_shufflevector(__a, __b, 0, 2+0);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
_mm_movepi64_pi64(__m128i __a)
{
  return (__m64)__a[0];
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_movpi64_epi64(__m64 __a)
{
  return (__m128i){ (long long)__a, 0 };
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_move_epi64(__m128i __a)
{
  return __builtin_shufflevector(__a, (__m128i){ 0 }, 0, 2);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
_mm_unpackhi_pd(__m128d __a, __m128d __b)
{
  return __builtin_shufflevector(__a, __b, 1, 2+1);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
_mm_unpacklo_pd(__m128d __a, __m128d __b)
{
  return __builtin_shufflevector(__a, __b, 0, 2+0);
}

static __inline__ int __attribute__((__always_inline__, __nodebug__))
_mm_movemask_pd(__m128d __a)
{
  return __builtin_ia32_movmskpd(__a);
}

#define _mm_shuffle_pd(a, b, i) __extension__ ({ \
  _Pragma("clang diagnostic push") _Pragma("clang diagnostic ignored \"-Wshadow\""); \
  __m128d __a = (a); \
  __m128d __b = (b); \
  _Pragma("clang diagnostic pop"); \
  __builtin_shufflevector(__a, __b, (i) & 1, (((i) & 2) >> 1) + 2); })

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
_mm_castpd_ps(__m128d __a)
{
  return (__m128)__a;
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_castpd_si128(__m128d __a)
{
  return (__m128i)__a;
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
_mm_castps_pd(__m128 __a)
{
  return (__m128d)__a;
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_castps_si128(__m128 __a)
{
  return (__m128i)__a;
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
_mm_castsi128_ps(__m128i __a)
{
  return (__m128)__a;
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
_mm_castsi128_pd(__m128i __a)
{
  return (__m128d)__a;
}

static __inline__ void __attribute__((__always_inline__, __nodebug__))
_mm_pause(void)
{
  __asm__ volatile ("pause");
}

#define _MM_SHUFFLE2(x, y) (((x) << 1) | (y))

#endif /* __SSE2__ */

#endif /* __EMMINTRIN_H */
                                                                                                                         usr/lib/llvm-3.5/lib/clang/3.5.0/include/f16cintrin.h                                               0100644 0000000 0000000 00000004307 12474130047 020163  0                                                                                                    ustar 00                                                                0000000 0000000                                                                                                                                                                        /*===---- f16cintrin.h - F16C intrinsics -----------------------------------===
 *
 * Permission is hereby granted, free of charge, to any person obtaining a copy
 * of this software and associated documentation files (the "Software"), to deal
 * in the Software without restriction, including without limitation the rights
 * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 * copies of the Software, and to permit persons to whom the Software is
 * furnished to do so, subject to the following conditions:
 *
 * The above copyright notice and this permission notice shall be included in
 * all copies or substantial portions of the Software.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
 * THE SOFTWARE.
 *
 *===-----------------------------------------------------------------------===
 */

#if !defined __X86INTRIN_H && !defined __IMMINTRIN_H
#error "Never use <f16cintrin.h> directly; include <x86intrin.h> instead."
#endif

#ifndef __F16C__
# error "F16C instruction is not enabled"
#endif /* __F16C__ */

#ifndef __F16CINTRIN_H
#define __F16CINTRIN_H

typedef float __v8sf __attribute__ ((__vector_size__ (32)));
typedef float __m256 __attribute__ ((__vector_size__ (32)));

#define _mm_cvtps_ph(a, imm) __extension__ ({ \
  __m128 __a = (a); \
 (__m128i)__builtin_ia32_vcvtps2ph((__v4sf)__a, (imm)); })

#define _mm256_cvtps_ph(a, imm) __extension__ ({ \
  __m256 __a = (a); \
 (__m128i)__builtin_ia32_vcvtps2ph256((__v8sf)__a, (imm)); })

static __inline __m128 __attribute__((__always_inline__, __nodebug__))
_mm_cvtph_ps(__m128i __a)
{
  return (__m128)__builtin_ia32_vcvtph2ps((__v8hi)__a);
}

static __inline __m256 __attribute__((__always_inline__, __nodebug__))
_mm256_cvtph_ps(__m128i __a)
{
  return (__m256)__builtin_ia32_vcvtph2ps256((__v8hi)__a);
}

#endif /* __F16CINTRIN_H */
                                                                                                                                                                                                                                                                                                                         usr/lib/llvm-3.5/lib/clang/3.5.0/include/float.h                                                    0100644 0000000 0000000 00000007426 12474130047 017312  0                                                                                                    ustar 00                                                                0000000 0000000                                                                                                                                                                        /*===---- float.h - Characteristics of floating point types ----------------===
 *
 * Permission is hereby granted, free of charge, to any person obtaining a copy
 * of this software and associated documentation files (the "Software"), to deal
 * in the Software without restriction, including without limitation the rights
 * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 * copies of the Software, and to permit persons to whom the Software is
 * furnished to do so, subject to the following conditions:
 *
 * The above copyright notice and this permission notice shall be included in
 * all copies or substantial portions of the Software.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
 * THE SOFTWARE.
 *
 *===-----------------------------------------------------------------------===
 */

#ifndef __FLOAT_H
#define __FLOAT_H

/* If we're on MinGW, fall back to the system's float.h, which might have
 * additional definitions provided for Windows.
 * For more details see http://msdn.microsoft.com/en-us/library/y0ybw9fy.aspx
 */
#if (defined(__MINGW32__) || defined(_MSC_VER)) && \
    __has_include_next(<float.h>)
#  include_next <float.h>

/* Undefine anything that we'll be redefining below. */
#  undef FLT_EVAL_METHOD
#  undef FLT_ROUNDS
#  undef FLT_RADIX
#  undef FLT_MANT_DIG
#  undef DBL_MANT_DIG
#  undef LDBL_MANT_DIG
#  undef DECIMAL_DIG
#  undef FLT_DIG
#  undef DBL_DIG
#  undef LDBL_DIG
#  undef FLT_MIN_EXP
#  undef DBL_MIN_EXP
#  undef LDBL_MIN_EXP
#  undef FLT_MIN_10_EXP
#  undef DBL_MIN_10_EXP
#  undef LDBL_MIN_10_EXP
#  undef FLT_MAX_EXP
#  undef DBL_MAX_EXP
#  undef LDBL_MAX_EXP
#  undef FLT_MAX_10_EXP
#  undef DBL_MAX_10_EXP
#  undef LDBL_MAX_10_EXP
#  undef FLT_MAX
#  undef DBL_MAX
#  undef LDBL_MAX
#  undef FLT_EPSILON
#  undef DBL_EPSILON
#  undef LDBL_EPSILON
#  undef FLT_MIN
#  undef DBL_MIN
#  undef LDBL_MIN
#  if __STDC_VERSION__ >= 201112L || !defined(__STRICT_ANSI__)
#    undef FLT_TRUE_MIN
#    undef DBL_TRUE_MIN
#    undef LDBL_TRUE_MIN
#  endif
#endif

/* Characteristics of floating point types, C99 5.2.4.2.2 */

#define FLT_EVAL_METHOD __FLT_EVAL_METHOD__
#define FLT_ROUNDS (__builtin_flt_rounds())
#define FLT_RADIX __FLT_RADIX__

#define FLT_MANT_DIG __FLT_MANT_DIG__
#define DBL_MANT_DIG __DBL_MANT_DIG__
#define LDBL_MANT_DIG __LDBL_MANT_DIG__

#define DECIMAL_DIG __DECIMAL_DIG__

#define FLT_DIG __FLT_DIG__
#define DBL_DIG __DBL_DIG__
#define LDBL_DIG __LDBL_DIG__

#define FLT_MIN_EXP __FLT_MIN_EXP__
#define DBL_MIN_EXP __DBL_MIN_EXP__
#define LDBL_MIN_EXP __LDBL_MIN_EXP__

#define FLT_MIN_10_EXP __FLT_MIN_10_EXP__
#define DBL_MIN_10_EXP __DBL_MIN_10_EXP__
#define LDBL_MIN_10_EXP __LDBL_MIN_10_EXP__

#define FLT_MAX_EXP __FLT_MAX_EXP__
#define DBL_MAX_EXP __DBL_MAX_EXP__
#define LDBL_MAX_EXP __LDBL_MAX_EXP__

#define FLT_MAX_10_EXP __FLT_MAX_10_EXP__
#define DBL_MAX_10_EXP __DBL_MAX_10_EXP__
#define LDBL_MAX_10_EXP __LDBL_MAX_10_EXP__

#define FLT_MAX __FLT_MAX__
#define DBL_MAX __DBL_MAX__
#define LDBL_MAX __LDBL_MAX__

#define FLT_EPSILON __FLT_EPSILON__
#define DBL_EPSILON __DBL_EPSILON__
#define LDBL_EPSILON __LDBL_EPSILON__

#define FLT_MIN __FLT_MIN__
#define DBL_MIN __DBL_MIN__
#define LDBL_MIN __LDBL_MIN__

#if __STDC_VERSION__ >= 201112L || !defined(__STRICT_ANSI__)
#  define FLT_TRUE_MIN __FLT_DENORM_MIN__
#  define DBL_TRUE_MIN __DBL_DENORM_MIN__
#  define LDBL_TRUE_MIN __LDBL_DENORM_MIN__
#endif

#endif /* __FLOAT_H */
                                                                                                                                                                                                                                          usr/lib/llvm-3.5/lib/clang/3.5.0/include/fma4intrin.h                                               0100644 0000000 0000000 00000016704 12474130047 020257  0                                                                                                    ustar 00                                                                0000000 0000000                                                                                                                                                                        /*===---- fma4intrin.h - FMA4 intrinsics -----------------------------------===
 *
 * Permission is hereby granted, free of charge, to any person obtaining a copy
 * of this software and associated documentation files (the "Software"), to deal
 * in the Software without restriction, including without limitation the rights
 * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 * copies of the Software, and to permit persons to whom the Software is
 * furnished to do so, subject to the following conditions:
 *
 * The above copyright notice and this permission notice shall be included in
 * all copies or substantial portions of the Software.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
 * THE SOFTWARE.
 *
 *===-----------------------------------------------------------------------===
 */

#ifndef __X86INTRIN_H
#error "Never use <fma4intrin.h> directly; include <x86intrin.h> instead."
#endif

#ifndef __FMA4INTRIN_H
#define __FMA4INTRIN_H

#ifndef __FMA4__
# error "FMA4 instruction set is not enabled"
#else

#include <pmmintrin.h>

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
_mm_macc_ps(__m128 __A, __m128 __B, __m128 __C)
{
  return (__m128)__builtin_ia32_vfmaddps(__A, __B, __C);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
_mm_macc_pd(__m128d __A, __m128d __B, __m128d __C)
{
  return (__m128d)__builtin_ia32_vfmaddpd(__A, __B, __C);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
_mm_macc_ss(__m128 __A, __m128 __B, __m128 __C)
{
  return (__m128)__builtin_ia32_vfmaddss(__A, __B, __C);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
_mm_macc_sd(__m128d __A, __m128d __B, __m128d __C)
{
  return (__m128d)__builtin_ia32_vfmaddsd(__A, __B, __C);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
_mm_msub_ps(__m128 __A, __m128 __B, __m128 __C)
{
  return (__m128)__builtin_ia32_vfmsubps(__A, __B, __C);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
_mm_msub_pd(__m128d __A, __m128d __B, __m128d __C)
{
  return (__m128d)__builtin_ia32_vfmsubpd(__A, __B, __C);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
_mm_msub_ss(__m128 __A, __m128 __B, __m128 __C)
{
  return (__m128)__builtin_ia32_vfmsubss(__A, __B, __C);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
_mm_msub_sd(__m128d __A, __m128d __B, __m128d __C)
{
  return (__m128d)__builtin_ia32_vfmsubsd(__A, __B, __C);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
_mm_nmacc_ps(__m128 __A, __m128 __B, __m128 __C)
{
  return (__m128)__builtin_ia32_vfnmaddps(__A, __B, __C);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
_mm_nmacc_pd(__m128d __A, __m128d __B, __m128d __C)
{
  return (__m128d)__builtin_ia32_vfnmaddpd(__A, __B, __C);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
_mm_nmacc_ss(__m128 __A, __m128 __B, __m128 __C)
{
  return (__m128)__builtin_ia32_vfnmaddss(__A, __B, __C);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
_mm_nmacc_sd(__m128d __A, __m128d __B, __m128d __C)
{
  return (__m128d)__builtin_ia32_vfnmaddsd(__A, __B, __C);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
_mm_nmsub_ps(__m128 __A, __m128 __B, __m128 __C)
{
  return (__m128)__builtin_ia32_vfnmsubps(__A, __B, __C);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
_mm_nmsub_pd(__m128d __A, __m128d __B, __m128d __C)
{
  return (__m128d)__builtin_ia32_vfnmsubpd(__A, __B, __C);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
_mm_nmsub_ss(__m128 __A, __m128 __B, __m128 __C)
{
  return (__m128)__builtin_ia32_vfnmsubss(__A, __B, __C);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
_mm_nmsub_sd(__m128d __A, __m128d __B, __m128d __C)
{
  return (__m128d)__builtin_ia32_vfnmsubsd(__A, __B, __C);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
_mm_maddsub_ps(__m128 __A, __m128 __B, __m128 __C)
{
  return (__m128)__builtin_ia32_vfmaddsubps(__A, __B, __C);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
_mm_maddsub_pd(__m128d __A, __m128d __B, __m128d __C)
{
  return (__m128d)__builtin_ia32_vfmaddsubpd(__A, __B, __C);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
_mm_msubadd_ps(__m128 __A, __m128 __B, __m128 __C)
{
  return (__m128)__builtin_ia32_vfmsubaddps(__A, __B, __C);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
_mm_msubadd_pd(__m128d __A, __m128d __B, __m128d __C)
{
  return (__m128d)__builtin_ia32_vfmsubaddpd(__A, __B, __C);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__))
_mm256_macc_ps(__m256 __A, __m256 __B, __m256 __C)
{
  return (__m256)__builtin_ia32_vfmaddps256(__A, __B, __C);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__))
_mm256_macc_pd(__m256d __A, __m256d __B, __m256d __C)
{
  return (__m256d)__builtin_ia32_vfmaddpd256(__A, __B, __C);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__))
_mm256_msub_ps(__m256 __A, __m256 __B, __m256 __C)
{
  return (__m256)__builtin_ia32_vfmsubps256(__A, __B, __C);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__))
_mm256_msub_pd(__m256d __A, __m256d __B, __m256d __C)
{
  return (__m256d)__builtin_ia32_vfmsubpd256(__A, __B, __C);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__))
_mm256_nmacc_ps(__m256 __A, __m256 __B, __m256 __C)
{
  return (__m256)__builtin_ia32_vfnmaddps256(__A, __B, __C);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__))
_mm256_nmacc_pd(__m256d __A, __m256d __B, __m256d __C)
{
  return (__m256d)__builtin_ia32_vfnmaddpd256(__A, __B, __C);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__))
_mm256_nmsub_ps(__m256 __A, __m256 __B, __m256 __C)
{
  return (__m256)__builtin_ia32_vfnmsubps256(__A, __B, __C);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__))
_mm256_nmsub_pd(__m256d __A, __m256d __B, __m256d __C)
{
  return (__m256d)__builtin_ia32_vfnmsubpd256(__A, __B, __C);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__))
_mm256_maddsub_ps(__m256 __A, __m256 __B, __m256 __C)
{
  return (__m256)__builtin_ia32_vfmaddsubps256(__A, __B, __C);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__))
_mm256_maddsub_pd(__m256d __A, __m256d __B, __m256d __C)
{
  return (__m256d)__builtin_ia32_vfmaddsubpd256(__A, __B, __C);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__))
_mm256_msubadd_ps(__m256 __A, __m256 __B, __m256 __C)
{
  return (__m256)__builtin_ia32_vfmsubaddps256(__A, __B, __C);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__))
_mm256_msubadd_pd(__m256d __A, __m256d __B, __m256d __C)
{
  return (__m256d)__builtin_ia32_vfmsubaddpd256(__A, __B, __C);
}

#endif /* __FMA4__ */

#endif /* __FMA4INTRIN_H */
                                                            usr/lib/llvm-3.5/lib/clang/3.5.0/include/fmaintrin.h                                                0100644 0000000 0000000 00000016705 12474130047 020174  0                                                                                                    ustar 00                                                                0000000 0000000                                                                                                                                                                        /*===---- fma4intrin.h - FMA4 intrinsics -----------------------------------===
 *
 * Permission is hereby granted, free of charge, to any person obtaining a copy
 * of this software and associated documentation files (the "Software"), to deal
 * in the Software without restriction, including without limitation the rights
 * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 * copies of the Software, and to permit persons to whom the Software is
 * furnished to do so, subject to the following conditions:
 *
 * The above copyright notice and this permission notice shall be included in
 * all copies or substantial portions of the Software.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
 * THE SOFTWARE.
 *
 *===-----------------------------------------------------------------------===
 */

#ifndef __IMMINTRIN_H
#error "Never use <fmaintrin.h> directly; include <immintrin.h> instead."
#endif

#ifndef __FMAINTRIN_H
#define __FMAINTRIN_H

#ifndef __FMA__
# error "FMA instruction set is not enabled"
#else

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
_mm_fmadd_ps(__m128 __A, __m128 __B, __m128 __C)
{
  return (__m128)__builtin_ia32_vfmaddps(__A, __B, __C);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
_mm_fmadd_pd(__m128d __A, __m128d __B, __m128d __C)
{
  return (__m128d)__builtin_ia32_vfmaddpd(__A, __B, __C);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
_mm_fmadd_ss(__m128 __A, __m128 __B, __m128 __C)
{
  return (__m128)__builtin_ia32_vfmaddss(__A, __B, __C);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
_mm_fmadd_sd(__m128d __A, __m128d __B, __m128d __C)
{
  return (__m128d)__builtin_ia32_vfmaddsd(__A, __B, __C);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
_mm_fmsub_ps(__m128 __A, __m128 __B, __m128 __C)
{
  return (__m128)__builtin_ia32_vfmsubps(__A, __B, __C);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
_mm_fmsub_pd(__m128d __A, __m128d __B, __m128d __C)
{
  return (__m128d)__builtin_ia32_vfmsubpd(__A, __B, __C);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
_mm_fmsub_ss(__m128 __A, __m128 __B, __m128 __C)
{
  return (__m128)__builtin_ia32_vfmsubss(__A, __B, __C);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
_mm_fmsub_sd(__m128d __A, __m128d __B, __m128d __C)
{
  return (__m128d)__builtin_ia32_vfmsubsd(__A, __B, __C);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
_mm_fnmadd_ps(__m128 __A, __m128 __B, __m128 __C)
{
  return (__m128)__builtin_ia32_vfnmaddps(__A, __B, __C);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
_mm_fnmadd_pd(__m128d __A, __m128d __B, __m128d __C)
{
  return (__m128d)__builtin_ia32_vfnmaddpd(__A, __B, __C);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
_mm_fnmadd_ss(__m128 __A, __m128 __B, __m128 __C)
{
  return (__m128)__builtin_ia32_vfnmaddss(__A, __B, __C);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
_mm_fnmadd_sd(__m128d __A, __m128d __B, __m128d __C)
{
  return (__m128d)__builtin_ia32_vfnmaddsd(__A, __B, __C);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
_mm_fnmsub_ps(__m128 __A, __m128 __B, __m128 __C)
{
  return (__m128)__builtin_ia32_vfnmsubps(__A, __B, __C);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
_mm_fnmsub_pd(__m128d __A, __m128d __B, __m128d __C)
{
  return (__m128d)__builtin_ia32_vfnmsubpd(__A, __B, __C);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
_mm_fnmsub_ss(__m128 __A, __m128 __B, __m128 __C)
{
  return (__m128)__builtin_ia32_vfnmsubss(__A, __B, __C);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
_mm_fnmsub_sd(__m128d __A, __m128d __B, __m128d __C)
{
  return (__m128d)__builtin_ia32_vfnmsubsd(__A, __B, __C);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
_mm_fmaddsub_ps(__m128 __A, __m128 __B, __m128 __C)
{
  return (__m128)__builtin_ia32_vfmaddsubps(__A, __B, __C);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
_mm_fmaddsub_pd(__m128d __A, __m128d __B, __m128d __C)
{
  return (__m128d)__builtin_ia32_vfmaddsubpd(__A, __B, __C);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
_mm_fmsubadd_ps(__m128 __A, __m128 __B, __m128 __C)
{
  return (__m128)__builtin_ia32_vfmsubaddps(__A, __B, __C);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
_mm_fmsubadd_pd(__m128d __A, __m128d __B, __m128d __C)
{
  return (__m128d)__builtin_ia32_vfmsubaddpd(__A, __B, __C);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__))
_mm256_fmadd_ps(__m256 __A, __m256 __B, __m256 __C)
{
  return (__m256)__builtin_ia32_vfmaddps256(__A, __B, __C);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__))
_mm256_fmadd_pd(__m256d __A, __m256d __B, __m256d __C)
{
  return (__m256d)__builtin_ia32_vfmaddpd256(__A, __B, __C);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__))
_mm256_fmsub_ps(__m256 __A, __m256 __B, __m256 __C)
{
  return (__m256)__builtin_ia32_vfmsubps256(__A, __B, __C);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__))
_mm256_fmsub_pd(__m256d __A, __m256d __B, __m256d __C)
{
  return (__m256d)__builtin_ia32_vfmsubpd256(__A, __B, __C);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__))
_mm256_fnmadd_ps(__m256 __A, __m256 __B, __m256 __C)
{
  return (__m256)__builtin_ia32_vfnmaddps256(__A, __B, __C);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__))
_mm256_fnmadd_pd(__m256d __A, __m256d __B, __m256d __C)
{
  return (__m256d)__builtin_ia32_vfnmaddpd256(__A, __B, __C);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__))
_mm256_fnmsub_ps(__m256 __A, __m256 __B, __m256 __C)
{
  return (__m256)__builtin_ia32_vfnmsubps256(__A, __B, __C);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__))
_mm256_fnmsub_pd(__m256d __A, __m256d __B, __m256d __C)
{
  return (__m256d)__builtin_ia32_vfnmsubpd256(__A, __B, __C);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__))
_mm256_fmaddsub_ps(__m256 __A, __m256 __B, __m256 __C)
{
  return (__m256)__builtin_ia32_vfmaddsubps256(__A, __B, __C);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__))
_mm256_fmaddsub_pd(__m256d __A, __m256d __B, __m256d __C)
{
  return (__m256d)__builtin_ia32_vfmaddsubpd256(__A, __B, __C);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__))
_mm256_fmsubadd_ps(__m256 __A, __m256 __B, __m256 __C)
{
  return (__m256)__builtin_ia32_vfmsubaddps256(__A, __B, __C);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__))
_mm256_fmsubadd_pd(__m256d __A, __m256d __B, __m256d __C)
{
  return (__m256d)__builtin_ia32_vfmsubaddpd256(__A, __B, __C);
}

#endif /* __FMA__ */

#endif /* __FMAINTRIN_H */
                                                           usr/lib/llvm-3.5/lib/clang/3.5.0/include/ia32intrin.h                                               0100644 0000000 0000000 00000006301 12474130047 020156  0                                                                                                    ustar 00                                                                0000000 0000000                                                                                                                                                                        /* ===-------- ia32intrin.h ---------------------------------------------------===
 *
 * Permission is hereby granted, free of charge, to any person obtaining a copy
 * of this software and associated documentation files (the "Software"), to deal
 * in the Software without restriction, including without limitation the rights
 * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 * copies of the Software, and to permit persons to whom the Software is
 * furnished to do so, subject to the following conditions:
 *
 * The above copyright notice and this permission notice shall be included in
 * all copies or substantial portions of the Software.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
 * THE SOFTWARE.
 *
 *===-----------------------------------------------------------------------===
 */

#ifndef __X86INTRIN_H
#error "Never use <ia32intrin.h> directly; include <x86intrin.h> instead."
#endif

#ifndef __IA32INTRIN_H
#define __IA32INTRIN_H

#ifdef __x86_64__
static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__))
__readeflags(void)
{
  unsigned long long __res = 0;
  __asm__ __volatile__ ("pushf\n\t"
                        "popq %0\n"
                        :"=r"(__res)
                        :
                        :
                       );
  return __res;
}

static __inline__ void __attribute__((__always_inline__, __nodebug__))
__writeeflags(unsigned long long __f)
{
  __asm__ __volatile__ ("pushq %0\n\t"
                        "popf\n"
                        :
                        :"r"(__f)
                        :"flags"
                       );
}

#else /* !__x86_64__ */
static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__))
__readeflags(void)
{
  unsigned int __res = 0;
  __asm__ __volatile__ ("pushf\n\t"
                        "popl %0\n"
                        :"=r"(__res)
                        :
                        :
                       );
  return __res;
}

static __inline__ void __attribute__((__always_inline__, __nodebug__))
__writeeflags(unsigned int __f)
{
  __asm__ __volatile__ ("pushl %0\n\t"
                        "popf\n"
                        :
                        :"r"(__f)
                        :"flags"
                       );
}
#endif /* !__x86_64__ */

static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__))
__rdpmc(int __A) {
  return __builtin_ia32_rdpmc(__A);
}

/* __rdtsc */
static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__))
__rdtsc(void) {
  return __builtin_ia32_rdtsc();
}

/* __rdtscp */
static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__))
__rdtscp(unsigned int *__A) {
  return __builtin_ia32_rdtscp(__A);
}

#define _rdtsc() __rdtsc()

#endif /* __IA32INTRIN_H */
                                                                                                                                                                                                                                                                                                                               usr/lib/llvm-3.5/lib/clang/3.5.0/include/immintrin.h                                                0100644 0000000 0000000 00000005405 12474130047 020206  0                                                                                                    ustar 00                                                                0000000 0000000                                                                                                                                                                        /*===---- immintrin.h - Intel intrinsics -----------------------------------===
 *
 * Permission is hereby granted, free of charge, to any person obtaining a copy
 * of this software and associated documentation files (the "Software"), to deal
 * in the Software without restriction, including without limitation the rights
 * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 * copies of the Software, and to permit persons to whom the Software is
 * furnished to do so, subject to the following conditions:
 *
 * The above copyright notice and this permission notice shall be included in
 * all copies or substantial portions of the Software.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
 * THE SOFTWARE.
 *
 *===-----------------------------------------------------------------------===
 */

#ifndef __IMMINTRIN_H
#define __IMMINTRIN_H

#ifdef __MMX__
#include <mmintrin.h>
#endif

#ifdef __SSE__
#include <xmmintrin.h>
#endif

#ifdef __SSE2__
#include <emmintrin.h>
#endif

#ifdef __SSE3__
#include <pmmintrin.h>
#endif

#ifdef __SSSE3__
#include <tmmintrin.h>
#endif

#if defined (__SSE4_2__) || defined (__SSE4_1__)
#include <smmintrin.h>
#endif

#if defined (__AES__) || defined (__PCLMUL__)
#include <wmmintrin.h>
#endif

#ifdef __AVX__
#include <avxintrin.h>
#endif

#ifdef __AVX2__
#include <avx2intrin.h>
#endif

#ifdef __BMI__
#include <bmiintrin.h>
#endif

#ifdef __BMI2__
#include <bmi2intrin.h>
#endif

#ifdef __LZCNT__
#include <lzcntintrin.h>
#endif

#ifdef __FMA__
#include <fmaintrin.h>
#endif

#ifdef __RDRND__
static __inline__ int __attribute__((__always_inline__, __nodebug__))
_rdrand16_step(unsigned short *__p)
{
  return __builtin_ia32_rdrand16_step(__p);
}

static __inline__ int __attribute__((__always_inline__, __nodebug__))
_rdrand32_step(unsigned int *__p)
{
  return __builtin_ia32_rdrand32_step(__p);
}

#ifdef __x86_64__
static __inline__ int __attribute__((__always_inline__, __nodebug__))
_rdrand64_step(unsigned long long *__p)
{
  return __builtin_ia32_rdrand64_step(__p);
}
#endif
#endif /* __RDRND__ */

#ifdef __RTM__
#include <rtmintrin.h>
#endif

/* FIXME: check __HLE__ as well when HLE is supported. */
#if defined (__RTM__)
static __inline__ int __attribute__((__always_inline__, __nodebug__))
_xtest(void)
{
  return __builtin_ia32_xtest();
}
#endif

#ifdef __SHA__
#include <shaintrin.h>
#endif

#endif /* __IMMINTRIN_H */
                                                                                                                                                                                                                                                           usr/lib/llvm-3.5/lib/clang/3.5.0/include/iso646.h                                                   0100644 0000000 0000000 00000003035 12474130047 017227  0                                                                                                    ustar 00                                                                0000000 0000000                                                                                                                                                                        /*===---- iso646.h - Standard header for alternate spellings of operators---===
 *
 * Copyright (c) 2008 Eli Friedman
 *
 * Permission is hereby granted, free of charge, to any person obtaining a copy
 * of this software and associated documentation files (the "Software"), to deal
 * in the Software without restriction, including without limitation the rights
 * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 * copies of the Software, and to permit persons to whom the Software is
 * furnished to do so, subject to the following conditions:
 *
 * The above copyright notice and this permission notice shall be included in
 * all copies or substantial portions of the Software.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
 * THE SOFTWARE.
 *
 *===-----------------------------------------------------------------------===
 */

#ifndef __ISO646_H
#define __ISO646_H

#ifndef __cplusplus
#define and    &&
#define and_eq &=
#define bitand &
#define bitor  |
#define compl  ~
#define not    !
#define not_eq !=
#define or     ||
#define or_eq  |=
#define xor    ^
#define xor_eq ^=
#endif

#endif /* __ISO646_H */
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   usr/lib/llvm-3.5/lib/clang/3.5.0/include/limits.h                                                   0100644 0000000 0000000 00000007226 12474130047 017504  0                                                                                                    ustar 00                                                                0000000 0000000                                                                                                                                                                        /*===---- limits.h - Standard header for integer sizes --------------------===*\
 *
 * Copyright (c) 2009 Chris Lattner
 *
 * Permission is hereby granted, free of charge, to any person obtaining a copy
 * of this software and associated documentation files (the "Software"), to deal
 * in the Software without restriction, including without limitation the rights
 * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 * copies of the Software, and to permit persons to whom the Software is
 * furnished to do so, subject to the following conditions:
 *
 * The above copyright notice and this permission notice shall be included in
 * all copies or substantial portions of the Software.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
 * THE SOFTWARE.
 *
\*===----------------------------------------------------------------------===*/

#ifndef __CLANG_LIMITS_H
#define __CLANG_LIMITS_H

/* The system's limits.h may, in turn, try to #include_next GCC's limits.h.
   Avert this #include_next madness. */
#if defined __GNUC__ && !defined _GCC_LIMITS_H_
#define _GCC_LIMITS_H_
#endif

/* System headers include a number of constants from POSIX in <limits.h>.
   Include it if we're hosted. */
#if __STDC_HOSTED__ && __has_include_next(<limits.h>)
#include_next <limits.h>
#endif

/* Many system headers try to "help us out" by defining these.  No really, we
   know how big each datatype is. */
#undef  SCHAR_MIN
#undef  SCHAR_MAX
#undef  UCHAR_MAX
#undef  SHRT_MIN
#undef  SHRT_MAX
#undef  USHRT_MAX
#undef  INT_MIN
#undef  INT_MAX
#undef  UINT_MAX
#undef  LONG_MIN
#undef  LONG_MAX
#undef  ULONG_MAX

#undef  CHAR_BIT
#undef  CHAR_MIN
#undef  CHAR_MAX

/* C90/99 5.2.4.2.1 */
#define SCHAR_MAX __SCHAR_MAX__
#define SHRT_MAX  __SHRT_MAX__
#define INT_MAX   __INT_MAX__
#define LONG_MAX  __LONG_MAX__

#define SCHAR_MIN (-__SCHAR_MAX__-1)
#define SHRT_MIN  (-__SHRT_MAX__ -1)
#define INT_MIN   (-__INT_MAX__  -1)
#define LONG_MIN  (-__LONG_MAX__ -1L)

#define UCHAR_MAX (__SCHAR_MAX__*2  +1)
#define USHRT_MAX (__SHRT_MAX__ *2  +1)
#define UINT_MAX  (__INT_MAX__  *2U +1U)
#define ULONG_MAX (__LONG_MAX__ *2UL+1UL)

#ifndef MB_LEN_MAX
#define MB_LEN_MAX 1
#endif

#define CHAR_BIT  __CHAR_BIT__

#ifdef __CHAR_UNSIGNED__  /* -funsigned-char */
#define CHAR_MIN 0
#define CHAR_MAX UCHAR_MAX
#else
#define CHAR_MIN SCHAR_MIN
#define CHAR_MAX __SCHAR_MAX__
#endif

/* C99 5.2.4.2.1: Added long long.
   C++11 18.3.3.2: same contents as the Standard C Library header <limits.h>.
 */
#if __STDC_VERSION__ >= 199901L || __cplusplus >= 201103L

#undef  LLONG_MIN
#undef  LLONG_MAX
#undef  ULLONG_MAX

#define LLONG_MAX  __LONG_LONG_MAX__
#define LLONG_MIN  (-__LONG_LONG_MAX__-1LL)
#define ULLONG_MAX (__LONG_LONG_MAX__*2ULL+1ULL)
#endif

/* LONG_LONG_MIN/LONG_LONG_MAX/ULONG_LONG_MAX are a GNU extension.  It's too bad
   that we don't have something like #pragma poison that could be used to
   deprecate a macro - the code should just use LLONG_MAX and friends.
 */
#if defined(__GNU_LIBRARY__) ? defined(__USE_GNU) : !defined(__STRICT_ANSI__)

#undef   LONG_LONG_MIN
#undef   LONG_LONG_MAX
#undef   ULONG_LONG_MAX

#define LONG_LONG_MAX  __LONG_LONG_MAX__
#define LONG_LONG_MIN  (-__LONG_LONG_MAX__-1LL)
#define ULONG_LONG_MAX (__LONG_LONG_MAX__*2ULL+1ULL)
#endif

#endif /* __CLANG_LIMITS_H */
                                                                                                                                                                                                                                                                                                                                                                          usr/lib/llvm-3.5/lib/clang/3.5.0/include/lzcntintrin.h                                              0100644 0000000 0000000 00000003732 12474130047 020557  0                                                                                                    ustar 00                                                                0000000 0000000                                                                                                                                                                        /*===---- lzcntintrin.h - LZCNT intrinsics ---------------------------------===
 *
 * Permission is hereby granted, free of charge, to any person obtaining a copy
 * of this software and associated documentation files (the "Software"), to deal
 * in the Software without restriction, including without limitation the rights
 * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 * copies of the Software, and to permit persons to whom the Software is
 * furnished to do so, subject to the following conditions:
 *
 * The above copyright notice and this permission notice shall be included in
 * all copies or substantial portions of the Software.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
 * THE SOFTWARE.
 *
 *===-----------------------------------------------------------------------===
 */

#if !defined __X86INTRIN_H && !defined __IMMINTRIN_H
#error "Never use <lzcntintrin.h> directly; include <x86intrin.h> instead."
#endif

#ifndef __LZCNT__
# error "LZCNT instruction is not enabled"
#endif /* __LZCNT__ */

#ifndef __LZCNTINTRIN_H
#define __LZCNTINTRIN_H

static __inline__ unsigned short __attribute__((__always_inline__, __nodebug__))
__lzcnt16(unsigned short __X)
{
  return __builtin_clzs(__X);
}

static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__))
__lzcnt32(unsigned int __X)
{
  return __builtin_clz(__X);
}

#ifdef __x86_64__
static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__))
__lzcnt64(unsigned long long __X)
{
  return __builtin_clzll(__X);
}
#endif

#endif /* __LZCNTINTRIN_H */
                                      usr/lib/llvm-3.5/lib/clang/3.5.0/include/mm3dnow.h                                                  0100644 0000000 0000000 00000013210 12474130047 017555  0                                                                                                    ustar 00                                                                0000000 0000000                                                                                                                                                                        /*===---- mm3dnow.h - 3DNow! intrinsics ------------------------------------===
 *
 * Permission is hereby granted, free of charge, to any person obtaining a copy
 * of this software and associated documentation files (the "Software"), to deal
 * in the Software without restriction, including without limitation the rights
 * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 * copies of the Software, and to permit persons to whom the Software is
 * furnished to do so, subject to the following conditions:
 *
 * The above copyright notice and this permission notice shall be included in
 * all copies or substantial portions of the Software.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
 * THE SOFTWARE.
 *
 *===-----------------------------------------------------------------------===
 */

#ifndef _MM3DNOW_H_INCLUDED
#define _MM3DNOW_H_INCLUDED

#include <mmintrin.h>
#include <prfchwintrin.h>

typedef float __v2sf __attribute__((__vector_size__(8)));

static __inline__ void __attribute__((__always_inline__, __nodebug__))
_m_femms() {
  __builtin_ia32_femms();
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
_m_pavgusb(__m64 __m1, __m64 __m2) {
  return (__m64)__builtin_ia32_pavgusb((__v8qi)__m1, (__v8qi)__m2);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
_m_pf2id(__m64 __m) {
  return (__m64)__builtin_ia32_pf2id((__v2sf)__m);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
_m_pfacc(__m64 __m1, __m64 __m2) {
  return (__m64)__builtin_ia32_pfacc((__v2sf)__m1, (__v2sf)__m2);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
_m_pfadd(__m64 __m1, __m64 __m2) {
  return (__m64)__builtin_ia32_pfadd((__v2sf)__m1, (__v2sf)__m2);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
_m_pfcmpeq(__m64 __m1, __m64 __m2) {
  return (__m64)__builtin_ia32_pfcmpeq((__v2sf)__m1, (__v2sf)__m2);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
_m_pfcmpge(__m64 __m1, __m64 __m2) {
  return (__m64)__builtin_ia32_pfcmpge((__v2sf)__m1, (__v2sf)__m2);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
_m_pfcmpgt(__m64 __m1, __m64 __m2) {
  return (__m64)__builtin_ia32_pfcmpgt((__v2sf)__m1, (__v2sf)__m2);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
_m_pfmax(__m64 __m1, __m64 __m2) {
  return (__m64)__builtin_ia32_pfmax((__v2sf)__m1, (__v2sf)__m2);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
_m_pfmin(__m64 __m1, __m64 __m2) {
  return (__m64)__builtin_ia32_pfmin((__v2sf)__m1, (__v2sf)__m2);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
_m_pfmul(__m64 __m1, __m64 __m2) {
  return (__m64)__builtin_ia32_pfmul((__v2sf)__m1, (__v2sf)__m2);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
_m_pfrcp(__m64 __m) {
  return (__m64)__builtin_ia32_pfrcp((__v2sf)__m);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
_m_pfrcpit1(__m64 __m1, __m64 __m2) {
  return (__m64)__builtin_ia32_pfrcpit1((__v2sf)__m1, (__v2sf)__m2);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
_m_pfrcpit2(__m64 __m1, __m64 __m2) {
  return (__m64)__builtin_ia32_pfrcpit2((__v2sf)__m1, (__v2sf)__m2);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
_m_pfrsqrt(__m64 __m) {
  return (__m64)__builtin_ia32_pfrsqrt((__v2sf)__m);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
_m_pfrsqrtit1(__m64 __m1, __m64 __m2) {
  return (__m64)__builtin_ia32_pfrsqit1((__v2sf)__m1, (__v2sf)__m2);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
_m_pfsub(__m64 __m1, __m64 __m2) {
  return (__m64)__builtin_ia32_pfsub((__v2sf)__m1, (__v2sf)__m2);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
_m_pfsubr(__m64 __m1, __m64 __m2) {
  return (__m64)__builtin_ia32_pfsubr((__v2sf)__m1, (__v2sf)__m2);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
_m_pi2fd(__m64 __m) {
  return (__m64)__builtin_ia32_pi2fd((__v2si)__m);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
_m_pmulhrw(__m64 __m1, __m64 __m2) {
  return (__m64)__builtin_ia32_pmulhrw((__v4hi)__m1, (__v4hi)__m2);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
_m_pf2iw(__m64 __m) {
  return (__m64)__builtin_ia32_pf2iw((__v2sf)__m);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
_m_pfnacc(__m64 __m1, __m64 __m2) {
  return (__m64)__builtin_ia32_pfnacc((__v2sf)__m1, (__v2sf)__m2);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
_m_pfpnacc(__m64 __m1, __m64 __m2) {
  return (__m64)__builtin_ia32_pfpnacc((__v2sf)__m1, (__v2sf)__m2);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
_m_pi2fw(__m64 __m) {
  return (__m64)__builtin_ia32_pi2fw((__v2si)__m);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
_m_pswapdsf(__m64 __m) {
  return (__m64)__builtin_ia32_pswapdsf((__v2sf)__m);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
_m_pswapdsi(__m64 __m) {
  return (__m64)__builtin_ia32_pswapdsi((__v2si)__m);
}

#endif
                                                                                                                                                                                                                                                                                                                                                                                        usr/lib/llvm-3.5/lib/clang/3.5.0/include/mm_malloc.h                                                0100644 0000000 0000000 00000005063 12474130047 020140  0                                                                                                    ustar 00                                                                0000000 0000000                                                                                                                                                                        /*===---- mm_malloc.h - Allocating and Freeing Aligned Memory Blocks -------===
 *
 * Permission is hereby granted, free of charge, to any person obtaining a copy
 * of this software and associated documentation files (the "Software"), to deal
 * in the Software without restriction, including without limitation the rights
 * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 * copies of the Software, and to permit persons to whom the Software is
 * furnished to do so, subject to the following conditions:
 *
 * The above copyright notice and this permission notice shall be included in
 * all copies or substantial portions of the Software.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
 * THE SOFTWARE.
 *
 *===-----------------------------------------------------------------------===
 */

#ifndef __MM_MALLOC_H
#define __MM_MALLOC_H

#include <stdlib.h>

#ifdef _WIN32
#include <malloc.h>
#else
#ifndef __cplusplus
extern int posix_memalign(void **__memptr, size_t __alignment, size_t __size);
#else
// Some systems (e.g. those with GNU libc) declare posix_memalign with an
// exception specifier. Via an "egregious workaround" in
// Sema::CheckEquivalentExceptionSpec, Clang accepts the following as a valid
// redeclaration of glibc's declaration.
extern "C" int posix_memalign(void **__memptr, size_t __alignment, size_t __size);
#endif
#endif

#if !(defined(_WIN32) && defined(_mm_malloc))
static __inline__ void *__attribute__((__always_inline__, __nodebug__,
                                       __malloc__))
_mm_malloc(size_t __size, size_t __align)
{
  if (__align == 1) {
    return malloc(__size);
  }

  if (!(__align & (__align - 1)) && __align < sizeof(void *))
    __align = sizeof(void *);

  void *__mallocedMemory;
#if defined(__MINGW32__)
  __mallocedMemory = __mingw_aligned_malloc(__size, __align);
#elif defined(_WIN32)
  __mallocedMemory = _aligned_malloc(__size, __align);
#else
  if (posix_memalign(&__mallocedMemory, __align, __size))
    return 0;
#endif

  return __mallocedMemory;
}

static __inline__ void __attribute__((__always_inline__, __nodebug__))
_mm_free(void *__p)
{
  free(__p);
}
#endif

#endif /* __MM_MALLOC_H */
                                                                                                                                                                                                                                                                                                                                                                                                                                                                             usr/lib/llvm-3.5/lib/clang/3.5.0/include/mmintrin.h                                                 0100644 0000000 0000000 00000036166 12474130047 020045  0                                                                                                    ustar 00                                                                0000000 0000000                                                                                                                                                                        /*===---- mmintrin.h - MMX intrinsics --------------------------------------===
 *
 * Permission is hereby granted, free of charge, to any person obtaining a copy
 * of this software and associated documentation files (the "Software"), to deal
 * in the Software without restriction, including without limitation the rights
 * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 * copies of the Software, and to permit persons to whom the Software is
 * furnished to do so, subject to the following conditions:
 *
 * The above copyright notice and this permission notice shall be included in
 * all copies or substantial portions of the Software.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
 * THE SOFTWARE.
 *
 *===-----------------------------------------------------------------------===
 */

#ifndef __MMINTRIN_H
#define __MMINTRIN_H

#ifndef __MMX__
#error "MMX instruction set not enabled"
#else

typedef long long __m64 __attribute__((__vector_size__(8)));

typedef int __v2si __attribute__((__vector_size__(8)));
typedef short __v4hi __attribute__((__vector_size__(8)));
typedef char __v8qi __attribute__((__vector_size__(8)));

static __inline__ void __attribute__((__always_inline__, __nodebug__))
_mm_empty(void)
{
    __builtin_ia32_emms();
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
_mm_cvtsi32_si64(int __i)
{
    return (__m64)__builtin_ia32_vec_init_v2si(__i, 0);
}

static __inline__ int __attribute__((__always_inline__, __nodebug__))
_mm_cvtsi64_si32(__m64 __m)
{
    return __builtin_ia32_vec_ext_v2si((__v2si)__m, 0);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
_mm_cvtsi64_m64(long long __i)
{
    return (__m64)__i;
}

static __inline__ long long __attribute__((__always_inline__, __nodebug__))
_mm_cvtm64_si64(__m64 __m)
{
    return (long long)__m;
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
_mm_packs_pi16(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_packsswb((__v4hi)__m1, (__v4hi)__m2);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
_mm_packs_pi32(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_packssdw((__v2si)__m1, (__v2si)__m2);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
_mm_packs_pu16(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_packuswb((__v4hi)__m1, (__v4hi)__m2);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
_mm_unpackhi_pi8(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_punpckhbw((__v8qi)__m1, (__v8qi)__m2);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
_mm_unpackhi_pi16(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_punpckhwd((__v4hi)__m1, (__v4hi)__m2);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
_mm_unpackhi_pi32(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_punpckhdq((__v2si)__m1, (__v2si)__m2);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
_mm_unpacklo_pi8(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_punpcklbw((__v8qi)__m1, (__v8qi)__m2);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
_mm_unpacklo_pi16(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_punpcklwd((__v4hi)__m1, (__v4hi)__m2);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
_mm_unpacklo_pi32(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_punpckldq((__v2si)__m1, (__v2si)__m2);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
_mm_add_pi8(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_paddb((__v8qi)__m1, (__v8qi)__m2);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
_mm_add_pi16(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_paddw((__v4hi)__m1, (__v4hi)__m2);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
_mm_add_pi32(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_paddd((__v2si)__m1, (__v2si)__m2);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
_mm_adds_pi8(__m64 __m1, __m64 __m2) 
{
    return (__m64)__builtin_ia32_paddsb((__v8qi)__m1, (__v8qi)__m2);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
_mm_adds_pi16(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_paddsw((__v4hi)__m1, (__v4hi)__m2);    
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
_mm_adds_pu8(__m64 __m1, __m64 __m2) 
{
    return (__m64)__builtin_ia32_paddusb((__v8qi)__m1, (__v8qi)__m2);
}
 
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
_mm_adds_pu16(__m64 __m1, __m64 __m2) 
{
    return (__m64)__builtin_ia32_paddusw((__v4hi)__m1, (__v4hi)__m2);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
_mm_sub_pi8(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_psubb((__v8qi)__m1, (__v8qi)__m2);
}
 
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
_mm_sub_pi16(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_psubw((__v4hi)__m1, (__v4hi)__m2);
}
 
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
_mm_sub_pi32(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_psubd((__v2si)__m1, (__v2si)__m2);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
_mm_subs_pi8(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_psubsb((__v8qi)__m1, (__v8qi)__m2);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
_mm_subs_pi16(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_psubsw((__v4hi)__m1, (__v4hi)__m2);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
_mm_subs_pu8(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_psubusb((__v8qi)__m1, (__v8qi)__m2);
}
 
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
_mm_subs_pu16(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_psubusw((__v4hi)__m1, (__v4hi)__m2);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
_mm_madd_pi16(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_pmaddwd((__v4hi)__m1, (__v4hi)__m2);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
_mm_mulhi_pi16(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_pmulhw((__v4hi)__m1, (__v4hi)__m2);
}
 
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
_mm_mullo_pi16(__m64 __m1, __m64 __m2) 
{
    return (__m64)__builtin_ia32_pmullw((__v4hi)__m1, (__v4hi)__m2);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
_mm_sll_pi16(__m64 __m, __m64 __count)
{
    return (__m64)__builtin_ia32_psllw((__v4hi)__m, __count);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
_mm_slli_pi16(__m64 __m, int __count)
{
    return (__m64)__builtin_ia32_psllwi((__v4hi)__m, __count);    
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
_mm_sll_pi32(__m64 __m, __m64 __count)
{
    return (__m64)__builtin_ia32_pslld((__v2si)__m, __count);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
_mm_slli_pi32(__m64 __m, int __count)
{
    return (__m64)__builtin_ia32_pslldi((__v2si)__m, __count);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
_mm_sll_si64(__m64 __m, __m64 __count)
{
    return (__m64)__builtin_ia32_psllq(__m, __count);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
_mm_slli_si64(__m64 __m, int __count)
{
    return (__m64)__builtin_ia32_psllqi(__m, __count);    
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
_mm_sra_pi16(__m64 __m, __m64 __count)
{
    return (__m64)__builtin_ia32_psraw((__v4hi)__m, __count);    
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
_mm_srai_pi16(__m64 __m, int __count)
{
    return (__m64)__builtin_ia32_psrawi((__v4hi)__m, __count);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
_mm_sra_pi32(__m64 __m, __m64 __count)
{
    return (__m64)__builtin_ia32_psrad((__v2si)__m, __count);    
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
_mm_srai_pi32(__m64 __m, int __count)
{
    return (__m64)__builtin_ia32_psradi((__v2si)__m, __count);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
_mm_srl_pi16(__m64 __m, __m64 __count)
{
    return (__m64)__builtin_ia32_psrlw((__v4hi)__m, __count);    
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
_mm_srli_pi16(__m64 __m, int __count)
{
    return (__m64)__builtin_ia32_psrlwi((__v4hi)__m, __count);    
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
_mm_srl_pi32(__m64 __m, __m64 __count)
{
    return (__m64)__builtin_ia32_psrld((__v2si)__m, __count);       
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
_mm_srli_pi32(__m64 __m, int __count)
{
    return (__m64)__builtin_ia32_psrldi((__v2si)__m, __count);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
_mm_srl_si64(__m64 __m, __m64 __count)
{
    return (__m64)__builtin_ia32_psrlq(__m, __count);    
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
_mm_srli_si64(__m64 __m, int __count)
{
    return (__m64)__builtin_ia32_psrlqi(__m, __count);    
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
_mm_and_si64(__m64 __m1, __m64 __m2)
{
    return __builtin_ia32_pand(__m1, __m2);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
_mm_andnot_si64(__m64 __m1, __m64 __m2)
{
    return __builtin_ia32_pandn(__m1, __m2);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
_mm_or_si64(__m64 __m1, __m64 __m2)
{
    return __builtin_ia32_por(__m1, __m2);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
_mm_xor_si64(__m64 __m1, __m64 __m2)
{
    return __builtin_ia32_pxor(__m1, __m2);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
_mm_cmpeq_pi8(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_pcmpeqb((__v8qi)__m1, (__v8qi)__m2);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
_mm_cmpeq_pi16(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_pcmpeqw((__v4hi)__m1, (__v4hi)__m2);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
_mm_cmpeq_pi32(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_pcmpeqd((__v2si)__m1, (__v2si)__m2);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
_mm_cmpgt_pi8(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_pcmpgtb((__v8qi)__m1, (__v8qi)__m2);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
_mm_cmpgt_pi16(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_pcmpgtw((__v4hi)__m1, (__v4hi)__m2);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
_mm_cmpgt_pi32(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_pcmpgtd((__v2si)__m1, (__v2si)__m2);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
_mm_setzero_si64(void)
{
    return (__m64){ 0LL };
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
_mm_set_pi32(int __i1, int __i0)
{
    return (__m64)__builtin_ia32_vec_init_v2si(__i0, __i1);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
_mm_set_pi16(short __s3, short __s2, short __s1, short __s0)
{
    return (__m64)__builtin_ia32_vec_init_v4hi(__s0, __s1, __s2, __s3);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
_mm_set_pi8(char __b7, char __b6, char __b5, char __b4, char __b3, char __b2,
            char __b1, char __b0)
{
    return (__m64)__builtin_ia32_vec_init_v8qi(__b0, __b1, __b2, __b3,
                                               __b4, __b5, __b6, __b7);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
_mm_set1_pi32(int __i)
{
    return _mm_set_pi32(__i, __i);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
_mm_set1_pi16(short __w)
{
    return _mm_set_pi16(__w, __w, __w, __w);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
_mm_set1_pi8(char __b)
{
    return _mm_set_pi8(__b, __b, __b, __b, __b, __b, __b, __b);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
_mm_setr_pi32(int __i0, int __i1)
{
    return _mm_set_pi32(__i1, __i0);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
_mm_setr_pi16(short __w0, short __w1, short __w2, short __w3)
{
    return _mm_set_pi16(__w3, __w2, __w1, __w0);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__))
_mm_setr_pi8(char __b0, char __b1, char __b2, char __b3, char __b4, char __b5,
             char __b6, char __b7)
{
    return _mm_set_pi8(__b7, __b6, __b5, __b4, __b3, __b2, __b1, __b0);
}


/* Aliases for compatibility. */
#define _m_empty _mm_empty
#define _m_from_int _mm_cvtsi32_si64
#define _m_to_int _mm_cvtsi64_si32
#define _m_packsswb _mm_packs_pi16
#define _m_packssdw _mm_packs_pi32
#define _m_packuswb _mm_packs_pu16
#define _m_punpckhbw _mm_unpackhi_pi8
#define _m_punpckhwd _mm_unpackhi_pi16
#define _m_punpckhdq _mm_unpackhi_pi32
#define _m_punpcklbw _mm_unpacklo_pi8
#define _m_punpcklwd _mm_unpacklo_pi16
#define _m_punpckldq _mm_unpacklo_pi32
#define _m_paddb _mm_add_pi8
#define _m_paddw _mm_add_pi16
#define _m_paddd _mm_add_pi32
#define _m_paddsb _mm_adds_pi8
#define _m_paddsw _mm_adds_pi16
#define _m_paddusb _mm_adds_pu8
#define _m_paddusw _mm_adds_pu16
#define _m_psubb _mm_sub_pi8
#define _m_psubw _mm_sub_pi16
#define _m_psubd _mm_sub_pi32
#define _m_psubsb _mm_subs_pi8
#define _m_psubsw _mm_subs_pi16
#define _m_psubusb _mm_subs_pu8
#define _m_psubusw _mm_subs_pu16
#define _m_pmaddwd _mm_madd_pi16
#define _m_pmulhw _mm_mulhi_pi16
#define _m_pmullw _mm_mullo_pi16
#define _m_psllw _mm_sll_pi16
#define _m_psllwi _mm_slli_pi16
#define _m_pslld _mm_sll_pi32
#define _m_pslldi _mm_slli_pi32
#define _m_psllq _mm_sll_si64
#define _m_psllqi _mm_slli_si64
#define _m_psraw _mm_sra_pi16
#define _m_psrawi _mm_srai_pi16
#define _m_psrad _mm_sra_pi32
#define _m_psradi _mm_srai_pi32
#define _m_psrlw _mm_srl_pi16
#define _m_psrlwi _mm_srli_pi16
#define _m_psrld _mm_srl_pi32
#define _m_psrldi _mm_srli_pi32
#define _m_psrlq _mm_srl_si64
#define _m_psrlqi _mm_srli_si64
#define _m_pand _mm_and_si64
#define _m_pandn _mm_andnot_si64
#define _m_por _mm_or_si64
#define _m_pxor _mm_xor_si64
#define _m_pcmpeqb _mm_cmpeq_pi8
#define _m_pcmpeqw _mm_cmpeq_pi16
#define _m_pcmpeqd _mm_cmpeq_pi32
#define _m_pcmpgtb _mm_cmpgt_pi8
#define _m_pcmpgtw _mm_cmpgt_pi16
#define _m_pcmpgtd _mm_cmpgt_pi32

#endif /* __MMX__ */

#endif /* __MMINTRIN_H */

                                                                                                                                                                                                                                                                                                                                                                                                          usr/lib/llvm-3.5/lib/clang/3.5.0/include/module.modulemap                                           0100644 0000000 0000000 00000005133 12474130047 021217  0                                                                                                    ustar 00                                                                0000000 0000000                                                                                                                                                                        module _Builtin_intrinsics [system] {
  explicit module altivec {
    requires altivec
    header "altivec.h"
  }

  explicit module arm {
    requires arm

    explicit module neon {
      requires neon
      header "arm_neon.h"
      export *
    }
  }

  explicit module intel {
    requires x86
    export *

    header "immintrin.h"
    header "x86intrin.h"

    explicit module mm_malloc {
      header "mm_malloc.h"
      export * // note: for <stdlib.h> dependency
    }

    explicit module cpuid {
      requires x86
      header "cpuid.h"
    }

    explicit module mmx {
      requires mmx
      header "mmintrin.h"
    }

    explicit module f16c {
      requires f16c
      header "f16cintrin.h"
    }

    explicit module sse {
      requires sse
      export mmx
      export * // note: for hackish <emmintrin.h> dependency
      header "xmmintrin.h"
    }

    explicit module sse2 {
      requires sse2
      export sse
      header "emmintrin.h"
    }

    explicit module sse3 {
      requires sse3
      export sse2
      header "pmmintrin.h"
    }

    explicit module ssse3 {
      requires ssse3
      export sse3
      header "tmmintrin.h"
    }

    explicit module sse4_1 {
      requires sse41
      export ssse3
      header "smmintrin.h"
    }

    explicit module sse4_2 {
      requires sse42
      export sse4_1
      header "nmmintrin.h"
    }

    explicit module sse4a {
      requires sse4a
      export sse3
      header "ammintrin.h"
    }

    explicit module avx {
      requires avx
      export sse4_2
      header "avxintrin.h"
    }

    explicit module avx2 {
      requires avx2
      export avx
      header "avx2intrin.h"
    }

    explicit module bmi {
      requires bmi
      header "bmiintrin.h"
    }

    explicit module bmi2 {
      requires bmi2
      header "bmi2intrin.h"
    }

    explicit module fma {
      requires fma
      header "fmaintrin.h"
    }

    explicit module fma4 {
      requires fma4
      export sse3
      header "fma4intrin.h"
    }

    explicit module lzcnt {
      requires lzcnt
      header "lzcntintrin.h"
    }

    explicit module popcnt {
      requires popcnt
      header "popcntintrin.h"
    }

    explicit module mm3dnow {
      requires mm3dnow
      header "mm3dnow.h"
    }

    explicit module xop {
      requires xop
      export fma4
      header "xopintrin.h"
    }

    explicit module aes_pclmul {
      requires aes, pclmul
      header "wmmintrin.h"
    }

    explicit module aes {
      requires aes
      header "__wmmintrin_aes.h"
    }

    explicit module pclmul {
      requires pclmul
      header "__wmmintrin_pclmul.h"
    }
  }
}
                                                                                                                                                                                                                                                                                                                                                                                                                                     usr/lib/llvm-3.5/lib/clang/3.5.0/include/nmmintrin.h                                                0100644 0000000 0000000 00000003006 12474130047 020206  0                                                                                                    ustar 00                                                                0000000 0000000                                                                                                                                                                        /*===---- nmmintrin.h - SSE4 intrinsics ------------------------------------===
 *
 * Permission is hereby granted, free of charge, to any person obtaining a copy
 * of this software and associated documentation files (the "Software"), to deal
 * in the Software without restriction, including without limitation the rights
 * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 * copies of the Software, and to permit persons to whom the Software is
 * furnished to do so, subject to the following conditions:
 *
 * The above copyright notice and this permission notice shall be included in
 * all copies or substantial portions of the Software.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
 * THE SOFTWARE.
 *
 *===-----------------------------------------------------------------------===
 */

#ifndef _NMMINTRIN_H
#define _NMMINTRIN_H

#ifndef __SSE4_2__
#error "SSE4.2 instruction set not enabled"
#else

/* To match expectations of gcc we put the sse4.2 definitions into smmintrin.h,
   just include it now then.  */
#include <smmintrin.h>
#endif /* __SSE4_2__ */
#endif /* _NMMINTRIN_H */
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          usr/lib/llvm-3.5/lib/clang/3.5.0/include/pmmintrin.h                                                0100644 0000000 0000000 00000007310 12474130047 020212  0                                                                                                    ustar 00                                                                0000000 0000000                                                                                                                                                                        /*===---- pmmintrin.h - SSE3 intrinsics ------------------------------------===
 *
 * Permission is hereby granted, free of charge, to any person obtaining a copy
 * of this software and associated documentation files (the "Software"), to deal
 * in the Software without restriction, including without limitation the rights
 * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 * copies of the Software, and to permit persons to whom the Software is
 * furnished to do so, subject to the following conditions:
 *
 * The above copyright notice and this permission notice shall be included in
 * all copies or substantial portions of the Software.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
 * THE SOFTWARE.
 *
 *===-----------------------------------------------------------------------===
 */
 
#ifndef __PMMINTRIN_H
#define __PMMINTRIN_H

#ifndef __SSE3__
#error "SSE3 instruction set not enabled"
#else

#include <emmintrin.h>

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__))
_mm_lddqu_si128(__m128i const *__p)
{
  return (__m128i)__builtin_ia32_lddqu((char const *)__p);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
_mm_addsub_ps(__m128 __a, __m128 __b)
{
  return __builtin_ia32_addsubps(__a, __b);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
_mm_hadd_ps(__m128 __a, __m128 __b)
{
  return __builtin_ia32_haddps(__a, __b);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
_mm_hsub_ps(__m128 __a, __m128 __b)
{
  return __builtin_ia32_hsubps(__a, __b);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
_mm_movehdup_ps(__m128 __a)
{
  return __builtin_shufflevector(__a, __a, 1, 1, 3, 3);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__))
_mm_moveldup_ps(__m128 __a)
{
  return __builtin_shufflevector(__a, __a, 0, 0, 2, 2);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
_mm_addsub_pd(__m128d __a, __m128d __b)
{
  return __builtin_ia32_addsubpd(__a, __b);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
_mm_hadd_pd(__m128d __a, __m128d __b)
{
  return __builtin_ia32_haddpd(__a, __b);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
_mm_hsub_pd(__m128d __a, __m128d __b)
{
  return __builtin_ia32_hsubpd(__a, __b);
}

#define        _mm_loaddup_pd(dp)        _mm_load1_pd(dp)

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__))
_mm_movedup_pd(__m128d __a)
{
  return __builtin_shufflevector(__a, __a, 0, 0);
}

#define _MM_DENORMALS_ZERO_ON   (0x0040)
#define _MM_DENORMALS_ZERO_OFF  (0x0000)

#define _MM_DENORMALS_ZERO_MASK (0x0040)

#define _MM_GET_DENORMALS_ZERO_MODE() (_mm_getcsr() & _MM_DENORMALS_ZERO_MASK)
#define _MM_SET_DENORMALS_ZERO_MODE(x) (_mm_setcsr((_mm_getcsr() & ~_MM_DENORMALS_ZERO_MASK) | (x)))

static __inline__ void __attribute__((__always_inline__, __nodebug__))
_mm_monitor(void const *__p, unsigned __extensions, unsigned __hints)
{
  __builtin_ia32_monitor((void *)__p, __extensions, __hints);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__))
_mm_mwait(unsigned __extensions, unsigned __hints)
{
  __builtin_ia32_mwait(__extensions, __hints);
}

#endif /* __SSE3__ */

#endif /* __PMMINTRIN_H */
                                                                                                                                                                                                                                                                                                                        