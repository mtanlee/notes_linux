lane_v(&__ret, __p0, __rev1.val[0], __rev1.val[1], __rev1.val[2], __rev1.val[3], __p2, 41); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 3, 2, 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 3, 2, 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 3, 2, 1, 0); \
  __ret.val[3] = __builtin_shufflevector(__ret.val[3], __ret.val[3], 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld4q_lane_f16(__p0, __p1, __p2) __extension__ ({ \
  float16x8x4_t __s1 = __p1; \
  float16x8x4_t __ret; \
  __builtin_neon_vld4q_lane_v(&__ret, __p0, __s1.val[0], __s1.val[1], __s1.val[2], __s1.val[3], __p2, 40); \
  __ret; \
})
#else
#define vld4q_lane_f16(__p0, __p1, __p2) __extension__ ({ \
  float16x8x4_t __s1 = __p1; \
  float16x8x4_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[3] = __builtin_shufflevector(__s1.val[3], __s1.val[3], 7, 6, 5, 4, 3, 2, 1, 0); \
  float16x8x4_t __ret; \
  __builtin_neon_vld4q_lane_v(&__ret, __p0, __rev1.val[0], __rev1.val[1], __rev1.val[2], __rev1.val[3], __p2, 40); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[3] = __builtin_shufflevector(__ret.val[3], __ret.val[3], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld4q_lane_s32(__p0, __p1, __p2) __extension__ ({ \
  int32x4x4_t __s1 = __p1; \
  int32x4x4_t __ret; \
  __builtin_neon_vld4q_lane_v(&__ret, __p0, __s1.val[0], __s1.val[1], __s1.val[2], __s1.val[3], __p2, 34); \
  __ret; \
})
#else
#define vld4q_lane_s32(__p0, __p1, __p2) __extension__ ({ \
  int32x4x4_t __s1 = __p1; \
  int32x4x4_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 3, 2, 1, 0); \
  __rev1.val[3] = __builtin_shufflevector(__s1.val[3], __s1.val[3], 3, 2, 1, 0); \
  int32x4x4_t __ret; \
  __builtin_neon_vld4q_lane_v(&__ret, __p0, __rev1.val[0], __rev1.val[1], __rev1.val[2], __rev1.val[3], __p2, 34); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 3, 2, 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 3, 2, 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 3, 2, 1, 0); \
  __ret.val[3] = __builtin_shufflevector(__ret.val[3], __ret.val[3], 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld4q_lane_s16(__p0, __p1, __p2) __extension__ ({ \
  int16x8x4_t __s1 = __p1; \
  int16x8x4_t __ret; \
  __builtin_neon_vld4q_lane_v(&__ret, __p0, __s1.val[0], __s1.val[1], __s1.val[2], __s1.val[3], __p2, 33); \
  __ret; \
})
#else
#define vld4q_lane_s16(__p0, __p1, __p2) __extension__ ({ \
  int16x8x4_t __s1 = __p1; \
  int16x8x4_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[3] = __builtin_shufflevector(__s1.val[3], __s1.val[3], 7, 6, 5, 4, 3, 2, 1, 0); \
  int16x8x4_t __ret; \
  __builtin_neon_vld4q_lane_v(&__ret, __p0, __rev1.val[0], __rev1.val[1], __rev1.val[2], __rev1.val[3], __p2, 33); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[3] = __builtin_shufflevector(__ret.val[3], __ret.val[3], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld4_lane_u8(__p0, __p1, __p2) __extension__ ({ \
  uint8x8x4_t __s1 = __p1; \
  uint8x8x4_t __ret; \
  __builtin_neon_vld4_lane_v(&__ret, __p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], (int8x8_t)__s1.val[2], (int8x8_t)__s1.val[3], __p2, 16); \
  __ret; \
})
#else
#define vld4_lane_u8(__p0, __p1, __p2) __extension__ ({ \
  uint8x8x4_t __s1 = __p1; \
  uint8x8x4_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[3] = __builtin_shufflevector(__s1.val[3], __s1.val[3], 7, 6, 5, 4, 3, 2, 1, 0); \
  uint8x8x4_t __ret; \
  __builtin_neon_vld4_lane_v(&__ret, __p0, (int8x8_t)__rev1.val[0], (int8x8_t)__rev1.val[1], (int8x8_t)__rev1.val[2], (int8x8_t)__rev1.val[3], __p2, 16); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[3] = __builtin_shufflevector(__ret.val[3], __ret.val[3], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld4_lane_u32(__p0, __p1, __p2) __extension__ ({ \
  uint32x2x4_t __s1 = __p1; \
  uint32x2x4_t __ret; \
  __builtin_neon_vld4_lane_v(&__ret, __p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], (int8x8_t)__s1.val[2], (int8x8_t)__s1.val[3], __p2, 18); \
  __ret; \
})
#else
#define vld4_lane_u32(__p0, __p1, __p2) __extension__ ({ \
  uint32x2x4_t __s1 = __p1; \
  uint32x2x4_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 1, 0); \
  __rev1.val[3] = __builtin_shufflevector(__s1.val[3], __s1.val[3], 1, 0); \
  uint32x2x4_t __ret; \
  __builtin_neon_vld4_lane_v(&__ret, __p0, (int8x8_t)__rev1.val[0], (int8x8_t)__rev1.val[1], (int8x8_t)__rev1.val[2], (int8x8_t)__rev1.val[3], __p2, 18); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 1, 0); \
  __ret.val[3] = __builtin_shufflevector(__ret.val[3], __ret.val[3], 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld4_lane_u16(__p0, __p1, __p2) __extension__ ({ \
  uint16x4x4_t __s1 = __p1; \
  uint16x4x4_t __ret; \
  __builtin_neon_vld4_lane_v(&__ret, __p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], (int8x8_t)__s1.val[2], (int8x8_t)__s1.val[3], __p2, 17); \
  __ret; \
})
#else
#define vld4_lane_u16(__p0, __p1, __p2) __extension__ ({ \
  uint16x4x4_t __s1 = __p1; \
  uint16x4x4_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 3, 2, 1, 0); \
  __rev1.val[3] = __builtin_shufflevector(__s1.val[3], __s1.val[3], 3, 2, 1, 0); \
  uint16x4x4_t __ret; \
  __builtin_neon_vld4_lane_v(&__ret, __p0, (int8x8_t)__rev1.val[0], (int8x8_t)__rev1.val[1], (int8x8_t)__rev1.val[2], (int8x8_t)__rev1.val[3], __p2, 17); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 3, 2, 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 3, 2, 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 3, 2, 1, 0); \
  __ret.val[3] = __builtin_shufflevector(__ret.val[3], __ret.val[3], 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld4_lane_s8(__p0, __p1, __p2) __extension__ ({ \
  int8x8x4_t __s1 = __p1; \
  int8x8x4_t __ret; \
  __builtin_neon_vld4_lane_v(&__ret, __p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], (int8x8_t)__s1.val[2], (int8x8_t)__s1.val[3], __p2, 0); \
  __ret; \
})
#else
#define vld4_lane_s8(__p0, __p1, __p2) __extension__ ({ \
  int8x8x4_t __s1 = __p1; \
  int8x8x4_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[3] = __builtin_shufflevector(__s1.val[3], __s1.val[3], 7, 6, 5, 4, 3, 2, 1, 0); \
  int8x8x4_t __ret; \
  __builtin_neon_vld4_lane_v(&__ret, __p0, (int8x8_t)__rev1.val[0], (int8x8_t)__rev1.val[1], (int8x8_t)__rev1.val[2], (int8x8_t)__rev1.val[3], __p2, 0); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret.val[3] = __builtin_shufflevector(__ret.val[3], __ret.val[3], 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld4_lane_f32(__p0, __p1, __p2) __extension__ ({ \
  float32x2x4_t __s1 = __p1; \
  float32x2x4_t __ret; \
  __builtin_neon_vld4_lane_v(&__ret, __p0, __s1.val[0], __s1.val[1], __s1.val[2], __s1.val[3], __p2, 9); \
  __ret; \
})
#else
#define vld4_lane_f32(__p0, __p1, __p2) __extension__ ({ \
  float32x2x4_t __s1 = __p1; \
  float32x2x4_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 1, 0); \
  __rev1.val[3] = __builtin_shufflevector(__s1.val[3], __s1.val[3], 1, 0); \
  float32x2x4_t __ret; \
  __builtin_neon_vld4_lane_v(&__ret, __p0, __rev1.val[0], __rev1.val[1], __rev1.val[2], __rev1.val[3], __p2, 9); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 1, 0); \
  __ret.val[3] = __builtin_shufflevector(__ret.val[3], __ret.val[3], 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld4_lane_f16(__p0, __p1, __p2) __extension__ ({ \
  float16x4x4_t __s1 = __p1; \
  float16x4x4_t __ret; \
  __builtin_neon_vld4_lane_v(&__ret, __p0, __s1.val[0], __s1.val[1], __s1.val[2], __s1.val[3], __p2, 8); \
  __ret; \
})
#else
#define vld4_lane_f16(__p0, __p1, __p2) __extension__ ({ \
  float16x4x4_t __s1 = __p1; \
  float16x4x4_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 3, 2, 1, 0); \
  __rev1.val[3] = __builtin_shufflevector(__s1.val[3], __s1.val[3], 3, 2, 1, 0); \
  float16x4x4_t __ret; \
  __builtin_neon_vld4_lane_v(&__ret, __p0, __rev1.val[0], __rev1.val[1], __rev1.val[2], __rev1.val[3], __p2, 8); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 3, 2, 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 3, 2, 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 3, 2, 1, 0); \
  __ret.val[3] = __builtin_shufflevector(__ret.val[3], __ret.val[3], 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld4_lane_s32(__p0, __p1, __p2) __extension__ ({ \
  int32x2x4_t __s1 = __p1; \
  int32x2x4_t __ret; \
  __builtin_neon_vld4_lane_v(&__ret, __p0, __s1.val[0], __s1.val[1], __s1.val[2], __s1.val[3], __p2, 2); \
  __ret; \
})
#else
#define vld4_lane_s32(__p0, __p1, __p2) __extension__ ({ \
  int32x2x4_t __s1 = __p1; \
  int32x2x4_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 1, 0); \
  __rev1.val[3] = __builtin_shufflevector(__s1.val[3], __s1.val[3], 1, 0); \
  int32x2x4_t __ret; \
  __builtin_neon_vld4_lane_v(&__ret, __p0, __rev1.val[0], __rev1.val[1], __rev1.val[2], __rev1.val[3], __p2, 2); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 1, 0); \
  __ret.val[3] = __builtin_shufflevector(__ret.val[3], __ret.val[3], 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vld4_lane_s16(__p0, __p1, __p2) __extension__ ({ \
  int16x4x4_t __s1 = __p1; \
  int16x4x4_t __ret; \
  __builtin_neon_vld4_lane_v(&__ret, __p0, __s1.val[0], __s1.val[1], __s1.val[2], __s1.val[3], __p2, 1); \
  __ret; \
})
#else
#define vld4_lane_s16(__p0, __p1, __p2) __extension__ ({ \
  int16x4x4_t __s1 = __p1; \
  int16x4x4_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 3, 2, 1, 0); \
  __rev1.val[3] = __builtin_shufflevector(__s1.val[3], __s1.val[3], 3, 2, 1, 0); \
  int16x4x4_t __ret; \
  __builtin_neon_vld4_lane_v(&__ret, __p0, __rev1.val[0], __rev1.val[1], __rev1.val[2], __rev1.val[3], __p2, 1); \
 \
  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 3, 2, 1, 0); \
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 3, 2, 1, 0); \
  __ret.val[2] = __builtin_shufflevector(__ret.val[2], __ret.val[2], 3, 2, 1, 0); \
  __ret.val[3] = __builtin_shufflevector(__ret.val[3], __ret.val[3], 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x16_t vmaxq_u8(uint8x16_t __p0, uint8x16_t __p1) {
  uint8x16_t __ret;
  __ret = (uint8x16_t) __builtin_neon_vmaxq_v((int8x16_t)__p0, (int8x16_t)__p1, 48);
  return __ret;
}
#else
__ai uint8x16_t vmaxq_u8(uint8x16_t __p0, uint8x16_t __p1) {
  uint8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __ret;
  __ret = (uint8x16_t) __builtin_neon_vmaxq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 48);
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vmaxq_u32(uint32x4_t __p0, uint32x4_t __p1) {
  uint32x4_t __ret;
  __ret = (uint32x4_t) __builtin_neon_vmaxq_v((int8x16_t)__p0, (int8x16_t)__p1, 50);
  return __ret;
}
#else
__ai uint32x4_t vmaxq_u32(uint32x4_t __p0, uint32x4_t __p1) {
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint32x4_t __ret;
  __ret = (uint32x4_t) __builtin_neon_vmaxq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 50);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x8_t vmaxq_u16(uint16x8_t __p0, uint16x8_t __p1) {
  uint16x8_t __ret;
  __ret = (uint16x8_t) __builtin_neon_vmaxq_v((int8x16_t)__p0, (int8x16_t)__p1, 49);
  return __ret;
}
#else
__ai uint16x8_t vmaxq_u16(uint16x8_t __p0, uint16x8_t __p1) {
  uint16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __ret;
  __ret = (uint16x8_t) __builtin_neon_vmaxq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 49);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x16_t vmaxq_s8(int8x16_t __p0, int8x16_t __p1) {
  int8x16_t __ret;
  __ret = (int8x16_t) __builtin_neon_vmaxq_v((int8x16_t)__p0, (int8x16_t)__p1, 32);
  return __ret;
}
#else
__ai int8x16_t vmaxq_s8(int8x16_t __p0, int8x16_t __p1) {
  int8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __ret;
  __ret = (int8x16_t) __builtin_neon_vmaxq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 32);
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x4_t vmaxq_f32(float32x4_t __p0, float32x4_t __p1) {
  float32x4_t __ret;
  __ret = (float32x4_t) __builtin_neon_vmaxq_v((int8x16_t)__p0, (int8x16_t)__p1, 41);
  return __ret;
}
#else
__ai float32x4_t vmaxq_f32(float32x4_t __p0, float32x4_t __p1) {
  float32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  float32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  float32x4_t __ret;
  __ret = (float32x4_t) __builtin_neon_vmaxq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 41);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vmaxq_s32(int32x4_t __p0, int32x4_t __p1) {
  int32x4_t __ret;
  __ret = (int32x4_t) __builtin_neon_vmaxq_v((int8x16_t)__p0, (int8x16_t)__p1, 34);
  return __ret;
}
#else
__ai int32x4_t vmaxq_s32(int32x4_t __p0, int32x4_t __p1) {
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int32x4_t __ret;
  __ret = (int32x4_t) __builtin_neon_vmaxq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 34);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x8_t vmaxq_s16(int16x8_t __p0, int16x8_t __p1) {
  int16x8_t __ret;
  __ret = (int16x8_t) __builtin_neon_vmaxq_v((int8x16_t)__p0, (int8x16_t)__p1, 33);
  return __ret;
}
#else
__ai int16x8_t vmaxq_s16(int16x8_t __p0, int16x8_t __p1) {
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __ret;
  __ret = (int16x8_t) __builtin_neon_vmaxq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 33);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x8_t vmax_u8(uint8x8_t __p0, uint8x8_t __p1) {
  uint8x8_t __ret;
  __ret = (uint8x8_t) __builtin_neon_vmax_v((int8x8_t)__p0, (int8x8_t)__p1, 16);
  return __ret;
}
#else
__ai uint8x8_t vmax_u8(uint8x8_t __p0, uint8x8_t __p1) {
  uint8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __ret;
  __ret = (uint8x8_t) __builtin_neon_vmax_v((int8x8_t)__rev0, (int8x8_t)__rev1, 16);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x2_t vmax_u32(uint32x2_t __p0, uint32x2_t __p1) {
  uint32x2_t __ret;
  __ret = (uint32x2_t) __builtin_neon_vmax_v((int8x8_t)__p0, (int8x8_t)__p1, 18);
  return __ret;
}
#else
__ai uint32x2_t vmax_u32(uint32x2_t __p0, uint32x2_t __p1) {
  uint32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  uint32x2_t __ret;
  __ret = (uint32x2_t) __builtin_neon_vmax_v((int8x8_t)__rev0, (int8x8_t)__rev1, 18);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x4_t vmax_u16(uint16x4_t __p0, uint16x4_t __p1) {
  uint16x4_t __ret;
  __ret = (uint16x4_t) __builtin_neon_vmax_v((int8x8_t)__p0, (int8x8_t)__p1, 17);
  return __ret;
}
#else
__ai uint16x4_t vmax_u16(uint16x4_t __p0, uint16x4_t __p1) {
  uint16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint16x4_t __ret;
  __ret = (uint16x4_t) __builtin_neon_vmax_v((int8x8_t)__rev0, (int8x8_t)__rev1, 17);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x8_t vmax_s8(int8x8_t __p0, int8x8_t __p1) {
  int8x8_t __ret;
  __ret = (int8x8_t) __builtin_neon_vmax_v((int8x8_t)__p0, (int8x8_t)__p1, 0);
  return __ret;
}
#else
__ai int8x8_t vmax_s8(int8x8_t __p0, int8x8_t __p1) {
  int8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __ret;
  __ret = (int8x8_t) __builtin_neon_vmax_v((int8x8_t)__rev0, (int8x8_t)__rev1, 0);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x2_t vmax_f32(float32x2_t __p0, float32x2_t __p1) {
  float32x2_t __ret;
  __ret = (float32x2_t) __builtin_neon_vmax_v((int8x8_t)__p0, (int8x8_t)__p1, 9);
  return __ret;
}
#else
__ai float32x2_t vmax_f32(float32x2_t __p0, float32x2_t __p1) {
  float32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  float32x2_t __ret;
  __ret = (float32x2_t) __builtin_neon_vmax_v((int8x8_t)__rev0, (int8x8_t)__rev1, 9);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x2_t vmax_s32(int32x2_t __p0, int32x2_t __p1) {
  int32x2_t __ret;
  __ret = (int32x2_t) __builtin_neon_vmax_v((int8x8_t)__p0, (int8x8_t)__p1, 2);
  return __ret;
}
#else
__ai int32x2_t vmax_s32(int32x2_t __p0, int32x2_t __p1) {
  int32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  int32x2_t __ret;
  __ret = (int32x2_t) __builtin_neon_vmax_v((int8x8_t)__rev0, (int8x8_t)__rev1, 2);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x4_t vmax_s16(int16x4_t __p0, int16x4_t __p1) {
  int16x4_t __ret;
  __ret = (int16x4_t) __builtin_neon_vmax_v((int8x8_t)__p0, (int8x8_t)__p1, 1);
  return __ret;
}
#else
__ai int16x4_t vmax_s16(int16x4_t __p0, int16x4_t __p1) {
  int16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int16x4_t __ret;
  __ret = (int16x4_t) __builtin_neon_vmax_v((int8x8_t)__rev0, (int8x8_t)__rev1, 1);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x16_t vminq_u8(uint8x16_t __p0, uint8x16_t __p1) {
  uint8x16_t __ret;
  __ret = (uint8x16_t) __builtin_neon_vminq_v((int8x16_t)__p0, (int8x16_t)__p1, 48);
  return __ret;
}
#else
__ai uint8x16_t vminq_u8(uint8x16_t __p0, uint8x16_t __p1) {
  uint8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __ret;
  __ret = (uint8x16_t) __builtin_neon_vminq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 48);
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vminq_u32(uint32x4_t __p0, uint32x4_t __p1) {
  uint32x4_t __ret;
  __ret = (uint32x4_t) __builtin_neon_vminq_v((int8x16_t)__p0, (int8x16_t)__p1, 50);
  return __ret;
}
#else
__ai uint32x4_t vminq_u32(uint32x4_t __p0, uint32x4_t __p1) {
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint32x4_t __ret;
  __ret = (uint32x4_t) __builtin_neon_vminq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 50);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x8_t vminq_u16(uint16x8_t __p0, uint16x8_t __p1) {
  uint16x8_t __ret;
  __ret = (uint16x8_t) __builtin_neon_vminq_v((int8x16_t)__p0, (int8x16_t)__p1, 49);
  return __ret;
}
#else
__ai uint16x8_t vminq_u16(uint16x8_t __p0, uint16x8_t __p1) {
  uint16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __ret;
  __ret = (uint16x8_t) __builtin_neon_vminq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 49);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x16_t vminq_s8(int8x16_t __p0, int8x16_t __p1) {
  int8x16_t __ret;
  __ret = (int8x16_t) __builtin_neon_vminq_v((int8x16_t)__p0, (int8x16_t)__p1, 32);
  return __ret;
}
#else
__ai int8x16_t vminq_s8(int8x16_t __p0, int8x16_t __p1) {
  int8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __ret;
  __ret = (int8x16_t) __builtin_neon_vminq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 32);
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x4_t vminq_f32(float32x4_t __p0, float32x4_t __p1) {
  float32x4_t __ret;
  __ret = (float32x4_t) __builtin_neon_vminq_v((int8x16_t)__p0, (int8x16_t)__p1, 41);
  return __ret;
}
#else
__ai float32x4_t vminq_f32(float32x4_t __p0, float32x4_t __p1) {
  float32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  float32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  float32x4_t __ret;
  __ret = (float32x4_t) __builtin_neon_vminq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 41);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vminq_s32(int32x4_t __p0, int32x4_t __p1) {
  int32x4_t __ret;
  __ret = (int32x4_t) __builtin_neon_vminq_v((int8x16_t)__p0, (int8x16_t)__p1, 34);
  return __ret;
}
#else
__ai int32x4_t vminq_s32(int32x4_t __p0, int32x4_t __p1) {
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int32x4_t __ret;
  __ret = (int32x4_t) __builtin_neon_vminq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 34);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x8_t vminq_s16(int16x8_t __p0, int16x8_t __p1) {
  int16x8_t __ret;
  __ret = (int16x8_t) __builtin_neon_vminq_v((int8x16_t)__p0, (int8x16_t)__p1, 33);
  return __ret;
}
#else
__ai int16x8_t vminq_s16(int16x8_t __p0, int16x8_t __p1) {
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __ret;
  __ret = (int16x8_t) __builtin_neon_vminq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 33);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x8_t vmin_u8(uint8x8_t __p0, uint8x8_t __p1) {
  uint8x8_t __ret;
  __ret = (uint8x8_t) __builtin_neon_vmin_v((int8x8_t)__p0, (int8x8_t)__p1, 16);
  return __ret;
}
#else
__ai uint8x8_t vmin_u8(uint8x8_t __p0, uint8x8_t __p1) {
  uint8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __ret;
  __ret = (uint8x8_t) __builtin_neon_vmin_v((int8x8_t)__rev0, (int8x8_t)__rev1, 16);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x2_t vmin_u32(uint32x2_t __p0, uint32x2_t __p1) {
  uint32x2_t __ret;
  __ret = (uint32x2_t) __builtin_neon_vmin_v((int8x8_t)__p0, (int8x8_t)__p1, 18);
  return __ret;
}
#else
__ai uint32x2_t vmin_u32(uint32x2_t __p0, uint32x2_t __p1) {
  uint32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  uint32x2_t __ret;
  __ret = (uint32x2_t) __builtin_neon_vmin_v((int8x8_t)__rev0, (int8x8_t)__rev1, 18);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x4_t vmin_u16(uint16x4_t __p0, uint16x4_t __p1) {
  uint16x4_t __ret;
  __ret = (uint16x4_t) __builtin_neon_vmin_v((int8x8_t)__p0, (int8x8_t)__p1, 17);
  return __ret;
}
#else
__ai uint16x4_t vmin_u16(uint16x4_t __p0, uint16x4_t __p1) {
  uint16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint16x4_t __ret;
  __ret = (uint16x4_t) __builtin_neon_vmin_v((int8x8_t)__rev0, (int8x8_t)__rev1, 17);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x8_t vmin_s8(int8x8_t __p0, int8x8_t __p1) {
  int8x8_t __ret;
  __ret = (int8x8_t) __builtin_neon_vmin_v((int8x8_t)__p0, (int8x8_t)__p1, 0);
  return __ret;
}
#else
__ai int8x8_t vmin_s8(int8x8_t __p0, int8x8_t __p1) {
  int8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __ret;
  __ret = (int8x8_t) __builtin_neon_vmin_v((int8x8_t)__rev0, (int8x8_t)__rev1, 0);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x2_t vmin_f32(float32x2_t __p0, float32x2_t __p1) {
  float32x2_t __ret;
  __ret = (float32x2_t) __builtin_neon_vmin_v((int8x8_t)__p0, (int8x8_t)__p1, 9);
  return __ret;
}
#else
__ai float32x2_t vmin_f32(float32x2_t __p0, float32x2_t __p1) {
  float32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  float32x2_t __ret;
  __ret = (float32x2_t) __builtin_neon_vmin_v((int8x8_t)__rev0, (int8x8_t)__rev1, 9);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x2_t vmin_s32(int32x2_t __p0, int32x2_t __p1) {
  int32x2_t __ret;
  __ret = (int32x2_t) __builtin_neon_vmin_v((int8x8_t)__p0, (int8x8_t)__p1, 2);
  return __ret;
}
#else
__ai int32x2_t vmin_s32(int32x2_t __p0, int32x2_t __p1) {
  int32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  int32x2_t __ret;
  __ret = (int32x2_t) __builtin_neon_vmin_v((int8x8_t)__rev0, (int8x8_t)__rev1, 2);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x4_t vmin_s16(int16x4_t __p0, int16x4_t __p1) {
  int16x4_t __ret;
  __ret = (int16x4_t) __builtin_neon_vmin_v((int8x8_t)__p0, (int8x8_t)__p1, 1);
  return __ret;
}
#else
__ai int16x4_t vmin_s16(int16x4_t __p0, int16x4_t __p1) {
  int16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int16x4_t __ret;
  __ret = (int16x4_t) __builtin_neon_vmin_v((int8x8_t)__rev0, (int8x8_t)__rev1, 1);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x16_t vmlaq_u8(uint8x16_t __p0, uint8x16_t __p1, uint8x16_t __p2) {
  uint8x16_t __ret;
  __ret = __p0 + __p1 * __p2;
  return __ret;
}
#else
__ai uint8x16_t vmlaq_u8(uint8x16_t __p0, uint8x16_t __p1, uint8x16_t __p2) {
  uint8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __ret;
  __ret = __rev0 + __rev1 * __rev2;
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vmlaq_u32(uint32x4_t __p0, uint32x4_t __p1, uint32x4_t __p2) {
  uint32x4_t __ret;
  __ret = __p0 + __p1 * __p2;
  return __ret;
}
#else
__ai uint32x4_t vmlaq_u32(uint32x4_t __p0, uint32x4_t __p1, uint32x4_t __p2) {
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint32x4_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 3, 2, 1, 0);
  uint32x4_t __ret;
  __ret = __rev0 + __rev1 * __rev2;
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x8_t vmlaq_u16(uint16x8_t __p0, uint16x8_t __p1, uint16x8_t __p2) {
  uint16x8_t __ret;
  __ret = __p0 + __p1 * __p2;
  return __ret;
}
#else
__ai uint16x8_t vmlaq_u16(uint16x8_t __p0, uint16x8_t __p1, uint16x8_t __p2) {
  uint16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __ret;
  __ret = __rev0 + __rev1 * __rev2;
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x16_t vmlaq_s8(int8x16_t __p0, int8x16_t __p1, int8x16_t __p2) {
  int8x16_t __ret;
  __ret = __p0 + __p1 * __p2;
  return __ret;
}
#else
__ai int8x16_t vmlaq_s8(int8x16_t __p0, int8x16_t __p1, int8x16_t __p2) {
  int8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __ret;
  __ret = __rev0 + __rev1 * __rev2;
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x4_t vmlaq_f32(float32x4_t __p0, float32x4_t __p1, float32x4_t __p2) {
  float32x4_t __ret;
  __ret = __p0 + __p1 * __p2;
  return __ret;
}
#else
__ai float32x4_t vmlaq_f32(float32x4_t __p0, float32x4_t __p1, float32x4_t __p2) {
  float32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  float32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  float32x4_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 3, 2, 1, 0);
  float32x4_t __ret;
  __ret = __rev0 + __rev1 * __rev2;
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vmlaq_s32(int32x4_t __p0, int32x4_t __p1, int32x4_t __p2) {
  int32x4_t __ret;
  __ret = __p0 + __p1 * __p2;
  return __ret;
}
#else
__ai int32x4_t vmlaq_s32(int32x4_t __p0, int32x4_t __p1, int32x4_t __p2) {
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int32x4_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 3, 2, 1, 0);
  int32x4_t __ret;
  __ret = __rev0 + __rev1 * __rev2;
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x8_t vmlaq_s16(int16x8_t __p0, int16x8_t __p1, int16x8_t __p2) {
  int16x8_t __ret;
  __ret = __p0 + __p1 * __p2;
  return __ret;
}
#else
__ai int16x8_t vmlaq_s16(int16x8_t __p0, int16x8_t __p1, int16x8_t __p2) {
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __ret;
  __ret = __rev0 + __rev1 * __rev2;
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x8_t vmla_u8(uint8x8_t __p0, uint8x8_t __p1, uint8x8_t __p2) {
  uint8x8_t __ret;
  __ret = __p0 + __p1 * __p2;
  return __ret;
}
#else
__ai uint8x8_t vmla_u8(uint8x8_t __p0, uint8x8_t __p1, uint8x8_t __p2) {
  uint8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __ret;
  __ret = __rev0 + __rev1 * __rev2;
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x2_t vmla_u32(uint32x2_t __p0, uint32x2_t __p1, uint32x2_t __p2) {
  uint32x2_t __ret;
  __ret = __p0 + __p1 * __p2;
  return __ret;
}
#else
__ai uint32x2_t vmla_u32(uint32x2_t __p0, uint32x2_t __p1, uint32x2_t __p2) {
  uint32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  uint32x2_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 1, 0);
  uint32x2_t __ret;
  __ret = __rev0 + __rev1 * __rev2;
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x4_t vmla_u16(uint16x4_t __p0, uint16x4_t __p1, uint16x4_t __p2) {
  uint16x4_t __ret;
  __ret = __p0 + __p1 * __p2;
  return __ret;
}
#else
__ai uint16x4_t vmla_u16(uint16x4_t __p0, uint16x4_t __p1, uint16x4_t __p2) {
  uint16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint16x4_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 3, 2, 1, 0);
  uint16x4_t __ret;
  __ret = __rev0 + __rev1 * __rev2;
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x8_t vmla_s8(int8x8_t __p0, int8x8_t __p1, int8x8_t __p2) {
  int8x8_t __ret;
  __ret = __p0 + __p1 * __p2;
  return __ret;
}
#else
__ai int8x8_t vmla_s8(int8x8_t __p0, int8x8_t __p1, int8x8_t __p2) {
  int8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __ret;
  __ret = __rev0 + __rev1 * __rev2;
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x2_t vmla_f32(float32x2_t __p0, float32x2_t __p1, float32x2_t __p2) {
  float32x2_t __ret;
  __ret = __p0 + __p1 * __p2;
  return __ret;
}
#else
__ai float32x2_t vmla_f32(float32x2_t __p0, float32x2_t __p1, float32x2_t __p2) {
  float32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  float32x2_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 1, 0);
  float32x2_t __ret;
  __ret = __rev0 + __rev1 * __rev2;
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x2_t vmla_s32(int32x2_t __p0, int32x2_t __p1, int32x2_t __p2) {
  int32x2_t __ret;
  __ret = __p0 + __p1 * __p2;
  return __ret;
}
#else
__ai int32x2_t vmla_s32(int32x2_t __p0, int32x2_t __p1, int32x2_t __p2) {
  int32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  int32x2_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 1, 0);
  int32x2_t __ret;
  __ret = __rev0 + __rev1 * __rev2;
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x4_t vmla_s16(int16x4_t __p0, int16x4_t __p1, int16x4_t __p2) {
  int16x4_t __ret;
  __ret = __p0 + __p1 * __p2;
  return __ret;
}
#else
__ai int16x4_t vmla_s16(int16x4_t __p0, int16x4_t __p1, int16x4_t __p2) {
  int16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int16x4_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 3, 2, 1, 0);
  int16x4_t __ret;
  __ret = __rev0 + __rev1 * __rev2;
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
#define vmlaq_lane_u32(__p0, __p1, __p2, __p3) __extension__ ({ \
  uint32x4_t __s0 = __p0; \
  uint32x4_t __s1 = __p1; \
  uint32x2_t __s2 = __p2; \
  uint32x4_t __ret; \
  __ret = __s0 + __s1 * __builtin_shufflevector(__s2, __s2, __p3, __p3, __p3, __p3); \
  __ret; \
})
#else
#define vmlaq_lane_u32(__p0, __p1, __p2, __p3) __extension__ ({ \
  uint32x4_t __s0 = __p0; \
  uint32x4_t __s1 = __p1; \
  uint32x2_t __s2 = __p2; \
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  uint32x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  uint32x2_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 1, 0); \
  uint32x4_t __ret; \
  __ret = __rev0 + __rev1 * __builtin_shufflevector(__rev2, __rev2, __p3, __p3, __p3, __p3); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmlaq_lane_u16(__p0, __p1, __p2, __p3) __extension__ ({ \
  uint16x8_t __s0 = __p0; \
  uint16x8_t __s1 = __p1; \
  uint16x4_t __s2 = __p2; \
  uint16x8_t __ret; \
  __ret = __s0 + __s1 * __builtin_shufflevector(__s2, __s2, __p3, __p3, __p3, __p3, __p3, __p3, __p3, __p3); \
  __ret; \
})
#else
#define vmlaq_lane_u16(__p0, __p1, __p2, __p3) __extension__ ({ \
  uint16x8_t __s0 = __p0; \
  uint16x8_t __s1 = __p1; \
  uint16x4_t __s2 = __p2; \
  uint16x8_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint16x8_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint16x4_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 3, 2, 1, 0); \
  uint16x8_t __ret; \
  __ret = __rev0 + __rev1 * __builtin_shufflevector(__rev2, __rev2, __p3, __p3, __p3, __p3, __p3, __p3, __p3, __p3); \
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmlaq_lane_f32(__p0, __p1, __p2, __p3) __extension__ ({ \
  float32x4_t __s0 = __p0; \
  float32x4_t __s1 = __p1; \
  float32x2_t __s2 = __p2; \
  float32x4_t __ret; \
  __ret = __s0 + __s1 * __builtin_shufflevector(__s2, __s2, __p3, __p3, __p3, __p3); \
  __ret; \
})
#else
#define vmlaq_lane_f32(__p0, __p1, __p2, __p3) __extension__ ({ \
  float32x4_t __s0 = __p0; \
  float32x4_t __s1 = __p1; \
  float32x2_t __s2 = __p2; \
  float32x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  float32x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  float32x2_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 1, 0); \
  float32x4_t __ret; \
  __ret = __rev0 + __rev1 * __builtin_shufflevector(__rev2, __rev2, __p3, __p3, __p3, __p3); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmlaq_lane_s32(__p0, __p1, __p2, __p3) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  int32x4_t __s1 = __p1; \
  int32x2_t __s2 = __p2; \
  int32x4_t __ret; \
  __ret = __s0 + __s1 * __builtin_shufflevector(__s2, __s2, __p3, __p3, __p3, __p3); \
  __ret; \
})
#else
#define vmlaq_lane_s32(__p0, __p1, __p2, __p3) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  int32x4_t __s1 = __p1; \
  int32x2_t __s2 = __p2; \
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  int32x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  int32x2_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 1, 0); \
  int32x4_t __ret; \
  __ret = __rev0 + __rev1 * __builtin_shufflevector(__rev2, __rev2, __p3, __p3, __p3, __p3); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmlaq_lane_s16(__p0, __p1, __p2, __p3) __extension__ ({ \
  int16x8_t __s0 = __p0; \
  int16x8_t __s1 = __p1; \
  int16x4_t __s2 = __p2; \
  int16x8_t __ret; \
  __ret = __s0 + __s1 * __builtin_shufflevector(__s2, __s2, __p3, __p3, __p3, __p3, __p3, __p3, __p3, __p3); \
  __ret; \
})
#else
#define vmlaq_lane_s16(__p0, __p1, __p2, __p3) __extension__ ({ \
  int16x8_t __s0 = __p0; \
  int16x8_t __s1 = __p1; \
  int16x4_t __s2 = __p2; \
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 7, 6, 5, 4, 3, 2, 1, 0); \
  int16x8_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 7, 6, 5, 4, 3, 2, 1, 0); \
  int16x4_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 3, 2, 1, 0); \
  int16x8_t __ret; \
  __ret = __rev0 + __rev1 * __builtin_shufflevector(__rev2, __rev2, __p3, __p3, __p3, __p3, __p3, __p3, __p3, __p3); \
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmla_lane_u32(__p0, __p1, __p2, __p3) __extension__ ({ \
  uint32x2_t __s0 = __p0; \
  uint32x2_t __s1 = __p1; \
  uint32x2_t __s2 = __p2; \
  uint32x2_t __ret; \
  __ret = __s0 + __s1 * __builtin_shufflevector(__s2, __s2, __p3, __p3); \
  __ret; \
})
#else
#define vmla_lane_u32(__p0, __p1, __p2, __p3) __extension__ ({ \
  uint32x2_t __s0 = __p0; \
  uint32x2_t __s1 = __p1; \
  uint32x2_t __s2 = __p2; \
  uint32x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  uint32x2_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 1, 0); \
  uint32x2_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 1, 0); \
  uint32x2_t __ret; \
  __ret = __rev0 + __rev1 * __builtin_shufflevector(__rev2, __rev2, __p3, __p3); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmla_lane_u16(__p0, __p1, __p2, __p3) __extension__ ({ \
  uint16x4_t __s0 = __p0; \
  uint16x4_t __s1 = __p1; \
  uint16x4_t __s2 = __p2; \
  uint16x4_t __ret; \
  __ret = __s0 + __s1 * __builtin_shufflevector(__s2, __s2, __p3, __p3, __p3, __p3); \
  __ret; \
})
#else
#define vmla_lane_u16(__p0, __p1, __p2, __p3) __extension__ ({ \
  uint16x4_t __s0 = __p0; \
  uint16x4_t __s1 = __p1; \
  uint16x4_t __s2 = __p2; \
  uint16x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  uint16x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  uint16x4_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 3, 2, 1, 0); \
  uint16x4_t __ret; \
  __ret = __rev0 + __rev1 * __builtin_shufflevector(__rev2, __rev2, __p3, __p3, __p3, __p3); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmla_lane_f32(__p0, __p1, __p2, __p3) __extension__ ({ \
  float32x2_t __s0 = __p0; \
  float32x2_t __s1 = __p1; \
  float32x2_t __s2 = __p2; \
  float32x2_t __ret; \
  __ret = __s0 + __s1 * __builtin_shufflevector(__s2, __s2, __p3, __p3); \
  __ret; \
})
#else
#define vmla_lane_f32(__p0, __p1, __p2, __p3) __extension__ ({ \
  float32x2_t __s0 = __p0; \
  float32x2_t __s1 = __p1; \
  float32x2_t __s2 = __p2; \
  float32x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  float32x2_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 1, 0); \
  float32x2_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 1, 0); \
  float32x2_t __ret; \
  __ret = __rev0 + __rev1 * __builtin_shufflevector(__rev2, __rev2, __p3, __p3); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmla_lane_s32(__p0, __p1, __p2, __p3) __extension__ ({ \
  int32x2_t __s0 = __p0; \
  int32x2_t __s1 = __p1; \
  int32x2_t __s2 = __p2; \
  int32x2_t __ret; \
  __ret = __s0 + __s1 * __builtin_shufflevector(__s2, __s2, __p3, __p3); \
  __ret; \
})
#else
#define vmla_lane_s32(__p0, __p1, __p2, __p3) __extension__ ({ \
  int32x2_t __s0 = __p0; \
  int32x2_t __s1 = __p1; \
  int32x2_t __s2 = __p2; \
  int32x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  int32x2_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 1, 0); \
  int32x2_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 1, 0); \
  int32x2_t __ret; \
  __ret = __rev0 + __rev1 * __builtin_shufflevector(__rev2, __rev2, __p3, __p3); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmla_lane_s16(__p0, __p1, __p2, __p3) __extension__ ({ \
  int16x4_t __s0 = __p0; \
  int16x4_t __s1 = __p1; \
  int16x4_t __s2 = __p2; \
  int16x4_t __ret; \
  __ret = __s0 + __s1 * __builtin_shufflevector(__s2, __s2, __p3, __p3, __p3, __p3); \
  __ret; \
})
#else
#define vmla_lane_s16(__p0, __p1, __p2, __p3) __extension__ ({ \
  int16x4_t __s0 = __p0; \
  int16x4_t __s1 = __p1; \
  int16x4_t __s2 = __p2; \
  int16x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  int16x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  int16x4_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 3, 2, 1, 0); \
  int16x4_t __ret; \
  __ret = __rev0 + __rev1 * __builtin_shufflevector(__rev2, __rev2, __p3, __p3, __p3, __p3); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vmlaq_n_u32(uint32x4_t __p0, uint32x4_t __p1, uint32_t __p2) {
  uint32x4_t __ret;
  __ret = __p0 + __p1 * (uint32x4_t) {__p2, __p2, __p2, __p2};
  return __ret;
}
#else
__ai uint32x4_t vmlaq_n_u32(uint32x4_t __p0, uint32x4_t __p1, uint32_t __p2) {
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint32x4_t __ret;
  __ret = __rev0 + __rev1 * (uint32x4_t) {__p2, __p2, __p2, __p2};
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x8_t vmlaq_n_u16(uint16x8_t __p0, uint16x8_t __p1, uint16_t __p2) {
  uint16x8_t __ret;
  __ret = __p0 + __p1 * (uint16x8_t) {__p2, __p2, __p2, __p2, __p2, __p2, __p2, __p2};
  return __ret;
}
#else
__ai uint16x8_t vmlaq_n_u16(uint16x8_t __p0, uint16x8_t __p1, uint16_t __p2) {
  uint16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __ret;
  __ret = __rev0 + __rev1 * (uint16x8_t) {__p2, __p2, __p2, __p2, __p2, __p2, __p2, __p2};
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x4_t vmlaq_n_f32(float32x4_t __p0, float32x4_t __p1, float32_t __p2) {
  float32x4_t __ret;
  __ret = __p0 + __p1 * (float32x4_t) {__p2, __p2, __p2, __p2};
  return __ret;
}
#else
__ai float32x4_t vmlaq_n_f32(float32x4_t __p0, float32x4_t __p1, float32_t __p2) {
  float32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  float32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  float32x4_t __ret;
  __ret = __rev0 + __rev1 * (float32x4_t) {__p2, __p2, __p2, __p2};
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vmlaq_n_s32(int32x4_t __p0, int32x4_t __p1, int32_t __p2) {
  int32x4_t __ret;
  __ret = __p0 + __p1 * (int32x4_t) {__p2, __p2, __p2, __p2};
  return __ret;
}
#else
__ai int32x4_t vmlaq_n_s32(int32x4_t __p0, int32x4_t __p1, int32_t __p2) {
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int32x4_t __ret;
  __ret = __rev0 + __rev1 * (int32x4_t) {__p2, __p2, __p2, __p2};
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x8_t vmlaq_n_s16(int16x8_t __p0, int16x8_t __p1, int16_t __p2) {
  int16x8_t __ret;
  __ret = __p0 + __p1 * (int16x8_t) {__p2, __p2, __p2, __p2, __p2, __p2, __p2, __p2};
  return __ret;
}
#else
__ai int16x8_t vmlaq_n_s16(int16x8_t __p0, int16x8_t __p1, int16_t __p2) {
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __ret;
  __ret = __rev0 + __rev1 * (int16x8_t) {__p2, __p2, __p2, __p2, __p2, __p2, __p2, __p2};
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x2_t vmla_n_u32(uint32x2_t __p0, uint32x2_t __p1, uint32_t __p2) {
  uint32x2_t __ret;
  __ret = __p0 + __p1 * (uint32x2_t) {__p2, __p2};
  return __ret;
}
#else
__ai uint32x2_t vmla_n_u32(uint32x2_t __p0, uint32x2_t __p1, uint32_t __p2) {
  uint32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  uint32x2_t __ret;
  __ret = __rev0 + __rev1 * (uint32x2_t) {__p2, __p2};
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x4_t vmla_n_u16(uint16x4_t __p0, uint16x4_t __p1, uint16_t __p2) {
  uint16x4_t __ret;
  __ret = __p0 + __p1 * (uint16x4_t) {__p2, __p2, __p2, __p2};
  return __ret;
}
#else
__ai uint16x4_t vmla_n_u16(uint16x4_t __p0, uint16x4_t __p1, uint16_t __p2) {
  uint16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint16x4_t __ret;
  __ret = __rev0 + __rev1 * (uint16x4_t) {__p2, __p2, __p2, __p2};
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x2_t vmla_n_f32(float32x2_t __p0, float32x2_t __p1, float32_t __p2) {
  float32x2_t __ret;
  __ret = __p0 + __p1 * (float32x2_t) {__p2, __p2};
  return __ret;
}
#else
__ai float32x2_t vmla_n_f32(float32x2_t __p0, float32x2_t __p1, float32_t __p2) {
  float32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  float32x2_t __ret;
  __ret = __rev0 + __rev1 * (float32x2_t) {__p2, __p2};
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x2_t vmla_n_s32(int32x2_t __p0, int32x2_t __p1, int32_t __p2) {
  int32x2_t __ret;
  __ret = __p0 + __p1 * (int32x2_t) {__p2, __p2};
  return __ret;
}
#else
__ai int32x2_t vmla_n_s32(int32x2_t __p0, int32x2_t __p1, int32_t __p2) {
  int32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  int32x2_t __ret;
  __ret = __rev0 + __rev1 * (int32x2_t) {__p2, __p2};
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x4_t vmla_n_s16(int16x4_t __p0, int16x4_t __p1, int16_t __p2) {
  int16x4_t __ret;
  __ret = __p0 + __p1 * (int16x4_t) {__p2, __p2, __p2, __p2};
  return __ret;
}
#else
__ai int16x4_t vmla_n_s16(int16x4_t __p0, int16x4_t __p1, int16_t __p2) {
  int16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int16x4_t __ret;
  __ret = __rev0 + __rev1 * (int16x4_t) {__p2, __p2, __p2, __p2};
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x16_t vmlsq_u8(uint8x16_t __p0, uint8x16_t __p1, uint8x16_t __p2) {
  uint8x16_t __ret;
  __ret = __p0 - __p1 * __p2;
  return __ret;
}
#else
__ai uint8x16_t vmlsq_u8(uint8x16_t __p0, uint8x16_t __p1, uint8x16_t __p2) {
  uint8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __ret;
  __ret = __rev0 - __rev1 * __rev2;
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vmlsq_u32(uint32x4_t __p0, uint32x4_t __p1, uint32x4_t __p2) {
  uint32x4_t __ret;
  __ret = __p0 - __p1 * __p2;
  return __ret;
}
#else
__ai uint32x4_t vmlsq_u32(uint32x4_t __p0, uint32x4_t __p1, uint32x4_t __p2) {
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint32x4_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 3, 2, 1, 0);
  uint32x4_t __ret;
  __ret = __rev0 - __rev1 * __rev2;
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x8_t vmlsq_u16(uint16x8_t __p0, uint16x8_t __p1, uint16x8_t __p2) {
  uint16x8_t __ret;
  __ret = __p0 - __p1 * __p2;
  return __ret;
}
#else
__ai uint16x8_t vmlsq_u16(uint16x8_t __p0, uint16x8_t __p1, uint16x8_t __p2) {
  uint16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __ret;
  __ret = __rev0 - __rev1 * __rev2;
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x16_t vmlsq_s8(int8x16_t __p0, int8x16_t __p1, int8x16_t __p2) {
  int8x16_t __ret;
  __ret = __p0 - __p1 * __p2;
  return __ret;
}
#else
__ai int8x16_t vmlsq_s8(int8x16_t __p0, int8x16_t __p1, int8x16_t __p2) {
  int8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __ret;
  __ret = __rev0 - __rev1 * __rev2;
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x4_t vmlsq_f32(float32x4_t __p0, float32x4_t __p1, float32x4_t __p2) {
  float32x4_t __ret;
  __ret = __p0 - __p1 * __p2;
  return __ret;
}
#else
__ai float32x4_t vmlsq_f32(float32x4_t __p0, float32x4_t __p1, float32x4_t __p2) {
  float32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  float32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  float32x4_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 3, 2, 1, 0);
  float32x4_t __ret;
  __ret = __rev0 - __rev1 * __rev2;
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vmlsq_s32(int32x4_t __p0, int32x4_t __p1, int32x4_t __p2) {
  int32x4_t __ret;
  __ret = __p0 - __p1 * __p2;
  return __ret;
}
#else
__ai int32x4_t vmlsq_s32(int32x4_t __p0, int32x4_t __p1, int32x4_t __p2) {
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int32x4_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 3, 2, 1, 0);
  int32x4_t __ret;
  __ret = __rev0 - __rev1 * __rev2;
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x8_t vmlsq_s16(int16x8_t __p0, int16x8_t __p1, int16x8_t __p2) {
  int16x8_t __ret;
  __ret = __p0 - __p1 * __p2;
  return __ret;
}
#else
__ai int16x8_t vmlsq_s16(int16x8_t __p0, int16x8_t __p1, int16x8_t __p2) {
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __ret;
  __ret = __rev0 - __rev1 * __rev2;
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x8_t vmls_u8(uint8x8_t __p0, uint8x8_t __p1, uint8x8_t __p2) {
  uint8x8_t __ret;
  __ret = __p0 - __p1 * __p2;
  return __ret;
}
#else
__ai uint8x8_t vmls_u8(uint8x8_t __p0, uint8x8_t __p1, uint8x8_t __p2) {
  uint8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __ret;
  __ret = __rev0 - __rev1 * __rev2;
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x2_t vmls_u32(uint32x2_t __p0, uint32x2_t __p1, uint32x2_t __p2) {
  uint32x2_t __ret;
  __ret = __p0 - __p1 * __p2;
  return __ret;
}
#else
__ai uint32x2_t vmls_u32(uint32x2_t __p0, uint32x2_t __p1, uint32x2_t __p2) {
  uint32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  uint32x2_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 1, 0);
  uint32x2_t __ret;
  __ret = __rev0 - __rev1 * __rev2;
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x4_t vmls_u16(uint16x4_t __p0, uint16x4_t __p1, uint16x4_t __p2) {
  uint16x4_t __ret;
  __ret = __p0 - __p1 * __p2;
  return __ret;
}
#else
__ai uint16x4_t vmls_u16(uint16x4_t __p0, uint16x4_t __p1, uint16x4_t __p2) {
  uint16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint16x4_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 3, 2, 1, 0);
  uint16x4_t __ret;
  __ret = __rev0 - __rev1 * __rev2;
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x8_t vmls_s8(int8x8_t __p0, int8x8_t __p1, int8x8_t __p2) {
  int8x8_t __ret;
  __ret = __p0 - __p1 * __p2;
  return __ret;
}
#else
__ai int8x8_t vmls_s8(int8x8_t __p0, int8x8_t __p1, int8x8_t __p2) {
  int8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __ret;
  __ret = __rev0 - __rev1 * __rev2;
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x2_t vmls_f32(float32x2_t __p0, float32x2_t __p1, float32x2_t __p2) {
  float32x2_t __ret;
  __ret = __p0 - __p1 * __p2;
  return __ret;
}
#else
__ai float32x2_t vmls_f32(float32x2_t __p0, float32x2_t __p1, float32x2_t __p2) {
  float32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  float32x2_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 1, 0);
  float32x2_t __ret;
  __ret = __rev0 - __rev1 * __rev2;
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x2_t vmls_s32(int32x2_t __p0, int32x2_t __p1, int32x2_t __p2) {
  int32x2_t __ret;
  __ret = __p0 - __p1 * __p2;
  return __ret;
}
#else
__ai int32x2_t vmls_s32(int32x2_t __p0, int32x2_t __p1, int32x2_t __p2) {
  int32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  int32x2_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 1, 0);
  int32x2_t __ret;
  __ret = __rev0 - __rev1 * __rev2;
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x4_t vmls_s16(int16x4_t __p0, int16x4_t __p1, int16x4_t __p2) {
  int16x4_t __ret;
  __ret = __p0 - __p1 * __p2;
  return __ret;
}
#else
__ai int16x4_t vmls_s16(int16x4_t __p0, int16x4_t __p1, int16x4_t __p2) {
  int16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int16x4_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 3, 2, 1, 0);
  int16x4_t __ret;
  __ret = __rev0 - __rev1 * __rev2;
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
#define vmlsq_lane_u32(__p0, __p1, __p2, __p3) __extension__ ({ \
  uint32x4_t __s0 = __p0; \
  uint32x4_t __s1 = __p1; \
  uint32x2_t __s2 = __p2; \
  uint32x4_t __ret; \
  __ret = __s0 - __s1 * __builtin_shufflevector(__s2, __s2, __p3, __p3, __p3, __p3); \
  __ret; \
})
#else
#define vmlsq_lane_u32(__p0, __p1, __p2, __p3) __extension__ ({ \
  uint32x4_t __s0 = __p0; \
  uint32x4_t __s1 = __p1; \
  uint32x2_t __s2 = __p2; \
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  uint32x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  uint32x2_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 1, 0); \
  uint32x4_t __ret; \
  __ret = __rev0 - __rev1 * __builtin_shufflevector(__rev2, __rev2, __p3, __p3, __p3, __p3); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmlsq_lane_u16(__p0, __p1, __p2, __p3) __extension__ ({ \
  uint16x8_t __s0 = __p0; \
  uint16x8_t __s1 = __p1; \
  uint16x4_t __s2 = __p2; \
  uint16x8_t __ret; \
  __ret = __s0 - __s1 * __builtin_shufflevector(__s2, __s2, __p3, __p3, __p3, __p3, __p3, __p3, __p3, __p3); \
  __ret; \
})
#else
#define vmlsq_lane_u16(__p0, __p1, __p2, __p3) __extension__ ({ \
  uint16x8_t __s0 = __p0; \
  uint16x8_t __s1 = __p1; \
  uint16x4_t __s2 = __p2; \
  uint16x8_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint16x8_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint16x4_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 3, 2, 1, 0); \
  uint16x8_t __ret; \
  __ret = __rev0 - __rev1 * __builtin_shufflevector(__rev2, __rev2, __p3, __p3, __p3, __p3, __p3, __p3, __p3, __p3); \
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmlsq_lane_f32(__p0, __p1, __p2, __p3) __extension__ ({ \
  float32x4_t __s0 = __p0; \
  float32x4_t __s1 = __p1; \
  float32x2_t __s2 = __p2; \
  float32x4_t __ret; \
  __ret = __s0 - __s1 * __builtin_shufflevector(__s2, __s2, __p3, __p3, __p3, __p3); \
  __ret; \
})
#else
#define vmlsq_lane_f32(__p0, __p1, __p2, __p3) __extension__ ({ \
  float32x4_t __s0 = __p0; \
  float32x4_t __s1 = __p1; \
  float32x2_t __s2 = __p2; \
  float32x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  float32x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  float32x2_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 1, 0); \
  float32x4_t __ret; \
  __ret = __rev0 - __rev1 * __builtin_shufflevector(__rev2, __rev2, __p3, __p3, __p3, __p3); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmlsq_lane_s32(__p0, __p1, __p2, __p3) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  int32x4_t __s1 = __p1; \
  int32x2_t __s2 = __p2; \
  int32x4_t __ret; \
  __ret = __s0 - __s1 * __builtin_shufflevector(__s2, __s2, __p3, __p3, __p3, __p3); \
  __ret; \
})
#else
#define vmlsq_lane_s32(__p0, __p1, __p2, __p3) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  int32x4_t __s1 = __p1; \
  int32x2_t __s2 = __p2; \
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  int32x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  int32x2_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 1, 0); \
  int32x4_t __ret; \
  __ret = __rev0 - __rev1 * __builtin_shufflevector(__rev2, __rev2, __p3, __p3, __p3, __p3); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmlsq_lane_s16(__p0, __p1, __p2, __p3) __extension__ ({ \
  int16x8_t __s0 = __p0; \
  int16x8_t __s1 = __p1; \
  int16x4_t __s2 = __p2; \
  int16x8_t __ret; \
  __ret = __s0 - __s1 * __builtin_shufflevector(__s2, __s2, __p3, __p3, __p3, __p3, __p3, __p3, __p3, __p3); \
  __ret; \
})
#else
#define vmlsq_lane_s16(__p0, __p1, __p2, __p3) __extension__ ({ \
  int16x8_t __s0 = __p0; \
  int16x8_t __s1 = __p1; \
  int16x4_t __s2 = __p2; \
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 7, 6, 5, 4, 3, 2, 1, 0); \
  int16x8_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 7, 6, 5, 4, 3, 2, 1, 0); \
  int16x4_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 3, 2, 1, 0); \
  int16x8_t __ret; \
  __ret = __rev0 - __rev1 * __builtin_shufflevector(__rev2, __rev2, __p3, __p3, __p3, __p3, __p3, __p3, __p3, __p3); \
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmls_lane_u32(__p0, __p1, __p2, __p3) __extension__ ({ \
  uint32x2_t __s0 = __p0; \
  uint32x2_t __s1 = __p1; \
  uint32x2_t __s2 = __p2; \
  uint32x2_t __ret; \
  __ret = __s0 - __s1 * __builtin_shufflevector(__s2, __s2, __p3, __p3); \
  __ret; \
})
#else
#define vmls_lane_u32(__p0, __p1, __p2, __p3) __extension__ ({ \
  uint32x2_t __s0 = __p0; \
  uint32x2_t __s1 = __p1; \
  uint32x2_t __s2 = __p2; \
  uint32x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  uint32x2_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 1, 0); \
  uint32x2_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 1, 0); \
  uint32x2_t __ret; \
  __ret = __rev0 - __rev1 * __builtin_shufflevector(__rev2, __rev2, __p3, __p3); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmls_lane_u16(__p0, __p1, __p2, __p3) __extension__ ({ \
  uint16x4_t __s0 = __p0; \
  uint16x4_t __s1 = __p1; \
  uint16x4_t __s2 = __p2; \
  uint16x4_t __ret; \
  __ret = __s0 - __s1 * __builtin_shufflevector(__s2, __s2, __p3, __p3, __p3, __p3); \
  __ret; \
})
#else
#define vmls_lane_u16(__p0, __p1, __p2, __p3) __extension__ ({ \
  uint16x4_t __s0 = __p0; \
  uint16x4_t __s1 = __p1; \
  uint16x4_t __s2 = __p2; \
  uint16x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  uint16x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  uint16x4_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 3, 2, 1, 0); \
  uint16x4_t __ret; \
  __ret = __rev0 - __rev1 * __builtin_shufflevector(__rev2, __rev2, __p3, __p3, __p3, __p3); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmls_lane_f32(__p0, __p1, __p2, __p3) __extension__ ({ \
  float32x2_t __s0 = __p0; \
  float32x2_t __s1 = __p1; \
  float32x2_t __s2 = __p2; \
  float32x2_t __ret; \
  __ret = __s0 - __s1 * __builtin_shufflevector(__s2, __s2, __p3, __p3); \
  __ret; \
})
#else
#define vmls_lane_f32(__p0, __p1, __p2, __p3) __extension__ ({ \
  float32x2_t __s0 = __p0; \
  float32x2_t __s1 = __p1; \
  float32x2_t __s2 = __p2; \
  float32x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  float32x2_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 1, 0); \
  float32x2_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 1, 0); \
  float32x2_t __ret; \
  __ret = __rev0 - __rev1 * __builtin_shufflevector(__rev2, __rev2, __p3, __p3); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmls_lane_s32(__p0, __p1, __p2, __p3) __extension__ ({ \
  int32x2_t __s0 = __p0; \
  int32x2_t __s1 = __p1; \
  int32x2_t __s2 = __p2; \
  int32x2_t __ret; \
  __ret = __s0 - __s1 * __builtin_shufflevector(__s2, __s2, __p3, __p3); \
  __ret; \
})
#else
#define vmls_lane_s32(__p0, __p1, __p2, __p3) __extension__ ({ \
  int32x2_t __s0 = __p0; \
  int32x2_t __s1 = __p1; \
  int32x2_t __s2 = __p2; \
  int32x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  int32x2_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 1, 0); \
  int32x2_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 1, 0); \
  int32x2_t __ret; \
  __ret = __rev0 - __rev1 * __builtin_shufflevector(__rev2, __rev2, __p3, __p3); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmls_lane_s16(__p0, __p1, __p2, __p3) __extension__ ({ \
  int16x4_t __s0 = __p0; \
  int16x4_t __s1 = __p1; \
  int16x4_t __s2 = __p2; \
  int16x4_t __ret; \
  __ret = __s0 - __s1 * __builtin_shufflevector(__s2, __s2, __p3, __p3, __p3, __p3); \
  __ret; \
})
#else
#define vmls_lane_s16(__p0, __p1, __p2, __p3) __extension__ ({ \
  int16x4_t __s0 = __p0; \
  int16x4_t __s1 = __p1; \
  int16x4_t __s2 = __p2; \
  int16x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  int16x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  int16x4_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 3, 2, 1, 0); \
  int16x4_t __ret; \
  __ret = __rev0 - __rev1 * __builtin_shufflevector(__rev2, __rev2, __p3, __p3, __p3, __p3); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vmlsq_n_u32(uint32x4_t __p0, uint32x4_t __p1, uint32_t __p2) {
  uint32x4_t __ret;
  __ret = __p0 - __p1 * (uint32x4_t) {__p2, __p2, __p2, __p2};
  return __ret;
}
#else
__ai uint32x4_t vmlsq_n_u32(uint32x4_t __p0, uint32x4_t __p1, uint32_t __p2) {
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint32x4_t __ret;
  __ret = __rev0 - __rev1 * (uint32x4_t) {__p2, __p2, __p2, __p2};
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x8_t vmlsq_n_u16(uint16x8_t __p0, uint16x8_t __p1, uint16_t __p2) {
  uint16x8_t __ret;
  __ret = __p0 - __p1 * (uint16x8_t) {__p2, __p2, __p2, __p2, __p2, __p2, __p2, __p2};
  return __ret;
}
#else
__ai uint16x8_t vmlsq_n_u16(uint16x8_t __p0, uint16x8_t __p1, uint16_t __p2) {
  uint16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __ret;
  __ret = __rev0 - __rev1 * (uint16x8_t) {__p2, __p2, __p2, __p2, __p2, __p2, __p2, __p2};
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x4_t vmlsq_n_f32(float32x4_t __p0, float32x4_t __p1, float32_t __p2) {
  float32x4_t __ret;
  __ret = __p0 - __p1 * (float32x4_t) {__p2, __p2, __p2, __p2};
  return __ret;
}
#else
__ai float32x4_t vmlsq_n_f32(float32x4_t __p0, float32x4_t __p1, float32_t __p2) {
  float32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  float32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  float32x4_t __ret;
  __ret = __rev0 - __rev1 * (float32x4_t) {__p2, __p2, __p2, __p2};
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vmlsq_n_s32(int32x4_t __p0, int32x4_t __p1, int32_t __p2) {
  int32x4_t __ret;
  __ret = __p0 - __p1 * (int32x4_t) {__p2, __p2, __p2, __p2};
  return __ret;
}
#else
__ai int32x4_t vmlsq_n_s32(int32x4_t __p0, int32x4_t __p1, int32_t __p2) {
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int32x4_t __ret;
  __ret = __rev0 - __rev1 * (int32x4_t) {__p2, __p2, __p2, __p2};
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x8_t vmlsq_n_s16(int16x8_t __p0, int16x8_t __p1, int16_t __p2) {
  int16x8_t __ret;
  __ret = __p0 - __p1 * (int16x8_t) {__p2, __p2, __p2, __p2, __p2, __p2, __p2, __p2};
  return __ret;
}
#else
__ai int16x8_t vmlsq_n_s16(int16x8_t __p0, int16x8_t __p1, int16_t __p2) {
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __ret;
  __ret = __rev0 - __rev1 * (int16x8_t) {__p2, __p2, __p2, __p2, __p2, __p2, __p2, __p2};
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x2_t vmls_n_u32(uint32x2_t __p0, uint32x2_t __p1, uint32_t __p2) {
  uint32x2_t __ret;
  __ret = __p0 - __p1 * (uint32x2_t) {__p2, __p2};
  return __ret;
}
#else
__ai uint32x2_t vmls_n_u32(uint32x2_t __p0, uint32x2_t __p1, uint32_t __p2) {
  uint32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  uint32x2_t __ret;
  __ret = __rev0 - __rev1 * (uint32x2_t) {__p2, __p2};
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x4_t vmls_n_u16(uint16x4_t __p0, uint16x4_t __p1, uint16_t __p2) {
  uint16x4_t __ret;
  __ret = __p0 - __p1 * (uint16x4_t) {__p2, __p2, __p2, __p2};
  return __ret;
}
#else
__ai uint16x4_t vmls_n_u16(uint16x4_t __p0, uint16x4_t __p1, uint16_t __p2) {
  uint16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint16x4_t __ret;
  __ret = __rev0 - __rev1 * (uint16x4_t) {__p2, __p2, __p2, __p2};
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x2_t vmls_n_f32(float32x2_t __p0, float32x2_t __p1, float32_t __p2) {
  float32x2_t __ret;
  __ret = __p0 - __p1 * (float32x2_t) {__p2, __p2};
  return __ret;
}
#else
__ai float32x2_t vmls_n_f32(float32x2_t __p0, float32x2_t __p1, float32_t __p2) {
  float32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  float32x2_t __ret;
  __ret = __rev0 - __rev1 * (float32x2_t) {__p2, __p2};
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x2_t vmls_n_s32(int32x2_t __p0, int32x2_t __p1, int32_t __p2) {
  int32x2_t __ret;
  __ret = __p0 - __p1 * (int32x2_t) {__p2, __p2};
  return __ret;
}
#else
__ai int32x2_t vmls_n_s32(int32x2_t __p0, int32x2_t __p1, int32_t __p2) {
  int32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  int32x2_t __ret;
  __ret = __rev0 - __rev1 * (int32x2_t) {__p2, __p2};
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x4_t vmls_n_s16(int16x4_t __p0, int16x4_t __p1, int16_t __p2) {
  int16x4_t __ret;
  __ret = __p0 - __p1 * (int16x4_t) {__p2, __p2, __p2, __p2};
  return __ret;
}
#else
__ai int16x4_t vmls_n_s16(int16x4_t __p0, int16x4_t __p1, int16_t __p2) {
  int16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int16x4_t __ret;
  __ret = __rev0 - __rev1 * (int16x4_t) {__p2, __p2, __p2, __p2};
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x8_t vmov_n_p8(poly8_t __p0) {
  poly8x8_t __ret;
  __ret = (poly8x8_t) {__p0, __p0, __p0, __p0, __p0, __p0, __p0, __p0};
  return __ret;
}
#else
__ai poly8x8_t vmov_n_p8(poly8_t __p0) {
  poly8x8_t __ret;
  __ret = (poly8x8_t) {__p0, __p0, __p0, __p0, __p0, __p0, __p0, __p0};
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly16x4_t vmov_n_p16(poly16_t __p0) {
  poly16x4_t __ret;
  __ret = (poly16x4_t) {__p0, __p0, __p0, __p0};
  return __ret;
}
#else
__ai poly16x4_t vmov_n_p16(poly16_t __p0) {
  poly16x4_t __ret;
  __ret = (poly16x4_t) {__p0, __p0, __p0, __p0};
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x16_t vmovq_n_p8(poly8_t __p0) {
  poly8x16_t __ret;
  __ret = (poly8x16_t) {__p0, __p0, __p0, __p0, __p0, __p0, __p0, __p0, __p0, __p0, __p0, __p0, __p0, __p0, __p0, __p0};
  return __ret;
}
#else
__ai poly8x16_t vmovq_n_p8(poly8_t __p0) {
  poly8x16_t __ret;
  __ret = (poly8x16_t) {__p0, __p0, __p0, __p0, __p0, __p0, __p0, __p0, __p0, __p0, __p0, __p0, __p0, __p0, __p0, __p0};
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly16x8_t vmovq_n_p16(poly16_t __p0) {
  poly16x8_t __ret;
  __ret = (poly16x8_t) {__p0, __p0, __p0, __p0, __p0, __p0, __p0, __p0};
  return __ret;
}
#else
__ai poly16x8_t vmovq_n_p16(poly16_t __p0) {
  poly16x8_t __ret;
  __ret = (poly16x8_t) {__p0, __p0, __p0, __p0, __p0, __p0, __p0, __p0};
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x16_t vmovq_n_u8(uint8_t __p0) {
  uint8x16_t __ret;
  __ret = (uint8x16_t) {__p0, __p0, __p0, __p0, __p0, __p0, __p0, __p0, __p0, __p0, __p0, __p0, __p0, __p0, __p0, __p0};
  return __ret;
}
#else
__ai uint8x16_t vmovq_n_u8(uint8_t __p0) {
  uint8x16_t __ret;
  __ret = (uint8x16_t) {__p0, __p0, __p0, __p0, __p0, __p0, __p0, __p0, __p0, __p0, __p0, __p0, __p0, __p0, __p0, __p0};
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vmovq_n_u32(uint32_t __p0) {
  uint32x4_t __ret;
  __ret = (uint32x4_t) {__p0, __p0, __p0, __p0};
  return __ret;
}
#else
__ai uint32x4_t vmovq_n_u32(uint32_t __p0) {
  uint32x4_t __ret;
  __ret = (uint32x4_t) {__p0, __p0, __p0, __p0};
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vmovq_n_u64(uint64_t __p0) {
  uint64x2_t __ret;
  __ret = (uint64x2_t) {__p0, __p0};
  return __ret;
}
#else
__ai uint64x2_t vmovq_n_u64(uint64_t __p0) {
  uint64x2_t __ret;
  __ret = (uint64x2_t) {__p0, __p0};
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x8_t vmovq_n_u16(uint16_t __p0) {
  uint16x8_t __ret;
  __ret = (uint16x8_t) {__p0, __p0, __p0, __p0, __p0, __p0, __p0, __p0};
  return __ret;
}
#else
__ai uint16x8_t vmovq_n_u16(uint16_t __p0) {
  uint16x8_t __ret;
  __ret = (uint16x8_t) {__p0, __p0, __p0, __p0, __p0, __p0, __p0, __p0};
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x16_t vmovq_n_s8(int8_t __p0) {
  int8x16_t __ret;
  __ret = (int8x16_t) {__p0, __p0, __p0, __p0, __p0, __p0, __p0, __p0, __p0, __p0, __p0, __p0, __p0, __p0, __p0, __p0};
  return __ret;
}
#else
__ai int8x16_t vmovq_n_s8(int8_t __p0) {
  int8x16_t __ret;
  __ret = (int8x16_t) {__p0, __p0, __p0, __p0, __p0, __p0, __p0, __p0, __p0, __p0, __p0, __p0, __p0, __p0, __p0, __p0};
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x4_t vmovq_n_f32(float32_t __p0) {
  float32x4_t __ret;
  __ret = (float32x4_t) {__p0, __p0, __p0, __p0};
  return __ret;
}
#else
__ai float32x4_t vmovq_n_f32(float32_t __p0) {
  float32x4_t __ret;
  __ret = (float32x4_t) {__p0, __p0, __p0, __p0};
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
#define vmovq_n_f16(__p0) __extension__ ({ \
  float16_t __s0 = __p0; \
  float16x8_t __ret; \
  __ret = (float16x8_t) {__s0, __s0, __s0, __s0, __s0, __s0, __s0, __s0}; \
  __ret; \
})
#else
#define vmovq_n_f16(__p0) __extension__ ({ \
  float16_t __s0 = __p0; \
  float16x8_t __ret; \
  __ret = (float16x8_t) {__s0, __s0, __s0, __s0, __s0, __s0, __s0, __s0}; \
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vmovq_n_s32(int32_t __p0) {
  int32x4_t __ret;
  __ret = (int32x4_t) {__p0, __p0, __p0, __p0};
  return __ret;
}
#else
__ai int32x4_t vmovq_n_s32(int32_t __p0) {
  int32x4_t __ret;
  __ret = (int32x4_t) {__p0, __p0, __p0, __p0};
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x2_t vmovq_n_s64(int64_t __p0) {
  int64x2_t __ret;
  __ret = (int64x2_t) {__p0, __p0};
  return __ret;
}
#else
__ai int64x2_t vmovq_n_s64(int64_t __p0) {
  int64x2_t __ret;
  __ret = (int64x2_t) {__p0, __p0};
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x8_t vmovq_n_s16(int16_t __p0) {
  int16x8_t __ret;
  __ret = (int16x8_t) {__p0, __p0, __p0, __p0, __p0, __p0, __p0, __p0};
  return __ret;
}
#else
__ai int16x8_t vmovq_n_s16(int16_t __p0) {
  int16x8_t __ret;
  __ret = (int16x8_t) {__p0, __p0, __p0, __p0, __p0, __p0, __p0, __p0};
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x8_t vmov_n_u8(uint8_t __p0) {
  uint8x8_t __ret;
  __ret = (uint8x8_t) {__p0, __p0, __p0, __p0, __p0, __p0, __p0, __p0};
  return __ret;
}
#else
__ai uint8x8_t vmov_n_u8(uint8_t __p0) {
  uint8x8_t __ret;
  __ret = (uint8x8_t) {__p0, __p0, __p0, __p0, __p0, __p0, __p0, __p0};
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x2_t vmov_n_u32(uint32_t __p0) {
  uint32x2_t __ret;
  __ret = (uint32x2_t) {__p0, __p0};
  return __ret;
}
#else
__ai uint32x2_t vmov_n_u32(uint32_t __p0) {
  uint32x2_t __ret;
  __ret = (uint32x2_t) {__p0, __p0};
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x1_t vmov_n_u64(uint64_t __p0) {
  uint64x1_t __ret;
  __ret = (uint64x1_t) {__p0};
  return __ret;
}
#else
__ai uint64x1_t vmov_n_u64(uint64_t __p0) {
  uint64x1_t __ret;
  __ret = (uint64x1_t) {__p0};
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x4_t vmov_n_u16(uint16_t __p0) {
  uint16x4_t __ret;
  __ret = (uint16x4_t) {__p0, __p0, __p0, __p0};
  return __ret;
}
#else
__ai uint16x4_t vmov_n_u16(uint16_t __p0) {
  uint16x4_t __ret;
  __ret = (uint16x4_t) {__p0, __p0, __p0, __p0};
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x8_t vmov_n_s8(int8_t __p0) {
  int8x8_t __ret;
  __ret = (int8x8_t) {__p0, __p0, __p0, __p0, __p0, __p0, __p0, __p0};
  return __ret;
}
#else
__ai int8x8_t vmov_n_s8(int8_t __p0) {
  int8x8_t __ret;
  __ret = (int8x8_t) {__p0, __p0, __p0, __p0, __p0, __p0, __p0, __p0};
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x2_t vmov_n_f32(float32_t __p0) {
  float32x2_t __ret;
  __ret = (float32x2_t) {__p0, __p0};
  return __ret;
}
#else
__ai float32x2_t vmov_n_f32(float32_t __p0) {
  float32x2_t __ret;
  __ret = (float32x2_t) {__p0, __p0};
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
#define vmov_n_f16(__p0) __extension__ ({ \
  float16_t __s0 = __p0; \
  float16x4_t __ret; \
  __ret = (float16x4_t) {__s0, __s0, __s0, __s0}; \
  __ret; \
})
#else
#define vmov_n_f16(__p0) __extension__ ({ \
  float16_t __s0 = __p0; \
  float16x4_t __ret; \
  __ret = (float16x4_t) {__s0, __s0, __s0, __s0}; \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x2_t vmov_n_s32(int32_t __p0) {
  int32x2_t __ret;
  __ret = (int32x2_t) {__p0, __p0};
  return __ret;
}
#else
__ai int32x2_t vmov_n_s32(int32_t __p0) {
  int32x2_t __ret;
  __ret = (int32x2_t) {__p0, __p0};
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x1_t vmov_n_s64(int64_t __p0) {
  int64x1_t __ret;
  __ret = (int64x1_t) {__p0};
  return __ret;
}
#else
__ai int64x1_t vmov_n_s64(int64_t __p0) {
  int64x1_t __ret;
  __ret = (int64x1_t) {__p0};
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x4_t vmov_n_s16(int16_t __p0) {
  int16x4_t __ret;
  __ret = (int16x4_t) {__p0, __p0, __p0, __p0};
  return __ret;
}
#else
__ai int16x4_t vmov_n_s16(int16_t __p0) {
  int16x4_t __ret;
  __ret = (int16x4_t) {__p0, __p0, __p0, __p0};
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x8_t vmovl_u8(uint8x8_t __p0) {
  uint16x8_t __ret;
  __ret = (uint16x8_t) __builtin_neon_vmovl_v((int8x8_t)__p0, 49);
  return __ret;
}
#else
__ai uint16x8_t vmovl_u8(uint8x8_t __p0) {
  uint8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __ret;
  __ret = (uint16x8_t) __builtin_neon_vmovl_v((int8x8_t)__rev0, 49);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
__ai uint16x8_t __noswap_vmovl_u8(uint8x8_t __p0) {
  uint16x8_t __ret;
  __ret = (uint16x8_t) __builtin_neon_vmovl_v((int8x8_t)__p0, 49);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vmovl_u32(uint32x2_t __p0) {
  uint64x2_t __ret;
  __ret = (uint64x2_t) __builtin_neon_vmovl_v((int8x8_t)__p0, 51);
  return __ret;
}
#else
__ai uint64x2_t vmovl_u32(uint32x2_t __p0) {
  uint32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint64x2_t __ret;
  __ret = (uint64x2_t) __builtin_neon_vmovl_v((int8x8_t)__rev0, 51);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
__ai uint64x2_t __noswap_vmovl_u32(uint32x2_t __p0) {
  uint64x2_t __ret;
  __ret = (uint64x2_t) __builtin_neon_vmovl_v((int8x8_t)__p0, 51);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vmovl_u16(uint16x4_t __p0) {
  uint32x4_t __ret;
  __ret = (uint32x4_t) __builtin_neon_vmovl_v((int8x8_t)__p0, 50);
  return __ret;
}
#else
__ai uint32x4_t vmovl_u16(uint16x4_t __p0) {
  uint16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint32x4_t __ret;
  __ret = (uint32x4_t) __builtin_neon_vmovl_v((int8x8_t)__rev0, 50);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
__ai uint32x4_t __noswap_vmovl_u16(uint16x4_t __p0) {
  uint32x4_t __ret;
  __ret = (uint32x4_t) __builtin_neon_vmovl_v((int8x8_t)__p0, 50);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x8_t vmovl_s8(int8x8_t __p0) {
  int16x8_t __ret;
  __ret = (int16x8_t) __builtin_neon_vmovl_v((int8x8_t)__p0, 33);
  return __ret;
}
#else
__ai int16x8_t vmovl_s8(int8x8_t __p0) {
  int8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __ret;
  __ret = (int16x8_t) __builtin_neon_vmovl_v((int8x8_t)__rev0, 33);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
__ai int16x8_t __noswap_vmovl_s8(int8x8_t __p0) {
  int16x8_t __ret;
  __ret = (int16x8_t) __builtin_neon_vmovl_v((int8x8_t)__p0, 33);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x2_t vmovl_s32(int32x2_t __p0) {
  int64x2_t __ret;
  __ret = (int64x2_t) __builtin_neon_vmovl_v((int8x8_t)__p0, 35);
  return __ret;
}
#else
__ai int64x2_t vmovl_s32(int32x2_t __p0) {
  int32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int64x2_t __ret;
  __ret = (int64x2_t) __builtin_neon_vmovl_v((int8x8_t)__rev0, 35);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
__ai int64x2_t __noswap_vmovl_s32(int32x2_t __p0) {
  int64x2_t __ret;
  __ret = (int64x2_t) __builtin_neon_vmovl_v((int8x8_t)__p0, 35);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vmovl_s16(int16x4_t __p0) {
  int32x4_t __ret;
  __ret = (int32x4_t) __builtin_neon_vmovl_v((int8x8_t)__p0, 34);
  return __ret;
}
#else
__ai int32x4_t vmovl_s16(int16x4_t __p0) {
  int16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int32x4_t __ret;
  __ret = (int32x4_t) __builtin_neon_vmovl_v((int8x8_t)__rev0, 34);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
__ai int32x4_t __noswap_vmovl_s16(int16x4_t __p0) {
  int32x4_t __ret;
  __ret = (int32x4_t) __builtin_neon_vmovl_v((int8x8_t)__p0, 34);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x4_t vmovn_u32(uint32x4_t __p0) {
  uint16x4_t __ret;
  __ret = (uint16x4_t) __builtin_neon_vmovn_v((int8x16_t)__p0, 17);
  return __ret;
}
#else
__ai uint16x4_t vmovn_u32(uint32x4_t __p0) {
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint16x4_t __ret;
  __ret = (uint16x4_t) __builtin_neon_vmovn_v((int8x16_t)__rev0, 17);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
__ai uint16x4_t __noswap_vmovn_u32(uint32x4_t __p0) {
  uint16x4_t __ret;
  __ret = (uint16x4_t) __builtin_neon_vmovn_v((int8x16_t)__p0, 17);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x2_t vmovn_u64(uint64x2_t __p0) {
  uint32x2_t __ret;
  __ret = (uint32x2_t) __builtin_neon_vmovn_v((int8x16_t)__p0, 18);
  return __ret;
}
#else
__ai uint32x2_t vmovn_u64(uint64x2_t __p0) {
  uint64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint32x2_t __ret;
  __ret = (uint32x2_t) __builtin_neon_vmovn_v((int8x16_t)__rev0, 18);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
__ai uint32x2_t __noswap_vmovn_u64(uint64x2_t __p0) {
  uint32x2_t __ret;
  __ret = (uint32x2_t) __builtin_neon_vmovn_v((int8x16_t)__p0, 18);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x8_t vmovn_u16(uint16x8_t __p0) {
  uint8x8_t __ret;
  __ret = (uint8x8_t) __builtin_neon_vmovn_v((int8x16_t)__p0, 16);
  return __ret;
}
#else
__ai uint8x8_t vmovn_u16(uint16x8_t __p0) {
  uint16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __ret;
  __ret = (uint8x8_t) __builtin_neon_vmovn_v((int8x16_t)__rev0, 16);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
__ai uint8x8_t __noswap_vmovn_u16(uint16x8_t __p0) {
  uint8x8_t __ret;
  __ret = (uint8x8_t) __builtin_neon_vmovn_v((int8x16_t)__p0, 16);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x4_t vmovn_s32(int32x4_t __p0) {
  int16x4_t __ret;
  __ret = (int16x4_t) __builtin_neon_vmovn_v((int8x16_t)__p0, 1);
  return __ret;
}
#else
__ai int16x4_t vmovn_s32(int32x4_t __p0) {
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int16x4_t __ret;
  __ret = (int16x4_t) __builtin_neon_vmovn_v((int8x16_t)__rev0, 1);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
__ai int16x4_t __noswap_vmovn_s32(int32x4_t __p0) {
  int16x4_t __ret;
  __ret = (int16x4_t) __builtin_neon_vmovn_v((int8x16_t)__p0, 1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x2_t vmovn_s64(int64x2_t __p0) {
  int32x2_t __ret;
  __ret = (int32x2_t) __builtin_neon_vmovn_v((int8x16_t)__p0, 2);
  return __ret;
}
#else
__ai int32x2_t vmovn_s64(int64x2_t __p0) {
  int64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int32x2_t __ret;
  __ret = (int32x2_t) __builtin_neon_vmovn_v((int8x16_t)__rev0, 2);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
__ai int32x2_t __noswap_vmovn_s64(int64x2_t __p0) {
  int32x2_t __ret;
  __ret = (int32x2_t) __builtin_neon_vmovn_v((int8x16_t)__p0, 2);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x8_t vmovn_s16(int16x8_t __p0) {
  int8x8_t __ret;
  __ret = (int8x8_t) __builtin_neon_vmovn_v((int8x16_t)__p0, 0);
  return __ret;
}
#else
__ai int8x8_t vmovn_s16(int16x8_t __p0) {
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __ret;
  __ret = (int8x8_t) __builtin_neon_vmovn_v((int8x16_t)__rev0, 0);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
__ai int8x8_t __noswap_vmovn_s16(int16x8_t __p0) {
  int8x8_t __ret;
  __ret = (int8x8_t) __builtin_neon_vmovn_v((int8x16_t)__p0, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x16_t vmulq_u8(uint8x16_t __p0, uint8x16_t __p1) {
  uint8x16_t __ret;
  __ret = __p0 * __p1;
  return __ret;
}
#else
__ai uint8x16_t vmulq_u8(uint8x16_t __p0, uint8x16_t __p1) {
  uint8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __ret;
  __ret = __rev0 * __rev1;
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vmulq_u32(uint32x4_t __p0, uint32x4_t __p1) {
  uint32x4_t __ret;
  __ret = __p0 * __p1;
  return __ret;
}
#else
__ai uint32x4_t vmulq_u32(uint32x4_t __p0, uint32x4_t __p1) {
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint32x4_t __ret;
  __ret = __rev0 * __rev1;
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x8_t vmulq_u16(uint16x8_t __p0, uint16x8_t __p1) {
  uint16x8_t __ret;
  __ret = __p0 * __p1;
  return __ret;
}
#else
__ai uint16x8_t vmulq_u16(uint16x8_t __p0, uint16x8_t __p1) {
  uint16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __ret;
  __ret = __rev0 * __rev1;
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x16_t vmulq_s8(int8x16_t __p0, int8x16_t __p1) {
  int8x16_t __ret;
  __ret = __p0 * __p1;
  return __ret;
}
#else
__ai int8x16_t vmulq_s8(int8x16_t __p0, int8x16_t __p1) {
  int8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __ret;
  __ret = __rev0 * __rev1;
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x4_t vmulq_f32(float32x4_t __p0, float32x4_t __p1) {
  float32x4_t __ret;
  __ret = __p0 * __p1;
  return __ret;
}
#else
__ai float32x4_t vmulq_f32(float32x4_t __p0, float32x4_t __p1) {
  float32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  float32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  float32x4_t __ret;
  __ret = __rev0 * __rev1;
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vmulq_s32(int32x4_t __p0, int32x4_t __p1) {
  int32x4_t __ret;
  __ret = __p0 * __p1;
  return __ret;
}
#else
__ai int32x4_t vmulq_s32(int32x4_t __p0, int32x4_t __p1) {
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int32x4_t __ret;
  __ret = __rev0 * __rev1;
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x8_t vmulq_s16(int16x8_t __p0, int16x8_t __p1) {
  int16x8_t __ret;
  __ret = __p0 * __p1;
  return __ret;
}
#else
__ai int16x8_t vmulq_s16(int16x8_t __p0, int16x8_t __p1) {
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __ret;
  __ret = __rev0 * __rev1;
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x8_t vmul_u8(uint8x8_t __p0, uint8x8_t __p1) {
  uint8x8_t __ret;
  __ret = __p0 * __p1;
  return __ret;
}
#else
__ai uint8x8_t vmul_u8(uint8x8_t __p0, uint8x8_t __p1) {
  uint8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __ret;
  __ret = __rev0 * __rev1;
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x2_t vmul_u32(uint32x2_t __p0, uint32x2_t __p1) {
  uint32x2_t __ret;
  __ret = __p0 * __p1;
  return __ret;
}
#else
__ai uint32x2_t vmul_u32(uint32x2_t __p0, uint32x2_t __p1) {
  uint32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  uint32x2_t __ret;
  __ret = __rev0 * __rev1;
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x4_t vmul_u16(uint16x4_t __p0, uint16x4_t __p1) {
  uint16x4_t __ret;
  __ret = __p0 * __p1;
  return __ret;
}
#else
__ai uint16x4_t vmul_u16(uint16x4_t __p0, uint16x4_t __p1) {
  uint16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint16x4_t __ret;
  __ret = __rev0 * __rev1;
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x8_t vmul_s8(int8x8_t __p0, int8x8_t __p1) {
  int8x8_t __ret;
  __ret = __p0 * __p1;
  return __ret;
}
#else
__ai int8x8_t vmul_s8(int8x8_t __p0, int8x8_t __p1) {
  int8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __ret;
  __ret = __rev0 * __rev1;
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x2_t vmul_f32(float32x2_t __p0, float32x2_t __p1) {
  float32x2_t __ret;
  __ret = __p0 * __p1;
  return __ret;
}
#else
__ai float32x2_t vmul_f32(float32x2_t __p0, float32x2_t __p1) {
  float32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  float32x2_t __ret;
  __ret = __rev0 * __rev1;
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x2_t vmul_s32(int32x2_t __p0, int32x2_t __p1) {
  int32x2_t __ret;
  __ret = __p0 * __p1;
  return __ret;
}
#else
__ai int32x2_t vmul_s32(int32x2_t __p0, int32x2_t __p1) {
  int32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  int32x2_t __ret;
  __ret = __rev0 * __rev1;
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x4_t vmul_s16(int16x4_t __p0, int16x4_t __p1) {
  int16x4_t __ret;
  __ret = __p0 * __p1;
  return __ret;
}
#else
__ai int16x4_t vmul_s16(int16x4_t __p0, int16x4_t __p1) {
  int16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int16x4_t __ret;
  __ret = __rev0 * __rev1;
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x8_t vmul_p8(poly8x8_t __p0, poly8x8_t __p1) {
  poly8x8_t __ret;
  __ret = (poly8x8_t) __builtin_neon_vmul_v((int8x8_t)__p0, (int8x8_t)__p1, 4);
  return __ret;
}
#else
__ai poly8x8_t vmul_p8(poly8x8_t __p0, poly8x8_t __p1) {
  poly8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  poly8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  poly8x8_t __ret;
  __ret = (poly8x8_t) __builtin_neon_vmul_v((int8x8_t)__rev0, (int8x8_t)__rev1, 4);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x16_t vmulq_p8(poly8x16_t __p0, poly8x16_t __p1) {
  poly8x16_t __ret;
  __ret = (poly8x16_t) __builtin_neon_vmulq_v((int8x16_t)__p0, (int8x16_t)__p1, 36);
  return __ret;
}
#else
__ai poly8x16_t vmulq_p8(poly8x16_t __p0, poly8x16_t __p1) {
  poly8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  poly8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  poly8x16_t __ret;
  __ret = (poly8x16_t) __builtin_neon_vmulq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 36);
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
#define vmulq_lane_u32(__p0, __p1, __p2) __extension__ ({ \
  uint32x4_t __s0 = __p0; \
  uint32x2_t __s1 = __p1; \
  uint32x4_t __ret; \
  __ret = __s0 * __builtin_shufflevector(__s1, __s1, __p2, __p2, __p2, __p2); \
  __ret; \
})
#else
#define vmulq_lane_u32(__p0, __p1, __p2) __extension__ ({ \
  uint32x4_t __s0 = __p0; \
  uint32x2_t __s1 = __p1; \
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  uint32x2_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 1, 0); \
  uint32x4_t __ret; \
  __ret = __rev0 * __builtin_shufflevector(__rev1, __rev1, __p2, __p2, __p2, __p2); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmulq_lane_u16(__p0, __p1, __p2) __extension__ ({ \
  uint16x8_t __s0 = __p0; \
  uint16x4_t __s1 = __p1; \
  uint16x8_t __ret; \
  __ret = __s0 * __builtin_shufflevector(__s1, __s1, __p2, __p2, __p2, __p2, __p2, __p2, __p2, __p2); \
  __ret; \
})
#else
#define vmulq_lane_u16(__p0, __p1, __p2) __extension__ ({ \
  uint16x8_t __s0 = __p0; \
  uint16x4_t __s1 = __p1; \
  uint16x8_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint16x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  uint16x8_t __ret; \
  __ret = __rev0 * __builtin_shufflevector(__rev1, __rev1, __p2, __p2, __p2, __p2, __p2, __p2, __p2, __p2); \
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmulq_lane_f32(__p0, __p1, __p2) __extension__ ({ \
  float32x4_t __s0 = __p0; \
  float32x2_t __s1 = __p1; \
  float32x4_t __ret; \
  __ret = __s0 * __builtin_shufflevector(__s1, __s1, __p2, __p2, __p2, __p2); \
  __ret; \
})
#else
#define vmulq_lane_f32(__p0, __p1, __p2) __extension__ ({ \
  float32x4_t __s0 = __p0; \
  float32x2_t __s1 = __p1; \
  float32x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  float32x2_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 1, 0); \
  float32x4_t __ret; \
  __ret = __rev0 * __builtin_shufflevector(__rev1, __rev1, __p2, __p2, __p2, __p2); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmulq_lane_s32(__p0, __p1, __p2) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  int32x2_t __s1 = __p1; \
  int32x4_t __ret; \
  __ret = __s0 * __builtin_shufflevector(__s1, __s1, __p2, __p2, __p2, __p2); \
  __ret; \
})
#else
#define vmulq_lane_s32(__p0, __p1, __p2) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  int32x2_t __s1 = __p1; \
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  int32x2_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 1, 0); \
  int32x4_t __ret; \
  __ret = __rev0 * __builtin_shufflevector(__rev1, __rev1, __p2, __p2, __p2, __p2); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmulq_lane_s16(__p0, __p1, __p2) __extension__ ({ \
  int16x8_t __s0 = __p0; \
  int16x4_t __s1 = __p1; \
  int16x8_t __ret; \
  __ret = __s0 * __builtin_shufflevector(__s1, __s1, __p2, __p2, __p2, __p2, __p2, __p2, __p2, __p2); \
  __ret; \
})
#else
#define vmulq_lane_s16(__p0, __p1, __p2) __extension__ ({ \
  int16x8_t __s0 = __p0; \
  int16x4_t __s1 = __p1; \
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 7, 6, 5, 4, 3, 2, 1, 0); \
  int16x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  int16x8_t __ret; \
  __ret = __rev0 * __builtin_shufflevector(__rev1, __rev1, __p2, __p2, __p2, __p2, __p2, __p2, __p2, __p2); \
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmul_lane_u32(__p0, __p1, __p2) __extension__ ({ \
  uint32x2_t __s0 = __p0; \
  uint32x2_t __s1 = __p1; \
  uint32x2_t __ret; \
  __ret = __s0 * __builtin_shufflevector(__s1, __s1, __p2, __p2); \
  __ret; \
})
#else
#define vmul_lane_u32(__p0, __p1, __p2) __extension__ ({ \
  uint32x2_t __s0 = __p0; \
  uint32x2_t __s1 = __p1; \
  uint32x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  uint32x2_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 1, 0); \
  uint32x2_t __ret; \
  __ret = __rev0 * __builtin_shufflevector(__rev1, __rev1, __p2, __p2); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmul_lane_u16(__p0, __p1, __p2) __extension__ ({ \
  uint16x4_t __s0 = __p0; \
  uint16x4_t __s1 = __p1; \
  uint16x4_t __ret; \
  __ret = __s0 * __builtin_shufflevector(__s1, __s1, __p2, __p2, __p2, __p2); \
  __ret; \
})
#else
#define vmul_lane_u16(__p0, __p1, __p2) __extension__ ({ \
  uint16x4_t __s0 = __p0; \
  uint16x4_t __s1 = __p1; \
  uint16x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  uint16x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  uint16x4_t __ret; \
  __ret = __rev0 * __builtin_shufflevector(__rev1, __rev1, __p2, __p2, __p2, __p2); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmul_lane_f32(__p0, __p1, __p2) __extension__ ({ \
  float32x2_t __s0 = __p0; \
  float32x2_t __s1 = __p1; \
  float32x2_t __ret; \
  __ret = __s0 * __builtin_shufflevector(__s1, __s1, __p2, __p2); \
  __ret; \
})
#else
#define vmul_lane_f32(__p0, __p1, __p2) __extension__ ({ \
  float32x2_t __s0 = __p0; \
  float32x2_t __s1 = __p1; \
  float32x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  float32x2_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 1, 0); \
  float32x2_t __ret; \
  __ret = __rev0 * __builtin_shufflevector(__rev1, __rev1, __p2, __p2); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmul_lane_s32(__p0, __p1, __p2) __extension__ ({ \
  int32x2_t __s0 = __p0; \
  int32x2_t __s1 = __p1; \
  int32x2_t __ret; \
  __ret = __s0 * __builtin_shufflevector(__s1, __s1, __p2, __p2); \
  __ret; \
})
#else
#define vmul_lane_s32(__p0, __p1, __p2) __extension__ ({ \
  int32x2_t __s0 = __p0; \
  int32x2_t __s1 = __p1; \
  int32x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  int32x2_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 1, 0); \
  int32x2_t __ret; \
  __ret = __rev0 * __builtin_shufflevector(__rev1, __rev1, __p2, __p2); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmul_lane_s16(__p0, __p1, __p2) __extension__ ({ \
  int16x4_t __s0 = __p0; \
  int16x4_t __s1 = __p1; \
  int16x4_t __ret; \
  __ret = __s0 * __builtin_shufflevector(__s1, __s1, __p2, __p2, __p2, __p2); \
  __ret; \
})
#else
#define vmul_lane_s16(__p0, __p1, __p2) __extension__ ({ \
  int16x4_t __s0 = __p0; \
  int16x4_t __s1 = __p1; \
  int16x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  int16x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  int16x4_t __ret; \
  __ret = __rev0 * __builtin_shufflevector(__rev1, __rev1, __p2, __p2, __p2, __p2); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vmulq_n_u32(uint32x4_t __p0, uint32_t __p1) {
  uint32x4_t __ret;
  __ret = __p0 * (uint32x4_t) {__p1, __p1, __p1, __p1};
  return __ret;
}
#else
__ai uint32x4_t vmulq_n_u32(uint32x4_t __p0, uint32_t __p1) {
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint32x4_t __ret;
  __ret = __rev0 * (uint32x4_t) {__p1, __p1, __p1, __p1};
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x8_t vmulq_n_u16(uint16x8_t __p0, uint16_t __p1) {
  uint16x8_t __ret;
  __ret = __p0 * (uint16x8_t) {__p1, __p1, __p1, __p1, __p1, __p1, __p1, __p1};
  return __ret;
}
#else
__ai uint16x8_t vmulq_n_u16(uint16x8_t __p0, uint16_t __p1) {
  uint16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __ret;
  __ret = __rev0 * (uint16x8_t) {__p1, __p1, __p1, __p1, __p1, __p1, __p1, __p1};
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x4_t vmulq_n_f32(float32x4_t __p0, float32_t __p1) {
  float32x4_t __ret;
  __ret = __p0 * (float32x4_t) {__p1, __p1, __p1, __p1};
  return __ret;
}
#else
__ai float32x4_t vmulq_n_f32(float32x4_t __p0, float32_t __p1) {
  float32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  float32x4_t __ret;
  __ret = __rev0 * (float32x4_t) {__p1, __p1, __p1, __p1};
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vmulq_n_s32(int32x4_t __p0, int32_t __p1) {
  int32x4_t __ret;
  __ret = __p0 * (int32x4_t) {__p1, __p1, __p1, __p1};
  return __ret;
}
#else
__ai int32x4_t vmulq_n_s32(int32x4_t __p0, int32_t __p1) {
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int32x4_t __ret;
  __ret = __rev0 * (int32x4_t) {__p1, __p1, __p1, __p1};
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x8_t vmulq_n_s16(int16x8_t __p0, int16_t __p1) {
  int16x8_t __ret;
  __ret = __p0 * (int16x8_t) {__p1, __p1, __p1, __p1, __p1, __p1, __p1, __p1};
  return __ret;
}
#else
__ai int16x8_t vmulq_n_s16(int16x8_t __p0, int16_t __p1) {
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __ret;
  __ret = __rev0 * (int16x8_t) {__p1, __p1, __p1, __p1, __p1, __p1, __p1, __p1};
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x2_t vmul_n_u32(uint32x2_t __p0, uint32_t __p1) {
  uint32x2_t __ret;
  __ret = __p0 * (uint32x2_t) {__p1, __p1};
  return __ret;
}
#else
__ai uint32x2_t vmul_n_u32(uint32x2_t __p0, uint32_t __p1) {
  uint32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint32x2_t __ret;
  __ret = __rev0 * (uint32x2_t) {__p1, __p1};
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x4_t vmul_n_u16(uint16x4_t __p0, uint16_t __p1) {
  uint16x4_t __ret;
  __ret = __p0 * (uint16x4_t) {__p1, __p1, __p1, __p1};
  return __ret;
}
#else
__ai uint16x4_t vmul_n_u16(uint16x4_t __p0, uint16_t __p1) {
  uint16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint16x4_t __ret;
  __ret = __rev0 * (uint16x4_t) {__p1, __p1, __p1, __p1};
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x2_t vmul_n_f32(float32x2_t __p0, float32_t __p1) {
  float32x2_t __ret;
  __ret = __p0 * (float32x2_t) {__p1, __p1};
  return __ret;
}
#else
__ai float32x2_t vmul_n_f32(float32x2_t __p0, float32_t __p1) {
  float32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float32x2_t __ret;
  __ret = __rev0 * (float32x2_t) {__p1, __p1};
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x2_t vmul_n_s32(int32x2_t __p0, int32_t __p1) {
  int32x2_t __ret;
  __ret = __p0 * (int32x2_t) {__p1, __p1};
  return __ret;
}
#else
__ai int32x2_t vmul_n_s32(int32x2_t __p0, int32_t __p1) {
  int32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int32x2_t __ret;
  __ret = __rev0 * (int32x2_t) {__p1, __p1};
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x4_t vmul_n_s16(int16x4_t __p0, int16_t __p1) {
  int16x4_t __ret;
  __ret = __p0 * (int16x4_t) {__p1, __p1, __p1, __p1};
  return __ret;
}
#else
__ai int16x4_t vmul_n_s16(int16x4_t __p0, int16_t __p1) {
  int16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int16x4_t __ret;
  __ret = __rev0 * (int16x4_t) {__p1, __p1, __p1, __p1};
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly16x8_t vmull_p8(poly8x8_t __p0, poly8x8_t __p1) {
  poly16x8_t __ret;
  __ret = (poly16x8_t) __builtin_neon_vmull_v((int8x8_t)__p0, (int8x8_t)__p1, 37);
  return __ret;
}
#else
__ai poly16x8_t vmull_p8(poly8x8_t __p0, poly8x8_t __p1) {
  poly8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  poly8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  poly16x8_t __ret;
  __ret = (poly16x8_t) __builtin_neon_vmull_v((int8x8_t)__rev0, (int8x8_t)__rev1, 37);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
__ai poly16x8_t __noswap_vmull_p8(poly8x8_t __p0, poly8x8_t __p1) {
  poly16x8_t __ret;
  __ret = (poly16x8_t) __builtin_neon_vmull_v((int8x8_t)__p0, (int8x8_t)__p1, 37);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x8_t vmull_u8(uint8x8_t __p0, uint8x8_t __p1) {
  uint16x8_t __ret;
  __ret = (uint16x8_t) __builtin_neon_vmull_v((int8x8_t)__p0, (int8x8_t)__p1, 49);
  return __ret;
}
#else
__ai uint16x8_t vmull_u8(uint8x8_t __p0, uint8x8_t __p1) {
  uint8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __ret;
  __ret = (uint16x8_t) __builtin_neon_vmull_v((int8x8_t)__rev0, (int8x8_t)__rev1, 49);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
__ai uint16x8_t __noswap_vmull_u8(uint8x8_t __p0, uint8x8_t __p1) {
  uint16x8_t __ret;
  __ret = (uint16x8_t) __builtin_neon_vmull_v((int8x8_t)__p0, (int8x8_t)__p1, 49);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vmull_u32(uint32x2_t __p0, uint32x2_t __p1) {
  uint64x2_t __ret;
  __ret = (uint64x2_t) __builtin_neon_vmull_v((int8x8_t)__p0, (int8x8_t)__p1, 51);
  return __ret;
}
#else
__ai uint64x2_t vmull_u32(uint32x2_t __p0, uint32x2_t __p1) {
  uint32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  uint64x2_t __ret;
  __ret = (uint64x2_t) __builtin_neon_vmull_v((int8x8_t)__rev0, (int8x8_t)__rev1, 51);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
__ai uint64x2_t __noswap_vmull_u32(uint32x2_t __p0, uint32x2_t __p1) {
  uint64x2_t __ret;
  __ret = (uint64x2_t) __builtin_neon_vmull_v((int8x8_t)__p0, (int8x8_t)__p1, 51);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vmull_u16(uint16x4_t __p0, uint16x4_t __p1) {
  uint32x4_t __ret;
  __ret = (uint32x4_t) __builtin_neon_vmull_v((int8x8_t)__p0, (int8x8_t)__p1, 50);
  return __ret;
}
#else
__ai uint32x4_t vmull_u16(uint16x4_t __p0, uint16x4_t __p1) {
  uint16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint32x4_t __ret;
  __ret = (uint32x4_t) __builtin_neon_vmull_v((int8x8_t)__rev0, (int8x8_t)__rev1, 50);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
__ai uint32x4_t __noswap_vmull_u16(uint16x4_t __p0, uint16x4_t __p1) {
  uint32x4_t __ret;
  __ret = (uint32x4_t) __builtin_neon_vmull_v((int8x8_t)__p0, (int8x8_t)__p1, 50);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x8_t vmull_s8(int8x8_t __p0, int8x8_t __p1) {
  int16x8_t __ret;
  __ret = (int16x8_t) __builtin_neon_vmull_v((int8x8_t)__p0, (int8x8_t)__p1, 33);
  return __ret;
}
#else
__ai int16x8_t vmull_s8(int8x8_t __p0, int8x8_t __p1) {
  int8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __ret;
  __ret = (int16x8_t) __builtin_neon_vmull_v((int8x8_t)__rev0, (int8x8_t)__rev1, 33);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
__ai int16x8_t __noswap_vmull_s8(int8x8_t __p0, int8x8_t __p1) {
  int16x8_t __ret;
  __ret = (int16x8_t) __builtin_neon_vmull_v((int8x8_t)__p0, (int8x8_t)__p1, 33);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x2_t vmull_s32(int32x2_t __p0, int32x2_t __p1) {
  int64x2_t __ret;
  __ret = (int64x2_t) __builtin_neon_vmull_v((int8x8_t)__p0, (int8x8_t)__p1, 35);
  return __ret;
}
#else
__ai int64x2_t vmull_s32(int32x2_t __p0, int32x2_t __p1) {
  int32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  int64x2_t __ret;
  __ret = (int64x2_t) __builtin_neon_vmull_v((int8x8_t)__rev0, (int8x8_t)__rev1, 35);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
__ai int64x2_t __noswap_vmull_s32(int32x2_t __p0, int32x2_t __p1) {
  int64x2_t __ret;
  __ret = (int64x2_t) __builtin_neon_vmull_v((int8x8_t)__p0, (int8x8_t)__p1, 35);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vmull_s16(int16x4_t __p0, int16x4_t __p1) {
  int32x4_t __ret;
  __ret = (int32x4_t) __builtin_neon_vmull_v((int8x8_t)__p0, (int8x8_t)__p1, 34);
  return __ret;
}
#else
__ai int32x4_t vmull_s16(int16x4_t __p0, int16x4_t __p1) {
  int16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int32x4_t __ret;
  __ret = (int32x4_t) __builtin_neon_vmull_v((int8x8_t)__rev0, (int8x8_t)__rev1, 34);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
__ai int32x4_t __noswap_vmull_s16(int16x4_t __p0, int16x4_t __p1) {
  int32x4_t __ret;
  __ret = (int32x4_t) __builtin_neon_vmull_v((int8x8_t)__p0, (int8x8_t)__p1, 34);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
#define vmull_lane_u32(__p0, __p1, __p2) __extension__ ({ \
  uint32x2_t __s0 = __p0; \
  uint32x2_t __s1 = __p1; \
  uint64x2_t __ret; \
  __ret = vmull_u32(__s0, __builtin_shufflevector(__s1, __s1, __p2, __p2)); \
  __ret; \
})
#else
#define vmull_lane_u32(__p0, __p1, __p2) __extension__ ({ \
  uint32x2_t __s0 = __p0; \
  uint32x2_t __s1 = __p1; \
  uint32x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  uint32x2_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 1, 0); \
  uint64x2_t __ret; \
  __ret = __noswap_vmull_u32(__rev0, __builtin_shufflevector(__rev1, __rev1, __p2, __p2)); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmull_lane_u16(__p0, __p1, __p2) __extension__ ({ \
  uint16x4_t __s0 = __p0; \
  uint16x4_t __s1 = __p1; \
  uint32x4_t __ret; \
  __ret = vmull_u16(__s0, __builtin_shufflevector(__s1, __s1, __p2, __p2, __p2, __p2)); \
  __ret; \
})
#else
#define vmull_lane_u16(__p0, __p1, __p2) __extension__ ({ \
  uint16x4_t __s0 = __p0; \
  uint16x4_t __s1 = __p1; \
  uint16x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  uint16x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  uint32x4_t __ret; \
  __ret = __noswap_vmull_u16(__rev0, __builtin_shufflevector(__rev1, __rev1, __p2, __p2, __p2, __p2)); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmull_lane_s32(__p0, __p1, __p2) __extension__ ({ \
  int32x2_t __s0 = __p0; \
  int32x2_t __s1 = __p1; \
  int64x2_t __ret; \
  __ret = vmull_s32(__s0, __builtin_shufflevector(__s1, __s1, __p2, __p2)); \
  __ret; \
})
#else
#define vmull_lane_s32(__p0, __p1, __p2) __extension__ ({ \
  int32x2_t __s0 = __p0; \
  int32x2_t __s1 = __p1; \
  int32x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  int32x2_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 1, 0); \
  int64x2_t __ret; \
  __ret = __noswap_vmull_s32(__rev0, __builtin_shufflevector(__rev1, __rev1, __p2, __p2)); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vmull_lane_s16(__p0, __p1, __p2) __extension__ ({ \
  int16x4_t __s0 = __p0; \
  int16x4_t __s1 = __p1; \
  int32x4_t __ret; \
  __ret = vmull_s16(__s0, __builtin_shufflevector(__s1, __s1, __p2, __p2, __p2, __p2)); \
  __ret; \
})
#else
#define vmull_lane_s16(__p0, __p1, __p2) __extension__ ({ \
  int16x4_t __s0 = __p0; \
  int16x4_t __s1 = __p1; \
  int16x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  int16x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  int32x4_t __ret; \
  __ret = __noswap_vmull_s16(__rev0, __builtin_shufflevector(__rev1, __rev1, __p2, __p2, __p2, __p2)); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vmull_n_u32(uint32x2_t __p0, uint32_t __p1) {
  uint64x2_t __ret;
  __ret = (uint64x2_t) __builtin_neon_vmull_v((int8x8_t)__p0, (int8x8_t)(uint32x2_t) {__p1, __p1}, 51);
  return __ret;
}
#else
__ai uint64x2_t vmull_n_u32(uint32x2_t __p0, uint32_t __p1) {
  uint32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint64x2_t __ret;
  __ret = (uint64x2_t) __builtin_neon_vmull_v((int8x8_t)__rev0, (int8x8_t)(uint32x2_t) {__p1, __p1}, 51);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
__ai uint64x2_t __noswap_vmull_n_u32(uint32x2_t __p0, uint32_t __p1) {
  uint64x2_t __ret;
  __ret = (uint64x2_t) __builtin_neon_vmull_v((int8x8_t)__p0, (int8x8_t)(uint32x2_t) {__p1, __p1}, 51);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vmull_n_u16(uint16x4_t __p0, uint16_t __p1) {
  uint32x4_t __ret;
  __ret = (uint32x4_t) __builtin_neon_vmull_v((int8x8_t)__p0, (int8x8_t)(uint16x4_t) {__p1, __p1, __p1, __p1}, 50);
  return __ret;
}
#else
__ai uint32x4_t vmull_n_u16(uint16x4_t __p0, uint16_t __p1) {
  uint16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint32x4_t __ret;
  __ret = (uint32x4_t) __builtin_neon_vmull_v((int8x8_t)__rev0, (int8x8_t)(uint16x4_t) {__p1, __p1, __p1, __p1}, 50);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
__ai uint32x4_t __noswap_vmull_n_u16(uint16x4_t __p0, uint16_t __p1) {
  uint32x4_t __ret;
  __ret = (uint32x4_t) __builtin_neon_vmull_v((int8x8_t)__p0, (int8x8_t)(uint16x4_t) {__p1, __p1, __p1, __p1}, 50);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x2_t vmull_n_s32(int32x2_t __p0, int32_t __p1) {
  int64x2_t __ret;
  __ret = (int64x2_t) __builtin_neon_vmull_v((int8x8_t)__p0, (int8x8_t)(int32x2_t) {__p1, __p1}, 35);
  return __ret;
}
#else
__ai int64x2_t vmull_n_s32(int32x2_t __p0, int32_t __p1) {
  int32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int64x2_t __ret;
  __ret = (int64x2_t) __builtin_neon_vmull_v((int8x8_t)__rev0, (int8x8_t)(int32x2_t) {__p1, __p1}, 35);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
__ai int64x2_t __noswap_vmull_n_s32(int32x2_t __p0, int32_t __p1) {
  int64x2_t __ret;
  __ret = (int64x2_t) __builtin_neon_vmull_v((int8x8_t)__p0, (int8x8_t)(int32x2_t) {__p1, __p1}, 35);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vmull_n_s16(int16x4_t __p0, int16_t __p1) {
  int32x4_t __ret;
  __ret = (int32x4_t) __builtin_neon_vmull_v((int8x8_t)__p0, (int8x8_t)(int16x4_t) {__p1, __p1, __p1, __p1}, 34);
  return __ret;
}
#else
__ai int32x4_t vmull_n_s16(int16x4_t __p0, int16_t __p1) {
  int16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int32x4_t __ret;
  __ret = (int32x4_t) __builtin_neon_vmull_v((int8x8_t)__rev0, (int8x8_t)(int16x4_t) {__p1, __p1, __p1, __p1}, 34);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
__ai int32x4_t __noswap_vmull_n_s16(int16x4_t __p0, int16_t __p1) {
  int32x4_t __ret;
  __ret = (int32x4_t) __builtin_neon_vmull_v((int8x8_t)__p0, (int8x8_t)(int16x4_t) {__p1, __p1, __p1, __p1}, 34);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x8_t vmvn_p8(poly8x8_t __p0) {
  poly8x8_t __ret;
  __ret = ~__p0;
  return __ret;
}
#else
__ai poly8x8_t vmvn_p8(poly8x8_t __p0) {
  poly8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  poly8x8_t __ret;
  __ret = ~__rev0;
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x16_t vmvnq_p8(poly8x16_t __p0) {
  poly8x16_t __ret;
  __ret = ~__p0;
  return __ret;
}
#else
__ai poly8x16_t vmvnq_p8(poly8x16_t __p0) {
  poly8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  poly8x16_t __ret;
  __ret = ~__rev0;
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x16_t vmvnq_u8(uint8x16_t __p0) {
  uint8x16_t __ret;
  __ret = ~__p0;
  return __ret;
}
#else
__ai uint8x16_t vmvnq_u8(uint8x16_t __p0) {
  uint8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __ret;
  __ret = ~__rev0;
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vmvnq_u32(uint32x4_t __p0) {
  uint32x4_t __ret;
  __ret = ~__p0;
  return __ret;
}
#else
__ai uint32x4_t vmvnq_u32(uint32x4_t __p0) {
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint32x4_t __ret;
  __ret = ~__rev0;
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x8_t vmvnq_u16(uint16x8_t __p0) {
  uint16x8_t __ret;
  __ret = ~__p0;
  return __ret;
}
#else
__ai uint16x8_t vmvnq_u16(uint16x8_t __p0) {
  uint16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __ret;
  __ret = ~__rev0;
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x16_t vmvnq_s8(int8x16_t __p0) {
  int8x16_t __ret;
  __ret = ~__p0;
  return __ret;
}
#else
__ai int8x16_t vmvnq_s8(int8x16_t __p0) {
  int8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __ret;
  __ret = ~__rev0;
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vmvnq_s32(int32x4_t __p0) {
  int32x4_t __ret;
  __ret = ~__p0;
  return __ret;
}
#else
__ai int32x4_t vmvnq_s32(int32x4_t __p0) {
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int32x4_t __ret;
  __ret = ~__rev0;
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x8_t vmvnq_s16(int16x8_t __p0) {
  int16x8_t __ret;
  __ret = ~__p0;
  return __ret;
}
#else
__ai int16x8_t vmvnq_s16(int16x8_t __p0) {
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __ret;
  __ret = ~__rev0;
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x8_t vmvn_u8(uint8x8_t __p0) {
  uint8x8_t __ret;
  __ret = ~__p0;
  return __ret;
}
#else
__ai uint8x8_t vmvn_u8(uint8x8_t __p0) {
  uint8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __ret;
  __ret = ~__rev0;
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x2_t vmvn_u32(uint32x2_t __p0) {
  uint32x2_t __ret;
  __ret = ~__p0;
  return __ret;
}
#else
__ai uint32x2_t vmvn_u32(uint32x2_t __p0) {
  uint32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint32x2_t __ret;
  __ret = ~__rev0;
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x4_t vmvn_u16(uint16x4_t __p0) {
  uint16x4_t __ret;
  __ret = ~__p0;
  return __ret;
}
#else
__ai uint16x4_t vmvn_u16(uint16x4_t __p0) {
  uint16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint16x4_t __ret;
  __ret = ~__rev0;
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x8_t vmvn_s8(int8x8_t __p0) {
  int8x8_t __ret;
  __ret = ~__p0;
  return __ret;
}
#else
__ai int8x8_t vmvn_s8(int8x8_t __p0) {
  int8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __ret;
  __ret = ~__rev0;
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x2_t vmvn_s32(int32x2_t __p0) {
  int32x2_t __ret;
  __ret = ~__p0;
  return __ret;
}
#else
__ai int32x2_t vmvn_s32(int32x2_t __p0) {
  int32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int32x2_t __ret;
  __ret = ~__rev0;
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x4_t vmvn_s16(int16x4_t __p0) {
  int16x4_t __ret;
  __ret = ~__p0;
  return __ret;
}
#else
__ai int16x4_t vmvn_s16(int16x4_t __p0) {
  int16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int16x4_t __ret;
  __ret = ~__rev0;
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x16_t vnegq_s8(int8x16_t __p0) {
  int8x16_t __ret;
  __ret = -__p0;
  return __ret;
}
#else
__ai int8x16_t vnegq_s8(int8x16_t __p0) {
  int8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __ret;
  __ret = -__rev0;
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x4_t vnegq_f32(float32x4_t __p0) {
  float32x4_t __ret;
  __ret = -__p0;
  return __ret;
}
#else
__ai float32x4_t vnegq_f32(float32x4_t __p0) {
  float32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  float32x4_t __ret;
  __ret = -__rev0;
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vnegq_s32(int32x4_t __p0) {
  int32x4_t __ret;
  __ret = -__p0;
  return __ret;
}
#else
__ai int32x4_t vnegq_s32(int32x4_t __p0) {
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int32x4_t __ret;
  __ret = -__rev0;
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x8_t vnegq_s16(int16x8_t __p0) {
  int16x8_t __ret;
  __ret = -__p0;
  return __ret;
}
#else
__ai int16x8_t vnegq_s16(int16x8_t __p0) {
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __ret;
  __ret = -__rev0;
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x8_t vneg_s8(int8x8_t __p0) {
  int8x8_t __ret;
  __ret = -__p0;
  return __ret;
}
#else
__ai int8x8_t vneg_s8(int8x8_t __p0) {
  int8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __ret;
  __ret = -__rev0;
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x2_t vneg_f32(float32x2_t __p0) {
  float32x2_t __ret;
  __ret = -__p0;
  return __ret;
}
#else
__ai float32x2_t vneg_f32(float32x2_t __p0) {
  float32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float32x2_t __ret;
  __ret = -__rev0;
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x2_t vneg_s32(int32x2_t __p0) {
  int32x2_t __ret;
  __ret = -__p0;
  return __ret;
}
#else
__ai int32x2_t vneg_s32(int32x2_t __p0) {
  int32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int32x2_t __ret;
  __ret = -__rev0;
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x4_t vneg_s16(int16x4_t __p0) {
  int16x4_t __ret;
  __ret = -__p0;
  return __ret;
}
#else
__ai int16x4_t vneg_s16(int16x4_t __p0) {
  int16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int16x4_t __ret;
  __ret = -__rev0;
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x16_t vornq_u8(uint8x16_t __p0, uint8x16_t __p1) {
  uint8x16_t __ret;
  __ret = __p0 | ~__p1;
  return __ret;
}
#else
__ai uint8x16_t vornq_u8(uint8x16_t __p0, uint8x16_t __p1) {
  uint8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __ret;
  __ret = __rev0 | ~__rev1;
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vornq_u32(uint32x4_t __p0, uint32x4_t __p1) {
  uint32x4_t __ret;
  __ret = __p0 | ~__p1;
  return __ret;
}
#else
__ai uint32x4_t vornq_u32(uint32x4_t __p0, uint32x4_t __p1) {
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint32x4_t __ret;
  __ret = __rev0 | ~__rev1;
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vornq_u64(uint64x2_t __p0, uint64x2_t __p1) {
  uint64x2_t __ret;
  __ret = __p0 | ~__p1;
  return __ret;
}
#else
__ai uint64x2_t vornq_u64(uint64x2_t __p0, uint64x2_t __p1) {
  uint64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  uint64x2_t __ret;
  __ret = __rev0 | ~__rev1;
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x8_t vornq_u16(uint16x8_t __p0, uint16x8_t __p1) {
  uint16x8_t __ret;
  __ret = __p0 | ~__p1;
  return __ret;
}
#else
__ai uint16x8_t vornq_u16(uint16x8_t __p0, uint16x8_t __p1) {
  uint16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __ret;
  __ret = __rev0 | ~__rev1;
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x16_t vornq_s8(int8x16_t __p0, int8x16_t __p1) {
  int8x16_t __ret;
  __ret = __p0 | ~__p1;
  return __ret;
}
#else
__ai int8x16_t vornq_s8(int8x16_t __p0, int8x16_t __p1) {
  int8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __ret;
  __ret = __rev0 | ~__rev1;
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vornq_s32(int32x4_t __p0, int32x4_t __p1) {
  int32x4_t __ret;
  __ret = __p0 | ~__p1;
  return __ret;
}
#else
__ai int32x4_t vornq_s32(int32x4_t __p0, int32x4_t __p1) {
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int32x4_t __ret;
  __ret = __rev0 | ~__rev1;
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x2_t vornq_s64(int64x2_t __p0, int64x2_t __p1) {
  int64x2_t __ret;
  __ret = __p0 | ~__p1;
  return __ret;
}
#else
__ai int64x2_t vornq_s64(int64x2_t __p0, int64x2_t __p1) {
  int64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  int64x2_t __ret;
  __ret = __rev0 | ~__rev1;
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x8_t vornq_s16(int16x8_t __p0, int16x8_t __p1) {
  int16x8_t __ret;
  __ret = __p0 | ~__p1;
  return __ret;
}
#else
__ai int16x8_t vornq_s16(int16x8_t __p0, int16x8_t __p1) {
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __ret;
  __ret = __rev0 | ~__rev1;
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x8_t vorn_u8(uint8x8_t __p0, uint8x8_t __p1) {
  uint8x8_t __ret;
  __ret = __p0 | ~__p1;
  return __ret;
}
#else
__ai uint8x8_t vorn_u8(uint8x8_t __p0, uint8x8_t __p1) {
  uint8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __ret;
  __ret = __rev0 | ~__rev1;
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x2_t vorn_u32(uint32x2_t __p0, uint32x2_t __p1) {
  uint32x2_t __ret;
  __ret = __p0 | ~__p1;
  return __ret;
}
#else
__ai uint32x2_t vorn_u32(uint32x2_t __p0, uint32x2_t __p1) {
  uint32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  uint32x2_t __ret;
  __ret = __rev0 | ~__rev1;
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x1_t vorn_u64(uint64x1_t __p0, uint64x1_t __p1) {
  uint64x1_t __ret;
  __ret = __p0 | ~__p1;
  return __ret;
}
#else
__ai uint64x1_t vorn_u64(uint64x1_t __p0, uint64x1_t __p1) {
  uint64x1_t __ret;
  __ret = __p0 | ~__p1;
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x4_t vorn_u16(uint16x4_t __p0, uint16x4_t __p1) {
  uint16x4_t __ret;
  __ret = __p0 | ~__p1;
  return __ret;
}
#else
__ai uint16x4_t vorn_u16(uint16x4_t __p0, uint16x4_t __p1) {
  uint16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint16x4_t __ret;
  __ret = __rev0 | ~__rev1;
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x8_t vorn_s8(int8x8_t __p0, int8x8_t __p1) {
  int8x8_t __ret;
  __ret = __p0 | ~__p1;
  return __ret;
}
#else
__ai int8x8_t vorn_s8(int8x8_t __p0, int8x8_t __p1) {
  int8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __ret;
  __ret = __rev0 | ~__rev1;
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x2_t vorn_s32(int32x2_t __p0, int32x2_t __p1) {
  int32x2_t __ret;
  __ret = __p0 | ~__p1;
  return __ret;
}
#else
__ai int32x2_t vorn_s32(int32x2_t __p0, int32x2_t __p1) {
  int32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  int32x2_t __ret;
  __ret = __rev0 | ~__rev1;
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x1_t vorn_s64(int64x1_t __p0, int64x1_t __p1) {
  int64x1_t __ret;
  __ret = __p0 | ~__p1;
  return __ret;
}
#else
__ai int64x1_t vorn_s64(int64x1_t __p0, int64x1_t __p1) {
  int64x1_t __ret;
  __ret = __p0 | ~__p1;
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x4_t vorn_s16(int16x4_t __p0, int16x4_t __p1) {
  int16x4_t __ret;
  __ret = __p0 | ~__p1;
  return __ret;
}
#else
__ai int16x4_t vorn_s16(int16x4_t __p0, int16x4_t __p1) {
  int16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int16x4_t __ret;
  __ret = __rev0 | ~__rev1;
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x16_t vorrq_u8(uint8x16_t __p0, uint8x16_t __p1) {
  uint8x16_t __ret;
  __ret = __p0 | __p1;
  return __ret;
}
#else
__ai uint8x16_t vorrq_u8(uint8x16_t __p0, uint8x16_t __p1) {
  uint8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __ret;
  __ret = __rev0 | __rev1;
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vorrq_u32(uint32x4_t __p0, uint32x4_t __p1) {
  uint32x4_t __ret;
  __ret = __p0 | __p1;
  return __ret;
}
#else
__ai uint32x4_t vorrq_u32(uint32x4_t __p0, uint32x4_t __p1) {
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint32x4_t __ret;
  __ret = __rev0 | __rev1;
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vorrq_u64(uint64x2_t __p0, uint64x2_t __p1) {
  uint64x2_t __ret;
  __ret = __p0 | __p1;
  return __ret;
}
#else
__ai uint64x2_t vorrq_u64(uint64x2_t __p0, uint64x2_t __p1) {
  uint64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  uint64x2_t __ret;
  __ret = __rev0 | __rev1;
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x8_t vorrq_u16(uint16x8_t __p0, uint16x8_t __p1) {
  uint16x8_t __ret;
  __ret = __p0 | __p1;
  return __ret;
}
#else
__ai uint16x8_t vorrq_u16(uint16x8_t __p0, uint16x8_t __p1) {
  uint16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __ret;
  __ret = __rev0 | __rev1;
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x16_t vorrq_s8(int8x16_t __p0, int8x16_t __p1) {
  int8x16_t __ret;
  __ret = __p0 | __p1;
  return __ret;
}
#else
__ai int8x16_t vorrq_s8(int8x16_t __p0, int8x16_t __p1) {
  int8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __ret;
  __ret = __rev0 | __rev1;
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vorrq_s32(int32x4_t __p0, int32x4_t __p1) {
  int32x4_t __ret;
  __ret = __p0 | __p1;
  return __ret;
}
#else
__ai int32x4_t vorrq_s32(int32x4_t __p0, int32x4_t __p1) {
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int32x4_t __ret;
  __ret = __rev0 | __rev1;
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x2_t vorrq_s64(int64x2_t __p0, int64x2_t __p1) {
  int64x2_t __ret;
  __ret = __p0 | __p1;
  return __ret;
}
#else
__ai int64x2_t vorrq_s64(int64x2_t __p0, int64x2_t __p1) {
  int64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  int64x2_t __ret;
  __ret = __rev0 | __rev1;
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x8_t vorrq_s16(int16x8_t __p0, int16x8_t __p1) {
  int16x8_t __ret;
  __ret = __p0 | __p1;
  return __ret;
}
#else
__ai int16x8_t vorrq_s16(int16x8_t __p0, int16x8_t __p1) {
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __ret;
  __ret = __rev0 | __rev1;
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x8_t vorr_u8(uint8x8_t __p0, uint8x8_t __p1) {
  uint8x8_t __ret;
  __ret = __p0 | __p1;
  return __ret;
}
#else
__ai uint8x8_t vorr_u8(uint8x8_t __p0, uint8x8_t __p1) {
  uint8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __ret;
  __ret = __rev0 | __rev1;
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x2_t vorr_u32(uint32x2_t __p0, uint32x2_t __p1) {
  uint32x2_t __ret;
  __ret = __p0 | __p1;
  return __ret;
}
#else
__ai uint32x2_t vorr_u32(uint32x2_t __p0, uint32x2_t __p1) {
  uint32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  uint32x2_t __ret;
  __ret = __rev0 | __rev1;
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x1_t vorr_u64(uint64x1_t __p0, uint64x1_t __p1) {
  uint64x1_t __ret;
  __ret = __p0 | __p1;
  return __ret;
}
#else
__ai uint64x1_t vorr_u64(uint64x1_t __p0, uint64x1_t __p1) {
  uint64x1_t __ret;
  __ret = __p0 | __p1;
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x4_t vorr_u16(uint16x4_t __p0, uint16x4_t __p1) {
  uint16x4_t __ret;
  __ret = __p0 | __p1;
  return __ret;
}
#else
__ai uint16x4_t vorr_u16(uint16x4_t __p0, uint16x4_t __p1) {
  uint16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint16x4_t __ret;
  __ret = __rev0 | __rev1;
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x8_t vorr_s8(int8x8_t __p0, int8x8_t __p1) {
  int8x8_t __ret;
  __ret = __p0 | __p1;
  return __ret;
}
#else
__ai int8x8_t vorr_s8(int8x8_t __p0, int8x8_t __p1) {
  int8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __ret;
  __ret = __rev0 | __rev1;
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x2_t vorr_s32(int32x2_t __p0, int32x2_t __p1) {
  int32x2_t __ret;
  __ret = __p0 | __p1;
  return __ret;
}
#else
__ai int32x2_t vorr_s32(int32x2_t __p0, int32x2_t __p1) {
  int32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  int32x2_t __ret;
  __ret = __rev0 | __rev1;
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x1_t vorr_s64(int64x1_t __p0, int64x1_t __p1) {
  int64x1_t __ret;
  __ret = __p0 | __p1;
  return __ret;
}
#else
__ai int64x1_t vorr_s64(int64x1_t __p0, int64x1_t __p1) {
  int64x1_t __ret;
  __ret = __p0 | __p1;
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x4_t vorr_s16(int16x4_t __p0, int16x4_t __p1) {
  int16x4_t __ret;
  __ret = __p0 | __p1;
  return __ret;
}
#else
__ai int16x4_t vorr_s16(int16x4_t __p0, int16x4_t __p1) {
  int16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int16x4_t __ret;
  __ret = __rev0 | __rev1;
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x8_t vpadalq_u8(uint16x8_t __p0, uint8x16_t __p1) {
  uint16x8_t __ret;
  __ret = (uint16x8_t) __builtin_neon_vpadalq_v((int8x16_t)__p0, (int8x16_t)__p1, 49);
  return __ret;
}
#else
__ai uint16x8_t vpadalq_u8(uint16x8_t __p0, uint8x16_t __p1) {
  uint16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __ret;
  __ret = (uint16x8_t) __builtin_neon_vpadalq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 49);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vpadalq_u32(uint64x2_t __p0, uint32x4_t __p1) {
  uint64x2_t __ret;
  __ret = (uint64x2_t) __builtin_neon_vpadalq_v((int8x16_t)__p0, (int8x16_t)__p1, 51);
  return __ret;
}
#else
__ai uint64x2_t vpadalq_u32(uint64x2_t __p0, uint32x4_t __p1) {
  uint64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint64x2_t __ret;
  __ret = (uint64x2_t) __builtin_neon_vpadalq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 51);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vpadalq_u16(uint32x4_t __p0, uint16x8_t __p1) {
  uint32x4_t __ret;
  __ret = (uint32x4_t) __builtin_neon_vpadalq_v((int8x16_t)__p0, (int8x16_t)__p1, 50);
  return __ret;
}
#else
__ai uint32x4_t vpadalq_u16(uint32x4_t __p0, uint16x8_t __p1) {
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint32x4_t __ret;
  __ret = (uint32x4_t) __builtin_neon_vpadalq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 50);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x8_t vpadalq_s8(int16x8_t __p0, int8x16_t __p1) {
  int16x8_t __ret;
  __ret = (int16x8_t) __builtin_neon_vpadalq_v((int8x16_t)__p0, (int8x16_t)__p1, 33);
  return __ret;
}
#else
__ai int16x8_t vpadalq_s8(int16x8_t __p0, int8x16_t __p1) {
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __ret;
  __ret = (int16x8_t) __builtin_neon_vpadalq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 33);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x2_t vpadalq_s32(int64x2_t __p0, int32x4_t __p1) {
  int64x2_t __ret;
  __ret = (int64x2_t) __builtin_neon_vpadalq_v((int8x16_t)__p0, (int8x16_t)__p1, 35);
  return __ret;
}
#else
__ai int64x2_t vpadalq_s32(int64x2_t __p0, int32x4_t __p1) {
  int64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int64x2_t __ret;
  __ret = (int64x2_t) __builtin_neon_vpadalq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 35);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vpadalq_s16(int32x4_t __p0, int16x8_t __p1) {
  int32x4_t __ret;
  __ret = (int32x4_t) __builtin_neon_vpadalq_v((int8x16_t)__p0, (int8x16_t)__p1, 34);
  return __ret;
}
#else
__ai int32x4_t vpadalq_s16(int32x4_t __p0, int16x8_t __p1) {
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int32x4_t __ret;
  __ret = (int32x4_t) __builtin_neon_vpadalq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 34);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x4_t vpadal_u8(uint16x4_t __p0, uint8x8_t __p1) {
  uint16x4_t __ret;
  __ret = (uint16x4_t) __builtin_neon_vpadal_v((int8x8_t)__p0, (int8x8_t)__p1, 17);
  return __ret;
}
#else
__ai uint16x4_t vpadal_u8(uint16x4_t __p0, uint8x8_t __p1) {
  uint16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x4_t __ret;
  __ret = (uint16x4_t) __builtin_neon_vpadal_v((int8x8_t)__rev0, (int8x8_t)__rev1, 17);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x1_t vpadal_u32(uint64x1_t __p0, uint32x2_t __p1) {
  uint64x1_t __ret;
  __ret = (uint64x1_t) __builtin_neon_vpadal_v((int8x8_t)__p0, (int8x8_t)__p1, 19);
  return __ret;
}
#else
__ai uint64x1_t vpadal_u32(uint64x1_t __p0, uint32x2_t __p1) {
  uint32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  uint64x1_t __ret;
  __ret = (uint64x1_t) __builtin_neon_vpadal_v((int8x8_t)__p0, (int8x8_t)__rev1, 19);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x2_t vpadal_u16(uint32x2_t __p0, uint16x4_t __p1) {
  uint32x2_t __ret;
  __ret = (uint32x2_t) __builtin_neon_vpadal_v((int8x8_t)__p0, (int8x8_t)__p1, 18);
  return __ret;
}
#else
__ai uint32x2_t vpadal_u16(uint32x2_t __p0, uint16x4_t __p1) {
  uint32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint32x2_t __ret;
  __ret = (uint32x2_t) __builtin_neon_vpadal_v((int8x8_t)__rev0, (int8x8_t)__rev1, 18);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x4_t vpadal_s8(int16x4_t __p0, int8x8_t __p1) {
  int16x4_t __ret;
  __ret = (int16x4_t) __builtin_neon_vpadal_v((int8x8_t)__p0, (int8x8_t)__p1, 1);
  return __ret;
}
#else
__ai int16x4_t vpadal_s8(int16x4_t __p0, int8x8_t __p1) {
  int16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x4_t __ret;
  __ret = (int16x4_t) __builtin_neon_vpadal_v((int8x8_t)__rev0, (int8x8_t)__rev1, 1);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x1_t vpadal_s32(int64x1_t __p0, int32x2_t __p1) {
  int64x1_t __ret;
  __ret = (int64x1_t) __builtin_neon_vpadal_v((int8x8_t)__p0, (int8x8_t)__p1, 3);
  return __ret;
}
#else
__ai int64x1_t vpadal_s32(int64x1_t __p0, int32x2_t __p1) {
  int32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  int64x1_t __ret;
  __ret = (int64x1_t) __builtin_neon_vpadal_v((int8x8_t)__p0, (int8x8_t)__rev1, 3);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x2_t vpadal_s16(int32x2_t __p0, int16x4_t __p1) {
  int32x2_t __ret;
  __ret = (int32x2_t) __builtin_neon_vpadal_v((int8x8_t)__p0, (int8x8_t)__p1, 2);
  return __ret;
}
#else
__ai int32x2_t vpadal_s16(int32x2_t __p0, int16x4_t __p1) {
  int32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int32x2_t __ret;
  __ret = (int32x2_t) __builtin_neon_vpadal_v((int8x8_t)__rev0, (int8x8_t)__rev1, 2);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x8_t vpadd_u8(uint8x8_t __p0, uint8x8_t __p1) {
  uint8x8_t __ret;
  __ret = (uint8x8_t) __builtin_neon_vpadd_v((int8x8_t)__p0, (int8x8_t)__p1, 16);
  return __ret;
}
#else
__ai uint8x8_t vpadd_u8(uint8x8_t __p0, uint8x8_t __p1) {
  uint8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __ret;
  __ret = (uint8x8_t) __builtin_neon_vpadd_v((int8x8_t)__rev0, (int8x8_t)__rev1, 16);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x2_t vpadd_u32(uint32x2_t __p0, uint32x2_t __p1) {
  uint32x2_t __ret;
  __ret = (uint32x2_t) __builtin_neon_vpadd_v((int8x8_t)__p0, (int8x8_t)__p1, 18);
  return __ret;
}
#else
__ai uint32x2_t vpadd_u32(uint32x2_t __p0, uint32x2_t __p1) {
  uint32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  uint32x2_t __ret;
  __ret = (uint32x2_t) __builtin_neon_vpadd_v((int8x8_t)__rev0, (int8x8_t)__rev1, 18);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x4_t vpadd_u16(uint16x4_t __p0, uint16x4_t __p1) {
  uint16x4_t __ret;
  __ret = (uint16x4_t) __builtin_neon_vpadd_v((int8x8_t)__p0, (int8x8_t)__p1, 17);
  return __ret;
}
#else
__ai uint16x4_t vpadd_u16(uint16x4_t __p0, uint16x4_t __p1) {
  uint16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint16x4_t __ret;
  __ret = (uint16x4_t) __builtin_neon_vpadd_v((int8x8_t)__rev0, (int8x8_t)__rev1, 17);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x8_t vpadd_s8(int8x8_t __p0, int8x8_t __p1) {
  int8x8_t __ret;
  __ret = (int8x8_t) __builtin_neon_vpadd_v((int8x8_t)__p0, (int8x8_t)__p1, 0);
  return __ret;
}
#else
__ai int8x8_t vpadd_s8(int8x8_t __p0, int8x8_t __p1) {
  int8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __ret;
  __ret = (int8x8_t) __builtin_neon_vpadd_v((int8x8_t)__rev0, (int8x8_t)__rev1, 0);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x2_t vpadd_f32(float32x2_t __p0, float32x2_t __p1) {
  float32x2_t __ret;
  __ret = (float32x2_t) __builtin_neon_vpadd_v((int8x8_t)__p0, (int8x8_t)__p1, 9);
  return __ret;
}
#else
__ai float32x2_t vpadd_f32(float32x2_t __p0, float32x2_t __p1) {
  float32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  float32x2_t __ret;
  __ret = (float32x2_t) __builtin_neon_vpadd_v((int8x8_t)__rev0, (int8x8_t)__rev1, 9);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x2_t vpadd_s32(int32x2_t __p0, int32x2_t __p1) {
  int32x2_t __ret;
  __ret = (int32x2_t) __builtin_neon_vpadd_v((int8x8_t)__p0, (int8x8_t)__p1, 2);
  return __ret;
}
#else
__ai int32x2_t vpadd_s32(int32x2_t __p0, int32x2_t __p1) {
  int32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  int32x2_t __ret;
  __ret = (int32x2_t) __builtin_neon_vpadd_v((int8x8_t)__rev0, (int8x8_t)__rev1, 2);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x4_t vpadd_s16(int16x4_t __p0, int16x4_t __p1) {
  int16x4_t __ret;
  __ret = (int16x4_t) __builtin_neon_vpadd_v((int8x8_t)__p0, (int8x8_t)__p1, 1);
  return __ret;
}
#else
__ai int16x4_t vpadd_s16(int16x4_t __p0, int16x4_t __p1) {
  int16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int16x4_t __ret;
  __ret = (int16x4_t) __builtin_neon_vpadd_v((int8x8_t)__rev0, (int8x8_t)__rev1, 1);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x8_t vpaddlq_u8(uint8x16_t __p0) {
  uint16x8_t __ret;
  __ret = (uint16x8_t) __builtin_neon_vpaddlq_v((int8x16_t)__p0, 49);
  return __ret;
}
#else
__ai uint16x8_t vpaddlq_u8(uint8x16_t __p0) {
  uint8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __ret;
  __ret = (uint16x8_t) __builtin_neon_vpaddlq_v((int8x16_t)__rev0, 49);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vpaddlq_u32(uint32x4_t __p0) {
  uint64x2_t __ret;
  __ret = (uint64x2_t) __builtin_neon_vpaddlq_v((int8x16_t)__p0, 51);
  return __ret;
}
#else
__ai uint64x2_t vpaddlq_u32(uint32x4_t __p0) {
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint64x2_t __ret;
  __ret = (uint64x2_t) __builtin_neon_vpaddlq_v((int8x16_t)__rev0, 51);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vpaddlq_u16(uint16x8_t __p0) {
  uint32x4_t __ret;
  __ret = (uint32x4_t) __builtin_neon_vpaddlq_v((int8x16_t)__p0, 50);
  return __ret;
}
#else
__ai uint32x4_t vpaddlq_u16(uint16x8_t __p0) {
  uint16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint32x4_t __ret;
  __ret = (uint32x4_t) __builtin_neon_vpaddlq_v((int8x16_t)__rev0, 50);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x8_t vpaddlq_s8(int8x16_t __p0) {
  int16x8_t __ret;
  __ret = (int16x8_t) __builtin_neon_vpaddlq_v((int8x16_t)__p0, 33);
  return __ret;
}
#else
__ai int16x8_t vpaddlq_s8(int8x16_t __p0) {
  int8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __ret;
  __ret = (int16x8_t) __builtin_neon_vpaddlq_v((int8x16_t)__rev0, 33);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x2_t vpaddlq_s32(int32x4_t __p0) {
  int64x2_t __ret;
  __ret = (int64x2_t) __builtin_neon_vpaddlq_v((int8x16_t)__p0, 35);
  return __ret;
}
#else
__ai int64x2_t vpaddlq_s32(int32x4_t __p0) {
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int64x2_t __ret;
  __ret = (int64x2_t) __builtin_neon_vpaddlq_v((int8x16_t)__rev0, 35);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vpaddlq_s16(int16x8_t __p0) {
  int32x4_t __ret;
  __ret = (int32x4_t) __builtin_neon_vpaddlq_v((int8x16_t)__p0, 34);
  return __ret;
}
#else
__ai int32x4_t vpaddlq_s16(int16x8_t __p0) {
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int32x4_t __ret;
  __ret = (int32x4_t) __builtin_neon_vpaddlq_v((int8x16_t)__rev0, 34);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x4_t vpaddl_u8(uint8x8_t __p0) {
  uint16x4_t __ret;
  __ret = (uint16x4_t) __builtin_neon_vpaddl_v((int8x8_t)__p0, 17);
  return __ret;
}
#else
__ai uint16x4_t vpaddl_u8(uint8x8_t __p0) {
  uint8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x4_t __ret;
  __ret = (uint16x4_t) __builtin_neon_vpaddl_v((int8x8_t)__rev0, 17);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x1_t vpaddl_u32(uint32x2_t __p0) {
  uint64x1_t __ret;
  __ret = (uint64x1_t) __builtin_neon_vpaddl_v((int8x8_t)__p0, 19);
  return __ret;
}
#else
__ai uint64x1_t vpaddl_u32(uint32x2_t __p0) {
  uint32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint64x1_t __ret;
  __ret = (uint64x1_t) __builtin_neon_vpaddl_v((int8x8_t)__rev0, 19);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x2_t vpaddl_u16(uint16x4_t __p0) {
  uint32x2_t __ret;
  __ret = (uint32x2_t) __builtin_neon_vpaddl_v((int8x8_t)__p0, 18);
  return __ret;
}
#else
__ai uint32x2_t vpaddl_u16(uint16x4_t __p0) {
  uint16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint32x2_t __ret;
  __ret = (uint32x2_t) __builtin_neon_vpaddl_v((int8x8_t)__rev0, 18);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x4_t vpaddl_s8(int8x8_t __p0) {
  int16x4_t __ret;
  __ret = (int16x4_t) __builtin_neon_vpaddl_v((int8x8_t)__p0, 1);
  return __ret;
}
#else
__ai int16x4_t vpaddl_s8(int8x8_t __p0) {
  int8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x4_t __ret;
  __ret = (int16x4_t) __builtin_neon_vpaddl_v((int8x8_t)__rev0, 1);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x1_t vpaddl_s32(int32x2_t __p0) {
  int64x1_t __ret;
  __ret = (int64x1_t) __builtin_neon_vpaddl_v((int8x8_t)__p0, 3);
  return __ret;
}
#else
__ai int64x1_t vpaddl_s32(int32x2_t __p0) {
  int32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int64x1_t __ret;
  __ret = (int64x1_t) __builtin_neon_vpaddl_v((int8x8_t)__rev0, 3);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x2_t vpaddl_s16(int16x4_t __p0) {
  int32x2_t __ret;
  __ret = (int32x2_t) __builtin_neon_vpaddl_v((int8x8_t)__p0, 2);
  return __ret;
}
#else
__ai int32x2_t vpaddl_s16(int16x4_t __p0) {
  int16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int32x2_t __ret;
  __ret = (int32x2_t) __builtin_neon_vpaddl_v((int8x8_t)__rev0, 2);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x8_t vpmax_u8(uint8x8_t __p0, uint8x8_t __p1) {
  uint8x8_t __ret;
  __ret = (uint8x8_t) __builtin_neon_vpmax_v((int8x8_t)__p0, (int8x8_t)__p1, 16);
  return __ret;
}
#else
__ai uint8x8_t vpmax_u8(uint8x8_t __p0, uint8x8_t __p1) {
  uint8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __ret;
  __ret = (uint8x8_t) __builtin_neon_vpmax_v((int8x8_t)__rev0, (int8x8_t)__rev1, 16);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x2_t vpmax_u32(uint32x2_t __p0, uint32x2_t __p1) {
  uint32x2_t __ret;
  __ret = (uint32x2_t) __builtin_neon_vpmax_v((int8x8_t)__p0, (int8x8_t)__p1, 18);
  return __ret;
}
#else
__ai uint32x2_t vpmax_u32(uint32x2_t __p0, uint32x2_t __p1) {
  uint32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  uint32x2_t __ret;
  __ret = (uint32x2_t) __builtin_neon_vpmax_v((int8x8_t)__rev0, (int8x8_t)__rev1, 18);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x4_t vpmax_u16(uint16x4_t __p0, uint16x4_t __p1) {
  uint16x4_t __ret;
  __ret = (uint16x4_t) __builtin_neon_vpmax_v((int8x8_t)__p0, (int8x8_t)__p1, 17);
  return __ret;
}
#else
__ai uint16x4_t vpmax_u16(uint16x4_t __p0, uint16x4_t __p1) {
  uint16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint16x4_t __ret;
  __ret = (uint16x4_t) __builtin_neon_vpmax_v((int8x8_t)__rev0, (int8x8_t)__rev1, 17);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x8_t vpmax_s8(int8x8_t __p0, int8x8_t __p1) {
  int8x8_t __ret;
  __ret = (int8x8_t) __builtin_neon_vpmax_v((int8x8_t)__p0, (int8x8_t)__p1, 0);
  return __ret;
}
#else
__ai int8x8_t vpmax_s8(int8x8_t __p0, int8x8_t __p1) {
  int8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __ret;
  __ret = (int8x8_t) __builtin_neon_vpmax_v((int8x8_t)__rev0, (int8x8_t)__rev1, 0);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x2_t vpmax_f32(float32x2_t __p0, float32x2_t __p1) {
  float32x2_t __ret;
  __ret = (float32x2_t) __builtin_neon_vpmax_v((int8x8_t)__p0, (int8x8_t)__p1, 9);
  return __ret;
}
#else
__ai float32x2_t vpmax_f32(float32x2_t __p0, float32x2_t __p1) {
  float32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  float32x2_t __ret;
  __ret = (float32x2_t) __builtin_neon_vpmax_v((int8x8_t)__rev0, (int8x8_t)__rev1, 9);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x2_t vpmax_s32(int32x2_t __p0, int32x2_t __p1) {
  int32x2_t __ret;
  __ret = (int32x2_t) __builtin_neon_vpmax_v((int8x8_t)__p0, (int8x8_t)__p1, 2);
  return __ret;
}
#else
__ai int32x2_t vpmax_s32(int32x2_t __p0, int32x2_t __p1) {
  int32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  int32x2_t __ret;
  __ret = (int32x2_t) __builtin_neon_vpmax_v((int8x8_t)__rev0, (int8x8_t)__rev1, 2);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x4_t vpmax_s16(int16x4_t __p0, int16x4_t __p1) {
  int16x4_t __ret;
  __ret = (int16x4_t) __builtin_neon_vpmax_v((int8x8_t)__p0, (int8x8_t)__p1, 1);
  return __ret;
}
#else
__ai int16x4_t vpmax_s16(int16x4_t __p0, int16x4_t __p1) {
  int16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int16x4_t __ret;
  __ret = (int16x4_t) __builtin_neon_vpmax_v((int8x8_t)__rev0, (int8x8_t)__rev1, 1);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x8_t vpmin_u8(uint8x8_t __p0, uint8x8_t __p1) {
  uint8x8_t __ret;
  __ret = (uint8x8_t) __builtin_neon_vpmin_v((int8x8_t)__p0, (int8x8_t)__p1, 16);
  return __ret;
}
#else
__ai uint8x8_t vpmin_u8(uint8x8_t __p0, uint8x8_t __p1) {
  uint8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __ret;
  __ret = (uint8x8_t) __builtin_neon_vpmin_v((int8x8_t)__rev0, (int8x8_t)__rev1, 16);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x2_t vpmin_u32(uint32x2_t __p0, uint32x2_t __p1) {
  uint32x2_t __ret;
  __ret = (uint32x2_t) __builtin_neon_vpmin_v((int8x8_t)__p0, (int8x8_t)__p1, 18);
  return __ret;
}
#else
__ai uint32x2_t vpmin_u32(uint32x2_t __p0, uint32x2_t __p1) {
  uint32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  uint32x2_t __ret;
  __ret = (uint32x2_t) __builtin_neon_vpmin_v((int8x8_t)__rev0, (int8x8_t)__rev1, 18);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x4_t vpmin_u16(uint16x4_t __p0, uint16x4_t __p1) {
  uint16x4_t __ret;
  __ret = (uint16x4_t) __builtin_neon_vpmin_v((int8x8_t)__p0, (int8x8_t)__p1, 17);
  return __ret;
}
#else
__ai uint16x4_t vpmin_u16(uint16x4_t __p0, uint16x4_t __p1) {
  uint16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint16x4_t __ret;
  __ret = (uint16x4_t) __builtin_neon_vpmin_v((int8x8_t)__rev0, (int8x8_t)__rev1, 17);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x8_t vpmin_s8(int8x8_t __p0, int8x8_t __p1) {
  int8x8_t __ret;
  __ret = (int8x8_t) __builtin_neon_vpmin_v((int8x8_t)__p0, (int8x8_t)__p1, 0);
  return __ret;
}
#else
__ai int8x8_t vpmin_s8(int8x8_t __p0, int8x8_t __p1) {
  int8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __ret;
  __ret = (int8x8_t) __builtin_neon_vpmin_v((int8x8_t)__rev0, (int8x8_t)__rev1, 0);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x2_t vpmin_f32(float32x2_t __p0, float32x2_t __p1) {
  float32x2_t __ret;
  __ret = (float32x2_t) __builtin_neon_vpmin_v((int8x8_t)__p0, (int8x8_t)__p1, 9);
  return __ret;
}
#else
__ai float32x2_t vpmin_f32(float32x2_t __p0, float32x2_t __p1) {
  float32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  float32x2_t __ret;
  __ret = (float32x2_t) __builtin_neon_vpmin_v((int8x8_t)__rev0, (int8x8_t)__rev1, 9);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x2_t vpmin_s32(int32x2_t __p0, int32x2_t __p1) {
  int32x2_t __ret;
  __ret = (int32x2_t) __builtin_neon_vpmin_v((int8x8_t)__p0, (int8x8_t)__p1, 2);
  return __ret;
}
#else
__ai int32x2_t vpmin_s32(int32x2_t __p0, int32x2_t __p1) {
  int32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  int32x2_t __ret;
  __ret = (int32x2_t) __builtin_neon_vpmin_v((int8x8_t)__rev0, (int8x8_t)__rev1, 2);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x4_t vpmin_s16(int16x4_t __p0, int16x4_t __p1) {
  int16x4_t __ret;
  __ret = (int16x4_t) __builtin_neon_vpmin_v((int8x8_t)__p0, (int8x8_t)__p1, 1);
  return __ret;
}
#else
__ai int16x4_t vpmin_s16(int16x4_t __p0, int16x4_t __p1) {
  int16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int16x4_t __ret;
  __ret = (int16x4_t) __builtin_neon_vpmin_v((int8x8_t)__rev0, (int8x8_t)__rev1, 1);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x16_t vqabsq_s8(int8x16_t __p0) {
  int8x16_t __ret;
  __ret = (int8x16_t) __builtin_neon_vqabsq_v((int8x16_t)__p0, 32);
  return __ret;
}
#else
__ai int8x16_t vqabsq_s8(int8x16_t __p0) {
  int8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __ret;
  __ret = (int8x16_t) __builtin_neon_vqabsq_v((int8x16_t)__rev0, 32);
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vqabsq_s32(int32x4_t __p0) {
  int32x4_t __ret;
  __ret = (int32x4_t) __builtin_neon_vqabsq_v((int8x16_t)__p0, 34);
  return __ret;
}
#else
__ai int32x4_t vqabsq_s32(int32x4_t __p0) {
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int32x4_t __ret;
  __ret = (int32x4_t) __builtin_neon_vqabsq_v((int8x16_t)__rev0, 34);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x8_t vqabsq_s16(int16x8_t __p0) {
  int16x8_t __ret;
  __ret = (int16x8_t) __builtin_neon_vqabsq_v((int8x16_t)__p0, 33);
  return __ret;
}
#else
__ai int16x8_t vqabsq_s16(int16x8_t __p0) {
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __ret;
  __ret = (int16x8_t) __builtin_neon_vqabsq_v((int8x16_t)__rev0, 33);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x8_t vqabs_s8(int8x8_t __p0) {
  int8x8_t __ret;
  __ret = (int8x8_t) __builtin_neon_vqabs_v((int8x8_t)__p0, 0);
  return __ret;
}
#else
__ai int8x8_t vqabs_s8(int8x8_t __p0) {
  int8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __ret;
  __ret = (int8x8_t) __builtin_neon_vqabs_v((int8x8_t)__rev0, 0);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x2_t vqabs_s32(int32x2_t __p0) {
  int32x2_t __ret;
  __ret = (int32x2_t) __builtin_neon_vqabs_v((int8x8_t)__p0, 2);
  return __ret;
}
#else
__ai int32x2_t vqabs_s32(int32x2_t __p0) {
  int32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int32x2_t __ret;
  __ret = (int32x2_t) __builtin_neon_vqabs_v((int8x8_t)__rev0, 2);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x4_t vqabs_s16(int16x4_t __p0) {
  int16x4_t __ret;
  __ret = (int16x4_t) __builtin_neon_vqabs_v((int8x8_t)__p0, 1);
  return __ret;
}
#else
__ai int16x4_t vqabs_s16(int16x4_t __p0) {
  int16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int16x4_t __ret;
  __ret = (int16x4_t) __builtin_neon_vqabs_v((int8x8_t)__rev0, 1);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x16_t vqaddq_u8(uint8x16_t __p0, uint8x16_t __p1) {
  uint8x16_t __ret;
  __ret = (uint8x16_t) __builtin_neon_vqaddq_v((int8x16_t)__p0, (int8x16_t)__p1, 48);
  return __ret;
}
#else
__ai uint8x16_t vqaddq_u8(uint8x16_t __p0, uint8x16_t __p1) {
  uint8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __ret;
  __ret = (uint8x16_t) __builtin_neon_vqaddq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 48);
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vqaddq_u32(uint32x4_t __p0, uint32x4_t __p1) {
  uint32x4_t __ret;
  __ret = (uint32x4_t) __builtin_neon_vqaddq_v((int8x16_t)__p0, (int8x16_t)__p1, 50);
  return __ret;
}
#else
__ai uint32x4_t vqaddq_u32(uint32x4_t __p0, uint32x4_t __p1) {
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint32x4_t __ret;
  __ret = (uint32x4_t) __builtin_neon_vqaddq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 50);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vqaddq_u64(uint64x2_t __p0, uint64x2_t __p1) {
  uint64x2_t __ret;
  __ret = (uint64x2_t) __builtin_neon_vqaddq_v((int8x16_t)__p0, (int8x16_t)__p1, 51);
  return __ret;
}
#else
__ai uint64x2_t vqaddq_u64(uint64x2_t __p0, uint64x2_t __p1) {
  uint64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  uint64x2_t __ret;
  __ret = (uint64x2_t) __builtin_neon_vqaddq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 51);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x8_t vqaddq_u16(uint16x8_t __p0, uint16x8_t __p1) {
  uint16x8_t __ret;
  __ret = (uint16x8_t) __builtin_neon_vqaddq_v((int8x16_t)__p0, (int8x16_t)__p1, 49);
  return __ret;
}
#else
__ai uint16x8_t vqaddq_u16(uint16x8_t __p0, uint16x8_t __p1) {
  uint16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __ret;
  __ret = (uint16x8_t) __builtin_neon_vqaddq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 49);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x16_t vqaddq_s8(int8x16_t __p0, int8x16_t __p1) {
  int8x16_t __ret;
  __ret = (int8x16_t) __builtin_neon_vqaddq_v((int8x16_t)__p0, (int8x16_t)__p1, 32);
  return __ret;
}
#else
__ai int8x16_t vqaddq_s8(int8x16_t __p0, int8x16_t __p1) {
  int8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __ret;
  __ret = (int8x16_t) __builtin_neon_vqaddq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 32);
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vqaddq_s32(int32x4_t __p0, int32x4_t __p1) {
  int32x4_t __ret;
  __ret = (int32x4_t) __builtin_neon_vqaddq_v((int8x16_t)__p0, (int8x16_t)__p1, 34);
  return __ret;
}
#else
__ai int32x4_t vqaddq_s32(int32x4_t __p0, int32x4_t __p1) {
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int32x4_t __ret;
  __ret = (int32x4_t) __builtin_neon_vqaddq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 34);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x2_t vqaddq_s64(int64x2_t __p0, int64x2_t __p1) {
  int64x2_t __ret;
  __ret = (int64x2_t) __builtin_neon_vqaddq_v((int8x16_t)__p0, (int8x16_t)__p1, 35);
  return __ret;
}
#else
__ai int64x2_t vqaddq_s64(int64x2_t __p0, int64x2_t __p1) {
  int64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  int64x2_t __ret;
  __ret = (int64x2_t) __builtin_neon_vqaddq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 35);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x8_t vqaddq_s16(int16x8_t __p0, int16x8_t __p1) {
  int16x8_t __ret;
  __ret = (int16x8_t) __builtin_neon_vqaddq_v((int8x16_t)__p0, (int8x16_t)__p1, 33);
  return __ret;
}
#else
__ai int16x8_t vqaddq_s16(int16x8_t __p0, int16x8_t __p1) {
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __ret;
  __ret = (int16x8_t) __builtin_neon_vqaddq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 33);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x8_t vqadd_u8(uint8x8_t __p0, uint8x8_t __p1) {
  uint8x8_t __ret;
  __ret = (uint8x8_t) __builtin_neon_vqadd_v((int8x8_t)__p0, (int8x8_t)__p1, 16);
  return __ret;
}
#else
__ai uint8x8_t vqadd_u8(uint8x8_t __p0, uint8x8_t __p1) {
  uint8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __ret;
  __ret = (uint8x8_t) __builtin_neon_vqadd_v((int8x8_t)__rev0, (int8x8_t)__rev1, 16);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x2_t vqadd_u32(uint32x2_t __p0, uint32x2_t __p1) {
  uint32x2_t __ret;
  __ret = (uint32x2_t) __builtin_neon_vqadd_v((int8x8_t)__p0, (int8x8_t)__p1, 18);
  return __ret;
}
#else
__ai uint32x2_t vqadd_u32(uint32x2_t __p0, uint32x2_t __p1) {
  uint32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  uint32x2_t __ret;
  __ret = (uint32x2_t) __builtin_neon_vqadd_v((int8x8_t)__rev0, (int8x8_t)__rev1, 18);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x1_t vqadd_u64(uint64x1_t __p0, uint64x1_t __p1) {
  uint64x1_t __ret;
  __ret = (uint64x1_t) __builtin_neon_vqadd_v((int8x8_t)__p0, (int8x8_t)__p1, 19);
  return __ret;
}
#else
__ai uint64x1_t vqadd_u64(uint64x1_t __p0, uint64x1_t __p1) {
  uint64x1_t __ret;
  __ret = (uint64x1_t) __builtin_neon_vqadd_v((int8x8_t)__p0, (int8x8_t)__p1, 19);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x4_t vqadd_u16(uint16x4_t __p0, uint16x4_t __p1) {
  uint16x4_t __ret;
  __ret = (uint16x4_t) __builtin_neon_vqadd_v((int8x8_t)__p0, (int8x8_t)__p1, 17);
  return __ret;
}
#else
__ai uint16x4_t vqadd_u16(uint16x4_t __p0, uint16x4_t __p1) {
  uint16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint16x4_t __ret;
  __ret = (uint16x4_t) __builtin_neon_vqadd_v((int8x8_t)__rev0, (int8x8_t)__rev1, 17);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x8_t vqadd_s8(int8x8_t __p0, int8x8_t __p1) {
  int8x8_t __ret;
  __ret = (int8x8_t) __builtin_neon_vqadd_v((int8x8_t)__p0, (int8x8_t)__p1, 0);
  return __ret;
}
#else
__ai int8x8_t vqadd_s8(int8x8_t __p0, int8x8_t __p1) {
  int8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __ret;
  __ret = (int8x8_t) __builtin_neon_vqadd_v((int8x8_t)__rev0, (int8x8_t)__rev1, 0);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x2_t vqadd_s32(int32x2_t __p0, int32x2_t __p1) {
  int32x2_t __ret;
  __ret = (int32x2_t) __builtin_neon_vqadd_v((int8x8_t)__p0, (int8x8_t)__p1, 2);
  return __ret;
}
#else
__ai int32x2_t vqadd_s32(int32x2_t __p0, int32x2_t __p1) {
  int32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  int32x2_t __ret;
  __ret = (int32x2_t) __builtin_neon_vqadd_v((int8x8_t)__rev0, (int8x8_t)__rev1, 2);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x1_t vqadd_s64(int64x1_t __p0, int64x1_t __p1) {
  int64x1_t __ret;
  __ret = (int64x1_t) __builtin_neon_vqadd_v((int8x8_t)__p0, (int8x8_t)__p1, 3);
  return __ret;
}
#else
__ai int64x1_t vqadd_s64(int64x1_t __p0, int64x1_t __p1) {
  int64x1_t __ret;
  __ret = (int64x1_t) __builtin_neon_vqadd_v((int8x8_t)__p0, (int8x8_t)__p1, 3);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x4_t vqadd_s16(int16x4_t __p0, int16x4_t __p1) {
  int16x4_t __ret;
  __ret = (int16x4_t) __builtin_neon_vqadd_v((int8x8_t)__p0, (int8x8_t)__p1, 1);
  return __ret;
}
#else
__ai int16x4_t vqadd_s16(int16x4_t __p0, int16x4_t __p1) {
  int16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int16x4_t __ret;
  __ret = (int16x4_t) __builtin_neon_vqadd_v((int8x8_t)__rev0, (int8x8_t)__rev1, 1);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x2_t vqdmlal_s32(int64x2_t __p0, int32x2_t __p1, int32x2_t __p2) {
  int64x2_t __ret;
  __ret = (int64x2_t) __builtin_neon_vqdmlal_v((int8x16_t)__p0, (int8x8_t)__p1, (int8x8_t)__p2, 35);
  return __ret;
}
#else
__ai int64x2_t vqdmlal_s32(int64x2_t __p0, int32x2_t __p1, int32x2_t __p2) {
  int64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  int32x2_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 1, 0);
  int64x2_t __ret;
  __ret = (int64x2_t) __builtin_neon_vqdmlal_v((int8x16_t)__rev0, (int8x8_t)__rev1, (int8x8_t)__rev2, 35);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
__ai int64x2_t __noswap_vqdmlal_s32(int64x2_t __p0, int32x2_t __p1, int32x2_t __p2) {
  int64x2_t __ret;
  __ret = (int64x2_t) __builtin_neon_vqdmlal_v((int8x16_t)__p0, (int8x8_t)__p1, (int8x8_t)__p2, 35);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vqdmlal_s16(int32x4_t __p0, int16x4_t __p1, int16x4_t __p2) {
  int32x4_t __ret;
  __ret = (int32x4_t) __builtin_neon_vqdmlal_v((int8x16_t)__p0, (int8x8_t)__p1, (int8x8_t)__p2, 34);
  return __ret;
}
#else
__ai int32x4_t vqdmlal_s16(int32x4_t __p0, int16x4_t __p1, int16x4_t __p2) {
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int16x4_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 3, 2, 1, 0);
  int32x4_t __ret;
  __ret = (int32x4_t) __builtin_neon_vqdmlal_v((int8x16_t)__rev0, (int8x8_t)__rev1, (int8x8_t)__rev2, 34);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
__ai int32x4_t __noswap_vqdmlal_s16(int32x4_t __p0, int16x4_t __p1, int16x4_t __p2) {
  int32x4_t __ret;
  __ret = (int32x4_t) __builtin_neon_vqdmlal_v((int8x16_t)__p0, (int8x8_t)__p1, (int8x8_t)__p2, 34);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
#define vqdmlal_lane_s32(__p0, __p1, __p2, __p3) __extension__ ({ \
  int64x2_t __s0 = __p0; \
  int32x2_t __s1 = __p1; \
  int32x2_t __s2 = __p2; \
  int64x2_t __ret; \
  __ret = vqdmlal_s32(__s0, __s1, __builtin_shufflevector(__s2, __s2, __p3, __p3)); \
  __ret; \
})
#else
#define vqdmlal_lane_s32(__p0, __p1, __p2, __p3) __extension__ ({ \
  int64x2_t __s0 = __p0; \
  int32x2_t __s1 = __p1; \
  int32x2_t __s2 = __p2; \
  int64x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  int32x2_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 1, 0); \
  int32x2_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 1, 0); \
  int64x2_t __ret; \
  __ret = __noswap_vqdmlal_s32(__rev0, __rev1, __builtin_shufflevector(__rev2, __rev2, __p3, __p3)); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqdmlal_lane_s16(__p0, __p1, __p2, __p3) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  int16x4_t __s1 = __p1; \
  int16x4_t __s2 = __p2; \
  int32x4_t __ret; \
  __ret = vqdmlal_s16(__s0, __s1, __builtin_shufflevector(__s2, __s2, __p3, __p3, __p3, __p3)); \
  __ret; \
})
#else
#define vqdmlal_lane_s16(__p0, __p1, __p2, __p3) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  int16x4_t __s1 = __p1; \
  int16x4_t __s2 = __p2; \
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  int16x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  int16x4_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 3, 2, 1, 0); \
  int32x4_t __ret; \
  __ret = __noswap_vqdmlal_s16(__rev0, __rev1, __builtin_shufflevector(__rev2, __rev2, __p3, __p3, __p3, __p3)); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x2_t vqdmlal_n_s32(int64x2_t __p0, int32x2_t __p1, int32_t __p2) {
  int64x2_t __ret;
  __ret = (int64x2_t) __builtin_neon_vqdmlal_v((int8x16_t)__p0, (int8x8_t)__p1, (int8x8_t)(int32x2_t) {__p2, __p2}, 35);
  return __ret;
}
#else
__ai int64x2_t vqdmlal_n_s32(int64x2_t __p0, int32x2_t __p1, int32_t __p2) {
  int64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  int64x2_t __ret;
  __ret = (int64x2_t) __builtin_neon_vqdmlal_v((int8x16_t)__rev0, (int8x8_t)__rev1, (int8x8_t)(int32x2_t) {__p2, __p2}, 35);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
__ai int64x2_t __noswap_vqdmlal_n_s32(int64x2_t __p0, int32x2_t __p1, int32_t __p2) {
  int64x2_t __ret;
  __ret = (int64x2_t) __builtin_neon_vqdmlal_v((int8x16_t)__p0, (int8x8_t)__p1, (int8x8_t)(int32x2_t) {__p2, __p2}, 35);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vqdmlal_n_s16(int32x4_t __p0, int16x4_t __p1, int16_t __p2) {
  int32x4_t __ret;
  __ret = (int32x4_t) __builtin_neon_vqdmlal_v((int8x16_t)__p0, (int8x8_t)__p1, (int8x8_t)(int16x4_t) {__p2, __p2, __p2, __p2}, 34);
  return __ret;
}
#else
__ai int32x4_t vqdmlal_n_s16(int32x4_t __p0, int16x4_t __p1, int16_t __p2) {
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int32x4_t __ret;
  __ret = (int32x4_t) __builtin_neon_vqdmlal_v((int8x16_t)__rev0, (int8x8_t)__rev1, (int8x8_t)(int16x4_t) {__p2, __p2, __p2, __p2}, 34);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
__ai int32x4_t __noswap_vqdmlal_n_s16(int32x4_t __p0, int16x4_t __p1, int16_t __p2) {
  int32x4_t __ret;
  __ret = (int32x4_t) __builtin_neon_vqdmlal_v((int8x16_t)__p0, (int8x8_t)__p1, (int8x8_t)(int16x4_t) {__p2, __p2, __p2, __p2}, 34);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x2_t vqdmlsl_s32(int64x2_t __p0, int32x2_t __p1, int32x2_t __p2) {
  int64x2_t __ret;
  __ret = (int64x2_t) __builtin_neon_vqdmlsl_v((int8x16_t)__p0, (int8x8_t)__p1, (int8x8_t)__p2, 35);
  return __ret;
}
#else
__ai int64x2_t vqdmlsl_s32(int64x2_t __p0, int32x2_t __p1, int32x2_t __p2) {
  int64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  int32x2_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 1, 0);
  int64x2_t __ret;
  __ret = (int64x2_t) __builtin_neon_vqdmlsl_v((int8x16_t)__rev0, (int8x8_t)__rev1, (int8x8_t)__rev2, 35);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
__ai int64x2_t __noswap_vqdmlsl_s32(int64x2_t __p0, int32x2_t __p1, int32x2_t __p2) {
  int64x2_t __ret;
  __ret = (int64x2_t) __builtin_neon_vqdmlsl_v((int8x16_t)__p0, (int8x8_t)__p1, (int8x8_t)__p2, 35);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vqdmlsl_s16(int32x4_t __p0, int16x4_t __p1, int16x4_t __p2) {
  int32x4_t __ret;
  __ret = (int32x4_t) __builtin_neon_vqdmlsl_v((int8x16_t)__p0, (int8x8_t)__p1, (int8x8_t)__p2, 34);
  return __ret;
}
#else
__ai int32x4_t vqdmlsl_s16(int32x4_t __p0, int16x4_t __p1, int16x4_t __p2) {
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int16x4_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 3, 2, 1, 0);
  int32x4_t __ret;
  __ret = (int32x4_t) __builtin_neon_vqdmlsl_v((int8x16_t)__rev0, (int8x8_t)__rev1, (int8x8_t)__rev2, 34);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
__ai int32x4_t __noswap_vqdmlsl_s16(int32x4_t __p0, int16x4_t __p1, int16x4_t __p2) {
  int32x4_t __ret;
  __ret = (int32x4_t) __builtin_neon_vqdmlsl_v((int8x16_t)__p0, (int8x8_t)__p1, (int8x8_t)__p2, 34);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
#define vqdmlsl_lane_s32(__p0, __p1, __p2, __p3) __extension__ ({ \
  int64x2_t __s0 = __p0; \
  int32x2_t __s1 = __p1; \
  int32x2_t __s2 = __p2; \
  int64x2_t __ret; \
  __ret = vqdmlsl_s32(__s0, __s1, __builtin_shufflevector(__s2, __s2, __p3, __p3)); \
  __ret; \
})
#else
#define vqdmlsl_lane_s32(__p0, __p1, __p2, __p3) __extension__ ({ \
  int64x2_t __s0 = __p0; \
  int32x2_t __s1 = __p1; \
  int32x2_t __s2 = __p2; \
  int64x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  int32x2_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 1, 0); \
  int32x2_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 1, 0); \
  int64x2_t __ret; \
  __ret = __noswap_vqdmlsl_s32(__rev0, __rev1, __builtin_shufflevector(__rev2, __rev2, __p3, __p3)); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqdmlsl_lane_s16(__p0, __p1, __p2, __p3) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  int16x4_t __s1 = __p1; \
  int16x4_t __s2 = __p2; \
  int32x4_t __ret; \
  __ret = vqdmlsl_s16(__s0, __s1, __builtin_shufflevector(__s2, __s2, __p3, __p3, __p3, __p3)); \
  __ret; \
})
#else
#define vqdmlsl_lane_s16(__p0, __p1, __p2, __p3) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  int16x4_t __s1 = __p1; \
  int16x4_t __s2 = __p2; \
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  int16x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  int16x4_t __rev2;  __rev2 = __builtin_shufflevector(__s2, __s2, 3, 2, 1, 0); \
  int32x4_t __ret; \
  __ret = __noswap_vqdmlsl_s16(__rev0, __rev1, __builtin_shufflevector(__rev2, __rev2, __p3, __p3, __p3, __p3)); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x2_t vqdmlsl_n_s32(int64x2_t __p0, int32x2_t __p1, int32_t __p2) {
  int64x2_t __ret;
  __ret = (int64x2_t) __builtin_neon_vqdmlsl_v((int8x16_t)__p0, (int8x8_t)__p1, (int8x8_t)(int32x2_t) {__p2, __p2}, 35);
  return __ret;
}
#else
__ai int64x2_t vqdmlsl_n_s32(int64x2_t __p0, int32x2_t __p1, int32_t __p2) {
  int64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  int64x2_t __ret;
  __ret = (int64x2_t) __builtin_neon_vqdmlsl_v((int8x16_t)__rev0, (int8x8_t)__rev1, (int8x8_t)(int32x2_t) {__p2, __p2}, 35);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
__ai int64x2_t __noswap_vqdmlsl_n_s32(int64x2_t __p0, int32x2_t __p1, int32_t __p2) {
  int64x2_t __ret;
  __ret = (int64x2_t) __builtin_neon_vqdmlsl_v((int8x16_t)__p0, (int8x8_t)__p1, (int8x8_t)(int32x2_t) {__p2, __p2}, 35);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vqdmlsl_n_s16(int32x4_t __p0, int16x4_t __p1, int16_t __p2) {
  int32x4_t __ret;
  __ret = (int32x4_t) __builtin_neon_vqdmlsl_v((int8x16_t)__p0, (int8x8_t)__p1, (int8x8_t)(int16x4_t) {__p2, __p2, __p2, __p2}, 34);
  return __ret;
}
#else
__ai int32x4_t vqdmlsl_n_s16(int32x4_t __p0, int16x4_t __p1, int16_t __p2) {
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int32x4_t __ret;
  __ret = (int32x4_t) __builtin_neon_vqdmlsl_v((int8x16_t)__rev0, (int8x8_t)__rev1, (int8x8_t)(int16x4_t) {__p2, __p2, __p2, __p2}, 34);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
__ai int32x4_t __noswap_vqdmlsl_n_s16(int32x4_t __p0, int16x4_t __p1, int16_t __p2) {
  int32x4_t __ret;
  __ret = (int32x4_t) __builtin_neon_vqdmlsl_v((int8x16_t)__p0, (int8x8_t)__p1, (int8x8_t)(int16x4_t) {__p2, __p2, __p2, __p2}, 34);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vqdmulhq_s32(int32x4_t __p0, int32x4_t __p1) {
  int32x4_t __ret;
  __ret = (int32x4_t) __builtin_neon_vqdmulhq_v((int8x16_t)__p0, (int8x16_t)__p1, 34);
  return __ret;
}
#else
__ai int32x4_t vqdmulhq_s32(int32x4_t __p0, int32x4_t __p1) {
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int32x4_t __ret;
  __ret = (int32x4_t) __builtin_neon_vqdmulhq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 34);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
__ai int32x4_t __noswap_vqdmulhq_s32(int32x4_t __p0, int32x4_t __p1) {
  int32x4_t __ret;
  __ret = (int32x4_t) __builtin_neon_vqdmulhq_v((int8x16_t)__p0, (int8x16_t)__p1, 34);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x8_t vqdmulhq_s16(int16x8_t __p0, int16x8_t __p1) {
  int16x8_t __ret;
  __ret = (int16x8_t) __builtin_neon_vqdmulhq_v((int8x16_t)__p0, (int8x16_t)__p1, 33);
  return __ret;
}
#else
__ai int16x8_t vqdmulhq_s16(int16x8_t __p0, int16x8_t __p1) {
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __ret;
  __ret = (int16x8_t) __builtin_neon_vqdmulhq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 33);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
__ai int16x8_t __noswap_vqdmulhq_s16(int16x8_t __p0, int16x8_t __p1) {
  int16x8_t __ret;
  __ret = (int16x8_t) __builtin_neon_vqdmulhq_v((int8x16_t)__p0, (int8x16_t)__p1, 33);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x2_t vqdmulh_s32(int32x2_t __p0, int32x2_t __p1) {
  int32x2_t __ret;
  __ret = (int32x2_t) __builtin_neon_vqdmulh_v((int8x8_t)__p0, (int8x8_t)__p1, 2);
  return __ret;
}
#else
__ai int32x2_t vqdmulh_s32(int32x2_t __p0, int32x2_t __p1) {
  int32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  int32x2_t __ret;
  __ret = (int32x2_t) __builtin_neon_vqdmulh_v((int8x8_t)__rev0, (int8x8_t)__rev1, 2);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
__ai int32x2_t __noswap_vqdmulh_s32(int32x2_t __p0, int32x2_t __p1) {
  int32x2_t __ret;
  __ret = (int32x2_t) __builtin_neon_vqdmulh_v((int8x8_t)__p0, (int8x8_t)__p1, 2);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x4_t vqdmulh_s16(int16x4_t __p0, int16x4_t __p1) {
  int16x4_t __ret;
  __ret = (int16x4_t) __builtin_neon_vqdmulh_v((int8x8_t)__p0, (int8x8_t)__p1, 1);
  return __ret;
}
#else
__ai int16x4_t vqdmulh_s16(int16x4_t __p0, int16x4_t __p1) {
  int16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int16x4_t __ret;
  __ret = (int16x4_t) __builtin_neon_vqdmulh_v((int8x8_t)__rev0, (int8x8_t)__rev1, 1);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
__ai int16x4_t __noswap_vqdmulh_s16(int16x4_t __p0, int16x4_t __p1) {
  int16x4_t __ret;
  __ret = (int16x4_t) __builtin_neon_vqdmulh_v((int8x8_t)__p0, (int8x8_t)__p1, 1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
#define vqdmulhq_lane_s32(__p0, __p1, __p2) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  int32x2_t __s1 = __p1; \
  int32x4_t __ret; \
  __ret = vqdmulhq_s32(__s0, __builtin_shufflevector(__s1, __s1, __p2, __p2, __p2, __p2)); \
  __ret; \
})
#else
#define vqdmulhq_lane_s32(__p0, __p1, __p2) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  int32x2_t __s1 = __p1; \
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  int32x2_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 1, 0); \
  int32x4_t __ret; \
  __ret = __noswap_vqdmulhq_s32(__rev0, __builtin_shufflevector(__rev1, __rev1, __p2, __p2, __p2, __p2)); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqdmulhq_lane_s16(__p0, __p1, __p2) __extension__ ({ \
  int16x8_t __s0 = __p0; \
  int16x4_t __s1 = __p1; \
  int16x8_t __ret; \
  __ret = vqdmulhq_s16(__s0, __builtin_shufflevector(__s1, __s1, __p2, __p2, __p2, __p2, __p2, __p2, __p2, __p2)); \
  __ret; \
})
#else
#define vqdmulhq_lane_s16(__p0, __p1, __p2) __extension__ ({ \
  int16x8_t __s0 = __p0; \
  int16x4_t __s1 = __p1; \
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 7, 6, 5, 4, 3, 2, 1, 0); \
  int16x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  int16x8_t __ret; \
  __ret = __noswap_vqdmulhq_s16(__rev0, __builtin_shufflevector(__rev1, __rev1, __p2, __p2, __p2, __p2, __p2, __p2, __p2, __p2)); \
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqdmulh_lane_s32(__p0, __p1, __p2) __extension__ ({ \
  int32x2_t __s0 = __p0; \
  int32x2_t __s1 = __p1; \
  int32x2_t __ret; \
  __ret = vqdmulh_s32(__s0, __builtin_shufflevector(__s1, __s1, __p2, __p2)); \
  __ret; \
})
#else
#define vqdmulh_lane_s32(__p0, __p1, __p2) __extension__ ({ \
  int32x2_t __s0 = __p0; \
  int32x2_t __s1 = __p1; \
  int32x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  int32x2_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 1, 0); \
  int32x2_t __ret; \
  __ret = __noswap_vqdmulh_s32(__rev0, __builtin_shufflevector(__rev1, __rev1, __p2, __p2)); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqdmulh_lane_s16(__p0, __p1, __p2) __extension__ ({ \
  int16x4_t __s0 = __p0; \
  int16x4_t __s1 = __p1; \
  int16x4_t __ret; \
  __ret = vqdmulh_s16(__s0, __builtin_shufflevector(__s1, __s1, __p2, __p2, __p2, __p2)); \
  __ret; \
})
#else
#define vqdmulh_lane_s16(__p0, __p1, __p2) __extension__ ({ \
  int16x4_t __s0 = __p0; \
  int16x4_t __s1 = __p1; \
  int16x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  int16x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  int16x4_t __ret; \
  __ret = __noswap_vqdmulh_s16(__rev0, __builtin_shufflevector(__rev1, __rev1, __p2, __p2, __p2, __p2)); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vqdmulhq_n_s32(int32x4_t __p0, int32_t __p1) {
  int32x4_t __ret;
  __ret = (int32x4_t) __builtin_neon_vqdmulhq_v((int8x16_t)__p0, (int8x16_t)(int32x4_t) {__p1, __p1, __p1, __p1}, 34);
  return __ret;
}
#else
__ai int32x4_t vqdmulhq_n_s32(int32x4_t __p0, int32_t __p1) {
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int32x4_t __ret;
  __ret = (int32x4_t) __builtin_neon_vqdmulhq_v((int8x16_t)__rev0, (int8x16_t)(int32x4_t) {__p1, __p1, __p1, __p1}, 34);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x8_t vqdmulhq_n_s16(int16x8_t __p0, int16_t __p1) {
  int16x8_t __ret;
  __ret = (int16x8_t) __builtin_neon_vqdmulhq_v((int8x16_t)__p0, (int8x16_t)(int16x8_t) {__p1, __p1, __p1, __p1, __p1, __p1, __p1, __p1}, 33);
  return __ret;
}
#else
__ai int16x8_t vqdmulhq_n_s16(int16x8_t __p0, int16_t __p1) {
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __ret;
  __ret = (int16x8_t) __builtin_neon_vqdmulhq_v((int8x16_t)__rev0, (int8x16_t)(int16x8_t) {__p1, __p1, __p1, __p1, __p1, __p1, __p1, __p1}, 33);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x2_t vqdmulh_n_s32(int32x2_t __p0, int32_t __p1) {
  int32x2_t __ret;
  __ret = (int32x2_t) __builtin_neon_vqdmulh_v((int8x8_t)__p0, (int8x8_t)(int32x2_t) {__p1, __p1}, 2);
  return __ret;
}
#else
__ai int32x2_t vqdmulh_n_s32(int32x2_t __p0, int32_t __p1) {
  int32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int32x2_t __ret;
  __ret = (int32x2_t) __builtin_neon_vqdmulh_v((int8x8_t)__rev0, (int8x8_t)(int32x2_t) {__p1, __p1}, 2);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x4_t vqdmulh_n_s16(int16x4_t __p0, int16_t __p1) {
  int16x4_t __ret;
  __ret = (int16x4_t) __builtin_neon_vqdmulh_v((int8x8_t)__p0, (int8x8_t)(int16x4_t) {__p1, __p1, __p1, __p1}, 1);
  return __ret;
}
#else
__ai int16x4_t vqdmulh_n_s16(int16x4_t __p0, int16_t __p1) {
  int16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int16x4_t __ret;
  __ret = (int16x4_t) __builtin_neon_vqdmulh_v((int8x8_t)__rev0, (int8x8_t)(int16x4_t) {__p1, __p1, __p1, __p1}, 1);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x2_t vqdmull_s32(int32x2_t __p0, int32x2_t __p1) {
  int64x2_t __ret;
  __ret = (int64x2_t) __builtin_neon_vqdmull_v((int8x8_t)__p0, (int8x8_t)__p1, 35);
  return __ret;
}
#else
__ai int64x2_t vqdmull_s32(int32x2_t __p0, int32x2_t __p1) {
  int32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  int64x2_t __ret;
  __ret = (int64x2_t) __builtin_neon_vqdmull_v((int8x8_t)__rev0, (int8x8_t)__rev1, 35);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
__ai int64x2_t __noswap_vqdmull_s32(int32x2_t __p0, int32x2_t __p1) {
  int64x2_t __ret;
  __ret = (int64x2_t) __builtin_neon_vqdmull_v((int8x8_t)__p0, (int8x8_t)__p1, 35);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vqdmull_s16(int16x4_t __p0, int16x4_t __p1) {
  int32x4_t __ret;
  __ret = (int32x4_t) __builtin_neon_vqdmull_v((int8x8_t)__p0, (int8x8_t)__p1, 34);
  return __ret;
}
#else
__ai int32x4_t vqdmull_s16(int16x4_t __p0, int16x4_t __p1) {
  int16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int32x4_t __ret;
  __ret = (int32x4_t) __builtin_neon_vqdmull_v((int8x8_t)__rev0, (int8x8_t)__rev1, 34);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
__ai int32x4_t __noswap_vqdmull_s16(int16x4_t __p0, int16x4_t __p1) {
  int32x4_t __ret;
  __ret = (int32x4_t) __builtin_neon_vqdmull_v((int8x8_t)__p0, (int8x8_t)__p1, 34);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
#define vqdmull_lane_s32(__p0, __p1, __p2) __extension__ ({ \
  int32x2_t __s0 = __p0; \
  int32x2_t __s1 = __p1; \
  int64x2_t __ret; \
  __ret = vqdmull_s32(__s0, __builtin_shufflevector(__s1, __s1, __p2, __p2)); \
  __ret; \
})
#else
#define vqdmull_lane_s32(__p0, __p1, __p2) __extension__ ({ \
  int32x2_t __s0 = __p0; \
  int32x2_t __s1 = __p1; \
  int32x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  int32x2_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 1, 0); \
  int64x2_t __ret; \
  __ret = __noswap_vqdmull_s32(__rev0, __builtin_shufflevector(__rev1, __rev1, __p2, __p2)); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqdmull_lane_s16(__p0, __p1, __p2) __extension__ ({ \
  int16x4_t __s0 = __p0; \
  int16x4_t __s1 = __p1; \
  int32x4_t __ret; \
  __ret = vqdmull_s16(__s0, __builtin_shufflevector(__s1, __s1, __p2, __p2, __p2, __p2)); \
  __ret; \
})
#else
#define vqdmull_lane_s16(__p0, __p1, __p2) __extension__ ({ \
  int16x4_t __s0 = __p0; \
  int16x4_t __s1 = __p1; \
  int16x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  int16x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  int32x4_t __ret; \
  __ret = __noswap_vqdmull_s16(__rev0, __builtin_shufflevector(__rev1, __rev1, __p2, __p2, __p2, __p2)); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x2_t vqdmull_n_s32(int32x2_t __p0, int32_t __p1) {
  int64x2_t __ret;
  __ret = (int64x2_t) __builtin_neon_vqdmull_v((int8x8_t)__p0, (int8x8_t)(int32x2_t) {__p1, __p1}, 35);
  return __ret;
}
#else
__ai int64x2_t vqdmull_n_s32(int32x2_t __p0, int32_t __p1) {
  int32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int64x2_t __ret;
  __ret = (int64x2_t) __builtin_neon_vqdmull_v((int8x8_t)__rev0, (int8x8_t)(int32x2_t) {__p1, __p1}, 35);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
__ai int64x2_t __noswap_vqdmull_n_s32(int32x2_t __p0, int32_t __p1) {
  int64x2_t __ret;
  __ret = (int64x2_t) __builtin_neon_vqdmull_v((int8x8_t)__p0, (int8x8_t)(int32x2_t) {__p1, __p1}, 35);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vqdmull_n_s16(int16x4_t __p0, int16_t __p1) {
  int32x4_t __ret;
  __ret = (int32x4_t) __builtin_neon_vqdmull_v((int8x8_t)__p0, (int8x8_t)(int16x4_t) {__p1, __p1, __p1, __p1}, 34);
  return __ret;
}
#else
__ai int32x4_t vqdmull_n_s16(int16x4_t __p0, int16_t __p1) {
  int16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int32x4_t __ret;
  __ret = (int32x4_t) __builtin_neon_vqdmull_v((int8x8_t)__rev0, (int8x8_t)(int16x4_t) {__p1, __p1, __p1, __p1}, 34);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
__ai int32x4_t __noswap_vqdmull_n_s16(int16x4_t __p0, int16_t __p1) {
  int32x4_t __ret;
  __ret = (int32x4_t) __builtin_neon_vqdmull_v((int8x8_t)__p0, (int8x8_t)(int16x4_t) {__p1, __p1, __p1, __p1}, 34);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x4_t vqmovn_u32(uint32x4_t __p0) {
  uint16x4_t __ret;
  __ret = (uint16x4_t) __builtin_neon_vqmovn_v((int8x16_t)__p0, 17);
  return __ret;
}
#else
__ai uint16x4_t vqmovn_u32(uint32x4_t __p0) {
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint16x4_t __ret;
  __ret = (uint16x4_t) __builtin_neon_vqmovn_v((int8x16_t)__rev0, 17);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
__ai uint16x4_t __noswap_vqmovn_u32(uint32x4_t __p0) {
  uint16x4_t __ret;
  __ret = (uint16x4_t) __builtin_neon_vqmovn_v((int8x16_t)__p0, 17);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x2_t vqmovn_u64(uint64x2_t __p0) {
  uint32x2_t __ret;
  __ret = (uint32x2_t) __builtin_neon_vqmovn_v((int8x16_t)__p0, 18);
  return __ret;
}
#else
__ai uint32x2_t vqmovn_u64(uint64x2_t __p0) {
  uint64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint32x2_t __ret;
  __ret = (uint32x2_t) __builtin_neon_vqmovn_v((int8x16_t)__rev0, 18);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
__ai uint32x2_t __noswap_vqmovn_u64(uint64x2_t __p0) {
  uint32x2_t __ret;
  __ret = (uint32x2_t) __builtin_neon_vqmovn_v((int8x16_t)__p0, 18);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x8_t vqmovn_u16(uint16x8_t __p0) {
  uint8x8_t __ret;
  __ret = (uint8x8_t) __builtin_neon_vqmovn_v((int8x16_t)__p0, 16);
  return __ret;
}
#else
__ai uint8x8_t vqmovn_u16(uint16x8_t __p0) {
  uint16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __ret;
  __ret = (uint8x8_t) __builtin_neon_vqmovn_v((int8x16_t)__rev0, 16);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
__ai uint8x8_t __noswap_vqmovn_u16(uint16x8_t __p0) {
  uint8x8_t __ret;
  __ret = (uint8x8_t) __builtin_neon_vqmovn_v((int8x16_t)__p0, 16);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x4_t vqmovn_s32(int32x4_t __p0) {
  int16x4_t __ret;
  __ret = (int16x4_t) __builtin_neon_vqmovn_v((int8x16_t)__p0, 1);
  return __ret;
}
#else
__ai int16x4_t vqmovn_s32(int32x4_t __p0) {
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int16x4_t __ret;
  __ret = (int16x4_t) __builtin_neon_vqmovn_v((int8x16_t)__rev0, 1);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
__ai int16x4_t __noswap_vqmovn_s32(int32x4_t __p0) {
  int16x4_t __ret;
  __ret = (int16x4_t) __builtin_neon_vqmovn_v((int8x16_t)__p0, 1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x2_t vqmovn_s64(int64x2_t __p0) {
  int32x2_t __ret;
  __ret = (int32x2_t) __builtin_neon_vqmovn_v((int8x16_t)__p0, 2);
  return __ret;
}
#else
__ai int32x2_t vqmovn_s64(int64x2_t __p0) {
  int64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int32x2_t __ret;
  __ret = (int32x2_t) __builtin_neon_vqmovn_v((int8x16_t)__rev0, 2);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
__ai int32x2_t __noswap_vqmovn_s64(int64x2_t __p0) {
  int32x2_t __ret;
  __ret = (int32x2_t) __builtin_neon_vqmovn_v((int8x16_t)__p0, 2);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x8_t vqmovn_s16(int16x8_t __p0) {
  int8x8_t __ret;
  __ret = (int8x8_t) __builtin_neon_vqmovn_v((int8x16_t)__p0, 0);
  return __ret;
}
#else
__ai int8x8_t vqmovn_s16(int16x8_t __p0) {
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __ret;
  __ret = (int8x8_t) __builtin_neon_vqmovn_v((int8x16_t)__rev0, 0);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
__ai int8x8_t __noswap_vqmovn_s16(int16x8_t __p0) {
  int8x8_t __ret;
  __ret = (int8x8_t) __builtin_neon_vqmovn_v((int8x16_t)__p0, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x4_t vqmovun_s32(int32x4_t __p0) {
  uint16x4_t __ret;
  __ret = (uint16x4_t) __builtin_neon_vqmovun_v((int8x16_t)__p0, 17);
  return __ret;
}
#else
__ai uint16x4_t vqmovun_s32(int32x4_t __p0) {
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint16x4_t __ret;
  __ret = (uint16x4_t) __builtin_neon_vqmovun_v((int8x16_t)__rev0, 17);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
__ai uint16x4_t __noswap_vqmovun_s32(int32x4_t __p0) {
  uint16x4_t __ret;
  __ret = (uint16x4_t) __builtin_neon_vqmovun_v((int8x16_t)__p0, 17);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x2_t vqmovun_s64(int64x2_t __p0) {
  uint32x2_t __ret;
  __ret = (uint32x2_t) __builtin_neon_vqmovun_v((int8x16_t)__p0, 18);
  return __ret;
}
#else
__ai uint32x2_t vqmovun_s64(int64x2_t __p0) {
  int64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint32x2_t __ret;
  __ret = (uint32x2_t) __builtin_neon_vqmovun_v((int8x16_t)__rev0, 18);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
__ai uint32x2_t __noswap_vqmovun_s64(int64x2_t __p0) {
  uint32x2_t __ret;
  __ret = (uint32x2_t) __builtin_neon_vqmovun_v((int8x16_t)__p0, 18);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x8_t vqmovun_s16(int16x8_t __p0) {
  uint8x8_t __ret;
  __ret = (uint8x8_t) __builtin_neon_vqmovun_v((int8x16_t)__p0, 16);
  return __ret;
}
#else
__ai uint8x8_t vqmovun_s16(int16x8_t __p0) {
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __ret;
  __ret = (uint8x8_t) __builtin_neon_vqmovun_v((int8x16_t)__rev0, 16);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
__ai uint8x8_t __noswap_vqmovun_s16(int16x8_t __p0) {
  uint8x8_t __ret;
  __ret = (uint8x8_t) __builtin_neon_vqmovun_v((int8x16_t)__p0, 16);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x16_t vqnegq_s8(int8x16_t __p0) {
  int8x16_t __ret;
  __ret = (int8x16_t) __builtin_neon_vqnegq_v((int8x16_t)__p0, 32);
  return __ret;
}
#else
__ai int8x16_t vqnegq_s8(int8x16_t __p0) {
  int8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __ret;
  __ret = (int8x16_t) __builtin_neon_vqnegq_v((int8x16_t)__rev0, 32);
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vqnegq_s32(int32x4_t __p0) {
  int32x4_t __ret;
  __ret = (int32x4_t) __builtin_neon_vqnegq_v((int8x16_t)__p0, 34);
  return __ret;
}
#else
__ai int32x4_t vqnegq_s32(int32x4_t __p0) {
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int32x4_t __ret;
  __ret = (int32x4_t) __builtin_neon_vqnegq_v((int8x16_t)__rev0, 34);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x8_t vqnegq_s16(int16x8_t __p0) {
  int16x8_t __ret;
  __ret = (int16x8_t) __builtin_neon_vqnegq_v((int8x16_t)__p0, 33);
  return __ret;
}
#else
__ai int16x8_t vqnegq_s16(int16x8_t __p0) {
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __ret;
  __ret = (int16x8_t) __builtin_neon_vqnegq_v((int8x16_t)__rev0, 33);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x8_t vqneg_s8(int8x8_t __p0) {
  int8x8_t __ret;
  __ret = (int8x8_t) __builtin_neon_vqneg_v((int8x8_t)__p0, 0);
  return __ret;
}
#else
__ai int8x8_t vqneg_s8(int8x8_t __p0) {
  int8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __ret;
  __ret = (int8x8_t) __builtin_neon_vqneg_v((int8x8_t)__rev0, 0);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x2_t vqneg_s32(int32x2_t __p0) {
  int32x2_t __ret;
  __ret = (int32x2_t) __builtin_neon_vqneg_v((int8x8_t)__p0, 2);
  return __ret;
}
#else
__ai int32x2_t vqneg_s32(int32x2_t __p0) {
  int32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int32x2_t __ret;
  __ret = (int32x2_t) __builtin_neon_vqneg_v((int8x8_t)__rev0, 2);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x4_t vqneg_s16(int16x4_t __p0) {
  int16x4_t __ret;
  __ret = (int16x4_t) __builtin_neon_vqneg_v((int8x8_t)__p0, 1);
  return __ret;
}
#else
__ai int16x4_t vqneg_s16(int16x4_t __p0) {
  int16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int16x4_t __ret;
  __ret = (int16x4_t) __builtin_neon_vqneg_v((int8x8_t)__rev0, 1);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vqrdmulhq_s32(int32x4_t __p0, int32x4_t __p1) {
  int32x4_t __ret;
  __ret = (int32x4_t) __builtin_neon_vqrdmulhq_v((int8x16_t)__p0, (int8x16_t)__p1, 34);
  return __ret;
}
#else
__ai int32x4_t vqrdmulhq_s32(int32x4_t __p0, int32x4_t __p1) {
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int32x4_t __ret;
  __ret = (int32x4_t) __builtin_neon_vqrdmulhq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 34);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
__ai int32x4_t __noswap_vqrdmulhq_s32(int32x4_t __p0, int32x4_t __p1) {
  int32x4_t __ret;
  __ret = (int32x4_t) __builtin_neon_vqrdmulhq_v((int8x16_t)__p0, (int8x16_t)__p1, 34);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x8_t vqrdmulhq_s16(int16x8_t __p0, int16x8_t __p1) {
  int16x8_t __ret;
  __ret = (int16x8_t) __builtin_neon_vqrdmulhq_v((int8x16_t)__p0, (int8x16_t)__p1, 33);
  return __ret;
}
#else
__ai int16x8_t vqrdmulhq_s16(int16x8_t __p0, int16x8_t __p1) {
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __ret;
  __ret = (int16x8_t) __builtin_neon_vqrdmulhq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 33);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
__ai int16x8_t __noswap_vqrdmulhq_s16(int16x8_t __p0, int16x8_t __p1) {
  int16x8_t __ret;
  __ret = (int16x8_t) __builtin_neon_vqrdmulhq_v((int8x16_t)__p0, (int8x16_t)__p1, 33);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x2_t vqrdmulh_s32(int32x2_t __p0, int32x2_t __p1) {
  int32x2_t __ret;
  __ret = (int32x2_t) __builtin_neon_vqrdmulh_v((int8x8_t)__p0, (int8x8_t)__p1, 2);
  return __ret;
}
#else
__ai int32x2_t vqrdmulh_s32(int32x2_t __p0, int32x2_t __p1) {
  int32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  int32x2_t __ret;
  __ret = (int32x2_t) __builtin_neon_vqrdmulh_v((int8x8_t)__rev0, (int8x8_t)__rev1, 2);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
__ai int32x2_t __noswap_vqrdmulh_s32(int32x2_t __p0, int32x2_t __p1) {
  int32x2_t __ret;
  __ret = (int32x2_t) __builtin_neon_vqrdmulh_v((int8x8_t)__p0, (int8x8_t)__p1, 2);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x4_t vqrdmulh_s16(int16x4_t __p0, int16x4_t __p1) {
  int16x4_t __ret;
  __ret = (int16x4_t) __builtin_neon_vqrdmulh_v((int8x8_t)__p0, (int8x8_t)__p1, 1);
  return __ret;
}
#else
__ai int16x4_t vqrdmulh_s16(int16x4_t __p0, int16x4_t __p1) {
  int16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int16x4_t __ret;
  __ret = (int16x4_t) __builtin_neon_vqrdmulh_v((int8x8_t)__rev0, (int8x8_t)__rev1, 1);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
__ai int16x4_t __noswap_vqrdmulh_s16(int16x4_t __p0, int16x4_t __p1) {
  int16x4_t __ret;
  __ret = (int16x4_t) __builtin_neon_vqrdmulh_v((int8x8_t)__p0, (int8x8_t)__p1, 1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
#define vqrdmulhq_lane_s32(__p0, __p1, __p2) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  int32x2_t __s1 = __p1; \
  int32x4_t __ret; \
  __ret = vqrdmulhq_s32(__s0, __builtin_shufflevector(__s1, __s1, __p2, __p2, __p2, __p2)); \
  __ret; \
})
#else
#define vqrdmulhq_lane_s32(__p0, __p1, __p2) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  int32x2_t __s1 = __p1; \
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  int32x2_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 1, 0); \
  int32x4_t __ret; \
  __ret = __noswap_vqrdmulhq_s32(__rev0, __builtin_shufflevector(__rev1, __rev1, __p2, __p2, __p2, __p2)); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqrdmulhq_lane_s16(__p0, __p1, __p2) __extension__ ({ \
  int16x8_t __s0 = __p0; \
  int16x4_t __s1 = __p1; \
  int16x8_t __ret; \
  __ret = vqrdmulhq_s16(__s0, __builtin_shufflevector(__s1, __s1, __p2, __p2, __p2, __p2, __p2, __p2, __p2, __p2)); \
  __ret; \
})
#else
#define vqrdmulhq_lane_s16(__p0, __p1, __p2) __extension__ ({ \
  int16x8_t __s0 = __p0; \
  int16x4_t __s1 = __p1; \
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 7, 6, 5, 4, 3, 2, 1, 0); \
  int16x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  int16x8_t __ret; \
  __ret = __noswap_vqrdmulhq_s16(__rev0, __builtin_shufflevector(__rev1, __rev1, __p2, __p2, __p2, __p2, __p2, __p2, __p2, __p2)); \
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqrdmulh_lane_s32(__p0, __p1, __p2) __extension__ ({ \
  int32x2_t __s0 = __p0; \
  int32x2_t __s1 = __p1; \
  int32x2_t __ret; \
  __ret = vqrdmulh_s32(__s0, __builtin_shufflevector(__s1, __s1, __p2, __p2)); \
  __ret; \
})
#else
#define vqrdmulh_lane_s32(__p0, __p1, __p2) __extension__ ({ \
  int32x2_t __s0 = __p0; \
  int32x2_t __s1 = __p1; \
  int32x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  int32x2_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 1, 0); \
  int32x2_t __ret; \
  __ret = __noswap_vqrdmulh_s32(__rev0, __builtin_shufflevector(__rev1, __rev1, __p2, __p2)); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqrdmulh_lane_s16(__p0, __p1, __p2) __extension__ ({ \
  int16x4_t __s0 = __p0; \
  int16x4_t __s1 = __p1; \
  int16x4_t __ret; \
  __ret = vqrdmulh_s16(__s0, __builtin_shufflevector(__s1, __s1, __p2, __p2, __p2, __p2)); \
  __ret; \
})
#else
#define vqrdmulh_lane_s16(__p0, __p1, __p2) __extension__ ({ \
  int16x4_t __s0 = __p0; \
  int16x4_t __s1 = __p1; \
  int16x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  int16x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  int16x4_t __ret; \
  __ret = __noswap_vqrdmulh_s16(__rev0, __builtin_shufflevector(__rev1, __rev1, __p2, __p2, __p2, __p2)); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vqrdmulhq_n_s32(int32x4_t __p0, int32_t __p1) {
  int32x4_t __ret;
  __ret = (int32x4_t) __builtin_neon_vqrdmulhq_v((int8x16_t)__p0, (int8x16_t)(int32x4_t) {__p1, __p1, __p1, __p1}, 34);
  return __ret;
}
#else
__ai int32x4_t vqrdmulhq_n_s32(int32x4_t __p0, int32_t __p1) {
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int32x4_t __ret;
  __ret = (int32x4_t) __builtin_neon_vqrdmulhq_v((int8x16_t)__rev0, (int8x16_t)(int32x4_t) {__p1, __p1, __p1, __p1}, 34);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x8_t vqrdmulhq_n_s16(int16x8_t __p0, int16_t __p1) {
  int16x8_t __ret;
  __ret = (int16x8_t) __builtin_neon_vqrdmulhq_v((int8x16_t)__p0, (int8x16_t)(int16x8_t) {__p1, __p1, __p1, __p1, __p1, __p1, __p1, __p1}, 33);
  return __ret;
}
#else
__ai int16x8_t vqrdmulhq_n_s16(int16x8_t __p0, int16_t __p1) {
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __ret;
  __ret = (int16x8_t) __builtin_neon_vqrdmulhq_v((int8x16_t)__rev0, (int8x16_t)(int16x8_t) {__p1, __p1, __p1, __p1, __p1, __p1, __p1, __p1}, 33);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x2_t vqrdmulh_n_s32(int32x2_t __p0, int32_t __p1) {
  int32x2_t __ret;
  __ret = (int32x2_t) __builtin_neon_vqrdmulh_v((int8x8_t)__p0, (int8x8_t)(int32x2_t) {__p1, __p1}, 2);
  return __ret;
}
#else
__ai int32x2_t vqrdmulh_n_s32(int32x2_t __p0, int32_t __p1) {
  int32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int32x2_t __ret;
  __ret = (int32x2_t) __builtin_neon_vqrdmulh_v((int8x8_t)__rev0, (int8x8_t)(int32x2_t) {__p1, __p1}, 2);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x4_t vqrdmulh_n_s16(int16x4_t __p0, int16_t __p1) {
  int16x4_t __ret;
  __ret = (int16x4_t) __builtin_neon_vqrdmulh_v((int8x8_t)__p0, (int8x8_t)(int16x4_t) {__p1, __p1, __p1, __p1}, 1);
  return __ret;
}
#else
__ai int16x4_t vqrdmulh_n_s16(int16x4_t __p0, int16_t __p1) {
  int16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int16x4_t __ret;
  __ret = (int16x4_t) __builtin_neon_vqrdmulh_v((int8x8_t)__rev0, (int8x8_t)(int16x4_t) {__p1, __p1, __p1, __p1}, 1);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x16_t vqrshlq_u8(uint8x16_t __p0, int8x16_t __p1) {
  uint8x16_t __ret;
  __ret = (uint8x16_t) __builtin_neon_vqrshlq_v((int8x16_t)__p0, (int8x16_t)__p1, 48);
  return __ret;
}
#else
__ai uint8x16_t vqrshlq_u8(uint8x16_t __p0, int8x16_t __p1) {
  uint8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __ret;
  __ret = (uint8x16_t) __builtin_neon_vqrshlq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 48);
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vqrshlq_u32(uint32x4_t __p0, int32x4_t __p1) {
  uint32x4_t __ret;
  __ret = (uint32x4_t) __builtin_neon_vqrshlq_v((int8x16_t)__p0, (int8x16_t)__p1, 50);
  return __ret;
}
#else
__ai uint32x4_t vqrshlq_u32(uint32x4_t __p0, int32x4_t __p1) {
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint32x4_t __ret;
  __ret = (uint32x4_t) __builtin_neon_vqrshlq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 50);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vqrshlq_u64(uint64x2_t __p0, int64x2_t __p1) {
  uint64x2_t __ret;
  __ret = (uint64x2_t) __builtin_neon_vqrshlq_v((int8x16_t)__p0, (int8x16_t)__p1, 51);
  return __ret;
}
#else
__ai uint64x2_t vqrshlq_u64(uint64x2_t __p0, int64x2_t __p1) {
  uint64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  uint64x2_t __ret;
  __ret = (uint64x2_t) __builtin_neon_vqrshlq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 51);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x8_t vqrshlq_u16(uint16x8_t __p0, int16x8_t __p1) {
  uint16x8_t __ret;
  __ret = (uint16x8_t) __builtin_neon_vqrshlq_v((int8x16_t)__p0, (int8x16_t)__p1, 49);
  return __ret;
}
#else
__ai uint16x8_t vqrshlq_u16(uint16x8_t __p0, int16x8_t __p1) {
  uint16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __ret;
  __ret = (uint16x8_t) __builtin_neon_vqrshlq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 49);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x16_t vqrshlq_s8(int8x16_t __p0, int8x16_t __p1) {
  int8x16_t __ret;
  __ret = (int8x16_t) __builtin_neon_vqrshlq_v((int8x16_t)__p0, (int8x16_t)__p1, 32);
  return __ret;
}
#else
__ai int8x16_t vqrshlq_s8(int8x16_t __p0, int8x16_t __p1) {
  int8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __ret;
  __ret = (int8x16_t) __builtin_neon_vqrshlq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 32);
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vqrshlq_s32(int32x4_t __p0, int32x4_t __p1) {
  int32x4_t __ret;
  __ret = (int32x4_t) __builtin_neon_vqrshlq_v((int8x16_t)__p0, (int8x16_t)__p1, 34);
  return __ret;
}
#else
__ai int32x4_t vqrshlq_s32(int32x4_t __p0, int32x4_t __p1) {
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int32x4_t __ret;
  __ret = (int32x4_t) __builtin_neon_vqrshlq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 34);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x2_t vqrshlq_s64(int64x2_t __p0, int64x2_t __p1) {
  int64x2_t __ret;
  __ret = (int64x2_t) __builtin_neon_vqrshlq_v((int8x16_t)__p0, (int8x16_t)__p1, 35);
  return __ret;
}
#else
__ai int64x2_t vqrshlq_s64(int64x2_t __p0, int64x2_t __p1) {
  int64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  int64x2_t __ret;
  __ret = (int64x2_t) __builtin_neon_vqrshlq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 35);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x8_t vqrshlq_s16(int16x8_t __p0, int16x8_t __p1) {
  int16x8_t __ret;
  __ret = (int16x8_t) __builtin_neon_vqrshlq_v((int8x16_t)__p0, (int8x16_t)__p1, 33);
  return __ret;
}
#else
__ai int16x8_t vqrshlq_s16(int16x8_t __p0, int16x8_t __p1) {
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __ret;
  __ret = (int16x8_t) __builtin_neon_vqrshlq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 33);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x8_t vqrshl_u8(uint8x8_t __p0, int8x8_t __p1) {
  uint8x8_t __ret;
  __ret = (uint8x8_t) __builtin_neon_vqrshl_v((int8x8_t)__p0, (int8x8_t)__p1, 16);
  return __ret;
}
#else
__ai uint8x8_t vqrshl_u8(uint8x8_t __p0, int8x8_t __p1) {
  uint8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __ret;
  __ret = (uint8x8_t) __builtin_neon_vqrshl_v((int8x8_t)__rev0, (int8x8_t)__rev1, 16);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x2_t vqrshl_u32(uint32x2_t __p0, int32x2_t __p1) {
  uint32x2_t __ret;
  __ret = (uint32x2_t) __builtin_neon_vqrshl_v((int8x8_t)__p0, (int8x8_t)__p1, 18);
  return __ret;
}
#else
__ai uint32x2_t vqrshl_u32(uint32x2_t __p0, int32x2_t __p1) {
  uint32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  uint32x2_t __ret;
  __ret = (uint32x2_t) __builtin_neon_vqrshl_v((int8x8_t)__rev0, (int8x8_t)__rev1, 18);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x1_t vqrshl_u64(uint64x1_t __p0, int64x1_t __p1) {
  uint64x1_t __ret;
  __ret = (uint64x1_t) __builtin_neon_vqrshl_v((int8x8_t)__p0, (int8x8_t)__p1, 19);
  return __ret;
}
#else
__ai uint64x1_t vqrshl_u64(uint64x1_t __p0, int64x1_t __p1) {
  uint64x1_t __ret;
  __ret = (uint64x1_t) __builtin_neon_vqrshl_v((int8x8_t)__p0, (int8x8_t)__p1, 19);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x4_t vqrshl_u16(uint16x4_t __p0, int16x4_t __p1) {
  uint16x4_t __ret;
  __ret = (uint16x4_t) __builtin_neon_vqrshl_v((int8x8_t)__p0, (int8x8_t)__p1, 17);
  return __ret;
}
#else
__ai uint16x4_t vqrshl_u16(uint16x4_t __p0, int16x4_t __p1) {
  uint16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint16x4_t __ret;
  __ret = (uint16x4_t) __builtin_neon_vqrshl_v((int8x8_t)__rev0, (int8x8_t)__rev1, 17);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x8_t vqrshl_s8(int8x8_t __p0, int8x8_t __p1) {
  int8x8_t __ret;
  __ret = (int8x8_t) __builtin_neon_vqrshl_v((int8x8_t)__p0, (int8x8_t)__p1, 0);
  return __ret;
}
#else
__ai int8x8_t vqrshl_s8(int8x8_t __p0, int8x8_t __p1) {
  int8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __ret;
  __ret = (int8x8_t) __builtin_neon_vqrshl_v((int8x8_t)__rev0, (int8x8_t)__rev1, 0);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x2_t vqrshl_s32(int32x2_t __p0, int32x2_t __p1) {
  int32x2_t __ret;
  __ret = (int32x2_t) __builtin_neon_vqrshl_v((int8x8_t)__p0, (int8x8_t)__p1, 2);
  return __ret;
}
#else
__ai int32x2_t vqrshl_s32(int32x2_t __p0, int32x2_t __p1) {
  int32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  int32x2_t __ret;
  __ret = (int32x2_t) __builtin_neon_vqrshl_v((int8x8_t)__rev0, (int8x8_t)__rev1, 2);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x1_t vqrshl_s64(int64x1_t __p0, int64x1_t __p1) {
  int64x1_t __ret;
  __ret = (int64x1_t) __builtin_neon_vqrshl_v((int8x8_t)__p0, (int8x8_t)__p1, 3);
  return __ret;
}
#else
__ai int64x1_t vqrshl_s64(int64x1_t __p0, int64x1_t __p1) {
  int64x1_t __ret;
  __ret = (int64x1_t) __builtin_neon_vqrshl_v((int8x8_t)__p0, (int8x8_t)__p1, 3);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x4_t vqrshl_s16(int16x4_t __p0, int16x4_t __p1) {
  int16x4_t __ret;
  __ret = (int16x4_t) __builtin_neon_vqrshl_v((int8x8_t)__p0, (int8x8_t)__p1, 1);
  return __ret;
}
#else
__ai int16x4_t vqrshl_s16(int16x4_t __p0, int16x4_t __p1) {
  int16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int16x4_t __ret;
  __ret = (int16x4_t) __builtin_neon_vqrshl_v((int8x8_t)__rev0, (int8x8_t)__rev1, 1);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
#define vqrshrn_n_u32(__p0, __p1) __extension__ ({ \
  uint32x4_t __s0 = __p0; \
  uint16x4_t __ret; \
  __ret = (uint16x4_t) __builtin_neon_vqrshrn_n_v((int8x16_t)__s0, __p1, 17); \
  __ret; \
})
#else
#define vqrshrn_n_u32(__p0, __p1) __extension__ ({ \
  uint32x4_t __s0 = __p0; \
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  uint16x4_t __ret; \
  __ret = (uint16x4_t) __builtin_neon_vqrshrn_n_v((int8x16_t)__rev0, __p1, 17); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#define __noswap_vqrshrn_n_u32(__p0, __p1) __extension__ ({ \
  uint32x4_t __s0 = __p0; \
  uint16x4_t __ret; \
  __ret = (uint16x4_t) __builtin_neon_vqrshrn_n_v((int8x16_t)__s0, __p1, 17); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqrshrn_n_u64(__p0, __p1) __extension__ ({ \
  uint64x2_t __s0 = __p0; \
  uint32x2_t __ret; \
  __ret = (uint32x2_t) __builtin_neon_vqrshrn_n_v((int8x16_t)__s0, __p1, 18); \
  __ret; \
})
#else
#define vqrshrn_n_u64(__p0, __p1) __extension__ ({ \
  uint64x2_t __s0 = __p0; \
  uint64x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  uint32x2_t __ret; \
  __ret = (uint32x2_t) __builtin_neon_vqrshrn_n_v((int8x16_t)__rev0, __p1, 18); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#define __noswap_vqrshrn_n_u64(__p0, __p1) __extension__ ({ \
  uint64x2_t __s0 = __p0; \
  uint32x2_t __ret; \
  __ret = (uint32x2_t) __builtin_neon_vqrshrn_n_v((int8x16_t)__s0, __p1, 18); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqrshrn_n_u16(__p0, __p1) __extension__ ({ \
  uint16x8_t __s0 = __p0; \
  uint8x8_t __ret; \
  __ret = (uint8x8_t) __builtin_neon_vqrshrn_n_v((int8x16_t)__s0, __p1, 16); \
  __ret; \
})
#else
#define vqrshrn_n_u16(__p0, __p1) __extension__ ({ \
  uint16x8_t __s0 = __p0; \
  uint16x8_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint8x8_t __ret; \
  __ret = (uint8x8_t) __builtin_neon_vqrshrn_n_v((int8x16_t)__rev0, __p1, 16); \
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#define __noswap_vqrshrn_n_u16(__p0, __p1) __extension__ ({ \
  uint16x8_t __s0 = __p0; \
  uint8x8_t __ret; \
  __ret = (uint8x8_t) __builtin_neon_vqrshrn_n_v((int8x16_t)__s0, __p1, 16); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqrshrn_n_s32(__p0, __p1) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  int16x4_t __ret; \
  __ret = (int16x4_t) __builtin_neon_vqrshrn_n_v((int8x16_t)__s0, __p1, 1); \
  __ret; \
})
#else
#define vqrshrn_n_s32(__p0, __p1) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  int16x4_t __ret; \
  __ret = (int16x4_t) __builtin_neon_vqrshrn_n_v((int8x16_t)__rev0, __p1, 1); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#define __noswap_vqrshrn_n_s32(__p0, __p1) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  int16x4_t __ret; \
  __ret = (int16x4_t) __builtin_neon_vqrshrn_n_v((int8x16_t)__s0, __p1, 1); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqrshrn_n_s64(__p0, __p1) __extension__ ({ \
  int64x2_t __s0 = __p0; \
  int32x2_t __ret; \
  __ret = (int32x2_t) __builtin_neon_vqrshrn_n_v((int8x16_t)__s0, __p1, 2); \
  __ret; \
})
#else
#define vqrshrn_n_s64(__p0, __p1) __extension__ ({ \
  int64x2_t __s0 = __p0; \
  int64x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  int32x2_t __ret; \
  __ret = (int32x2_t) __builtin_neon_vqrshrn_n_v((int8x16_t)__rev0, __p1, 2); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#define __noswap_vqrshrn_n_s64(__p0, __p1) __extension__ ({ \
  int64x2_t __s0 = __p0; \
  int32x2_t __ret; \
  __ret = (int32x2_t) __builtin_neon_vqrshrn_n_v((int8x16_t)__s0, __p1, 2); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqrshrn_n_s16(__p0, __p1) __extension__ ({ \
  int16x8_t __s0 = __p0; \
  int8x8_t __ret; \
  __ret = (int8x8_t) __builtin_neon_vqrshrn_n_v((int8x16_t)__s0, __p1, 0); \
  __ret; \
})
#else
#define vqrshrn_n_s16(__p0, __p1) __extension__ ({ \
  int16x8_t __s0 = __p0; \
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 7, 6, 5, 4, 3, 2, 1, 0); \
  int8x8_t __ret; \
  __ret = (int8x8_t) __builtin_neon_vqrshrn_n_v((int8x16_t)__rev0, __p1, 0); \
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#define __noswap_vqrshrn_n_s16(__p0, __p1) __extension__ ({ \
  int16x8_t __s0 = __p0; \
  int8x8_t __ret; \
  __ret = (int8x8_t) __builtin_neon_vqrshrn_n_v((int8x16_t)__s0, __p1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqrshrun_n_s32(__p0, __p1) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  uint16x4_t __ret; \
  __ret = (uint16x4_t) __builtin_neon_vqrshrun_n_v((int8x16_t)__s0, __p1, 17); \
  __ret; \
})
#else
#define vqrshrun_n_s32(__p0, __p1) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  uint16x4_t __ret; \
  __ret = (uint16x4_t) __builtin_neon_vqrshrun_n_v((int8x16_t)__rev0, __p1, 17); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#define __noswap_vqrshrun_n_s32(__p0, __p1) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  uint16x4_t __ret; \
  __ret = (uint16x4_t) __builtin_neon_vqrshrun_n_v((int8x16_t)__s0, __p1, 17); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqrshrun_n_s64(__p0, __p1) __extension__ ({ \
  int64x2_t __s0 = __p0; \
  uint32x2_t __ret; \
  __ret = (uint32x2_t) __builtin_neon_vqrshrun_n_v((int8x16_t)__s0, __p1, 18); \
  __ret; \
})
#else
#define vqrshrun_n_s64(__p0, __p1) __extension__ ({ \
  int64x2_t __s0 = __p0; \
  int64x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  uint32x2_t __ret; \
  __ret = (uint32x2_t) __builtin_neon_vqrshrun_n_v((int8x16_t)__rev0, __p1, 18); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#define __noswap_vqrshrun_n_s64(__p0, __p1) __extension__ ({ \
  int64x2_t __s0 = __p0; \
  uint32x2_t __ret; \
  __ret = (uint32x2_t) __builtin_neon_vqrshrun_n_v((int8x16_t)__s0, __p1, 18); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqrshrun_n_s16(__p0, __p1) __extension__ ({ \
  int16x8_t __s0 = __p0; \
  uint8x8_t __ret; \
  __ret = (uint8x8_t) __builtin_neon_vqrshrun_n_v((int8x16_t)__s0, __p1, 16); \
  __ret; \
})
#else
#define vqrshrun_n_s16(__p0, __p1) __extension__ ({ \
  int16x8_t __s0 = __p0; \
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint8x8_t __ret; \
  __ret = (uint8x8_t) __builtin_neon_vqrshrun_n_v((int8x16_t)__rev0, __p1, 16); \
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#define __noswap_vqrshrun_n_s16(__p0, __p1) __extension__ ({ \
  int16x8_t __s0 = __p0; \
  uint8x8_t __ret; \
  __ret = (uint8x8_t) __builtin_neon_vqrshrun_n_v((int8x16_t)__s0, __p1, 16); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x16_t vqshlq_u8(uint8x16_t __p0, int8x16_t __p1) {
  uint8x16_t __ret;
  __ret = (uint8x16_t) __builtin_neon_vqshlq_v((int8x16_t)__p0, (int8x16_t)__p1, 48);
  return __ret;
}
#else
__ai uint8x16_t vqshlq_u8(uint8x16_t __p0, int8x16_t __p1) {
  uint8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __ret;
  __ret = (uint8x16_t) __builtin_neon_vqshlq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 48);
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vqshlq_u32(uint32x4_t __p0, int32x4_t __p1) {
  uint32x4_t __ret;
  __ret = (uint32x4_t) __builtin_neon_vqshlq_v((int8x16_t)__p0, (int8x16_t)__p1, 50);
  return __ret;
}
#else
__ai uint32x4_t vqshlq_u32(uint32x4_t __p0, int32x4_t __p1) {
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint32x4_t __ret;
  __ret = (uint32x4_t) __builtin_neon_vqshlq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 50);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vqshlq_u64(uint64x2_t __p0, int64x2_t __p1) {
  uint64x2_t __ret;
  __ret = (uint64x2_t) __builtin_neon_vqshlq_v((int8x16_t)__p0, (int8x16_t)__p1, 51);
  return __ret;
}
#else
__ai uint64x2_t vqshlq_u64(uint64x2_t __p0, int64x2_t __p1) {
  uint64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  uint64x2_t __ret;
  __ret = (uint64x2_t) __builtin_neon_vqshlq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 51);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x8_t vqshlq_u16(uint16x8_t __p0, int16x8_t __p1) {
  uint16x8_t __ret;
  __ret = (uint16x8_t) __builtin_neon_vqshlq_v((int8x16_t)__p0, (int8x16_t)__p1, 49);
  return __ret;
}
#else
__ai uint16x8_t vqshlq_u16(uint16x8_t __p0, int16x8_t __p1) {
  uint16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __ret;
  __ret = (uint16x8_t) __builtin_neon_vqshlq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 49);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x16_t vqshlq_s8(int8x16_t __p0, int8x16_t __p1) {
  int8x16_t __ret;
  __ret = (int8x16_t) __builtin_neon_vqshlq_v((int8x16_t)__p0, (int8x16_t)__p1, 32);
  return __ret;
}
#else
__ai int8x16_t vqshlq_s8(int8x16_t __p0, int8x16_t __p1) {
  int8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __ret;
  __ret = (int8x16_t) __builtin_neon_vqshlq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 32);
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vqshlq_s32(int32x4_t __p0, int32x4_t __p1) {
  int32x4_t __ret;
  __ret = (int32x4_t) __builtin_neon_vqshlq_v((int8x16_t)__p0, (int8x16_t)__p1, 34);
  return __ret;
}
#else
__ai int32x4_t vqshlq_s32(int32x4_t __p0, int32x4_t __p1) {
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int32x4_t __ret;
  __ret = (int32x4_t) __builtin_neon_vqshlq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 34);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x2_t vqshlq_s64(int64x2_t __p0, int64x2_t __p1) {
  int64x2_t __ret;
  __ret = (int64x2_t) __builtin_neon_vqshlq_v((int8x16_t)__p0, (int8x16_t)__p1, 35);
  return __ret;
}
#else
__ai int64x2_t vqshlq_s64(int64x2_t __p0, int64x2_t __p1) {
  int64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  int64x2_t __ret;
  __ret = (int64x2_t) __builtin_neon_vqshlq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 35);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x8_t vqshlq_s16(int16x8_t __p0, int16x8_t __p1) {
  int16x8_t __ret;
  __ret = (int16x8_t) __builtin_neon_vqshlq_v((int8x16_t)__p0, (int8x16_t)__p1, 33);
  return __ret;
}
#else
__ai int16x8_t vqshlq_s16(int16x8_t __p0, int16x8_t __p1) {
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __ret;
  __ret = (int16x8_t) __builtin_neon_vqshlq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 33);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x8_t vqshl_u8(uint8x8_t __p0, int8x8_t __p1) {
  uint8x8_t __ret;
  __ret = (uint8x8_t) __builtin_neon_vqshl_v((int8x8_t)__p0, (int8x8_t)__p1, 16);
  return __ret;
}
#else
__ai uint8x8_t vqshl_u8(uint8x8_t __p0, int8x8_t __p1) {
  uint8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __ret;
  __ret = (uint8x8_t) __builtin_neon_vqshl_v((int8x8_t)__rev0, (int8x8_t)__rev1, 16);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x2_t vqshl_u32(uint32x2_t __p0, int32x2_t __p1) {
  uint32x2_t __ret;
  __ret = (uint32x2_t) __builtin_neon_vqshl_v((int8x8_t)__p0, (int8x8_t)__p1, 18);
  return __ret;
}
#else
__ai uint32x2_t vqshl_u32(uint32x2_t __p0, int32x2_t __p1) {
  uint32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  uint32x2_t __ret;
  __ret = (uint32x2_t) __builtin_neon_vqshl_v((int8x8_t)__rev0, (int8x8_t)__rev1, 18);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x1_t vqshl_u64(uint64x1_t __p0, int64x1_t __p1) {
  uint64x1_t __ret;
  __ret = (uint64x1_t) __builtin_neon_vqshl_v((int8x8_t)__p0, (int8x8_t)__p1, 19);
  return __ret;
}
#else
__ai uint64x1_t vqshl_u64(uint64x1_t __p0, int64x1_t __p1) {
  uint64x1_t __ret;
  __ret = (uint64x1_t) __builtin_neon_vqshl_v((int8x8_t)__p0, (int8x8_t)__p1, 19);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x4_t vqshl_u16(uint16x4_t __p0, int16x4_t __p1) {
  uint16x4_t __ret;
  __ret = (uint16x4_t) __builtin_neon_vqshl_v((int8x8_t)__p0, (int8x8_t)__p1, 17);
  return __ret;
}
#else
__ai uint16x4_t vqshl_u16(uint16x4_t __p0, int16x4_t __p1) {
  uint16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint16x4_t __ret;
  __ret = (uint16x4_t) __builtin_neon_vqshl_v((int8x8_t)__rev0, (int8x8_t)__rev1, 17);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x8_t vqshl_s8(int8x8_t __p0, int8x8_t __p1) {
  int8x8_t __ret;
  __ret = (int8x8_t) __builtin_neon_vqshl_v((int8x8_t)__p0, (int8x8_t)__p1, 0);
  return __ret;
}
#else
__ai int8x8_t vqshl_s8(int8x8_t __p0, int8x8_t __p1) {
  int8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __ret;
  __ret = (int8x8_t) __builtin_neon_vqshl_v((int8x8_t)__rev0, (int8x8_t)__rev1, 0);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x2_t vqshl_s32(int32x2_t __p0, int32x2_t __p1) {
  int32x2_t __ret;
  __ret = (int32x2_t) __builtin_neon_vqshl_v((int8x8_t)__p0, (int8x8_t)__p1, 2);
  return __ret;
}
#else
__ai int32x2_t vqshl_s32(int32x2_t __p0, int32x2_t __p1) {
  int32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  int32x2_t __ret;
  __ret = (int32x2_t) __builtin_neon_vqshl_v((int8x8_t)__rev0, (int8x8_t)__rev1, 2);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x1_t vqshl_s64(int64x1_t __p0, int64x1_t __p1) {
  int64x1_t __ret;
  __ret = (int64x1_t) __builtin_neon_vqshl_v((int8x8_t)__p0, (int8x8_t)__p1, 3);
  return __ret;
}
#else
__ai int64x1_t vqshl_s64(int64x1_t __p0, int64x1_t __p1) {
  int64x1_t __ret;
  __ret = (int64x1_t) __builtin_neon_vqshl_v((int8x8_t)__p0, (int8x8_t)__p1, 3);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x4_t vqshl_s16(int16x4_t __p0, int16x4_t __p1) {
  int16x4_t __ret;
  __ret = (int16x4_t) __builtin_neon_vqshl_v((int8x8_t)__p0, (int8x8_t)__p1, 1);
  return __ret;
}
#else
__ai int16x4_t vqshl_s16(int16x4_t __p0, int16x4_t __p1) {
  int16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int16x4_t __ret;
  __ret = (int16x4_t) __builtin_neon_vqshl_v((int8x8_t)__rev0, (int8x8_t)__rev1, 1);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
#define vqshlq_n_u8(__p0, __p1) __extension__ ({ \
  uint8x16_t __s0 = __p0; \
  uint8x16_t __ret; \
  __ret = (uint8x16_t) __builtin_neon_vqshlq_n_v((int8x16_t)__s0, __p1, 48); \
  __ret; \
})
#else
#define vqshlq_n_u8(__p0, __p1) __extension__ ({ \
  uint8x16_t __s0 = __p0; \
  uint8x16_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint8x16_t __ret; \
  __ret = (uint8x16_t) __builtin_neon_vqshlq_n_v((int8x16_t)__rev0, __p1, 48); \
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqshlq_n_u32(__p0, __p1) __extension__ ({ \
  uint32x4_t __s0 = __p0; \
  uint32x4_t __ret; \
  __ret = (uint32x4_t) __builtin_neon_vqshlq_n_v((int8x16_t)__s0, __p1, 50); \
  __ret; \
})
#else
#define vqshlq_n_u32(__p0, __p1) __extension__ ({ \
  uint32x4_t __s0 = __p0; \
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  uint32x4_t __ret; \
  __ret = (uint32x4_t) __builtin_neon_vqshlq_n_v((int8x16_t)__rev0, __p1, 50); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqshlq_n_u64(__p0, __p1) __extension__ ({ \
  uint64x2_t __s0 = __p0; \
  uint64x2_t __ret; \
  __ret = (uint64x2_t) __builtin_neon_vqshlq_n_v((int8x16_t)__s0, __p1, 51); \
  __ret; \
})
#else
#define vqshlq_n_u64(__p0, __p1) __extension__ ({ \
  uint64x2_t __s0 = __p0; \
  uint64x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  uint64x2_t __ret; \
  __ret = (uint64x2_t) __builtin_neon_vqshlq_n_v((int8x16_t)__rev0, __p1, 51); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqshlq_n_u16(__p0, __p1) __extension__ ({ \
  uint16x8_t __s0 = __p0; \
  uint16x8_t __ret; \
  __ret = (uint16x8_t) __builtin_neon_vqshlq_n_v((int8x16_t)__s0, __p1, 49); \
  __ret; \
})
#else
#define vqshlq_n_u16(__p0, __p1) __extension__ ({ \
  uint16x8_t __s0 = __p0; \
  uint16x8_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint16x8_t __ret; \
  __ret = (uint16x8_t) __builtin_neon_vqshlq_n_v((int8x16_t)__rev0, __p1, 49); \
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqshlq_n_s8(__p0, __p1) __extension__ ({ \
  int8x16_t __s0 = __p0; \
  int8x16_t __ret; \
  __ret = (int8x16_t) __builtin_neon_vqshlq_n_v((int8x16_t)__s0, __p1, 32); \
  __ret; \
})
#else
#define vqshlq_n_s8(__p0, __p1) __extension__ ({ \
  int8x16_t __s0 = __p0; \
  int8x16_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  int8x16_t __ret; \
  __ret = (int8x16_t) __builtin_neon_vqshlq_n_v((int8x16_t)__rev0, __p1, 32); \
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqshlq_n_s32(__p0, __p1) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  int32x4_t __ret; \
  __ret = (int32x4_t) __builtin_neon_vqshlq_n_v((int8x16_t)__s0, __p1, 34); \
  __ret; \
})
#else
#define vqshlq_n_s32(__p0, __p1) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  int32x4_t __ret; \
  __ret = (int32x4_t) __builtin_neon_vqshlq_n_v((int8x16_t)__rev0, __p1, 34); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqshlq_n_s64(__p0, __p1) __extension__ ({ \
  int64x2_t __s0 = __p0; \
  int64x2_t __ret; \
  __ret = (int64x2_t) __builtin_neon_vqshlq_n_v((int8x16_t)__s0, __p1, 35); \
  __ret; \
})
#else
#define vqshlq_n_s64(__p0, __p1) __extension__ ({ \
  int64x2_t __s0 = __p0; \
  int64x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  int64x2_t __ret; \
  __ret = (int64x2_t) __builtin_neon_vqshlq_n_v((int8x16_t)__rev0, __p1, 35); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqshlq_n_s16(__p0, __p1) __extension__ ({ \
  int16x8_t __s0 = __p0; \
  int16x8_t __ret; \
  __ret = (int16x8_t) __builtin_neon_vqshlq_n_v((int8x16_t)__s0, __p1, 33); \
  __ret; \
})
#else
#define vqshlq_n_s16(__p0, __p1) __extension__ ({ \
  int16x8_t __s0 = __p0; \
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 7, 6, 5, 4, 3, 2, 1, 0); \
  int16x8_t __ret; \
  __ret = (int16x8_t) __builtin_neon_vqshlq_n_v((int8x16_t)__rev0, __p1, 33); \
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqshl_n_u8(__p0, __p1) __extension__ ({ \
  uint8x8_t __s0 = __p0; \
  uint8x8_t __ret; \
  __ret = (uint8x8_t) __builtin_neon_vqshl_n_v((int8x8_t)__s0, __p1, 16); \
  __ret; \
})
#else
#define vqshl_n_u8(__p0, __p1) __extension__ ({ \
  uint8x8_t __s0 = __p0; \
  uint8x8_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint8x8_t __ret; \
  __ret = (uint8x8_t) __builtin_neon_vqshl_n_v((int8x8_t)__rev0, __p1, 16); \
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqshl_n_u32(__p0, __p1) __extension__ ({ \
  uint32x2_t __s0 = __p0; \
  uint32x2_t __ret; \
  __ret = (uint32x2_t) __builtin_neon_vqshl_n_v((int8x8_t)__s0, __p1, 18); \
  __ret; \
})
#else
#define vqshl_n_u32(__p0, __p1) __extension__ ({ \
  uint32x2_t __s0 = __p0; \
  uint32x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  uint32x2_t __ret; \
  __ret = (uint32x2_t) __builtin_neon_vqshl_n_v((int8x8_t)__rev0, __p1, 18); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqshl_n_u64(__p0, __p1) __extension__ ({ \
  uint64x1_t __s0 = __p0; \
  uint64x1_t __ret; \
  __ret = (uint64x1_t) __builtin_neon_vqshl_n_v((int8x8_t)__s0, __p1, 19); \
  __ret; \
})
#else
#define vqshl_n_u64(__p0, __p1) __extension__ ({ \
  uint64x1_t __s0 = __p0; \
  uint64x1_t __ret; \
  __ret = (uint64x1_t) __builtin_neon_vqshl_n_v((int8x8_t)__s0, __p1, 19); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqshl_n_u16(__p0, __p1) __extension__ ({ \
  uint16x4_t __s0 = __p0; \
  uint16x4_t __ret; \
  __ret = (uint16x4_t) __builtin_neon_vqshl_n_v((int8x8_t)__s0, __p1, 17); \
  __ret; \
})
#else
#define vqshl_n_u16(__p0, __p1) __extension__ ({ \
  uint16x4_t __s0 = __p0; \
  uint16x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  uint16x4_t __ret; \
  __ret = (uint16x4_t) __builtin_neon_vqshl_n_v((int8x8_t)__rev0, __p1, 17); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqshl_n_s8(__p0, __p1) __extension__ ({ \
  int8x8_t __s0 = __p0; \
  int8x8_t __ret; \
  __ret = (int8x8_t) __builtin_neon_vqshl_n_v((int8x8_t)__s0, __p1, 0); \
  __ret; \
})
#else
#define vqshl_n_s8(__p0, __p1) __extension__ ({ \
  int8x8_t __s0 = __p0; \
  int8x8_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 7, 6, 5, 4, 3, 2, 1, 0); \
  int8x8_t __ret; \
  __ret = (int8x8_t) __builtin_neon_vqshl_n_v((int8x8_t)__rev0, __p1, 0); \
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqshl_n_s32(__p0, __p1) __extension__ ({ \
  int32x2_t __s0 = __p0; \
  int32x2_t __ret; \
  __ret = (int32x2_t) __builtin_neon_vqshl_n_v((int8x8_t)__s0, __p1, 2); \
  __ret; \
})
#else
#define vqshl_n_s32(__p0, __p1) __extension__ ({ \
  int32x2_t __s0 = __p0; \
  int32x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  int32x2_t __ret; \
  __ret = (int32x2_t) __builtin_neon_vqshl_n_v((int8x8_t)__rev0, __p1, 2); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqshl_n_s64(__p0, __p1) __extension__ ({ \
  int64x1_t __s0 = __p0; \
  int64x1_t __ret; \
  __ret = (int64x1_t) __builtin_neon_vqshl_n_v((int8x8_t)__s0, __p1, 3); \
  __ret; \
})
#else
#define vqshl_n_s64(__p0, __p1) __extension__ ({ \
  int64x1_t __s0 = __p0; \
  int64x1_t __ret; \
  __ret = (int64x1_t) __builtin_neon_vqshl_n_v((int8x8_t)__s0, __p1, 3); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqshl_n_s16(__p0, __p1) __extension__ ({ \
  int16x4_t __s0 = __p0; \
  int16x4_t __ret; \
  __ret = (int16x4_t) __builtin_neon_vqshl_n_v((int8x8_t)__s0, __p1, 1); \
  __ret; \
})
#else
#define vqshl_n_s16(__p0, __p1) __extension__ ({ \
  int16x4_t __s0 = __p0; \
  int16x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  int16x4_t __ret; \
  __ret = (int16x4_t) __builtin_neon_vqshl_n_v((int8x8_t)__rev0, __p1, 1); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqshluq_n_s8(__p0, __p1) __extension__ ({ \
  int8x16_t __s0 = __p0; \
  uint8x16_t __ret; \
  __ret = (uint8x16_t) __builtin_neon_vqshluq_n_v((int8x16_t)__s0, __p1, 48); \
  __ret; \
})
#else
#define vqshluq_n_s8(__p0, __p1) __extension__ ({ \
  int8x16_t __s0 = __p0; \
  int8x16_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint8x16_t __ret; \
  __ret = (uint8x16_t) __builtin_neon_vqshluq_n_v((int8x16_t)__rev0, __p1, 48); \
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqshluq_n_s32(__p0, __p1) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  uint32x4_t __ret; \
  __ret = (uint32x4_t) __builtin_neon_vqshluq_n_v((int8x16_t)__s0, __p1, 50); \
  __ret; \
})
#else
#define vqshluq_n_s32(__p0, __p1) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  uint32x4_t __ret; \
  __ret = (uint32x4_t) __builtin_neon_vqshluq_n_v((int8x16_t)__rev0, __p1, 50); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqshluq_n_s64(__p0, __p1) __extension__ ({ \
  int64x2_t __s0 = __p0; \
  uint64x2_t __ret; \
  __ret = (uint64x2_t) __builtin_neon_vqshluq_n_v((int8x16_t)__s0, __p1, 51); \
  __ret; \
})
#else
#define vqshluq_n_s64(__p0, __p1) __extension__ ({ \
  int64x2_t __s0 = __p0; \
  int64x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  uint64x2_t __ret; \
  __ret = (uint64x2_t) __builtin_neon_vqshluq_n_v((int8x16_t)__rev0, __p1, 51); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqshluq_n_s16(__p0, __p1) __extension__ ({ \
  int16x8_t __s0 = __p0; \
  uint16x8_t __ret; \
  __ret = (uint16x8_t) __builtin_neon_vqshluq_n_v((int8x16_t)__s0, __p1, 49); \
  __ret; \
})
#else
#define vqshluq_n_s16(__p0, __p1) __extension__ ({ \
  int16x8_t __s0 = __p0; \
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint16x8_t __ret; \
  __ret = (uint16x8_t) __builtin_neon_vqshluq_n_v((int8x16_t)__rev0, __p1, 49); \
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqshlu_n_s8(__p0, __p1) __extension__ ({ \
  int8x8_t __s0 = __p0; \
  uint8x8_t __ret; \
  __ret = (uint8x8_t) __builtin_neon_vqshlu_n_v((int8x8_t)__s0, __p1, 16); \
  __ret; \
})
#else
#define vqshlu_n_s8(__p0, __p1) __extension__ ({ \
  int8x8_t __s0 = __p0; \
  int8x8_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint8x8_t __ret; \
  __ret = (uint8x8_t) __builtin_neon_vqshlu_n_v((int8x8_t)__rev0, __p1, 16); \
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqshlu_n_s32(__p0, __p1) __extension__ ({ \
  int32x2_t __s0 = __p0; \
  uint32x2_t __ret; \
  __ret = (uint32x2_t) __builtin_neon_vqshlu_n_v((int8x8_t)__s0, __p1, 18); \
  __ret; \
})
#else
#define vqshlu_n_s32(__p0, __p1) __extension__ ({ \
  int32x2_t __s0 = __p0; \
  int32x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  uint32x2_t __ret; \
  __ret = (uint32x2_t) __builtin_neon_vqshlu_n_v((int8x8_t)__rev0, __p1, 18); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqshlu_n_s64(__p0, __p1) __extension__ ({ \
  int64x1_t __s0 = __p0; \
  uint64x1_t __ret; \
  __ret = (uint64x1_t) __builtin_neon_vqshlu_n_v((int8x8_t)__s0, __p1, 19); \
  __ret; \
})
#else
#define vqshlu_n_s64(__p0, __p1) __extension__ ({ \
  int64x1_t __s0 = __p0; \
  uint64x1_t __ret; \
  __ret = (uint64x1_t) __builtin_neon_vqshlu_n_v((int8x8_t)__s0, __p1, 19); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqshlu_n_s16(__p0, __p1) __extension__ ({ \
  int16x4_t __s0 = __p0; \
  uint16x4_t __ret; \
  __ret = (uint16x4_t) __builtin_neon_vqshlu_n_v((int8x8_t)__s0, __p1, 17); \
  __ret; \
})
#else
#define vqshlu_n_s16(__p0, __p1) __extension__ ({ \
  int16x4_t __s0 = __p0; \
  int16x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  uint16x4_t __ret; \
  __ret = (uint16x4_t) __builtin_neon_vqshlu_n_v((int8x8_t)__rev0, __p1, 17); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqshrn_n_u32(__p0, __p1) __extension__ ({ \
  uint32x4_t __s0 = __p0; \
  uint16x4_t __ret; \
  __ret = (uint16x4_t) __builtin_neon_vqshrn_n_v((int8x16_t)__s0, __p1, 17); \
  __ret; \
})
#else
#define vqshrn_n_u32(__p0, __p1) __extension__ ({ \
  uint32x4_t __s0 = __p0; \
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  uint16x4_t __ret; \
  __ret = (uint16x4_t) __builtin_neon_vqshrn_n_v((int8x16_t)__rev0, __p1, 17); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#define __noswap_vqshrn_n_u32(__p0, __p1) __extension__ ({ \
  uint32x4_t __s0 = __p0; \
  uint16x4_t __ret; \
  __ret = (uint16x4_t) __builtin_neon_vqshrn_n_v((int8x16_t)__s0, __p1, 17); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqshrn_n_u64(__p0, __p1) __extension__ ({ \
  uint64x2_t __s0 = __p0; \
  uint32x2_t __ret; \
  __ret = (uint32x2_t) __builtin_neon_vqshrn_n_v((int8x16_t)__s0, __p1, 18); \
  __ret; \
})
#else
#define vqshrn_n_u64(__p0, __p1) __extension__ ({ \
  uint64x2_t __s0 = __p0; \
  uint64x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  uint32x2_t __ret; \
  __ret = (uint32x2_t) __builtin_neon_vqshrn_n_v((int8x16_t)__rev0, __p1, 18); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#define __noswap_vqshrn_n_u64(__p0, __p1) __extension__ ({ \
  uint64x2_t __s0 = __p0; \
  uint32x2_t __ret; \
  __ret = (uint32x2_t) __builtin_neon_vqshrn_n_v((int8x16_t)__s0, __p1, 18); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqshrn_n_u16(__p0, __p1) __extension__ ({ \
  uint16x8_t __s0 = __p0; \
  uint8x8_t __ret; \
  __ret = (uint8x8_t) __builtin_neon_vqshrn_n_v((int8x16_t)__s0, __p1, 16); \
  __ret; \
})
#else
#define vqshrn_n_u16(__p0, __p1) __extension__ ({ \
  uint16x8_t __s0 = __p0; \
  uint16x8_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint8x8_t __ret; \
  __ret = (uint8x8_t) __builtin_neon_vqshrn_n_v((int8x16_t)__rev0, __p1, 16); \
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#define __noswap_vqshrn_n_u16(__p0, __p1) __extension__ ({ \
  uint16x8_t __s0 = __p0; \
  uint8x8_t __ret; \
  __ret = (uint8x8_t) __builtin_neon_vqshrn_n_v((int8x16_t)__s0, __p1, 16); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqshrn_n_s32(__p0, __p1) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  int16x4_t __ret; \
  __ret = (int16x4_t) __builtin_neon_vqshrn_n_v((int8x16_t)__s0, __p1, 1); \
  __ret; \
})
#else
#define vqshrn_n_s32(__p0, __p1) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  int16x4_t __ret; \
  __ret = (int16x4_t) __builtin_neon_vqshrn_n_v((int8x16_t)__rev0, __p1, 1); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#define __noswap_vqshrn_n_s32(__p0, __p1) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  int16x4_t __ret; \
  __ret = (int16x4_t) __builtin_neon_vqshrn_n_v((int8x16_t)__s0, __p1, 1); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqshrn_n_s64(__p0, __p1) __extension__ ({ \
  int64x2_t __s0 = __p0; \
  int32x2_t __ret; \
  __ret = (int32x2_t) __builtin_neon_vqshrn_n_v((int8x16_t)__s0, __p1, 2); \
  __ret; \
})
#else
#define vqshrn_n_s64(__p0, __p1) __extension__ ({ \
  int64x2_t __s0 = __p0; \
  int64x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  int32x2_t __ret; \
  __ret = (int32x2_t) __builtin_neon_vqshrn_n_v((int8x16_t)__rev0, __p1, 2); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#define __noswap_vqshrn_n_s64(__p0, __p1) __extension__ ({ \
  int64x2_t __s0 = __p0; \
  int32x2_t __ret; \
  __ret = (int32x2_t) __builtin_neon_vqshrn_n_v((int8x16_t)__s0, __p1, 2); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqshrn_n_s16(__p0, __p1) __extension__ ({ \
  int16x8_t __s0 = __p0; \
  int8x8_t __ret; \
  __ret = (int8x8_t) __builtin_neon_vqshrn_n_v((int8x16_t)__s0, __p1, 0); \
  __ret; \
})
#else
#define vqshrn_n_s16(__p0, __p1) __extension__ ({ \
  int16x8_t __s0 = __p0; \
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 7, 6, 5, 4, 3, 2, 1, 0); \
  int8x8_t __ret; \
  __ret = (int8x8_t) __builtin_neon_vqshrn_n_v((int8x16_t)__rev0, __p1, 0); \
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#define __noswap_vqshrn_n_s16(__p0, __p1) __extension__ ({ \
  int16x8_t __s0 = __p0; \
  int8x8_t __ret; \
  __ret = (int8x8_t) __builtin_neon_vqshrn_n_v((int8x16_t)__s0, __p1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqshrun_n_s32(__p0, __p1) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  uint16x4_t __ret; \
  __ret = (uint16x4_t) __builtin_neon_vqshrun_n_v((int8x16_t)__s0, __p1, 17); \
  __ret; \
})
#else
#define vqshrun_n_s32(__p0, __p1) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  uint16x4_t __ret; \
  __ret = (uint16x4_t) __builtin_neon_vqshrun_n_v((int8x16_t)__rev0, __p1, 17); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#define __noswap_vqshrun_n_s32(__p0, __p1) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  uint16x4_t __ret; \
  __ret = (uint16x4_t) __builtin_neon_vqshrun_n_v((int8x16_t)__s0, __p1, 17); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqshrun_n_s64(__p0, __p1) __extension__ ({ \
  int64x2_t __s0 = __p0; \
  uint32x2_t __ret; \
  __ret = (uint32x2_t) __builtin_neon_vqshrun_n_v((int8x16_t)__s0, __p1, 18); \
  __ret; \
})
#else
#define vqshrun_n_s64(__p0, __p1) __extension__ ({ \
  int64x2_t __s0 = __p0; \
  int64x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  uint32x2_t __ret; \
  __ret = (uint32x2_t) __builtin_neon_vqshrun_n_v((int8x16_t)__rev0, __p1, 18); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#define __noswap_vqshrun_n_s64(__p0, __p1) __extension__ ({ \
  int64x2_t __s0 = __p0; \
  uint32x2_t __ret; \
  __ret = (uint32x2_t) __builtin_neon_vqshrun_n_v((int8x16_t)__s0, __p1, 18); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vqshrun_n_s16(__p0, __p1) __extension__ ({ \
  int16x8_t __s0 = __p0; \
  uint8x8_t __ret; \
  __ret = (uint8x8_t) __builtin_neon_vqshrun_n_v((int8x16_t)__s0, __p1, 16); \
  __ret; \
})
#else
#define vqshrun_n_s16(__p0, __p1) __extension__ ({ \
  int16x8_t __s0 = __p0; \
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint8x8_t __ret; \
  __ret = (uint8x8_t) __builtin_neon_vqshrun_n_v((int8x16_t)__rev0, __p1, 16); \
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#define __noswap_vqshrun_n_s16(__p0, __p1) __extension__ ({ \
  int16x8_t __s0 = __p0; \
  uint8x8_t __ret; \
  __ret = (uint8x8_t) __builtin_neon_vqshrun_n_v((int8x16_t)__s0, __p1, 16); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x16_t vqsubq_u8(uint8x16_t __p0, uint8x16_t __p1) {
  uint8x16_t __ret;
  __ret = (uint8x16_t) __builtin_neon_vqsubq_v((int8x16_t)__p0, (int8x16_t)__p1, 48);
  return __ret;
}
#else
__ai uint8x16_t vqsubq_u8(uint8x16_t __p0, uint8x16_t __p1) {
  uint8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __ret;
  __ret = (uint8x16_t) __builtin_neon_vqsubq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 48);
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vqsubq_u32(uint32x4_t __p0, uint32x4_t __p1) {
  uint32x4_t __ret;
  __ret = (uint32x4_t) __builtin_neon_vqsubq_v((int8x16_t)__p0, (int8x16_t)__p1, 50);
  return __ret;
}
#else
__ai uint32x4_t vqsubq_u32(uint32x4_t __p0, uint32x4_t __p1) {
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint32x4_t __ret;
  __ret = (uint32x4_t) __builtin_neon_vqsubq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 50);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vqsubq_u64(uint64x2_t __p0, uint64x2_t __p1) {
  uint64x2_t __ret;
  __ret = (uint64x2_t) __builtin_neon_vqsubq_v((int8x16_t)__p0, (int8x16_t)__p1, 51);
  return __ret;
}
#else
__ai uint64x2_t vqsubq_u64(uint64x2_t __p0, uint64x2_t __p1) {
  uint64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  uint64x2_t __ret;
  __ret = (uint64x2_t) __builtin_neon_vqsubq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 51);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x8_t vqsubq_u16(uint16x8_t __p0, uint16x8_t __p1) {
  uint16x8_t __ret;
  __ret = (uint16x8_t) __builtin_neon_vqsubq_v((int8x16_t)__p0, (int8x16_t)__p1, 49);
  return __ret;
}
#else
__ai uint16x8_t vqsubq_u16(uint16x8_t __p0, uint16x8_t __p1) {
  uint16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __ret;
  __ret = (uint16x8_t) __builtin_neon_vqsubq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 49);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x16_t vqsubq_s8(int8x16_t __p0, int8x16_t __p1) {
  int8x16_t __ret;
  __ret = (int8x16_t) __builtin_neon_vqsubq_v((int8x16_t)__p0, (int8x16_t)__p1, 32);
  return __ret;
}
#else
__ai int8x16_t vqsubq_s8(int8x16_t __p0, int8x16_t __p1) {
  int8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __ret;
  __ret = (int8x16_t) __builtin_neon_vqsubq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 32);
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vqsubq_s32(int32x4_t __p0, int32x4_t __p1) {
  int32x4_t __ret;
  __ret = (int32x4_t) __builtin_neon_vqsubq_v((int8x16_t)__p0, (int8x16_t)__p1, 34);
  return __ret;
}
#else
__ai int32x4_t vqsubq_s32(int32x4_t __p0, int32x4_t __p1) {
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int32x4_t __ret;
  __ret = (int32x4_t) __builtin_neon_vqsubq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 34);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x2_t vqsubq_s64(int64x2_t __p0, int64x2_t __p1) {
  int64x2_t __ret;
  __ret = (int64x2_t) __builtin_neon_vqsubq_v((int8x16_t)__p0, (int8x16_t)__p1, 35);
  return __ret;
}
#else
__ai int64x2_t vqsubq_s64(int64x2_t __p0, int64x2_t __p1) {
  int64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  int64x2_t __ret;
  __ret = (int64x2_t) __builtin_neon_vqsubq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 35);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x8_t vqsubq_s16(int16x8_t __p0, int16x8_t __p1) {
  int16x8_t __ret;
  __ret = (int16x8_t) __builtin_neon_vqsubq_v((int8x16_t)__p0, (int8x16_t)__p1, 33);
  return __ret;
}
#else
__ai int16x8_t vqsubq_s16(int16x8_t __p0, int16x8_t __p1) {
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __ret;
  __ret = (int16x8_t) __builtin_neon_vqsubq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 33);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x8_t vqsub_u8(uint8x8_t __p0, uint8x8_t __p1) {
  uint8x8_t __ret;
  __ret = (uint8x8_t) __builtin_neon_vqsub_v((int8x8_t)__p0, (int8x8_t)__p1, 16);
  return __ret;
}
#else
__ai uint8x8_t vqsub_u8(uint8x8_t __p0, uint8x8_t __p1) {
  uint8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __ret;
  __ret = (uint8x8_t) __builtin_neon_vqsub_v((int8x8_t)__rev0, (int8x8_t)__rev1, 16);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x2_t vqsub_u32(uint32x2_t __p0, uint32x2_t __p1) {
  uint32x2_t __ret;
  __ret = (uint32x2_t) __builtin_neon_vqsub_v((int8x8_t)__p0, (int8x8_t)__p1, 18);
  return __ret;
}
#else
__ai uint32x2_t vqsub_u32(uint32x2_t __p0, uint32x2_t __p1) {
  uint32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  uint32x2_t __ret;
  __ret = (uint32x2_t) __builtin_neon_vqsub_v((int8x8_t)__rev0, (int8x8_t)__rev1, 18);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x1_t vqsub_u64(uint64x1_t __p0, uint64x1_t __p1) {
  uint64x1_t __ret;
  __ret = (uint64x1_t) __builtin_neon_vqsub_v((int8x8_t)__p0, (int8x8_t)__p1, 19);
  return __ret;
}
#else
__ai uint64x1_t vqsub_u64(uint64x1_t __p0, uint64x1_t __p1) {
  uint64x1_t __ret;
  __ret = (uint64x1_t) __builtin_neon_vqsub_v((int8x8_t)__p0, (int8x8_t)__p1, 19);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x4_t vqsub_u16(uint16x4_t __p0, uint16x4_t __p1) {
  uint16x4_t __ret;
  __ret = (uint16x4_t) __builtin_neon_vqsub_v((int8x8_t)__p0, (int8x8_t)__p1, 17);
  return __ret;
}
#else
__ai uint16x4_t vqsub_u16(uint16x4_t __p0, uint16x4_t __p1) {
  uint16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint16x4_t __ret;
  __ret = (uint16x4_t) __builtin_neon_vqsub_v((int8x8_t)__rev0, (int8x8_t)__rev1, 17);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x8_t vqsub_s8(int8x8_t __p0, int8x8_t __p1) {
  int8x8_t __ret;
  __ret = (int8x8_t) __builtin_neon_vqsub_v((int8x8_t)__p0, (int8x8_t)__p1, 0);
  return __ret;
}
#else
__ai int8x8_t vqsub_s8(int8x8_t __p0, int8x8_t __p1) {
  int8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __ret;
  __ret = (int8x8_t) __builtin_neon_vqsub_v((int8x8_t)__rev0, (int8x8_t)__rev1, 0);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x2_t vqsub_s32(int32x2_t __p0, int32x2_t __p1) {
  int32x2_t __ret;
  __ret = (int32x2_t) __builtin_neon_vqsub_v((int8x8_t)__p0, (int8x8_t)__p1, 2);
  return __ret;
}
#else
__ai int32x2_t vqsub_s32(int32x2_t __p0, int32x2_t __p1) {
  int32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  int32x2_t __ret;
  __ret = (int32x2_t) __builtin_neon_vqsub_v((int8x8_t)__rev0, (int8x8_t)__rev1, 2);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x1_t vqsub_s64(int64x1_t __p0, int64x1_t __p1) {
  int64x1_t __ret;
  __ret = (int64x1_t) __builtin_neon_vqsub_v((int8x8_t)__p0, (int8x8_t)__p1, 3);
  return __ret;
}
#else
__ai int64x1_t vqsub_s64(int64x1_t __p0, int64x1_t __p1) {
  int64x1_t __ret;
  __ret = (int64x1_t) __builtin_neon_vqsub_v((int8x8_t)__p0, (int8x8_t)__p1, 3);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x4_t vqsub_s16(int16x4_t __p0, int16x4_t __p1) {
  int16x4_t __ret;
  __ret = (int16x4_t) __builtin_neon_vqsub_v((int8x8_t)__p0, (int8x8_t)__p1, 1);
  return __ret;
}
#else
__ai int16x4_t vqsub_s16(int16x4_t __p0, int16x4_t __p1) {
  int16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int16x4_t __ret;
  __ret = (int16x4_t) __builtin_neon_vqsub_v((int8x8_t)__rev0, (int8x8_t)__rev1, 1);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x4_t vraddhn_u32(uint32x4_t __p0, uint32x4_t __p1) {
  uint16x4_t __ret;
  __ret = (uint16x4_t) __builtin_neon_vraddhn_v((int8x16_t)__p0, (int8x16_t)__p1, 17);
  return __ret;
}
#else
__ai uint16x4_t vraddhn_u32(uint32x4_t __p0, uint32x4_t __p1) {
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint16x4_t __ret;
  __ret = (uint16x4_t) __builtin_neon_vraddhn_v((int8x16_t)__rev0, (int8x16_t)__rev1, 17);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
__ai uint16x4_t __noswap_vraddhn_u32(uint32x4_t __p0, uint32x4_t __p1) {
  uint16x4_t __ret;
  __ret = (uint16x4_t) __builtin_neon_vraddhn_v((int8x16_t)__p0, (int8x16_t)__p1, 17);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x2_t vraddhn_u64(uint64x2_t __p0, uint64x2_t __p1) {
  uint32x2_t __ret;
  __ret = (uint32x2_t) __builtin_neon_vraddhn_v((int8x16_t)__p0, (int8x16_t)__p1, 18);
  return __ret;
}
#else
__ai uint32x2_t vraddhn_u64(uint64x2_t __p0, uint64x2_t __p1) {
  uint64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  uint32x2_t __ret;
  __ret = (uint32x2_t) __builtin_neon_vraddhn_v((int8x16_t)__rev0, (int8x16_t)__rev1, 18);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
__ai uint32x2_t __noswap_vraddhn_u64(uint64x2_t __p0, uint64x2_t __p1) {
  uint32x2_t __ret;
  __ret = (uint32x2_t) __builtin_neon_vraddhn_v((int8x16_t)__p0, (int8x16_t)__p1, 18);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x8_t vraddhn_u16(uint16x8_t __p0, uint16x8_t __p1) {
  uint8x8_t __ret;
  __ret = (uint8x8_t) __builtin_neon_vraddhn_v((int8x16_t)__p0, (int8x16_t)__p1, 16);
  return __ret;
}
#else
__ai uint8x8_t vraddhn_u16(uint16x8_t __p0, uint16x8_t __p1) {
  uint16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __ret;
  __ret = (uint8x8_t) __builtin_neon_vraddhn_v((int8x16_t)__rev0, (int8x16_t)__rev1, 16);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
__ai uint8x8_t __noswap_vraddhn_u16(uint16x8_t __p0, uint16x8_t __p1) {
  uint8x8_t __ret;
  __ret = (uint8x8_t) __builtin_neon_vraddhn_v((int8x16_t)__p0, (int8x16_t)__p1, 16);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x4_t vraddhn_s32(int32x4_t __p0, int32x4_t __p1) {
  int16x4_t __ret;
  __ret = (int16x4_t) __builtin_neon_vraddhn_v((int8x16_t)__p0, (int8x16_t)__p1, 1);
  return __ret;
}
#else
__ai int16x4_t vraddhn_s32(int32x4_t __p0, int32x4_t __p1) {
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int16x4_t __ret;
  __ret = (int16x4_t) __builtin_neon_vraddhn_v((int8x16_t)__rev0, (int8x16_t)__rev1, 1);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
__ai int16x4_t __noswap_vraddhn_s32(int32x4_t __p0, int32x4_t __p1) {
  int16x4_t __ret;
  __ret = (int16x4_t) __builtin_neon_vraddhn_v((int8x16_t)__p0, (int8x16_t)__p1, 1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x2_t vraddhn_s64(int64x2_t __p0, int64x2_t __p1) {
  int32x2_t __ret;
  __ret = (int32x2_t) __builtin_neon_vraddhn_v((int8x16_t)__p0, (int8x16_t)__p1, 2);
  return __ret;
}
#else
__ai int32x2_t vraddhn_s64(int64x2_t __p0, int64x2_t __p1) {
  int64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  int32x2_t __ret;
  __ret = (int32x2_t) __builtin_neon_vraddhn_v((int8x16_t)__rev0, (int8x16_t)__rev1, 2);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
__ai int32x2_t __noswap_vraddhn_s64(int64x2_t __p0, int64x2_t __p1) {
  int32x2_t __ret;
  __ret = (int32x2_t) __builtin_neon_vraddhn_v((int8x16_t)__p0, (int8x16_t)__p1, 2);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x8_t vraddhn_s16(int16x8_t __p0, int16x8_t __p1) {
  int8x8_t __ret;
  __ret = (int8x8_t) __builtin_neon_vraddhn_v((int8x16_t)__p0, (int8x16_t)__p1, 0);
  return __ret;
}
#else
__ai int8x8_t vraddhn_s16(int16x8_t __p0, int16x8_t __p1) {
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __ret;
  __ret = (int8x8_t) __builtin_neon_vraddhn_v((int8x16_t)__rev0, (int8x16_t)__rev1, 0);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
__ai int8x8_t __noswap_vraddhn_s16(int16x8_t __p0, int16x8_t __p1) {
  int8x8_t __ret;
  __ret = (int8x8_t) __builtin_neon_vraddhn_v((int8x16_t)__p0, (int8x16_t)__p1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vrecpeq_u32(uint32x4_t __p0) {
  uint32x4_t __ret;
  __ret = (uint32x4_t) __builtin_neon_vrecpeq_v((int8x16_t)__p0, 50);
  return __ret;
}
#else
__ai uint32x4_t vrecpeq_u32(uint32x4_t __p0) {
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint32x4_t __ret;
  __ret = (uint32x4_t) __builtin_neon_vrecpeq_v((int8x16_t)__rev0, 50);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x4_t vrecpeq_f32(float32x4_t __p0) {
  float32x4_t __ret;
  __ret = (float32x4_t) __builtin_neon_vrecpeq_v((int8x16_t)__p0, 41);
  return __ret;
}
#else
__ai float32x4_t vrecpeq_f32(float32x4_t __p0) {
  float32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  float32x4_t __ret;
  __ret = (float32x4_t) __builtin_neon_vrecpeq_v((int8x16_t)__rev0, 41);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x2_t vrecpe_u32(uint32x2_t __p0) {
  uint32x2_t __ret;
  __ret = (uint32x2_t) __builtin_neon_vrecpe_v((int8x8_t)__p0, 18);
  return __ret;
}
#else
__ai uint32x2_t vrecpe_u32(uint32x2_t __p0) {
  uint32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint32x2_t __ret;
  __ret = (uint32x2_t) __builtin_neon_vrecpe_v((int8x8_t)__rev0, 18);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x2_t vrecpe_f32(float32x2_t __p0) {
  float32x2_t __ret;
  __ret = (float32x2_t) __builtin_neon_vrecpe_v((int8x8_t)__p0, 9);
  return __ret;
}
#else
__ai float32x2_t vrecpe_f32(float32x2_t __p0) {
  float32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float32x2_t __ret;
  __ret = (float32x2_t) __builtin_neon_vrecpe_v((int8x8_t)__rev0, 9);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x4_t vrecpsq_f32(float32x4_t __p0, float32x4_t __p1) {
  float32x4_t __ret;
  __ret = (float32x4_t) __builtin_neon_vrecpsq_v((int8x16_t)__p0, (int8x16_t)__p1, 41);
  return __ret;
}
#else
__ai float32x4_t vrecpsq_f32(float32x4_t __p0, float32x4_t __p1) {
  float32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  float32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  float32x4_t __ret;
  __ret = (float32x4_t) __builtin_neon_vrecpsq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 41);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x2_t vrecps_f32(float32x2_t __p0, float32x2_t __p1) {
  float32x2_t __ret;
  __ret = (float32x2_t) __builtin_neon_vrecps_v((int8x8_t)__p0, (int8x8_t)__p1, 9);
  return __ret;
}
#else
__ai float32x2_t vrecps_f32(float32x2_t __p0, float32x2_t __p1) {
  float32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  float32x2_t __ret;
  __ret = (float32x2_t) __builtin_neon_vrecps_v((int8x8_t)__rev0, (int8x8_t)__rev1, 9);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x8_t vrev16_p8(poly8x8_t __p0) {
  poly8x8_t __ret;
  __ret = __builtin_shufflevector(__p0, __p0, 1, 0, 3, 2, 5, 4, 7, 6);
  return __ret;
}
#else
__ai poly8x8_t vrev16_p8(poly8x8_t __p0) {
  poly8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  poly8x8_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev0, 1, 0, 3, 2, 5, 4, 7, 6);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x16_t vrev16q_p8(poly8x16_t __p0) {
  poly8x16_t __ret;
  __ret = __builtin_shufflevector(__p0, __p0, 1, 0, 3, 2, 5, 4, 7, 6, 9, 8, 11, 10, 13, 12, 15, 14);
  return __ret;
}
#else
__ai poly8x16_t vrev16q_p8(poly8x16_t __p0) {
  poly8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  poly8x16_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev0, 1, 0, 3, 2, 5, 4, 7, 6, 9, 8, 11, 10, 13, 12, 15, 14);
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x16_t vrev16q_u8(uint8x16_t __p0) {
  uint8x16_t __ret;
  __ret = __builtin_shufflevector(__p0, __p0, 1, 0, 3, 2, 5, 4, 7, 6, 9, 8, 11, 10, 13, 12, 15, 14);
  return __ret;
}
#else
__ai uint8x16_t vrev16q_u8(uint8x16_t __p0) {
  uint8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev0, 1, 0, 3, 2, 5, 4, 7, 6, 9, 8, 11, 10, 13, 12, 15, 14);
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x16_t vrev16q_s8(int8x16_t __p0) {
  int8x16_t __ret;
  __ret = __builtin_shufflevector(__p0, __p0, 1, 0, 3, 2, 5, 4, 7, 6, 9, 8, 11, 10, 13, 12, 15, 14);
  return __ret;
}
#else
__ai int8x16_t vrev16q_s8(int8x16_t __p0) {
  int8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev0, 1, 0, 3, 2, 5, 4, 7, 6, 9, 8, 11, 10, 13, 12, 15, 14);
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x8_t vrev16_u8(uint8x8_t __p0) {
  uint8x8_t __ret;
  __ret = __builtin_shufflevector(__p0, __p0, 1, 0, 3, 2, 5, 4, 7, 6);
  return __ret;
}
#else
__ai uint8x8_t vrev16_u8(uint8x8_t __p0) {
  uint8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev0, 1, 0, 3, 2, 5, 4, 7, 6);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x8_t vrev16_s8(int8x8_t __p0) {
  int8x8_t __ret;
  __ret = __builtin_shufflevector(__p0, __p0, 1, 0, 3, 2, 5, 4, 7, 6);
  return __ret;
}
#else
__ai int8x8_t vrev16_s8(int8x8_t __p0) {
  int8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev0, 1, 0, 3, 2, 5, 4, 7, 6);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x8_t vrev32_p8(poly8x8_t __p0) {
  poly8x8_t __ret;
  __ret = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0, 7, 6, 5, 4);
  return __ret;
}
#else
__ai poly8x8_t vrev32_p8(poly8x8_t __p0) {
  poly8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  poly8x8_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev0, 3, 2, 1, 0, 7, 6, 5, 4);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly16x4_t vrev32_p16(poly16x4_t __p0) {
  poly16x4_t __ret;
  __ret = __builtin_shufflevector(__p0, __p0, 1, 0, 3, 2);
  return __ret;
}
#else
__ai poly16x4_t vrev32_p16(poly16x4_t __p0) {
  poly16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  poly16x4_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev0, 1, 0, 3, 2);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x16_t vrev32q_p8(poly8x16_t __p0) {
  poly8x16_t __ret;
  __ret = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0, 7, 6, 5, 4, 11, 10, 9, 8, 15, 14, 13, 12);
  return __ret;
}
#else
__ai poly8x16_t vrev32q_p8(poly8x16_t __p0) {
  poly8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  poly8x16_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev0, 3, 2, 1, 0, 7, 6, 5, 4, 11, 10, 9, 8, 15, 14, 13, 12);
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly16x8_t vrev32q_p16(poly16x8_t __p0) {
  poly16x8_t __ret;
  __ret = __builtin_shufflevector(__p0, __p0, 1, 0, 3, 2, 5, 4, 7, 6);
  return __ret;
}
#else
__ai poly16x8_t vrev32q_p16(poly16x8_t __p0) {
  poly16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  poly16x8_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev0, 1, 0, 3, 2, 5, 4, 7, 6);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x16_t vrev32q_u8(uint8x16_t __p0) {
  uint8x16_t __ret;
  __ret = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0, 7, 6, 5, 4, 11, 10, 9, 8, 15, 14, 13, 12);
  return __ret;
}
#else
__ai uint8x16_t vrev32q_u8(uint8x16_t __p0) {
  uint8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev0, 3, 2, 1, 0, 7, 6, 5, 4, 11, 10, 9, 8, 15, 14, 13, 12);
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x8_t vrev32q_u16(uint16x8_t __p0) {
  uint16x8_t __ret;
  __ret = __builtin_shufflevector(__p0, __p0, 1, 0, 3, 2, 5, 4, 7, 6);
  return __ret;
}
#else
__ai uint16x8_t vrev32q_u16(uint16x8_t __p0) {
  uint16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev0, 1, 0, 3, 2, 5, 4, 7, 6);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x16_t vrev32q_s8(int8x16_t __p0) {
  int8x16_t __ret;
  __ret = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0, 7, 6, 5, 4, 11, 10, 9, 8, 15, 14, 13, 12);
  return __ret;
}
#else
__ai int8x16_t vrev32q_s8(int8x16_t __p0) {
  int8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev0, 3, 2, 1, 0, 7, 6, 5, 4, 11, 10, 9, 8, 15, 14, 13, 12);
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x8_t vrev32q_s16(int16x8_t __p0) {
  int16x8_t __ret;
  __ret = __builtin_shufflevector(__p0, __p0, 1, 0, 3, 2, 5, 4, 7, 6);
  return __ret;
}
#else
__ai int16x8_t vrev32q_s16(int16x8_t __p0) {
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev0, 1, 0, 3, 2, 5, 4, 7, 6);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x8_t vrev32_u8(uint8x8_t __p0) {
  uint8x8_t __ret;
  __ret = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0, 7, 6, 5, 4);
  return __ret;
}
#else
__ai uint8x8_t vrev32_u8(uint8x8_t __p0) {
  uint8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev0, 3, 2, 1, 0, 7, 6, 5, 4);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x4_t vrev32_u16(uint16x4_t __p0) {
  uint16x4_t __ret;
  __ret = __builtin_shufflevector(__p0, __p0, 1, 0, 3, 2);
  return __ret;
}
#else
__ai uint16x4_t vrev32_u16(uint16x4_t __p0) {
  uint16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint16x4_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev0, 1, 0, 3, 2);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x8_t vrev32_s8(int8x8_t __p0) {
  int8x8_t __ret;
  __ret = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0, 7, 6, 5, 4);
  return __ret;
}
#else
__ai int8x8_t vrev32_s8(int8x8_t __p0) {
  int8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev0, 3, 2, 1, 0, 7, 6, 5, 4);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x4_t vrev32_s16(int16x4_t __p0) {
  int16x4_t __ret;
  __ret = __builtin_shufflevector(__p0, __p0, 1, 0, 3, 2);
  return __ret;
}
#else
__ai int16x4_t vrev32_s16(int16x4_t __p0) {
  int16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int16x4_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev0, 1, 0, 3, 2);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x8_t vrev64_p8(poly8x8_t __p0) {
  poly8x8_t __ret;
  __ret = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#else
__ai poly8x8_t vrev64_p8(poly8x8_t __p0) {
  poly8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  poly8x8_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev0, 7, 6, 5, 4, 3, 2, 1, 0);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly16x4_t vrev64_p16(poly16x4_t __p0) {
  poly16x4_t __ret;
  __ret = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  return __ret;
}
#else
__ai poly16x4_t vrev64_p16(poly16x4_t __p0) {
  poly16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  poly16x4_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev0, 3, 2, 1, 0);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x16_t vrev64q_p8(poly8x16_t __p0) {
  poly8x16_t __ret;
  __ret = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0, 15, 14, 13, 12, 11, 10, 9, 8);
  return __ret;
}
#else
__ai poly8x16_t vrev64q_p8(poly8x16_t __p0) {
  poly8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  poly8x16_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev0, 7, 6, 5, 4, 3, 2, 1, 0, 15, 14, 13, 12, 11, 10, 9, 8);
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly16x8_t vrev64q_p16(poly16x8_t __p0) {
  poly16x8_t __ret;
  __ret = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0, 7, 6, 5, 4);
  return __ret;
}
#else
__ai poly16x8_t vrev64q_p16(poly16x8_t __p0) {
  poly16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  poly16x8_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev0, 3, 2, 1, 0, 7, 6, 5, 4);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x16_t vrev64q_u8(uint8x16_t __p0) {
  uint8x16_t __ret;
  __ret = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0, 15, 14, 13, 12, 11, 10, 9, 8);
  return __ret;
}
#else
__ai uint8x16_t vrev64q_u8(uint8x16_t __p0) {
  uint8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev0, 7, 6, 5, 4, 3, 2, 1, 0, 15, 14, 13, 12, 11, 10, 9, 8);
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vrev64q_u32(uint32x4_t __p0) {
  uint32x4_t __ret;
  __ret = __builtin_shufflevector(__p0, __p0, 1, 0, 3, 2);
  return __ret;
}
#else
__ai uint32x4_t vrev64q_u32(uint32x4_t __p0) {
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint32x4_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev0, 1, 0, 3, 2);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x8_t vrev64q_u16(uint16x8_t __p0) {
  uint16x8_t __ret;
  __ret = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0, 7, 6, 5, 4);
  return __ret;
}
#else
__ai uint16x8_t vrev64q_u16(uint16x8_t __p0) {
  uint16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev0, 3, 2, 1, 0, 7, 6, 5, 4);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x16_t vrev64q_s8(int8x16_t __p0) {
  int8x16_t __ret;
  __ret = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0, 15, 14, 13, 12, 11, 10, 9, 8);
  return __ret;
}
#else
__ai int8x16_t vrev64q_s8(int8x16_t __p0) {
  int8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev0, 7, 6, 5, 4, 3, 2, 1, 0, 15, 14, 13, 12, 11, 10, 9, 8);
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x4_t vrev64q_f32(float32x4_t __p0) {
  float32x4_t __ret;
  __ret = __builtin_shufflevector(__p0, __p0, 1, 0, 3, 2);
  return __ret;
}
#else
__ai float32x4_t vrev64q_f32(float32x4_t __p0) {
  float32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  float32x4_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev0, 1, 0, 3, 2);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vrev64q_s32(int32x4_t __p0) {
  int32x4_t __ret;
  __ret = __builtin_shufflevector(__p0, __p0, 1, 0, 3, 2);
  return __ret;
}
#else
__ai int32x4_t vrev64q_s32(int32x4_t __p0) {
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int32x4_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev0, 1, 0, 3, 2);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x8_t vrev64q_s16(int16x8_t __p0) {
  int16x8_t __ret;
  __ret = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0, 7, 6, 5, 4);
  return __ret;
}
#else
__ai int16x8_t vrev64q_s16(int16x8_t __p0) {
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev0, 3, 2, 1, 0, 7, 6, 5, 4);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x8_t vrev64_u8(uint8x8_t __p0) {
  uint8x8_t __ret;
  __ret = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#else
__ai uint8x8_t vrev64_u8(uint8x8_t __p0) {
  uint8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev0, 7, 6, 5, 4, 3, 2, 1, 0);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x2_t vrev64_u32(uint32x2_t __p0) {
  uint32x2_t __ret;
  __ret = __builtin_shufflevector(__p0, __p0, 1, 0);
  return __ret;
}
#else
__ai uint32x2_t vrev64_u32(uint32x2_t __p0) {
  uint32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint32x2_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev0, 1, 0);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x4_t vrev64_u16(uint16x4_t __p0) {
  uint16x4_t __ret;
  __ret = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  return __ret;
}
#else
__ai uint16x4_t vrev64_u16(uint16x4_t __p0) {
  uint16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint16x4_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev0, 3, 2, 1, 0);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x8_t vrev64_s8(int8x8_t __p0) {
  int8x8_t __ret;
  __ret = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#else
__ai int8x8_t vrev64_s8(int8x8_t __p0) {
  int8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev0, 7, 6, 5, 4, 3, 2, 1, 0);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x2_t vrev64_f32(float32x2_t __p0) {
  float32x2_t __ret;
  __ret = __builtin_shufflevector(__p0, __p0, 1, 0);
  return __ret;
}
#else
__ai float32x2_t vrev64_f32(float32x2_t __p0) {
  float32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float32x2_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev0, 1, 0);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x2_t vrev64_s32(int32x2_t __p0) {
  int32x2_t __ret;
  __ret = __builtin_shufflevector(__p0, __p0, 1, 0);
  return __ret;
}
#else
__ai int32x2_t vrev64_s32(int32x2_t __p0) {
  int32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int32x2_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev0, 1, 0);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x4_t vrev64_s16(int16x4_t __p0) {
  int16x4_t __ret;
  __ret = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  return __ret;
}
#else
__ai int16x4_t vrev64_s16(int16x4_t __p0) {
  int16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int16x4_t __ret;
  __ret = __builtin_shufflevector(__rev0, __rev0, 3, 2, 1, 0);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x16_t vrhaddq_u8(uint8x16_t __p0, uint8x16_t __p1) {
  uint8x16_t __ret;
  __ret = (uint8x16_t) __builtin_neon_vrhaddq_v((int8x16_t)__p0, (int8x16_t)__p1, 48);
  return __ret;
}
#else
__ai uint8x16_t vrhaddq_u8(uint8x16_t __p0, uint8x16_t __p1) {
  uint8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __ret;
  __ret = (uint8x16_t) __builtin_neon_vrhaddq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 48);
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vrhaddq_u32(uint32x4_t __p0, uint32x4_t __p1) {
  uint32x4_t __ret;
  __ret = (uint32x4_t) __builtin_neon_vrhaddq_v((int8x16_t)__p0, (int8x16_t)__p1, 50);
  return __ret;
}
#else
__ai uint32x4_t vrhaddq_u32(uint32x4_t __p0, uint32x4_t __p1) {
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint32x4_t __ret;
  __ret = (uint32x4_t) __builtin_neon_vrhaddq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 50);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x8_t vrhaddq_u16(uint16x8_t __p0, uint16x8_t __p1) {
  uint16x8_t __ret;
  __ret = (uint16x8_t) __builtin_neon_vrhaddq_v((int8x16_t)__p0, (int8x16_t)__p1, 49);
  return __ret;
}
#else
__ai uint16x8_t vrhaddq_u16(uint16x8_t __p0, uint16x8_t __p1) {
  uint16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __ret;
  __ret = (uint16x8_t) __builtin_neon_vrhaddq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 49);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x16_t vrhaddq_s8(int8x16_t __p0, int8x16_t __p1) {
  int8x16_t __ret;
  __ret = (int8x16_t) __builtin_neon_vrhaddq_v((int8x16_t)__p0, (int8x16_t)__p1, 32);
  return __ret;
}
#else
__ai int8x16_t vrhaddq_s8(int8x16_t __p0, int8x16_t __p1) {
  int8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __ret;
  __ret = (int8x16_t) __builtin_neon_vrhaddq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 32);
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vrhaddq_s32(int32x4_t __p0, int32x4_t __p1) {
  int32x4_t __ret;
  __ret = (int32x4_t) __builtin_neon_vrhaddq_v((int8x16_t)__p0, (int8x16_t)__p1, 34);
  return __ret;
}
#else
__ai int32x4_t vrhaddq_s32(int32x4_t __p0, int32x4_t __p1) {
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int32x4_t __ret;
  __ret = (int32x4_t) __builtin_neon_vrhaddq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 34);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x8_t vrhaddq_s16(int16x8_t __p0, int16x8_t __p1) {
  int16x8_t __ret;
  __ret = (int16x8_t) __builtin_neon_vrhaddq_v((int8x16_t)__p0, (int8x16_t)__p1, 33);
  return __ret;
}
#else
__ai int16x8_t vrhaddq_s16(int16x8_t __p0, int16x8_t __p1) {
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __ret;
  __ret = (int16x8_t) __builtin_neon_vrhaddq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 33);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x8_t vrhadd_u8(uint8x8_t __p0, uint8x8_t __p1) {
  uint8x8_t __ret;
  __ret = (uint8x8_t) __builtin_neon_vrhadd_v((int8x8_t)__p0, (int8x8_t)__p1, 16);
  return __ret;
}
#else
__ai uint8x8_t vrhadd_u8(uint8x8_t __p0, uint8x8_t __p1) {
  uint8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __ret;
  __ret = (uint8x8_t) __builtin_neon_vrhadd_v((int8x8_t)__rev0, (int8x8_t)__rev1, 16);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x2_t vrhadd_u32(uint32x2_t __p0, uint32x2_t __p1) {
  uint32x2_t __ret;
  __ret = (uint32x2_t) __builtin_neon_vrhadd_v((int8x8_t)__p0, (int8x8_t)__p1, 18);
  return __ret;
}
#else
__ai uint32x2_t vrhadd_u32(uint32x2_t __p0, uint32x2_t __p1) {
  uint32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  uint32x2_t __ret;
  __ret = (uint32x2_t) __builtin_neon_vrhadd_v((int8x8_t)__rev0, (int8x8_t)__rev1, 18);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x4_t vrhadd_u16(uint16x4_t __p0, uint16x4_t __p1) {
  uint16x4_t __ret;
  __ret = (uint16x4_t) __builtin_neon_vrhadd_v((int8x8_t)__p0, (int8x8_t)__p1, 17);
  return __ret;
}
#else
__ai uint16x4_t vrhadd_u16(uint16x4_t __p0, uint16x4_t __p1) {
  uint16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint16x4_t __ret;
  __ret = (uint16x4_t) __builtin_neon_vrhadd_v((int8x8_t)__rev0, (int8x8_t)__rev1, 17);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x8_t vrhadd_s8(int8x8_t __p0, int8x8_t __p1) {
  int8x8_t __ret;
  __ret = (int8x8_t) __builtin_neon_vrhadd_v((int8x8_t)__p0, (int8x8_t)__p1, 0);
  return __ret;
}
#else
__ai int8x8_t vrhadd_s8(int8x8_t __p0, int8x8_t __p1) {
  int8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __ret;
  __ret = (int8x8_t) __builtin_neon_vrhadd_v((int8x8_t)__rev0, (int8x8_t)__rev1, 0);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x2_t vrhadd_s32(int32x2_t __p0, int32x2_t __p1) {
  int32x2_t __ret;
  __ret = (int32x2_t) __builtin_neon_vrhadd_v((int8x8_t)__p0, (int8x8_t)__p1, 2);
  return __ret;
}
#else
__ai int32x2_t vrhadd_s32(int32x2_t __p0, int32x2_t __p1) {
  int32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  int32x2_t __ret;
  __ret = (int32x2_t) __builtin_neon_vrhadd_v((int8x8_t)__rev0, (int8x8_t)__rev1, 2);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x4_t vrhadd_s16(int16x4_t __p0, int16x4_t __p1) {
  int16x4_t __ret;
  __ret = (int16x4_t) __builtin_neon_vrhadd_v((int8x8_t)__p0, (int8x8_t)__p1, 1);
  return __ret;
}
#else
__ai int16x4_t vrhadd_s16(int16x4_t __p0, int16x4_t __p1) {
  int16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int16x4_t __ret;
  __ret = (int16x4_t) __builtin_neon_vrhadd_v((int8x8_t)__rev0, (int8x8_t)__rev1, 1);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x16_t vrshlq_u8(uint8x16_t __p0, int8x16_t __p1) {
  uint8x16_t __ret;
  __ret = (uint8x16_t) __builtin_neon_vrshlq_v((int8x16_t)__p0, (int8x16_t)__p1, 48);
  return __ret;
}
#else
__ai uint8x16_t vrshlq_u8(uint8x16_t __p0, int8x16_t __p1) {
  uint8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __ret;
  __ret = (uint8x16_t) __builtin_neon_vrshlq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 48);
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vrshlq_u32(uint32x4_t __p0, int32x4_t __p1) {
  uint32x4_t __ret;
  __ret = (uint32x4_t) __builtin_neon_vrshlq_v((int8x16_t)__p0, (int8x16_t)__p1, 50);
  return __ret;
}
#else
__ai uint32x4_t vrshlq_u32(uint32x4_t __p0, int32x4_t __p1) {
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint32x4_t __ret;
  __ret = (uint32x4_t) __builtin_neon_vrshlq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 50);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vrshlq_u64(uint64x2_t __p0, int64x2_t __p1) {
  uint64x2_t __ret;
  __ret = (uint64x2_t) __builtin_neon_vrshlq_v((int8x16_t)__p0, (int8x16_t)__p1, 51);
  return __ret;
}
#else
__ai uint64x2_t vrshlq_u64(uint64x2_t __p0, int64x2_t __p1) {
  uint64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  uint64x2_t __ret;
  __ret = (uint64x2_t) __builtin_neon_vrshlq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 51);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x8_t vrshlq_u16(uint16x8_t __p0, int16x8_t __p1) {
  uint16x8_t __ret;
  __ret = (uint16x8_t) __builtin_neon_vrshlq_v((int8x16_t)__p0, (int8x16_t)__p1, 49);
  return __ret;
}
#else
__ai uint16x8_t vrshlq_u16(uint16x8_t __p0, int16x8_t __p1) {
  uint16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __ret;
  __ret = (uint16x8_t) __builtin_neon_vrshlq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 49);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x16_t vrshlq_s8(int8x16_t __p0, int8x16_t __p1) {
  int8x16_t __ret;
  __ret = (int8x16_t) __builtin_neon_vrshlq_v((int8x16_t)__p0, (int8x16_t)__p1, 32);
  return __ret;
}
#else
__ai int8x16_t vrshlq_s8(int8x16_t __p0, int8x16_t __p1) {
  int8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __ret;
  __ret = (int8x16_t) __builtin_neon_vrshlq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 32);
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vrshlq_s32(int32x4_t __p0, int32x4_t __p1) {
  int32x4_t __ret;
  __ret = (int32x4_t) __builtin_neon_vrshlq_v((int8x16_t)__p0, (int8x16_t)__p1, 34);
  return __ret;
}
#else
__ai int32x4_t vrshlq_s32(int32x4_t __p0, int32x4_t __p1) {
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int32x4_t __ret;
  __ret = (int32x4_t) __builtin_neon_vrshlq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 34);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x2_t vrshlq_s64(int64x2_t __p0, int64x2_t __p1) {
  int64x2_t __ret;
  __ret = (int64x2_t) __builtin_neon_vrshlq_v((int8x16_t)__p0, (int8x16_t)__p1, 35);
  return __ret;
}
#else
__ai int64x2_t vrshlq_s64(int64x2_t __p0, int64x2_t __p1) {
  int64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  int64x2_t __ret;
  __ret = (int64x2_t) __builtin_neon_vrshlq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 35);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x8_t vrshlq_s16(int16x8_t __p0, int16x8_t __p1) {
  int16x8_t __ret;
  __ret = (int16x8_t) __builtin_neon_vrshlq_v((int8x16_t)__p0, (int8x16_t)__p1, 33);
  return __ret;
}
#else
__ai int16x8_t vrshlq_s16(int16x8_t __p0, int16x8_t __p1) {
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __ret;
  __ret = (int16x8_t) __builtin_neon_vrshlq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 33);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x8_t vrshl_u8(uint8x8_t __p0, int8x8_t __p1) {
  uint8x8_t __ret;
  __ret = (uint8x8_t) __builtin_neon_vrshl_v((int8x8_t)__p0, (int8x8_t)__p1, 16);
  return __ret;
}
#else
__ai uint8x8_t vrshl_u8(uint8x8_t __p0, int8x8_t __p1) {
  uint8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __ret;
  __ret = (uint8x8_t) __builtin_neon_vrshl_v((int8x8_t)__rev0, (int8x8_t)__rev1, 16);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x2_t vrshl_u32(uint32x2_t __p0, int32x2_t __p1) {
  uint32x2_t __ret;
  __ret = (uint32x2_t) __builtin_neon_vrshl_v((int8x8_t)__p0, (int8x8_t)__p1, 18);
  return __ret;
}
#else
__ai uint32x2_t vrshl_u32(uint32x2_t __p0, int32x2_t __p1) {
  uint32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  uint32x2_t __ret;
  __ret = (uint32x2_t) __builtin_neon_vrshl_v((int8x8_t)__rev0, (int8x8_t)__rev1, 18);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x1_t vrshl_u64(uint64x1_t __p0, int64x1_t __p1) {
  uint64x1_t __ret;
  __ret = (uint64x1_t) __builtin_neon_vrshl_v((int8x8_t)__p0, (int8x8_t)__p1, 19);
  return __ret;
}
#else
__ai uint64x1_t vrshl_u64(uint64x1_t __p0, int64x1_t __p1) {
  uint64x1_t __ret;
  __ret = (uint64x1_t) __builtin_neon_vrshl_v((int8x8_t)__p0, (int8x8_t)__p1, 19);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x4_t vrshl_u16(uint16x4_t __p0, int16x4_t __p1) {
  uint16x4_t __ret;
  __ret = (uint16x4_t) __builtin_neon_vrshl_v((int8x8_t)__p0, (int8x8_t)__p1, 17);
  return __ret;
}
#else
__ai uint16x4_t vrshl_u16(uint16x4_t __p0, int16x4_t __p1) {
  uint16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint16x4_t __ret;
  __ret = (uint16x4_t) __builtin_neon_vrshl_v((int8x8_t)__rev0, (int8x8_t)__rev1, 17);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x8_t vrshl_s8(int8x8_t __p0, int8x8_t __p1) {
  int8x8_t __ret;
  __ret = (int8x8_t) __builtin_neon_vrshl_v((int8x8_t)__p0, (int8x8_t)__p1, 0);
  return __ret;
}
#else
__ai int8x8_t vrshl_s8(int8x8_t __p0, int8x8_t __p1) {
  int8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __ret;
  __ret = (int8x8_t) __builtin_neon_vrshl_v((int8x8_t)__rev0, (int8x8_t)__rev1, 0);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x2_t vrshl_s32(int32x2_t __p0, int32x2_t __p1) {
  int32x2_t __ret;
  __ret = (int32x2_t) __builtin_neon_vrshl_v((int8x8_t)__p0, (int8x8_t)__p1, 2);
  return __ret;
}
#else
__ai int32x2_t vrshl_s32(int32x2_t __p0, int32x2_t __p1) {
  int32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  int32x2_t __ret;
  __ret = (int32x2_t) __builtin_neon_vrshl_v((int8x8_t)__rev0, (int8x8_t)__rev1, 2);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x1_t vrshl_s64(int64x1_t __p0, int64x1_t __p1) {
  int64x1_t __ret;
  __ret = (int64x1_t) __builtin_neon_vrshl_v((int8x8_t)__p0, (int8x8_t)__p1, 3);
  return __ret;
}
#else
__ai int64x1_t vrshl_s64(int64x1_t __p0, int64x1_t __p1) {
  int64x1_t __ret;
  __ret = (int64x1_t) __builtin_neon_vrshl_v((int8x8_t)__p0, (int8x8_t)__p1, 3);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x4_t vrshl_s16(int16x4_t __p0, int16x4_t __p1) {
  int16x4_t __ret;
  __ret = (int16x4_t) __builtin_neon_vrshl_v((int8x8_t)__p0, (int8x8_t)__p1, 1);
  return __ret;
}
#else
__ai int16x4_t vrshl_s16(int16x4_t __p0, int16x4_t __p1) {
  int16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int16x4_t __ret;
  __ret = (int16x4_t) __builtin_neon_vrshl_v((int8x8_t)__rev0, (int8x8_t)__rev1, 1);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
#define vrshrq_n_u8(__p0, __p1) __extension__ ({ \
  uint8x16_t __s0 = __p0; \
  uint8x16_t __ret; \
  __ret = (uint8x16_t) __builtin_neon_vrshrq_n_v((int8x16_t)__s0, __p1, 48); \
  __ret; \
})
#else
#define vrshrq_n_u8(__p0, __p1) __extension__ ({ \
  uint8x16_t __s0 = __p0; \
  uint8x16_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint8x16_t __ret; \
  __ret = (uint8x16_t) __builtin_neon_vrshrq_n_v((int8x16_t)__rev0, __p1, 48); \
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vrshrq_n_u32(__p0, __p1) __extension__ ({ \
  uint32x4_t __s0 = __p0; \
  uint32x4_t __ret; \
  __ret = (uint32x4_t) __builtin_neon_vrshrq_n_v((int8x16_t)__s0, __p1, 50); \
  __ret; \
})
#else
#define vrshrq_n_u32(__p0, __p1) __extension__ ({ \
  uint32x4_t __s0 = __p0; \
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  uint32x4_t __ret; \
  __ret = (uint32x4_t) __builtin_neon_vrshrq_n_v((int8x16_t)__rev0, __p1, 50); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vrshrq_n_u64(__p0, __p1) __extension__ ({ \
  uint64x2_t __s0 = __p0; \
  uint64x2_t __ret; \
  __ret = (uint64x2_t) __builtin_neon_vrshrq_n_v((int8x16_t)__s0, __p1, 51); \
  __ret; \
})
#else
#define vrshrq_n_u64(__p0, __p1) __extension__ ({ \
  uint64x2_t __s0 = __p0; \
  uint64x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  uint64x2_t __ret; \
  __ret = (uint64x2_t) __builtin_neon_vrshrq_n_v((int8x16_t)__rev0, __p1, 51); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vrshrq_n_u16(__p0, __p1) __extension__ ({ \
  uint16x8_t __s0 = __p0; \
  uint16x8_t __ret; \
  __ret = (uint16x8_t) __builtin_neon_vrshrq_n_v((int8x16_t)__s0, __p1, 49); \
  __ret; \
})
#else
#define vrshrq_n_u16(__p0, __p1) __extension__ ({ \
  uint16x8_t __s0 = __p0; \
  uint16x8_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint16x8_t __ret; \
  __ret = (uint16x8_t) __builtin_neon_vrshrq_n_v((int8x16_t)__rev0, __p1, 49); \
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vrshrq_n_s8(__p0, __p1) __extension__ ({ \
  int8x16_t __s0 = __p0; \
  int8x16_t __ret; \
  __ret = (int8x16_t) __builtin_neon_vrshrq_n_v((int8x16_t)__s0, __p1, 32); \
  __ret; \
})
#else
#define vrshrq_n_s8(__p0, __p1) __extension__ ({ \
  int8x16_t __s0 = __p0; \
  int8x16_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  int8x16_t __ret; \
  __ret = (int8x16_t) __builtin_neon_vrshrq_n_v((int8x16_t)__rev0, __p1, 32); \
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vrshrq_n_s32(__p0, __p1) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  int32x4_t __ret; \
  __ret = (int32x4_t) __builtin_neon_vrshrq_n_v((int8x16_t)__s0, __p1, 34); \
  __ret; \
})
#else
#define vrshrq_n_s32(__p0, __p1) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  int32x4_t __ret; \
  __ret = (int32x4_t) __builtin_neon_vrshrq_n_v((int8x16_t)__rev0, __p1, 34); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vrshrq_n_s64(__p0, __p1) __extension__ ({ \
  int64x2_t __s0 = __p0; \
  int64x2_t __ret; \
  __ret = (int64x2_t) __builtin_neon_vrshrq_n_v((int8x16_t)__s0, __p1, 35); \
  __ret; \
})
#else
#define vrshrq_n_s64(__p0, __p1) __extension__ ({ \
  int64x2_t __s0 = __p0; \
  int64x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  int64x2_t __ret; \
  __ret = (int64x2_t) __builtin_neon_vrshrq_n_v((int8x16_t)__rev0, __p1, 35); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vrshrq_n_s16(__p0, __p1) __extension__ ({ \
  int16x8_t __s0 = __p0; \
  int16x8_t __ret; \
  __ret = (int16x8_t) __builtin_neon_vrshrq_n_v((int8x16_t)__s0, __p1, 33); \
  __ret; \
})
#else
#define vrshrq_n_s16(__p0, __p1) __extension__ ({ \
  int16x8_t __s0 = __p0; \
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 7, 6, 5, 4, 3, 2, 1, 0); \
  int16x8_t __ret; \
  __ret = (int16x8_t) __builtin_neon_vrshrq_n_v((int8x16_t)__rev0, __p1, 33); \
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vrshr_n_u8(__p0, __p1) __extension__ ({ \
  uint8x8_t __s0 = __p0; \
  uint8x8_t __ret; \
  __ret = (uint8x8_t) __builtin_neon_vrshr_n_v((int8x8_t)__s0, __p1, 16); \
  __ret; \
})
#else
#define vrshr_n_u8(__p0, __p1) __extension__ ({ \
  uint8x8_t __s0 = __p0; \
  uint8x8_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint8x8_t __ret; \
  __ret = (uint8x8_t) __builtin_neon_vrshr_n_v((int8x8_t)__rev0, __p1, 16); \
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vrshr_n_u32(__p0, __p1) __extension__ ({ \
  uint32x2_t __s0 = __p0; \
  uint32x2_t __ret; \
  __ret = (uint32x2_t) __builtin_neon_vrshr_n_v((int8x8_t)__s0, __p1, 18); \
  __ret; \
})
#else
#define vrshr_n_u32(__p0, __p1) __extension__ ({ \
  uint32x2_t __s0 = __p0; \
  uint32x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  uint32x2_t __ret; \
  __ret = (uint32x2_t) __builtin_neon_vrshr_n_v((int8x8_t)__rev0, __p1, 18); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vrshr_n_u64(__p0, __p1) __extension__ ({ \
  uint64x1_t __s0 = __p0; \
  uint64x1_t __ret; \
  __ret = (uint64x1_t) __builtin_neon_vrshr_n_v((int8x8_t)__s0, __p1, 19); \
  __ret; \
})
#else
#define vrshr_n_u64(__p0, __p1) __extension__ ({ \
  uint64x1_t __s0 = __p0; \
  uint64x1_t __ret; \
  __ret = (uint64x1_t) __builtin_neon_vrshr_n_v((int8x8_t)__s0, __p1, 19); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vrshr_n_u16(__p0, __p1) __extension__ ({ \
  uint16x4_t __s0 = __p0; \
  uint16x4_t __ret; \
  __ret = (uint16x4_t) __builtin_neon_vrshr_n_v((int8x8_t)__s0, __p1, 17); \
  __ret; \
})
#else
#define vrshr_n_u16(__p0, __p1) __extension__ ({ \
  uint16x4_t __s0 = __p0; \
  uint16x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  uint16x4_t __ret; \
  __ret = (uint16x4_t) __builtin_neon_vrshr_n_v((int8x8_t)__rev0, __p1, 17); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vrshr_n_s8(__p0, __p1) __extension__ ({ \
  int8x8_t __s0 = __p0; \
  int8x8_t __ret; \
  __ret = (int8x8_t) __builtin_neon_vrshr_n_v((int8x8_t)__s0, __p1, 0); \
  __ret; \
})
#else
#define vrshr_n_s8(__p0, __p1) __extension__ ({ \
  int8x8_t __s0 = __p0; \
  int8x8_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 7, 6, 5, 4, 3, 2, 1, 0); \
  int8x8_t __ret; \
  __ret = (int8x8_t) __builtin_neon_vrshr_n_v((int8x8_t)__rev0, __p1, 0); \
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vrshr_n_s32(__p0, __p1) __extension__ ({ \
  int32x2_t __s0 = __p0; \
  int32x2_t __ret; \
  __ret = (int32x2_t) __builtin_neon_vrshr_n_v((int8x8_t)__s0, __p1, 2); \
  __ret; \
})
#else
#define vrshr_n_s32(__p0, __p1) __extension__ ({ \
  int32x2_t __s0 = __p0; \
  int32x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  int32x2_t __ret; \
  __ret = (int32x2_t) __builtin_neon_vrshr_n_v((int8x8_t)__rev0, __p1, 2); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vrshr_n_s64(__p0, __p1) __extension__ ({ \
  int64x1_t __s0 = __p0; \
  int64x1_t __ret; \
  __ret = (int64x1_t) __builtin_neon_vrshr_n_v((int8x8_t)__s0, __p1, 3); \
  __ret; \
})
#else
#define vrshr_n_s64(__p0, __p1) __extension__ ({ \
  int64x1_t __s0 = __p0; \
  int64x1_t __ret; \
  __ret = (int64x1_t) __builtin_neon_vrshr_n_v((int8x8_t)__s0, __p1, 3); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vrshr_n_s16(__p0, __p1) __extension__ ({ \
  int16x4_t __s0 = __p0; \
  int16x4_t __ret; \
  __ret = (int16x4_t) __builtin_neon_vrshr_n_v((int8x8_t)__s0, __p1, 1); \
  __ret; \
})
#else
#define vrshr_n_s16(__p0, __p1) __extension__ ({ \
  int16x4_t __s0 = __p0; \
  int16x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  int16x4_t __ret; \
  __ret = (int16x4_t) __builtin_neon_vrshr_n_v((int8x8_t)__rev0, __p1, 1); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vrshrn_n_u32(__p0, __p1) __extension__ ({ \
  uint32x4_t __s0 = __p0; \
  uint16x4_t __ret; \
  __ret = (uint16x4_t) __builtin_neon_vrshrn_n_v((int8x16_t)__s0, __p1, 17); \
  __ret; \
})
#else
#define vrshrn_n_u32(__p0, __p1) __extension__ ({ \
  uint32x4_t __s0 = __p0; \
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  uint16x4_t __ret; \
  __ret = (uint16x4_t) __builtin_neon_vrshrn_n_v((int8x16_t)__rev0, __p1, 17); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#define __noswap_vrshrn_n_u32(__p0, __p1) __extension__ ({ \
  uint32x4_t __s0 = __p0; \
  uint16x4_t __ret; \
  __ret = (uint16x4_t) __builtin_neon_vrshrn_n_v((int8x16_t)__s0, __p1, 17); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vrshrn_n_u64(__p0, __p1) __extension__ ({ \
  uint64x2_t __s0 = __p0; \
  uint32x2_t __ret; \
  __ret = (uint32x2_t) __builtin_neon_vrshrn_n_v((int8x16_t)__s0, __p1, 18); \
  __ret; \
})
#else
#define vrshrn_n_u64(__p0, __p1) __extension__ ({ \
  uint64x2_t __s0 = __p0; \
  uint64x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  uint32x2_t __ret; \
  __ret = (uint32x2_t) __builtin_neon_vrshrn_n_v((int8x16_t)__rev0, __p1, 18); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#define __noswap_vrshrn_n_u64(__p0, __p1) __extension__ ({ \
  uint64x2_t __s0 = __p0; \
  uint32x2_t __ret; \
  __ret = (uint32x2_t) __builtin_neon_vrshrn_n_v((int8x16_t)__s0, __p1, 18); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vrshrn_n_u16(__p0, __p1) __extension__ ({ \
  uint16x8_t __s0 = __p0; \
  uint8x8_t __ret; \
  __ret = (uint8x8_t) __builtin_neon_vrshrn_n_v((int8x16_t)__s0, __p1, 16); \
  __ret; \
})
#else
#define vrshrn_n_u16(__p0, __p1) __extension__ ({ \
  uint16x8_t __s0 = __p0; \
  uint16x8_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint8x8_t __ret; \
  __ret = (uint8x8_t) __builtin_neon_vrshrn_n_v((int8x16_t)__rev0, __p1, 16); \
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#define __noswap_vrshrn_n_u16(__p0, __p1) __extension__ ({ \
  uint16x8_t __s0 = __p0; \
  uint8x8_t __ret; \
  __ret = (uint8x8_t) __builtin_neon_vrshrn_n_v((int8x16_t)__s0, __p1, 16); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vrshrn_n_s32(__p0, __p1) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  int16x4_t __ret; \
  __ret = (int16x4_t) __builtin_neon_vrshrn_n_v((int8x16_t)__s0, __p1, 1); \
  __ret; \
})
#else
#define vrshrn_n_s32(__p0, __p1) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  int16x4_t __ret; \
  __ret = (int16x4_t) __builtin_neon_vrshrn_n_v((int8x16_t)__rev0, __p1, 1); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#define __noswap_vrshrn_n_s32(__p0, __p1) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  int16x4_t __ret; \
  __ret = (int16x4_t) __builtin_neon_vrshrn_n_v((int8x16_t)__s0, __p1, 1); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vrshrn_n_s64(__p0, __p1) __extension__ ({ \
  int64x2_t __s0 = __p0; \
  int32x2_t __ret; \
  __ret = (int32x2_t) __builtin_neon_vrshrn_n_v((int8x16_t)__s0, __p1, 2); \
  __ret; \
})
#else
#define vrshrn_n_s64(__p0, __p1) __extension__ ({ \
  int64x2_t __s0 = __p0; \
  int64x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  int32x2_t __ret; \
  __ret = (int32x2_t) __builtin_neon_vrshrn_n_v((int8x16_t)__rev0, __p1, 2); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#define __noswap_vrshrn_n_s64(__p0, __p1) __extension__ ({ \
  int64x2_t __s0 = __p0; \
  int32x2_t __ret; \
  __ret = (int32x2_t) __builtin_neon_vrshrn_n_v((int8x16_t)__s0, __p1, 2); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vrshrn_n_s16(__p0, __p1) __extension__ ({ \
  int16x8_t __s0 = __p0; \
  int8x8_t __ret; \
  __ret = (int8x8_t) __builtin_neon_vrshrn_n_v((int8x16_t)__s0, __p1, 0); \
  __ret; \
})
#else
#define vrshrn_n_s16(__p0, __p1) __extension__ ({ \
  int16x8_t __s0 = __p0; \
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 7, 6, 5, 4, 3, 2, 1, 0); \
  int8x8_t __ret; \
  __ret = (int8x8_t) __builtin_neon_vrshrn_n_v((int8x16_t)__rev0, __p1, 0); \
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#define __noswap_vrshrn_n_s16(__p0, __p1) __extension__ ({ \
  int16x8_t __s0 = __p0; \
  int8x8_t __ret; \
  __ret = (int8x8_t) __builtin_neon_vrshrn_n_v((int8x16_t)__s0, __p1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vrsqrteq_u32(uint32x4_t __p0) {
  uint32x4_t __ret;
  __ret = (uint32x4_t) __builtin_neon_vrsqrteq_v((int8x16_t)__p0, 50);
  return __ret;
}
#else
__ai uint32x4_t vrsqrteq_u32(uint32x4_t __p0) {
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint32x4_t __ret;
  __ret = (uint32x4_t) __builtin_neon_vrsqrteq_v((int8x16_t)__rev0, 50);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x4_t vrsqrteq_f32(float32x4_t __p0) {
  float32x4_t __ret;
  __ret = (float32x4_t) __builtin_neon_vrsqrteq_v((int8x16_t)__p0, 41);
  return __ret;
}
#else
__ai float32x4_t vrsqrteq_f32(float32x4_t __p0) {
  float32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  float32x4_t __ret;
  __ret = (float32x4_t) __builtin_neon_vrsqrteq_v((int8x16_t)__rev0, 41);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x2_t vrsqrte_u32(uint32x2_t __p0) {
  uint32x2_t __ret;
  __ret = (uint32x2_t) __builtin_neon_vrsqrte_v((int8x8_t)__p0, 18);
  return __ret;
}
#else
__ai uint32x2_t vrsqrte_u32(uint32x2_t __p0) {
  uint32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint32x2_t __ret;
  __ret = (uint32x2_t) __builtin_neon_vrsqrte_v((int8x8_t)__rev0, 18);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x2_t vrsqrte_f32(float32x2_t __p0) {
  float32x2_t __ret;
  __ret = (float32x2_t) __builtin_neon_vrsqrte_v((int8x8_t)__p0, 9);
  return __ret;
}
#else
__ai float32x2_t vrsqrte_f32(float32x2_t __p0) {
  float32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float32x2_t __ret;
  __ret = (float32x2_t) __builtin_neon_vrsqrte_v((int8x8_t)__rev0, 9);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x4_t vrsqrtsq_f32(float32x4_t __p0, float32x4_t __p1) {
  float32x4_t __ret;
  __ret = (float32x4_t) __builtin_neon_vrsqrtsq_v((int8x16_t)__p0, (int8x16_t)__p1, 41);
  return __ret;
}
#else
__ai float32x4_t vrsqrtsq_f32(float32x4_t __p0, float32x4_t __p1) {
  float32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  float32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  float32x4_t __ret;
  __ret = (float32x4_t) __builtin_neon_vrsqrtsq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 41);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x2_t vrsqrts_f32(float32x2_t __p0, float32x2_t __p1) {
  float32x2_t __ret;
  __ret = (float32x2_t) __builtin_neon_vrsqrts_v((int8x8_t)__p0, (int8x8_t)__p1, 9);
  return __ret;
}
#else
__ai float32x2_t vrsqrts_f32(float32x2_t __p0, float32x2_t __p1) {
  float32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  float32x2_t __ret;
  __ret = (float32x2_t) __builtin_neon_vrsqrts_v((int8x8_t)__rev0, (int8x8_t)__rev1, 9);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
#define vrsraq_n_u8(__p0, __p1, __p2) __extension__ ({ \
  uint8x16_t __s0 = __p0; \
  uint8x16_t __s1 = __p1; \
  uint8x16_t __ret; \
  __ret = (uint8x16_t) __builtin_neon_vrsraq_n_v((int8x16_t)__s0, (int8x16_t)__s1, __p2, 48); \
  __ret; \
})
#else
#define vrsraq_n_u8(__p0, __p1, __p2) __extension__ ({ \
  uint8x16_t __s0 = __p0; \
  uint8x16_t __s1 = __p1; \
  uint8x16_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint8x16_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint8x16_t __ret; \
  __ret = (uint8x16_t) __builtin_neon_vrsraq_n_v((int8x16_t)__rev0, (int8x16_t)__rev1, __p2, 48); \
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vrsraq_n_u32(__p0, __p1, __p2) __extension__ ({ \
  uint32x4_t __s0 = __p0; \
  uint32x4_t __s1 = __p1; \
  uint32x4_t __ret; \
  __ret = (uint32x4_t) __builtin_neon_vrsraq_n_v((int8x16_t)__s0, (int8x16_t)__s1, __p2, 50); \
  __ret; \
})
#else
#define vrsraq_n_u32(__p0, __p1, __p2) __extension__ ({ \
  uint32x4_t __s0 = __p0; \
  uint32x4_t __s1 = __p1; \
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  uint32x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  uint32x4_t __ret; \
  __ret = (uint32x4_t) __builtin_neon_vrsraq_n_v((int8x16_t)__rev0, (int8x16_t)__rev1, __p2, 50); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vrsraq_n_u64(__p0, __p1, __p2) __extension__ ({ \
  uint64x2_t __s0 = __p0; \
  uint64x2_t __s1 = __p1; \
  uint64x2_t __ret; \
  __ret = (uint64x2_t) __builtin_neon_vrsraq_n_v((int8x16_t)__s0, (int8x16_t)__s1, __p2, 51); \
  __ret; \
})
#else
#define vrsraq_n_u64(__p0, __p1, __p2) __extension__ ({ \
  uint64x2_t __s0 = __p0; \
  uint64x2_t __s1 = __p1; \
  uint64x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  uint64x2_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 1, 0); \
  uint64x2_t __ret; \
  __ret = (uint64x2_t) __builtin_neon_vrsraq_n_v((int8x16_t)__rev0, (int8x16_t)__rev1, __p2, 51); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vrsraq_n_u16(__p0, __p1, __p2) __extension__ ({ \
  uint16x8_t __s0 = __p0; \
  uint16x8_t __s1 = __p1; \
  uint16x8_t __ret; \
  __ret = (uint16x8_t) __builtin_neon_vrsraq_n_v((int8x16_t)__s0, (int8x16_t)__s1, __p2, 49); \
  __ret; \
})
#else
#define vrsraq_n_u16(__p0, __p1, __p2) __extension__ ({ \
  uint16x8_t __s0 = __p0; \
  uint16x8_t __s1 = __p1; \
  uint16x8_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint16x8_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint16x8_t __ret; \
  __ret = (uint16x8_t) __builtin_neon_vrsraq_n_v((int8x16_t)__rev0, (int8x16_t)__rev1, __p2, 49); \
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vrsraq_n_s8(__p0, __p1, __p2) __extension__ ({ \
  int8x16_t __s0 = __p0; \
  int8x16_t __s1 = __p1; \
  int8x16_t __ret; \
  __ret = (int8x16_t) __builtin_neon_vrsraq_n_v((int8x16_t)__s0, (int8x16_t)__s1, __p2, 32); \
  __ret; \
})
#else
#define vrsraq_n_s8(__p0, __p1, __p2) __extension__ ({ \
  int8x16_t __s0 = __p0; \
  int8x16_t __s1 = __p1; \
  int8x16_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  int8x16_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  int8x16_t __ret; \
  __ret = (int8x16_t) __builtin_neon_vrsraq_n_v((int8x16_t)__rev0, (int8x16_t)__rev1, __p2, 32); \
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vrsraq_n_s32(__p0, __p1, __p2) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  int32x4_t __s1 = __p1; \
  int32x4_t __ret; \
  __ret = (int32x4_t) __builtin_neon_vrsraq_n_v((int8x16_t)__s0, (int8x16_t)__s1, __p2, 34); \
  __ret; \
})
#else
#define vrsraq_n_s32(__p0, __p1, __p2) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  int32x4_t __s1 = __p1; \
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  int32x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  int32x4_t __ret; \
  __ret = (int32x4_t) __builtin_neon_vrsraq_n_v((int8x16_t)__rev0, (int8x16_t)__rev1, __p2, 34); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vrsraq_n_s64(__p0, __p1, __p2) __extension__ ({ \
  int64x2_t __s0 = __p0; \
  int64x2_t __s1 = __p1; \
  int64x2_t __ret; \
  __ret = (int64x2_t) __builtin_neon_vrsraq_n_v((int8x16_t)__s0, (int8x16_t)__s1, __p2, 35); \
  __ret; \
})
#else
#define vrsraq_n_s64(__p0, __p1, __p2) __extension__ ({ \
  int64x2_t __s0 = __p0; \
  int64x2_t __s1 = __p1; \
  int64x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  int64x2_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 1, 0); \
  int64x2_t __ret; \
  __ret = (int64x2_t) __builtin_neon_vrsraq_n_v((int8x16_t)__rev0, (int8x16_t)__rev1, __p2, 35); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vrsraq_n_s16(__p0, __p1, __p2) __extension__ ({ \
  int16x8_t __s0 = __p0; \
  int16x8_t __s1 = __p1; \
  int16x8_t __ret; \
  __ret = (int16x8_t) __builtin_neon_vrsraq_n_v((int8x16_t)__s0, (int8x16_t)__s1, __p2, 33); \
  __ret; \
})
#else
#define vrsraq_n_s16(__p0, __p1, __p2) __extension__ ({ \
  int16x8_t __s0 = __p0; \
  int16x8_t __s1 = __p1; \
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 7, 6, 5, 4, 3, 2, 1, 0); \
  int16x8_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 7, 6, 5, 4, 3, 2, 1, 0); \
  int16x8_t __ret; \
  __ret = (int16x8_t) __builtin_neon_vrsraq_n_v((int8x16_t)__rev0, (int8x16_t)__rev1, __p2, 33); \
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vrsra_n_u8(__p0, __p1, __p2) __extension__ ({ \
  uint8x8_t __s0 = __p0; \
  uint8x8_t __s1 = __p1; \
  uint8x8_t __ret; \
  __ret = (uint8x8_t) __builtin_neon_vrsra_n_v((int8x8_t)__s0, (int8x8_t)__s1, __p2, 16); \
  __ret; \
})
#else
#define vrsra_n_u8(__p0, __p1, __p2) __extension__ ({ \
  uint8x8_t __s0 = __p0; \
  uint8x8_t __s1 = __p1; \
  uint8x8_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint8x8_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint8x8_t __ret; \
  __ret = (uint8x8_t) __builtin_neon_vrsra_n_v((int8x8_t)__rev0, (int8x8_t)__rev1, __p2, 16); \
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vrsra_n_u32(__p0, __p1, __p2) __extension__ ({ \
  uint32x2_t __s0 = __p0; \
  uint32x2_t __s1 = __p1; \
  uint32x2_t __ret; \
  __ret = (uint32x2_t) __builtin_neon_vrsra_n_v((int8x8_t)__s0, (int8x8_t)__s1, __p2, 18); \
  __ret; \
})
#else
#define vrsra_n_u32(__p0, __p1, __p2) __extension__ ({ \
  uint32x2_t __s0 = __p0; \
  uint32x2_t __s1 = __p1; \
  uint32x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  uint32x2_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 1, 0); \
  uint32x2_t __ret; \
  __ret = (uint32x2_t) __builtin_neon_vrsra_n_v((int8x8_t)__rev0, (int8x8_t)__rev1, __p2, 18); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vrsra_n_u64(__p0, __p1, __p2) __extension__ ({ \
  uint64x1_t __s0 = __p0; \
  uint64x1_t __s1 = __p1; \
  uint64x1_t __ret; \
  __ret = (uint64x1_t) __builtin_neon_vrsra_n_v((int8x8_t)__s0, (int8x8_t)__s1, __p2, 19); \
  __ret; \
})
#else
#define vrsra_n_u64(__p0, __p1, __p2) __extension__ ({ \
  uint64x1_t __s0 = __p0; \
  uint64x1_t __s1 = __p1; \
  uint64x1_t __ret; \
  __ret = (uint64x1_t) __builtin_neon_vrsra_n_v((int8x8_t)__s0, (int8x8_t)__s1, __p2, 19); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vrsra_n_u16(__p0, __p1, __p2) __extension__ ({ \
  uint16x4_t __s0 = __p0; \
  uint16x4_t __s1 = __p1; \
  uint16x4_t __ret; \
  __ret = (uint16x4_t) __builtin_neon_vrsra_n_v((int8x8_t)__s0, (int8x8_t)__s1, __p2, 17); \
  __ret; \
})
#else
#define vrsra_n_u16(__p0, __p1, __p2) __extension__ ({ \
  uint16x4_t __s0 = __p0; \
  uint16x4_t __s1 = __p1; \
  uint16x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  uint16x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  uint16x4_t __ret; \
  __ret = (uint16x4_t) __builtin_neon_vrsra_n_v((int8x8_t)__rev0, (int8x8_t)__rev1, __p2, 17); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vrsra_n_s8(__p0, __p1, __p2) __extension__ ({ \
  int8x8_t __s0 = __p0; \
  int8x8_t __s1 = __p1; \
  int8x8_t __ret; \
  __ret = (int8x8_t) __builtin_neon_vrsra_n_v((int8x8_t)__s0, (int8x8_t)__s1, __p2, 0); \
  __ret; \
})
#else
#define vrsra_n_s8(__p0, __p1, __p2) __extension__ ({ \
  int8x8_t __s0 = __p0; \
  int8x8_t __s1 = __p1; \
  int8x8_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 7, 6, 5, 4, 3, 2, 1, 0); \
  int8x8_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 7, 6, 5, 4, 3, 2, 1, 0); \
  int8x8_t __ret; \
  __ret = (int8x8_t) __builtin_neon_vrsra_n_v((int8x8_t)__rev0, (int8x8_t)__rev1, __p2, 0); \
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vrsra_n_s32(__p0, __p1, __p2) __extension__ ({ \
  int32x2_t __s0 = __p0; \
  int32x2_t __s1 = __p1; \
  int32x2_t __ret; \
  __ret = (int32x2_t) __builtin_neon_vrsra_n_v((int8x8_t)__s0, (int8x8_t)__s1, __p2, 2); \
  __ret; \
})
#else
#define vrsra_n_s32(__p0, __p1, __p2) __extension__ ({ \
  int32x2_t __s0 = __p0; \
  int32x2_t __s1 = __p1; \
  int32x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  int32x2_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 1, 0); \
  int32x2_t __ret; \
  __ret = (int32x2_t) __builtin_neon_vrsra_n_v((int8x8_t)__rev0, (int8x8_t)__rev1, __p2, 2); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vrsra_n_s64(__p0, __p1, __p2) __extension__ ({ \
  int64x1_t __s0 = __p0; \
  int64x1_t __s1 = __p1; \
  int64x1_t __ret; \
  __ret = (int64x1_t) __builtin_neon_vrsra_n_v((int8x8_t)__s0, (int8x8_t)__s1, __p2, 3); \
  __ret; \
})
#else
#define vrsra_n_s64(__p0, __p1, __p2) __extension__ ({ \
  int64x1_t __s0 = __p0; \
  int64x1_t __s1 = __p1; \
  int64x1_t __ret; \
  __ret = (int64x1_t) __builtin_neon_vrsra_n_v((int8x8_t)__s0, (int8x8_t)__s1, __p2, 3); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vrsra_n_s16(__p0, __p1, __p2) __extension__ ({ \
  int16x4_t __s0 = __p0; \
  int16x4_t __s1 = __p1; \
  int16x4_t __ret; \
  __ret = (int16x4_t) __builtin_neon_vrsra_n_v((int8x8_t)__s0, (int8x8_t)__s1, __p2, 1); \
  __ret; \
})
#else
#define vrsra_n_s16(__p0, __p1, __p2) __extension__ ({ \
  int16x4_t __s0 = __p0; \
  int16x4_t __s1 = __p1; \
  int16x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  int16x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  int16x4_t __ret; \
  __ret = (int16x4_t) __builtin_neon_vrsra_n_v((int8x8_t)__rev0, (int8x8_t)__rev1, __p2, 1); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x4_t vrsubhn_u32(uint32x4_t __p0, uint32x4_t __p1) {
  uint16x4_t __ret;
  __ret = (uint16x4_t) __builtin_neon_vrsubhn_v((int8x16_t)__p0, (int8x16_t)__p1, 17);
  return __ret;
}
#else
__ai uint16x4_t vrsubhn_u32(uint32x4_t __p0, uint32x4_t __p1) {
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint16x4_t __ret;
  __ret = (uint16x4_t) __builtin_neon_vrsubhn_v((int8x16_t)__rev0, (int8x16_t)__rev1, 17);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
__ai uint16x4_t __noswap_vrsubhn_u32(uint32x4_t __p0, uint32x4_t __p1) {
  uint16x4_t __ret;
  __ret = (uint16x4_t) __builtin_neon_vrsubhn_v((int8x16_t)__p0, (int8x16_t)__p1, 17);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x2_t vrsubhn_u64(uint64x2_t __p0, uint64x2_t __p1) {
  uint32x2_t __ret;
  __ret = (uint32x2_t) __builtin_neon_vrsubhn_v((int8x16_t)__p0, (int8x16_t)__p1, 18);
  return __ret;
}
#else
__ai uint32x2_t vrsubhn_u64(uint64x2_t __p0, uint64x2_t __p1) {
  uint64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  uint32x2_t __ret;
  __ret = (uint32x2_t) __builtin_neon_vrsubhn_v((int8x16_t)__rev0, (int8x16_t)__rev1, 18);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
__ai uint32x2_t __noswap_vrsubhn_u64(uint64x2_t __p0, uint64x2_t __p1) {
  uint32x2_t __ret;
  __ret = (uint32x2_t) __builtin_neon_vrsubhn_v((int8x16_t)__p0, (int8x16_t)__p1, 18);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x8_t vrsubhn_u16(uint16x8_t __p0, uint16x8_t __p1) {
  uint8x8_t __ret;
  __ret = (uint8x8_t) __builtin_neon_vrsubhn_v((int8x16_t)__p0, (int8x16_t)__p1, 16);
  return __ret;
}
#else
__ai uint8x8_t vrsubhn_u16(uint16x8_t __p0, uint16x8_t __p1) {
  uint16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __ret;
  __ret = (uint8x8_t) __builtin_neon_vrsubhn_v((int8x16_t)__rev0, (int8x16_t)__rev1, 16);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
__ai uint8x8_t __noswap_vrsubhn_u16(uint16x8_t __p0, uint16x8_t __p1) {
  uint8x8_t __ret;
  __ret = (uint8x8_t) __builtin_neon_vrsubhn_v((int8x16_t)__p0, (int8x16_t)__p1, 16);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x4_t vrsubhn_s32(int32x4_t __p0, int32x4_t __p1) {
  int16x4_t __ret;
  __ret = (int16x4_t) __builtin_neon_vrsubhn_v((int8x16_t)__p0, (int8x16_t)__p1, 1);
  return __ret;
}
#else
__ai int16x4_t vrsubhn_s32(int32x4_t __p0, int32x4_t __p1) {
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int16x4_t __ret;
  __ret = (int16x4_t) __builtin_neon_vrsubhn_v((int8x16_t)__rev0, (int8x16_t)__rev1, 1);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
__ai int16x4_t __noswap_vrsubhn_s32(int32x4_t __p0, int32x4_t __p1) {
  int16x4_t __ret;
  __ret = (int16x4_t) __builtin_neon_vrsubhn_v((int8x16_t)__p0, (int8x16_t)__p1, 1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x2_t vrsubhn_s64(int64x2_t __p0, int64x2_t __p1) {
  int32x2_t __ret;
  __ret = (int32x2_t) __builtin_neon_vrsubhn_v((int8x16_t)__p0, (int8x16_t)__p1, 2);
  return __ret;
}
#else
__ai int32x2_t vrsubhn_s64(int64x2_t __p0, int64x2_t __p1) {
  int64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  int32x2_t __ret;
  __ret = (int32x2_t) __builtin_neon_vrsubhn_v((int8x16_t)__rev0, (int8x16_t)__rev1, 2);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
__ai int32x2_t __noswap_vrsubhn_s64(int64x2_t __p0, int64x2_t __p1) {
  int32x2_t __ret;
  __ret = (int32x2_t) __builtin_neon_vrsubhn_v((int8x16_t)__p0, (int8x16_t)__p1, 2);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x8_t vrsubhn_s16(int16x8_t __p0, int16x8_t __p1) {
  int8x8_t __ret;
  __ret = (int8x8_t) __builtin_neon_vrsubhn_v((int8x16_t)__p0, (int8x16_t)__p1, 0);
  return __ret;
}
#else
__ai int8x8_t vrsubhn_s16(int16x8_t __p0, int16x8_t __p1) {
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __ret;
  __ret = (int8x8_t) __builtin_neon_vrsubhn_v((int8x16_t)__rev0, (int8x16_t)__rev1, 0);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
__ai int8x8_t __noswap_vrsubhn_s16(int16x8_t __p0, int16x8_t __p1) {
  int8x8_t __ret;
  __ret = (int8x8_t) __builtin_neon_vrsubhn_v((int8x16_t)__p0, (int8x16_t)__p1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
#define vset_lane_p8(__p0, __p1, __p2) __extension__ ({ \
  poly8_t __s0 = __p0; \
  poly8x8_t __s1 = __p1; \
  poly8x8_t __ret; \
  __ret = (poly8x8_t) __builtin_neon_vset_lane_i8(__s0, (int8x8_t)__s1, __p2); \
  __ret; \
})
#else
#define vset_lane_p8(__p0, __p1, __p2) __extension__ ({ \
  poly8_t __s0 = __p0; \
  poly8x8_t __s1 = __p1; \
  poly8x8_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 7, 6, 5, 4, 3, 2, 1, 0); \
  poly8x8_t __ret; \
  __ret = (poly8x8_t) __builtin_neon_vset_lane_i8(__s0, (int8x8_t)__rev1, __p2); \
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#define __noswap_vset_lane_p8(__p0, __p1, __p2) __extension__ ({ \
  poly8_t __s0 = __p0; \
  poly8x8_t __s1 = __p1; \
  poly8x8_t __ret; \
  __ret = (poly8x8_t) __builtin_neon_vset_lane_i8(__s0, (int8x8_t)__s1, __p2); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vset_lane_p16(__p0, __p1, __p2) __extension__ ({ \
  poly16_t __s0 = __p0; \
  poly16x4_t __s1 = __p1; \
  poly16x4_t __ret; \
  __ret = (poly16x4_t) __builtin_neon_vset_lane_i16(__s0, (int8x8_t)__s1, __p2); \
  __ret; \
})
#else
#define vset_lane_p16(__p0, __p1, __p2) __extension__ ({ \
  poly16_t __s0 = __p0; \
  poly16x4_t __s1 = __p1; \
  poly16x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  poly16x4_t __ret; \
  __ret = (poly16x4_t) __builtin_neon_vset_lane_i16(__s0, (int8x8_t)__rev1, __p2); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#define __noswap_vset_lane_p16(__p0, __p1, __p2) __extension__ ({ \
  poly16_t __s0 = __p0; \
  poly16x4_t __s1 = __p1; \
  poly16x4_t __ret; \
  __ret = (poly16x4_t) __builtin_neon_vset_lane_i16(__s0, (int8x8_t)__s1, __p2); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vsetq_lane_p8(__p0, __p1, __p2) __extension__ ({ \
  poly8_t __s0 = __p0; \
  poly8x16_t __s1 = __p1; \
  poly8x16_t __ret; \
  __ret = (poly8x16_t) __builtin_neon_vsetq_lane_i8(__s0, (int8x16_t)__s1, __p2); \
  __ret; \
})
#else
#define vsetq_lane_p8(__p0, __p1, __p2) __extension__ ({ \
  poly8_t __s0 = __p0; \
  poly8x16_t __s1 = __p1; \
  poly8x16_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  poly8x16_t __ret; \
  __ret = (poly8x16_t) __builtin_neon_vsetq_lane_i8(__s0, (int8x16_t)__rev1, __p2); \
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#define __noswap_vsetq_lane_p8(__p0, __p1, __p2) __extension__ ({ \
  poly8_t __s0 = __p0; \
  poly8x16_t __s1 = __p1; \
  poly8x16_t __ret; \
  __ret = (poly8x16_t) __builtin_neon_vsetq_lane_i8(__s0, (int8x16_t)__s1, __p2); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vsetq_lane_p16(__p0, __p1, __p2) __extension__ ({ \
  poly16_t __s0 = __p0; \
  poly16x8_t __s1 = __p1; \
  poly16x8_t __ret; \
  __ret = (poly16x8_t) __builtin_neon_vsetq_lane_i16(__s0, (int8x16_t)__s1, __p2); \
  __ret; \
})
#else
#define vsetq_lane_p16(__p0, __p1, __p2) __extension__ ({ \
  poly16_t __s0 = __p0; \
  poly16x8_t __s1 = __p1; \
  poly16x8_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 7, 6, 5, 4, 3, 2, 1, 0); \
  poly16x8_t __ret; \
  __ret = (poly16x8_t) __builtin_neon_vsetq_lane_i16(__s0, (int8x16_t)__rev1, __p2); \
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#define __noswap_vsetq_lane_p16(__p0, __p1, __p2) __extension__ ({ \
  poly16_t __s0 = __p0; \
  poly16x8_t __s1 = __p1; \
  poly16x8_t __ret; \
  __ret = (poly16x8_t) __builtin_neon_vsetq_lane_i16(__s0, (int8x16_t)__s1, __p2); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vsetq_lane_u8(__p0, __p1, __p2) __extension__ ({ \
  uint8_t __s0 = __p0; \
  uint8x16_t __s1 = __p1; \
  uint8x16_t __ret; \
  __ret = (uint8x16_t) __builtin_neon_vsetq_lane_i8(__s0, (int8x16_t)__s1, __p2); \
  __ret; \
})
#else
#define vsetq_lane_u8(__p0, __p1, __p2) __extension__ ({ \
  uint8_t __s0 = __p0; \
  uint8x16_t __s1 = __p1; \
  uint8x16_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint8x16_t __ret; \
  __ret = (uint8x16_t) __builtin_neon_vsetq_lane_i8(__s0, (int8x16_t)__rev1, __p2); \
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#define __noswap_vsetq_lane_u8(__p0, __p1, __p2) __extension__ ({ \
  uint8_t __s0 = __p0; \
  uint8x16_t __s1 = __p1; \
  uint8x16_t __ret; \
  __ret = (uint8x16_t) __builtin_neon_vsetq_lane_i8(__s0, (int8x16_t)__s1, __p2); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vsetq_lane_u32(__p0, __p1, __p2) __extension__ ({ \
  uint32_t __s0 = __p0; \
  uint32x4_t __s1 = __p1; \
  uint32x4_t __ret; \
  __ret = (uint32x4_t) __builtin_neon_vsetq_lane_i32(__s0, (int8x16_t)__s1, __p2); \
  __ret; \
})
#else
#define vsetq_lane_u32(__p0, __p1, __p2) __extension__ ({ \
  uint32_t __s0 = __p0; \
  uint32x4_t __s1 = __p1; \
  uint32x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  uint32x4_t __ret; \
  __ret = (uint32x4_t) __builtin_neon_vsetq_lane_i32(__s0, (int8x16_t)__rev1, __p2); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#define __noswap_vsetq_lane_u32(__p0, __p1, __p2) __extension__ ({ \
  uint32_t __s0 = __p0; \
  uint32x4_t __s1 = __p1; \
  uint32x4_t __ret; \
  __ret = (uint32x4_t) __builtin_neon_vsetq_lane_i32(__s0, (int8x16_t)__s1, __p2); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vsetq_lane_u64(__p0, __p1, __p2) __extension__ ({ \
  uint64_t __s0 = __p0; \
  uint64x2_t __s1 = __p1; \
  uint64x2_t __ret; \
  __ret = (uint64x2_t) __builtin_neon_vsetq_lane_i64(__s0, (int8x16_t)__s1, __p2); \
  __ret; \
})
#else
#define vsetq_lane_u64(__p0, __p1, __p2) __extension__ ({ \
  uint64_t __s0 = __p0; \
  uint64x2_t __s1 = __p1; \
  uint64x2_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 1, 0); \
  uint64x2_t __ret; \
  __ret = (uint64x2_t) __builtin_neon_vsetq_lane_i64(__s0, (int8x16_t)__rev1, __p2); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#define __noswap_vsetq_lane_u64(__p0, __p1, __p2) __extension__ ({ \
  uint64_t __s0 = __p0; \
  uint64x2_t __s1 = __p1; \
  uint64x2_t __ret; \
  __ret = (uint64x2_t) __builtin_neon_vsetq_lane_i64(__s0, (int8x16_t)__s1, __p2); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vsetq_lane_u16(__p0, __p1, __p2) __extension__ ({ \
  uint16_t __s0 = __p0; \
  uint16x8_t __s1 = __p1; \
  uint16x8_t __ret; \
  __ret = (uint16x8_t) __builtin_neon_vsetq_lane_i16(__s0, (int8x16_t)__s1, __p2); \
  __ret; \
})
#else
#define vsetq_lane_u16(__p0, __p1, __p2) __extension__ ({ \
  uint16_t __s0 = __p0; \
  uint16x8_t __s1 = __p1; \
  uint16x8_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint16x8_t __ret; \
  __ret = (uint16x8_t) __builtin_neon_vsetq_lane_i16(__s0, (int8x16_t)__rev1, __p2); \
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#define __noswap_vsetq_lane_u16(__p0, __p1, __p2) __extension__ ({ \
  uint16_t __s0 = __p0; \
  uint16x8_t __s1 = __p1; \
  uint16x8_t __ret; \
  __ret = (uint16x8_t) __builtin_neon_vsetq_lane_i16(__s0, (int8x16_t)__s1, __p2); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vsetq_lane_s8(__p0, __p1, __p2) __extension__ ({ \
  int8_t __s0 = __p0; \
  int8x16_t __s1 = __p1; \
  int8x16_t __ret; \
  __ret = (int8x16_t) __builtin_neon_vsetq_lane_i8(__s0, (int8x16_t)__s1, __p2); \
  __ret; \
})
#else
#define vsetq_lane_s8(__p0, __p1, __p2) __extension__ ({ \
  int8_t __s0 = __p0; \
  int8x16_t __s1 = __p1; \
  int8x16_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  int8x16_t __ret; \
  __ret = (int8x16_t) __builtin_neon_vsetq_lane_i8(__s0, (int8x16_t)__rev1, __p2); \
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#define __noswap_vsetq_lane_s8(__p0, __p1, __p2) __extension__ ({ \
  int8_t __s0 = __p0; \
  int8x16_t __s1 = __p1; \
  int8x16_t __ret; \
  __ret = (int8x16_t) __builtin_neon_vsetq_lane_i8(__s0, (int8x16_t)__s1, __p2); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vsetq_lane_f32(__p0, __p1, __p2) __extension__ ({ \
  float32_t __s0 = __p0; \
  float32x4_t __s1 = __p1; \
  float32x4_t __ret; \
  __ret = (float32x4_t) __builtin_neon_vsetq_lane_f32(__s0, (int8x16_t)__s1, __p2); \
  __ret; \
})
#else
#define vsetq_lane_f32(__p0, __p1, __p2) __extension__ ({ \
  float32_t __s0 = __p0; \
  float32x4_t __s1 = __p1; \
  float32x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  float32x4_t __ret; \
  __ret = (float32x4_t) __builtin_neon_vsetq_lane_f32(__s0, (int8x16_t)__rev1, __p2); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#define __noswap_vsetq_lane_f32(__p0, __p1, __p2) __extension__ ({ \
  float32_t __s0 = __p0; \
  float32x4_t __s1 = __p1; \
  float32x4_t __ret; \
  __ret = (float32x4_t) __builtin_neon_vsetq_lane_f32(__s0, (int8x16_t)__s1, __p2); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vsetq_lane_s32(__p0, __p1, __p2) __extension__ ({ \
  int32_t __s0 = __p0; \
  int32x4_t __s1 = __p1; \
  int32x4_t __ret; \
  __ret = (int32x4_t) __builtin_neon_vsetq_lane_i32(__s0, (int8x16_t)__s1, __p2); \
  __ret; \
})
#else
#define vsetq_lane_s32(__p0, __p1, __p2) __extension__ ({ \
  int32_t __s0 = __p0; \
  int32x4_t __s1 = __p1; \
  int32x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  int32x4_t __ret; \
  __ret = (int32x4_t) __builtin_neon_vsetq_lane_i32(__s0, (int8x16_t)__rev1, __p2); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#define __noswap_vsetq_lane_s32(__p0, __p1, __p2) __extension__ ({ \
  int32_t __s0 = __p0; \
  int32x4_t __s1 = __p1; \
  int32x4_t __ret; \
  __ret = (int32x4_t) __builtin_neon_vsetq_lane_i32(__s0, (int8x16_t)__s1, __p2); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vsetq_lane_s64(__p0, __p1, __p2) __extension__ ({ \
  int64_t __s0 = __p0; \
  int64x2_t __s1 = __p1; \
  int64x2_t __ret; \
  __ret = (int64x2_t) __builtin_neon_vsetq_lane_i64(__s0, (int8x16_t)__s1, __p2); \
  __ret; \
})
#else
#define vsetq_lane_s64(__p0, __p1, __p2) __extension__ ({ \
  int64_t __s0 = __p0; \
  int64x2_t __s1 = __p1; \
  int64x2_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 1, 0); \
  int64x2_t __ret; \
  __ret = (int64x2_t) __builtin_neon_vsetq_lane_i64(__s0, (int8x16_t)__rev1, __p2); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#define __noswap_vsetq_lane_s64(__p0, __p1, __p2) __extension__ ({ \
  int64_t __s0 = __p0; \
  int64x2_t __s1 = __p1; \
  int64x2_t __ret; \
  __ret = (int64x2_t) __builtin_neon_vsetq_lane_i64(__s0, (int8x16_t)__s1, __p2); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vsetq_lane_s16(__p0, __p1, __p2) __extension__ ({ \
  int16_t __s0 = __p0; \
  int16x8_t __s1 = __p1; \
  int16x8_t __ret; \
  __ret = (int16x8_t) __builtin_neon_vsetq_lane_i16(__s0, (int8x16_t)__s1, __p2); \
  __ret; \
})
#else
#define vsetq_lane_s16(__p0, __p1, __p2) __extension__ ({ \
  int16_t __s0 = __p0; \
  int16x8_t __s1 = __p1; \
  int16x8_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 7, 6, 5, 4, 3, 2, 1, 0); \
  int16x8_t __ret; \
  __ret = (int16x8_t) __builtin_neon_vsetq_lane_i16(__s0, (int8x16_t)__rev1, __p2); \
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#define __noswap_vsetq_lane_s16(__p0, __p1, __p2) __extension__ ({ \
  int16_t __s0 = __p0; \
  int16x8_t __s1 = __p1; \
  int16x8_t __ret; \
  __ret = (int16x8_t) __builtin_neon_vsetq_lane_i16(__s0, (int8x16_t)__s1, __p2); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vset_lane_u8(__p0, __p1, __p2) __extension__ ({ \
  uint8_t __s0 = __p0; \
  uint8x8_t __s1 = __p1; \
  uint8x8_t __ret; \
  __ret = (uint8x8_t) __builtin_neon_vset_lane_i8(__s0, (int8x8_t)__s1, __p2); \
  __ret; \
})
#else
#define vset_lane_u8(__p0, __p1, __p2) __extension__ ({ \
  uint8_t __s0 = __p0; \
  uint8x8_t __s1 = __p1; \
  uint8x8_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint8x8_t __ret; \
  __ret = (uint8x8_t) __builtin_neon_vset_lane_i8(__s0, (int8x8_t)__rev1, __p2); \
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#define __noswap_vset_lane_u8(__p0, __p1, __p2) __extension__ ({ \
  uint8_t __s0 = __p0; \
  uint8x8_t __s1 = __p1; \
  uint8x8_t __ret; \
  __ret = (uint8x8_t) __builtin_neon_vset_lane_i8(__s0, (int8x8_t)__s1, __p2); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vset_lane_u32(__p0, __p1, __p2) __extension__ ({ \
  uint32_t __s0 = __p0; \
  uint32x2_t __s1 = __p1; \
  uint32x2_t __ret; \
  __ret = (uint32x2_t) __builtin_neon_vset_lane_i32(__s0, (int8x8_t)__s1, __p2); \
  __ret; \
})
#else
#define vset_lane_u32(__p0, __p1, __p2) __extension__ ({ \
  uint32_t __s0 = __p0; \
  uint32x2_t __s1 = __p1; \
  uint32x2_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 1, 0); \
  uint32x2_t __ret; \
  __ret = (uint32x2_t) __builtin_neon_vset_lane_i32(__s0, (int8x8_t)__rev1, __p2); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#define __noswap_vset_lane_u32(__p0, __p1, __p2) __extension__ ({ \
  uint32_t __s0 = __p0; \
  uint32x2_t __s1 = __p1; \
  uint32x2_t __ret; \
  __ret = (uint32x2_t) __builtin_neon_vset_lane_i32(__s0, (int8x8_t)__s1, __p2); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vset_lane_u64(__p0, __p1, __p2) __extension__ ({ \
  uint64_t __s0 = __p0; \
  uint64x1_t __s1 = __p1; \
  uint64x1_t __ret; \
  __ret = (uint64x1_t) __builtin_neon_vset_lane_i64(__s0, (int8x8_t)__s1, __p2); \
  __ret; \
})
#else
#define vset_lane_u64(__p0, __p1, __p2) __extension__ ({ \
  uint64_t __s0 = __p0; \
  uint64x1_t __s1 = __p1; \
  uint64x1_t __ret; \
  __ret = (uint64x1_t) __builtin_neon_vset_lane_i64(__s0, (int8x8_t)__s1, __p2); \
  __ret; \
})
#define __noswap_vset_lane_u64(__p0, __p1, __p2) __extension__ ({ \
  uint64_t __s0 = __p0; \
  uint64x1_t __s1 = __p1; \
  uint64x1_t __ret; \
  __ret = (uint64x1_t) __builtin_neon_vset_lane_i64(__s0, (int8x8_t)__s1, __p2); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vset_lane_u16(__p0, __p1, __p2) __extension__ ({ \
  uint16_t __s0 = __p0; \
  uint16x4_t __s1 = __p1; \
  uint16x4_t __ret; \
  __ret = (uint16x4_t) __builtin_neon_vset_lane_i16(__s0, (int8x8_t)__s1, __p2); \
  __ret; \
})
#else
#define vset_lane_u16(__p0, __p1, __p2) __extension__ ({ \
  uint16_t __s0 = __p0; \
  uint16x4_t __s1 = __p1; \
  uint16x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  uint16x4_t __ret; \
  __ret = (uint16x4_t) __builtin_neon_vset_lane_i16(__s0, (int8x8_t)__rev1, __p2); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#define __noswap_vset_lane_u16(__p0, __p1, __p2) __extension__ ({ \
  uint16_t __s0 = __p0; \
  uint16x4_t __s1 = __p1; \
  uint16x4_t __ret; \
  __ret = (uint16x4_t) __builtin_neon_vset_lane_i16(__s0, (int8x8_t)__s1, __p2); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vset_lane_s8(__p0, __p1, __p2) __extension__ ({ \
  int8_t __s0 = __p0; \
  int8x8_t __s1 = __p1; \
  int8x8_t __ret; \
  __ret = (int8x8_t) __builtin_neon_vset_lane_i8(__s0, (int8x8_t)__s1, __p2); \
  __ret; \
})
#else
#define vset_lane_s8(__p0, __p1, __p2) __extension__ ({ \
  int8_t __s0 = __p0; \
  int8x8_t __s1 = __p1; \
  int8x8_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 7, 6, 5, 4, 3, 2, 1, 0); \
  int8x8_t __ret; \
  __ret = (int8x8_t) __builtin_neon_vset_lane_i8(__s0, (int8x8_t)__rev1, __p2); \
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#define __noswap_vset_lane_s8(__p0, __p1, __p2) __extension__ ({ \
  int8_t __s0 = __p0; \
  int8x8_t __s1 = __p1; \
  int8x8_t __ret; \
  __ret = (int8x8_t) __builtin_neon_vset_lane_i8(__s0, (int8x8_t)__s1, __p2); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vset_lane_f32(__p0, __p1, __p2) __extension__ ({ \
  float32_t __s0 = __p0; \
  float32x2_t __s1 = __p1; \
  float32x2_t __ret; \
  __ret = (float32x2_t) __builtin_neon_vset_lane_f32(__s0, (int8x8_t)__s1, __p2); \
  __ret; \
})
#else
#define vset_lane_f32(__p0, __p1, __p2) __extension__ ({ \
  float32_t __s0 = __p0; \
  float32x2_t __s1 = __p1; \
  float32x2_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 1, 0); \
  float32x2_t __ret; \
  __ret = (float32x2_t) __builtin_neon_vset_lane_f32(__s0, (int8x8_t)__rev1, __p2); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#define __noswap_vset_lane_f32(__p0, __p1, __p2) __extension__ ({ \
  float32_t __s0 = __p0; \
  float32x2_t __s1 = __p1; \
  float32x2_t __ret; \
  __ret = (float32x2_t) __builtin_neon_vset_lane_f32(__s0, (int8x8_t)__s1, __p2); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vset_lane_s32(__p0, __p1, __p2) __extension__ ({ \
  int32_t __s0 = __p0; \
  int32x2_t __s1 = __p1; \
  int32x2_t __ret; \
  __ret = (int32x2_t) __builtin_neon_vset_lane_i32(__s0, (int8x8_t)__s1, __p2); \
  __ret; \
})
#else
#define vset_lane_s32(__p0, __p1, __p2) __extension__ ({ \
  int32_t __s0 = __p0; \
  int32x2_t __s1 = __p1; \
  int32x2_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 1, 0); \
  int32x2_t __ret; \
  __ret = (int32x2_t) __builtin_neon_vset_lane_i32(__s0, (int8x8_t)__rev1, __p2); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#define __noswap_vset_lane_s32(__p0, __p1, __p2) __extension__ ({ \
  int32_t __s0 = __p0; \
  int32x2_t __s1 = __p1; \
  int32x2_t __ret; \
  __ret = (int32x2_t) __builtin_neon_vset_lane_i32(__s0, (int8x8_t)__s1, __p2); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vset_lane_s64(__p0, __p1, __p2) __extension__ ({ \
  int64_t __s0 = __p0; \
  int64x1_t __s1 = __p1; \
  int64x1_t __ret; \
  __ret = (int64x1_t) __builtin_neon_vset_lane_i64(__s0, (int8x8_t)__s1, __p2); \
  __ret; \
})
#else
#define vset_lane_s64(__p0, __p1, __p2) __extension__ ({ \
  int64_t __s0 = __p0; \
  int64x1_t __s1 = __p1; \
  int64x1_t __ret; \
  __ret = (int64x1_t) __builtin_neon_vset_lane_i64(__s0, (int8x8_t)__s1, __p2); \
  __ret; \
})
#define __noswap_vset_lane_s64(__p0, __p1, __p2) __extension__ ({ \
  int64_t __s0 = __p0; \
  int64x1_t __s1 = __p1; \
  int64x1_t __ret; \
  __ret = (int64x1_t) __builtin_neon_vset_lane_i64(__s0, (int8x8_t)__s1, __p2); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vset_lane_s16(__p0, __p1, __p2) __extension__ ({ \
  int16_t __s0 = __p0; \
  int16x4_t __s1 = __p1; \
  int16x4_t __ret; \
  __ret = (int16x4_t) __builtin_neon_vset_lane_i16(__s0, (int8x8_t)__s1, __p2); \
  __ret; \
})
#else
#define vset_lane_s16(__p0, __p1, __p2) __extension__ ({ \
  int16_t __s0 = __p0; \
  int16x4_t __s1 = __p1; \
  int16x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  int16x4_t __ret; \
  __ret = (int16x4_t) __builtin_neon_vset_lane_i16(__s0, (int8x8_t)__rev1, __p2); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#define __noswap_vset_lane_s16(__p0, __p1, __p2) __extension__ ({ \
  int16_t __s0 = __p0; \
  int16x4_t __s1 = __p1; \
  int16x4_t __ret; \
  __ret = (int16x4_t) __builtin_neon_vset_lane_i16(__s0, (int8x8_t)__s1, __p2); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x16_t vshlq_u8(uint8x16_t __p0, int8x16_t __p1) {
  uint8x16_t __ret;
  __ret = (uint8x16_t) __builtin_neon_vshlq_v((int8x16_t)__p0, (int8x16_t)__p1, 48);
  return __ret;
}
#else
__ai uint8x16_t vshlq_u8(uint8x16_t __p0, int8x16_t __p1) {
  uint8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __ret;
  __ret = (uint8x16_t) __builtin_neon_vshlq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 48);
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vshlq_u32(uint32x4_t __p0, int32x4_t __p1) {
  uint32x4_t __ret;
  __ret = (uint32x4_t) __builtin_neon_vshlq_v((int8x16_t)__p0, (int8x16_t)__p1, 50);
  return __ret;
}
#else
__ai uint32x4_t vshlq_u32(uint32x4_t __p0, int32x4_t __p1) {
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint32x4_t __ret;
  __ret = (uint32x4_t) __builtin_neon_vshlq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 50);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vshlq_u64(uint64x2_t __p0, int64x2_t __p1) {
  uint64x2_t __ret;
  __ret = (uint64x2_t) __builtin_neon_vshlq_v((int8x16_t)__p0, (int8x16_t)__p1, 51);
  return __ret;
}
#else
__ai uint64x2_t vshlq_u64(uint64x2_t __p0, int64x2_t __p1) {
  uint64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  uint64x2_t __ret;
  __ret = (uint64x2_t) __builtin_neon_vshlq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 51);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x8_t vshlq_u16(uint16x8_t __p0, int16x8_t __p1) {
  uint16x8_t __ret;
  __ret = (uint16x8_t) __builtin_neon_vshlq_v((int8x16_t)__p0, (int8x16_t)__p1, 49);
  return __ret;
}
#else
__ai uint16x8_t vshlq_u16(uint16x8_t __p0, int16x8_t __p1) {
  uint16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __ret;
  __ret = (uint16x8_t) __builtin_neon_vshlq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 49);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x16_t vshlq_s8(int8x16_t __p0, int8x16_t __p1) {
  int8x16_t __ret;
  __ret = (int8x16_t) __builtin_neon_vshlq_v((int8x16_t)__p0, (int8x16_t)__p1, 32);
  return __ret;
}
#else
__ai int8x16_t vshlq_s8(int8x16_t __p0, int8x16_t __p1) {
  int8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __ret;
  __ret = (int8x16_t) __builtin_neon_vshlq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 32);
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vshlq_s32(int32x4_t __p0, int32x4_t __p1) {
  int32x4_t __ret;
  __ret = (int32x4_t) __builtin_neon_vshlq_v((int8x16_t)__p0, (int8x16_t)__p1, 34);
  return __ret;
}
#else
__ai int32x4_t vshlq_s32(int32x4_t __p0, int32x4_t __p1) {
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int32x4_t __ret;
  __ret = (int32x4_t) __builtin_neon_vshlq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 34);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x2_t vshlq_s64(int64x2_t __p0, int64x2_t __p1) {
  int64x2_t __ret;
  __ret = (int64x2_t) __builtin_neon_vshlq_v((int8x16_t)__p0, (int8x16_t)__p1, 35);
  return __ret;
}
#else
__ai int64x2_t vshlq_s64(int64x2_t __p0, int64x2_t __p1) {
  int64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  int64x2_t __ret;
  __ret = (int64x2_t) __builtin_neon_vshlq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 35);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x8_t vshlq_s16(int16x8_t __p0, int16x8_t __p1) {
  int16x8_t __ret;
  __ret = (int16x8_t) __builtin_neon_vshlq_v((int8x16_t)__p0, (int8x16_t)__p1, 33);
  return __ret;
}
#else
__ai int16x8_t vshlq_s16(int16x8_t __p0, int16x8_t __p1) {
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __ret;
  __ret = (int16x8_t) __builtin_neon_vshlq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 33);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x8_t vshl_u8(uint8x8_t __p0, int8x8_t __p1) {
  uint8x8_t __ret;
  __ret = (uint8x8_t) __builtin_neon_vshl_v((int8x8_t)__p0, (int8x8_t)__p1, 16);
  return __ret;
}
#else
__ai uint8x8_t vshl_u8(uint8x8_t __p0, int8x8_t __p1) {
  uint8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __ret;
  __ret = (uint8x8_t) __builtin_neon_vshl_v((int8x8_t)__rev0, (int8x8_t)__rev1, 16);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x2_t vshl_u32(uint32x2_t __p0, int32x2_t __p1) {
  uint32x2_t __ret;
  __ret = (uint32x2_t) __builtin_neon_vshl_v((int8x8_t)__p0, (int8x8_t)__p1, 18);
  return __ret;
}
#else
__ai uint32x2_t vshl_u32(uint32x2_t __p0, int32x2_t __p1) {
  uint32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  uint32x2_t __ret;
  __ret = (uint32x2_t) __builtin_neon_vshl_v((int8x8_t)__rev0, (int8x8_t)__rev1, 18);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x1_t vshl_u64(uint64x1_t __p0, int64x1_t __p1) {
  uint64x1_t __ret;
  __ret = (uint64x1_t) __builtin_neon_vshl_v((int8x8_t)__p0, (int8x8_t)__p1, 19);
  return __ret;
}
#else
__ai uint64x1_t vshl_u64(uint64x1_t __p0, int64x1_t __p1) {
  uint64x1_t __ret;
  __ret = (uint64x1_t) __builtin_neon_vshl_v((int8x8_t)__p0, (int8x8_t)__p1, 19);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x4_t vshl_u16(uint16x4_t __p0, int16x4_t __p1) {
  uint16x4_t __ret;
  __ret = (uint16x4_t) __builtin_neon_vshl_v((int8x8_t)__p0, (int8x8_t)__p1, 17);
  return __ret;
}
#else
__ai uint16x4_t vshl_u16(uint16x4_t __p0, int16x4_t __p1) {
  uint16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint16x4_t __ret;
  __ret = (uint16x4_t) __builtin_neon_vshl_v((int8x8_t)__rev0, (int8x8_t)__rev1, 17);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x8_t vshl_s8(int8x8_t __p0, int8x8_t __p1) {
  int8x8_t __ret;
  __ret = (int8x8_t) __builtin_neon_vshl_v((int8x8_t)__p0, (int8x8_t)__p1, 0);
  return __ret;
}
#else
__ai int8x8_t vshl_s8(int8x8_t __p0, int8x8_t __p1) {
  int8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __ret;
  __ret = (int8x8_t) __builtin_neon_vshl_v((int8x8_t)__rev0, (int8x8_t)__rev1, 0);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x2_t vshl_s32(int32x2_t __p0, int32x2_t __p1) {
  int32x2_t __ret;
  __ret = (int32x2_t) __builtin_neon_vshl_v((int8x8_t)__p0, (int8x8_t)__p1, 2);
  return __ret;
}
#else
__ai int32x2_t vshl_s32(int32x2_t __p0, int32x2_t __p1) {
  int32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  int32x2_t __ret;
  __ret = (int32x2_t) __builtin_neon_vshl_v((int8x8_t)__rev0, (int8x8_t)__rev1, 2);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x1_t vshl_s64(int64x1_t __p0, int64x1_t __p1) {
  int64x1_t __ret;
  __ret = (int64x1_t) __builtin_neon_vshl_v((int8x8_t)__p0, (int8x8_t)__p1, 3);
  return __ret;
}
#else
__ai int64x1_t vshl_s64(int64x1_t __p0, int64x1_t __p1) {
  int64x1_t __ret;
  __ret = (int64x1_t) __builtin_neon_vshl_v((int8x8_t)__p0, (int8x8_t)__p1, 3);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x4_t vshl_s16(int16x4_t __p0, int16x4_t __p1) {
  int16x4_t __ret;
  __ret = (int16x4_t) __builtin_neon_vshl_v((int8x8_t)__p0, (int8x8_t)__p1, 1);
  return __ret;
}
#else
__ai int16x4_t vshl_s16(int16x4_t __p0, int16x4_t __p1) {
  int16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int16x4_t __ret;
  __ret = (int16x4_t) __builtin_neon_vshl_v((int8x8_t)__rev0, (int8x8_t)__rev1, 1);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
#define vshlq_n_u8(__p0, __p1) __extension__ ({ \
  uint8x16_t __s0 = __p0; \
  uint8x16_t __ret; \
  __ret = (uint8x16_t) __builtin_neon_vshlq_n_v((int8x16_t)__s0, __p1, 48); \
  __ret; \
})
#else
#define vshlq_n_u8(__p0, __p1) __extension__ ({ \
  uint8x16_t __s0 = __p0; \
  uint8x16_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint8x16_t __ret; \
  __ret = (uint8x16_t) __builtin_neon_vshlq_n_v((int8x16_t)__rev0, __p1, 48); \
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vshlq_n_u32(__p0, __p1) __extension__ ({ \
  uint32x4_t __s0 = __p0; \
  uint32x4_t __ret; \
  __ret = (uint32x4_t) __builtin_neon_vshlq_n_v((int8x16_t)__s0, __p1, 50); \
  __ret; \
})
#else
#define vshlq_n_u32(__p0, __p1) __extension__ ({ \
  uint32x4_t __s0 = __p0; \
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  uint32x4_t __ret; \
  __ret = (uint32x4_t) __builtin_neon_vshlq_n_v((int8x16_t)__rev0, __p1, 50); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vshlq_n_u64(__p0, __p1) __extension__ ({ \
  uint64x2_t __s0 = __p0; \
  uint64x2_t __ret; \
  __ret = (uint64x2_t) __builtin_neon_vshlq_n_v((int8x16_t)__s0, __p1, 51); \
  __ret; \
})
#else
#define vshlq_n_u64(__p0, __p1) __extension__ ({ \
  uint64x2_t __s0 = __p0; \
  uint64x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  uint64x2_t __ret; \
  __ret = (uint64x2_t) __builtin_neon_vshlq_n_v((int8x16_t)__rev0, __p1, 51); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vshlq_n_u16(__p0, __p1) __extension__ ({ \
  uint16x8_t __s0 = __p0; \
  uint16x8_t __ret; \
  __ret = (uint16x8_t) __builtin_neon_vshlq_n_v((int8x16_t)__s0, __p1, 49); \
  __ret; \
})
#else
#define vshlq_n_u16(__p0, __p1) __extension__ ({ \
  uint16x8_t __s0 = __p0; \
  uint16x8_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint16x8_t __ret; \
  __ret = (uint16x8_t) __builtin_neon_vshlq_n_v((int8x16_t)__rev0, __p1, 49); \
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vshlq_n_s8(__p0, __p1) __extension__ ({ \
  int8x16_t __s0 = __p0; \
  int8x16_t __ret; \
  __ret = (int8x16_t) __builtin_neon_vshlq_n_v((int8x16_t)__s0, __p1, 32); \
  __ret; \
})
#else
#define vshlq_n_s8(__p0, __p1) __extension__ ({ \
  int8x16_t __s0 = __p0; \
  int8x16_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  int8x16_t __ret; \
  __ret = (int8x16_t) __builtin_neon_vshlq_n_v((int8x16_t)__rev0, __p1, 32); \
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vshlq_n_s32(__p0, __p1) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  int32x4_t __ret; \
  __ret = (int32x4_t) __builtin_neon_vshlq_n_v((int8x16_t)__s0, __p1, 34); \
  __ret; \
})
#else
#define vshlq_n_s32(__p0, __p1) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  int32x4_t __ret; \
  __ret = (int32x4_t) __builtin_neon_vshlq_n_v((int8x16_t)__rev0, __p1, 34); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vshlq_n_s64(__p0, __p1) __extension__ ({ \
  int64x2_t __s0 = __p0; \
  int64x2_t __ret; \
  __ret = (int64x2_t) __builtin_neon_vshlq_n_v((int8x16_t)__s0, __p1, 35); \
  __ret; \
})
#else
#define vshlq_n_s64(__p0, __p1) __extension__ ({ \
  int64x2_t __s0 = __p0; \
  int64x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  int64x2_t __ret; \
  __ret = (int64x2_t) __builtin_neon_vshlq_n_v((int8x16_t)__rev0, __p1, 35); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vshlq_n_s16(__p0, __p1) __extension__ ({ \
  int16x8_t __s0 = __p0; \
  int16x8_t __ret; \
  __ret = (int16x8_t) __builtin_neon_vshlq_n_v((int8x16_t)__s0, __p1, 33); \
  __ret; \
})
#else
#define vshlq_n_s16(__p0, __p1) __extension__ ({ \
  int16x8_t __s0 = __p0; \
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 7, 6, 5, 4, 3, 2, 1, 0); \
  int16x8_t __ret; \
  __ret = (int16x8_t) __builtin_neon_vshlq_n_v((int8x16_t)__rev0, __p1, 33); \
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vshl_n_u8(__p0, __p1) __extension__ ({ \
  uint8x8_t __s0 = __p0; \
  uint8x8_t __ret; \
  __ret = (uint8x8_t) __builtin_neon_vshl_n_v((int8x8_t)__s0, __p1, 16); \
  __ret; \
})
#else
#define vshl_n_u8(__p0, __p1) __extension__ ({ \
  uint8x8_t __s0 = __p0; \
  uint8x8_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint8x8_t __ret; \
  __ret = (uint8x8_t) __builtin_neon_vshl_n_v((int8x8_t)__rev0, __p1, 16); \
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vshl_n_u32(__p0, __p1) __extension__ ({ \
  uint32x2_t __s0 = __p0; \
  uint32x2_t __ret; \
  __ret = (uint32x2_t) __builtin_neon_vshl_n_v((int8x8_t)__s0, __p1, 18); \
  __ret; \
})
#else
#define vshl_n_u32(__p0, __p1) __extension__ ({ \
  uint32x2_t __s0 = __p0; \
  uint32x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  uint32x2_t __ret; \
  __ret = (uint32x2_t) __builtin_neon_vshl_n_v((int8x8_t)__rev0, __p1, 18); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vshl_n_u64(__p0, __p1) __extension__ ({ \
  uint64x1_t __s0 = __p0; \
  uint64x1_t __ret; \
  __ret = (uint64x1_t) __builtin_neon_vshl_n_v((int8x8_t)__s0, __p1, 19); \
  __ret; \
})
#else
#define vshl_n_u64(__p0, __p1) __extension__ ({ \
  uint64x1_t __s0 = __p0; \
  uint64x1_t __ret; \
  __ret = (uint64x1_t) __builtin_neon_vshl_n_v((int8x8_t)__s0, __p1, 19); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vshl_n_u16(__p0, __p1) __extension__ ({ \
  uint16x4_t __s0 = __p0; \
  uint16x4_t __ret; \
  __ret = (uint16x4_t) __builtin_neon_vshl_n_v((int8x8_t)__s0, __p1, 17); \
  __ret; \
})
#else
#define vshl_n_u16(__p0, __p1) __extension__ ({ \
  uint16x4_t __s0 = __p0; \
  uint16x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  uint16x4_t __ret; \
  __ret = (uint16x4_t) __builtin_neon_vshl_n_v((int8x8_t)__rev0, __p1, 17); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vshl_n_s8(__p0, __p1) __extension__ ({ \
  int8x8_t __s0 = __p0; \
  int8x8_t __ret; \
  __ret = (int8x8_t) __builtin_neon_vshl_n_v((int8x8_t)__s0, __p1, 0); \
  __ret; \
})
#else
#define vshl_n_s8(__p0, __p1) __extension__ ({ \
  int8x8_t __s0 = __p0; \
  int8x8_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 7, 6, 5, 4, 3, 2, 1, 0); \
  int8x8_t __ret; \
  __ret = (int8x8_t) __builtin_neon_vshl_n_v((int8x8_t)__rev0, __p1, 0); \
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vshl_n_s32(__p0, __p1) __extension__ ({ \
  int32x2_t __s0 = __p0; \
  int32x2_t __ret; \
  __ret = (int32x2_t) __builtin_neon_vshl_n_v((int8x8_t)__s0, __p1, 2); \
  __ret; \
})
#else
#define vshl_n_s32(__p0, __p1) __extension__ ({ \
  int32x2_t __s0 = __p0; \
  int32x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  int32x2_t __ret; \
  __ret = (int32x2_t) __builtin_neon_vshl_n_v((int8x8_t)__rev0, __p1, 2); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vshl_n_s64(__p0, __p1) __extension__ ({ \
  int64x1_t __s0 = __p0; \
  int64x1_t __ret; \
  __ret = (int64x1_t) __builtin_neon_vshl_n_v((int8x8_t)__s0, __p1, 3); \
  __ret; \
})
#else
#define vshl_n_s64(__p0, __p1) __extension__ ({ \
  int64x1_t __s0 = __p0; \
  int64x1_t __ret; \
  __ret = (int64x1_t) __builtin_neon_vshl_n_v((int8x8_t)__s0, __p1, 3); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vshl_n_s16(__p0, __p1) __extension__ ({ \
  int16x4_t __s0 = __p0; \
  int16x4_t __ret; \
  __ret = (int16x4_t) __builtin_neon_vshl_n_v((int8x8_t)__s0, __p1, 1); \
  __ret; \
})
#else
#define vshl_n_s16(__p0, __p1) __extension__ ({ \
  int16x4_t __s0 = __p0; \
  int16x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  int16x4_t __ret; \
  __ret = (int16x4_t) __builtin_neon_vshl_n_v((int8x8_t)__rev0, __p1, 1); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vshll_n_u8(__p0, __p1) __extension__ ({ \
  uint8x8_t __s0 = __p0; \
  uint16x8_t __ret; \
  __ret = (uint16x8_t) __builtin_neon_vshll_n_v((int8x8_t)__s0, __p1, 49); \
  __ret; \
})
#else
#define vshll_n_u8(__p0, __p1) __extension__ ({ \
  uint8x8_t __s0 = __p0; \
  uint8x8_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint16x8_t __ret; \
  __ret = (uint16x8_t) __builtin_neon_vshll_n_v((int8x8_t)__rev0, __p1, 49); \
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#define __noswap_vshll_n_u8(__p0, __p1) __extension__ ({ \
  uint8x8_t __s0 = __p0; \
  uint16x8_t __ret; \
  __ret = (uint16x8_t) __builtin_neon_vshll_n_v((int8x8_t)__s0, __p1, 49); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vshll_n_u32(__p0, __p1) __extension__ ({ \
  uint32x2_t __s0 = __p0; \
  uint64x2_t __ret; \
  __ret = (uint64x2_t) __builtin_neon_vshll_n_v((int8x8_t)__s0, __p1, 51); \
  __ret; \
})
#else
#define vshll_n_u32(__p0, __p1) __extension__ ({ \
  uint32x2_t __s0 = __p0; \
  uint32x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  uint64x2_t __ret; \
  __ret = (uint64x2_t) __builtin_neon_vshll_n_v((int8x8_t)__rev0, __p1, 51); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#define __noswap_vshll_n_u32(__p0, __p1) __extension__ ({ \
  uint32x2_t __s0 = __p0; \
  uint64x2_t __ret; \
  __ret = (uint64x2_t) __builtin_neon_vshll_n_v((int8x8_t)__s0, __p1, 51); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vshll_n_u16(__p0, __p1) __extension__ ({ \
  uint16x4_t __s0 = __p0; \
  uint32x4_t __ret; \
  __ret = (uint32x4_t) __builtin_neon_vshll_n_v((int8x8_t)__s0, __p1, 50); \
  __ret; \
})
#else
#define vshll_n_u16(__p0, __p1) __extension__ ({ \
  uint16x4_t __s0 = __p0; \
  uint16x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  uint32x4_t __ret; \
  __ret = (uint32x4_t) __builtin_neon_vshll_n_v((int8x8_t)__rev0, __p1, 50); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#define __noswap_vshll_n_u16(__p0, __p1) __extension__ ({ \
  uint16x4_t __s0 = __p0; \
  uint32x4_t __ret; \
  __ret = (uint32x4_t) __builtin_neon_vshll_n_v((int8x8_t)__s0, __p1, 50); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vshll_n_s8(__p0, __p1) __extension__ ({ \
  int8x8_t __s0 = __p0; \
  int16x8_t __ret; \
  __ret = (int16x8_t) __builtin_neon_vshll_n_v((int8x8_t)__s0, __p1, 33); \
  __ret; \
})
#else
#define vshll_n_s8(__p0, __p1) __extension__ ({ \
  int8x8_t __s0 = __p0; \
  int8x8_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 7, 6, 5, 4, 3, 2, 1, 0); \
  int16x8_t __ret; \
  __ret = (int16x8_t) __builtin_neon_vshll_n_v((int8x8_t)__rev0, __p1, 33); \
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#define __noswap_vshll_n_s8(__p0, __p1) __extension__ ({ \
  int8x8_t __s0 = __p0; \
  int16x8_t __ret; \
  __ret = (int16x8_t) __builtin_neon_vshll_n_v((int8x8_t)__s0, __p1, 33); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vshll_n_s32(__p0, __p1) __extension__ ({ \
  int32x2_t __s0 = __p0; \
  int64x2_t __ret; \
  __ret = (int64x2_t) __builtin_neon_vshll_n_v((int8x8_t)__s0, __p1, 35); \
  __ret; \
})
#else
#define vshll_n_s32(__p0, __p1) __extension__ ({ \
  int32x2_t __s0 = __p0; \
  int32x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  int64x2_t __ret; \
  __ret = (int64x2_t) __builtin_neon_vshll_n_v((int8x8_t)__rev0, __p1, 35); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#define __noswap_vshll_n_s32(__p0, __p1) __extension__ ({ \
  int32x2_t __s0 = __p0; \
  int64x2_t __ret; \
  __ret = (int64x2_t) __builtin_neon_vshll_n_v((int8x8_t)__s0, __p1, 35); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vshll_n_s16(__p0, __p1) __extension__ ({ \
  int16x4_t __s0 = __p0; \
  int32x4_t __ret; \
  __ret = (int32x4_t) __builtin_neon_vshll_n_v((int8x8_t)__s0, __p1, 34); \
  __ret; \
})
#else
#define vshll_n_s16(__p0, __p1) __extension__ ({ \
  int16x4_t __s0 = __p0; \
  int16x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  int32x4_t __ret; \
  __ret = (int32x4_t) __builtin_neon_vshll_n_v((int8x8_t)__rev0, __p1, 34); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#define __noswap_vshll_n_s16(__p0, __p1) __extension__ ({ \
  int16x4_t __s0 = __p0; \
  int32x4_t __ret; \
  __ret = (int32x4_t) __builtin_neon_vshll_n_v((int8x8_t)__s0, __p1, 34); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vshrq_n_u8(__p0, __p1) __extension__ ({ \
  uint8x16_t __s0 = __p0; \
  uint8x16_t __ret; \
  __ret = (uint8x16_t) __builtin_neon_vshrq_n_v((int8x16_t)__s0, __p1, 48); \
  __ret; \
})
#else
#define vshrq_n_u8(__p0, __p1) __extension__ ({ \
  uint8x16_t __s0 = __p0; \
  uint8x16_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint8x16_t __ret; \
  __ret = (uint8x16_t) __builtin_neon_vshrq_n_v((int8x16_t)__rev0, __p1, 48); \
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vshrq_n_u32(__p0, __p1) __extension__ ({ \
  uint32x4_t __s0 = __p0; \
  uint32x4_t __ret; \
  __ret = (uint32x4_t) __builtin_neon_vshrq_n_v((int8x16_t)__s0, __p1, 50); \
  __ret; \
})
#else
#define vshrq_n_u32(__p0, __p1) __extension__ ({ \
  uint32x4_t __s0 = __p0; \
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  uint32x4_t __ret; \
  __ret = (uint32x4_t) __builtin_neon_vshrq_n_v((int8x16_t)__rev0, __p1, 50); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vshrq_n_u64(__p0, __p1) __extension__ ({ \
  uint64x2_t __s0 = __p0; \
  uint64x2_t __ret; \
  __ret = (uint64x2_t) __builtin_neon_vshrq_n_v((int8x16_t)__s0, __p1, 51); \
  __ret; \
})
#else
#define vshrq_n_u64(__p0, __p1) __extension__ ({ \
  uint64x2_t __s0 = __p0; \
  uint64x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  uint64x2_t __ret; \
  __ret = (uint64x2_t) __builtin_neon_vshrq_n_v((int8x16_t)__rev0, __p1, 51); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vshrq_n_u16(__p0, __p1) __extension__ ({ \
  uint16x8_t __s0 = __p0; \
  uint16x8_t __ret; \
  __ret = (uint16x8_t) __builtin_neon_vshrq_n_v((int8x16_t)__s0, __p1, 49); \
  __ret; \
})
#else
#define vshrq_n_u16(__p0, __p1) __extension__ ({ \
  uint16x8_t __s0 = __p0; \
  uint16x8_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint16x8_t __ret; \
  __ret = (uint16x8_t) __builtin_neon_vshrq_n_v((int8x16_t)__rev0, __p1, 49); \
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vshrq_n_s8(__p0, __p1) __extension__ ({ \
  int8x16_t __s0 = __p0; \
  int8x16_t __ret; \
  __ret = (int8x16_t) __builtin_neon_vshrq_n_v((int8x16_t)__s0, __p1, 32); \
  __ret; \
})
#else
#define vshrq_n_s8(__p0, __p1) __extension__ ({ \
  int8x16_t __s0 = __p0; \
  int8x16_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  int8x16_t __ret; \
  __ret = (int8x16_t) __builtin_neon_vshrq_n_v((int8x16_t)__rev0, __p1, 32); \
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vshrq_n_s32(__p0, __p1) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  int32x4_t __ret; \
  __ret = (int32x4_t) __builtin_neon_vshrq_n_v((int8x16_t)__s0, __p1, 34); \
  __ret; \
})
#else
#define vshrq_n_s32(__p0, __p1) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  int32x4_t __ret; \
  __ret = (int32x4_t) __builtin_neon_vshrq_n_v((int8x16_t)__rev0, __p1, 34); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vshrq_n_s64(__p0, __p1) __extension__ ({ \
  int64x2_t __s0 = __p0; \
  int64x2_t __ret; \
  __ret = (int64x2_t) __builtin_neon_vshrq_n_v((int8x16_t)__s0, __p1, 35); \
  __ret; \
})
#else
#define vshrq_n_s64(__p0, __p1) __extension__ ({ \
  int64x2_t __s0 = __p0; \
  int64x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  int64x2_t __ret; \
  __ret = (int64x2_t) __builtin_neon_vshrq_n_v((int8x16_t)__rev0, __p1, 35); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vshrq_n_s16(__p0, __p1) __extension__ ({ \
  int16x8_t __s0 = __p0; \
  int16x8_t __ret; \
  __ret = (int16x8_t) __builtin_neon_vshrq_n_v((int8x16_t)__s0, __p1, 33); \
  __ret; \
})
#else
#define vshrq_n_s16(__p0, __p1) __extension__ ({ \
  int16x8_t __s0 = __p0; \
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 7, 6, 5, 4, 3, 2, 1, 0); \
  int16x8_t __ret; \
  __ret = (int16x8_t) __builtin_neon_vshrq_n_v((int8x16_t)__rev0, __p1, 33); \
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vshr_n_u8(__p0, __p1) __extension__ ({ \
  uint8x8_t __s0 = __p0; \
  uint8x8_t __ret; \
  __ret = (uint8x8_t) __builtin_neon_vshr_n_v((int8x8_t)__s0, __p1, 16); \
  __ret; \
})
#else
#define vshr_n_u8(__p0, __p1) __extension__ ({ \
  uint8x8_t __s0 = __p0; \
  uint8x8_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint8x8_t __ret; \
  __ret = (uint8x8_t) __builtin_neon_vshr_n_v((int8x8_t)__rev0, __p1, 16); \
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vshr_n_u32(__p0, __p1) __extension__ ({ \
  uint32x2_t __s0 = __p0; \
  uint32x2_t __ret; \
  __ret = (uint32x2_t) __builtin_neon_vshr_n_v((int8x8_t)__s0, __p1, 18); \
  __ret; \
})
#else
#define vshr_n_u32(__p0, __p1) __extension__ ({ \
  uint32x2_t __s0 = __p0; \
  uint32x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  uint32x2_t __ret; \
  __ret = (uint32x2_t) __builtin_neon_vshr_n_v((int8x8_t)__rev0, __p1, 18); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vshr_n_u64(__p0, __p1) __extension__ ({ \
  uint64x1_t __s0 = __p0; \
  uint64x1_t __ret; \
  __ret = (uint64x1_t) __builtin_neon_vshr_n_v((int8x8_t)__s0, __p1, 19); \
  __ret; \
})
#else
#define vshr_n_u64(__p0, __p1) __extension__ ({ \
  uint64x1_t __s0 = __p0; \
  uint64x1_t __ret; \
  __ret = (uint64x1_t) __builtin_neon_vshr_n_v((int8x8_t)__s0, __p1, 19); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vshr_n_u16(__p0, __p1) __extension__ ({ \
  uint16x4_t __s0 = __p0; \
  uint16x4_t __ret; \
  __ret = (uint16x4_t) __builtin_neon_vshr_n_v((int8x8_t)__s0, __p1, 17); \
  __ret; \
})
#else
#define vshr_n_u16(__p0, __p1) __extension__ ({ \
  uint16x4_t __s0 = __p0; \
  uint16x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  uint16x4_t __ret; \
  __ret = (uint16x4_t) __builtin_neon_vshr_n_v((int8x8_t)__rev0, __p1, 17); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vshr_n_s8(__p0, __p1) __extension__ ({ \
  int8x8_t __s0 = __p0; \
  int8x8_t __ret; \
  __ret = (int8x8_t) __builtin_neon_vshr_n_v((int8x8_t)__s0, __p1, 0); \
  __ret; \
})
#else
#define vshr_n_s8(__p0, __p1) __extension__ ({ \
  int8x8_t __s0 = __p0; \
  int8x8_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 7, 6, 5, 4, 3, 2, 1, 0); \
  int8x8_t __ret; \
  __ret = (int8x8_t) __builtin_neon_vshr_n_v((int8x8_t)__rev0, __p1, 0); \
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vshr_n_s32(__p0, __p1) __extension__ ({ \
  int32x2_t __s0 = __p0; \
  int32x2_t __ret; \
  __ret = (int32x2_t) __builtin_neon_vshr_n_v((int8x8_t)__s0, __p1, 2); \
  __ret; \
})
#else
#define vshr_n_s32(__p0, __p1) __extension__ ({ \
  int32x2_t __s0 = __p0; \
  int32x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  int32x2_t __ret; \
  __ret = (int32x2_t) __builtin_neon_vshr_n_v((int8x8_t)__rev0, __p1, 2); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vshr_n_s64(__p0, __p1) __extension__ ({ \
  int64x1_t __s0 = __p0; \
  int64x1_t __ret; \
  __ret = (int64x1_t) __builtin_neon_vshr_n_v((int8x8_t)__s0, __p1, 3); \
  __ret; \
})
#else
#define vshr_n_s64(__p0, __p1) __extension__ ({ \
  int64x1_t __s0 = __p0; \
  int64x1_t __ret; \
  __ret = (int64x1_t) __builtin_neon_vshr_n_v((int8x8_t)__s0, __p1, 3); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vshr_n_s16(__p0, __p1) __extension__ ({ \
  int16x4_t __s0 = __p0; \
  int16x4_t __ret; \
  __ret = (int16x4_t) __builtin_neon_vshr_n_v((int8x8_t)__s0, __p1, 1); \
  __ret; \
})
#else
#define vshr_n_s16(__p0, __p1) __extension__ ({ \
  int16x4_t __s0 = __p0; \
  int16x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  int16x4_t __ret; \
  __ret = (int16x4_t) __builtin_neon_vshr_n_v((int8x8_t)__rev0, __p1, 1); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vshrn_n_u32(__p0, __p1) __extension__ ({ \
  uint32x4_t __s0 = __p0; \
  uint16x4_t __ret; \
  __ret = (uint16x4_t) __builtin_neon_vshrn_n_v((int8x16_t)__s0, __p1, 17); \
  __ret; \
})
#else
#define vshrn_n_u32(__p0, __p1) __extension__ ({ \
  uint32x4_t __s0 = __p0; \
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  uint16x4_t __ret; \
  __ret = (uint16x4_t) __builtin_neon_vshrn_n_v((int8x16_t)__rev0, __p1, 17); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#define __noswap_vshrn_n_u32(__p0, __p1) __extension__ ({ \
  uint32x4_t __s0 = __p0; \
  uint16x4_t __ret; \
  __ret = (uint16x4_t) __builtin_neon_vshrn_n_v((int8x16_t)__s0, __p1, 17); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vshrn_n_u64(__p0, __p1) __extension__ ({ \
  uint64x2_t __s0 = __p0; \
  uint32x2_t __ret; \
  __ret = (uint32x2_t) __builtin_neon_vshrn_n_v((int8x16_t)__s0, __p1, 18); \
  __ret; \
})
#else
#define vshrn_n_u64(__p0, __p1) __extension__ ({ \
  uint64x2_t __s0 = __p0; \
  uint64x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  uint32x2_t __ret; \
  __ret = (uint32x2_t) __builtin_neon_vshrn_n_v((int8x16_t)__rev0, __p1, 18); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#define __noswap_vshrn_n_u64(__p0, __p1) __extension__ ({ \
  uint64x2_t __s0 = __p0; \
  uint32x2_t __ret; \
  __ret = (uint32x2_t) __builtin_neon_vshrn_n_v((int8x16_t)__s0, __p1, 18); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vshrn_n_u16(__p0, __p1) __extension__ ({ \
  uint16x8_t __s0 = __p0; \
  uint8x8_t __ret; \
  __ret = (uint8x8_t) __builtin_neon_vshrn_n_v((int8x16_t)__s0, __p1, 16); \
  __ret; \
})
#else
#define vshrn_n_u16(__p0, __p1) __extension__ ({ \
  uint16x8_t __s0 = __p0; \
  uint16x8_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint8x8_t __ret; \
  __ret = (uint8x8_t) __builtin_neon_vshrn_n_v((int8x16_t)__rev0, __p1, 16); \
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#define __noswap_vshrn_n_u16(__p0, __p1) __extension__ ({ \
  uint16x8_t __s0 = __p0; \
  uint8x8_t __ret; \
  __ret = (uint8x8_t) __builtin_neon_vshrn_n_v((int8x16_t)__s0, __p1, 16); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vshrn_n_s32(__p0, __p1) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  int16x4_t __ret; \
  __ret = (int16x4_t) __builtin_neon_vshrn_n_v((int8x16_t)__s0, __p1, 1); \
  __ret; \
})
#else
#define vshrn_n_s32(__p0, __p1) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  int16x4_t __ret; \
  __ret = (int16x4_t) __builtin_neon_vshrn_n_v((int8x16_t)__rev0, __p1, 1); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#define __noswap_vshrn_n_s32(__p0, __p1) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  int16x4_t __ret; \
  __ret = (int16x4_t) __builtin_neon_vshrn_n_v((int8x16_t)__s0, __p1, 1); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vshrn_n_s64(__p0, __p1) __extension__ ({ \
  int64x2_t __s0 = __p0; \
  int32x2_t __ret; \
  __ret = (int32x2_t) __builtin_neon_vshrn_n_v((int8x16_t)__s0, __p1, 2); \
  __ret; \
})
#else
#define vshrn_n_s64(__p0, __p1) __extension__ ({ \
  int64x2_t __s0 = __p0; \
  int64x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  int32x2_t __ret; \
  __ret = (int32x2_t) __builtin_neon_vshrn_n_v((int8x16_t)__rev0, __p1, 2); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#define __noswap_vshrn_n_s64(__p0, __p1) __extension__ ({ \
  int64x2_t __s0 = __p0; \
  int32x2_t __ret; \
  __ret = (int32x2_t) __builtin_neon_vshrn_n_v((int8x16_t)__s0, __p1, 2); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vshrn_n_s16(__p0, __p1) __extension__ ({ \
  int16x8_t __s0 = __p0; \
  int8x8_t __ret; \
  __ret = (int8x8_t) __builtin_neon_vshrn_n_v((int8x16_t)__s0, __p1, 0); \
  __ret; \
})
#else
#define vshrn_n_s16(__p0, __p1) __extension__ ({ \
  int16x8_t __s0 = __p0; \
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 7, 6, 5, 4, 3, 2, 1, 0); \
  int8x8_t __ret; \
  __ret = (int8x8_t) __builtin_neon_vshrn_n_v((int8x16_t)__rev0, __p1, 0); \
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#define __noswap_vshrn_n_s16(__p0, __p1) __extension__ ({ \
  int16x8_t __s0 = __p0; \
  int8x8_t __ret; \
  __ret = (int8x8_t) __builtin_neon_vshrn_n_v((int8x16_t)__s0, __p1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vsli_n_p8(__p0, __p1, __p2) __extension__ ({ \
  poly8x8_t __s0 = __p0; \
  poly8x8_t __s1 = __p1; \
  poly8x8_t __ret; \
  __ret = (poly8x8_t) __builtin_neon_vsli_n_v((int8x8_t)__s0, (int8x8_t)__s1, __p2, 4); \
  __ret; \
})
#else
#define vsli_n_p8(__p0, __p1, __p2) __extension__ ({ \
  poly8x8_t __s0 = __p0; \
  poly8x8_t __s1 = __p1; \
  poly8x8_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 7, 6, 5, 4, 3, 2, 1, 0); \
  poly8x8_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 7, 6, 5, 4, 3, 2, 1, 0); \
  poly8x8_t __ret; \
  __ret = (poly8x8_t) __builtin_neon_vsli_n_v((int8x8_t)__rev0, (int8x8_t)__rev1, __p2, 4); \
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vsli_n_p16(__p0, __p1, __p2) __extension__ ({ \
  poly16x4_t __s0 = __p0; \
  poly16x4_t __s1 = __p1; \
  poly16x4_t __ret; \
  __ret = (poly16x4_t) __builtin_neon_vsli_n_v((int8x8_t)__s0, (int8x8_t)__s1, __p2, 5); \
  __ret; \
})
#else
#define vsli_n_p16(__p0, __p1, __p2) __extension__ ({ \
  poly16x4_t __s0 = __p0; \
  poly16x4_t __s1 = __p1; \
  poly16x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  poly16x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  poly16x4_t __ret; \
  __ret = (poly16x4_t) __builtin_neon_vsli_n_v((int8x8_t)__rev0, (int8x8_t)__rev1, __p2, 5); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vsliq_n_p8(__p0, __p1, __p2) __extension__ ({ \
  poly8x16_t __s0 = __p0; \
  poly8x16_t __s1 = __p1; \
  poly8x16_t __ret; \
  __ret = (poly8x16_t) __builtin_neon_vsliq_n_v((int8x16_t)__s0, (int8x16_t)__s1, __p2, 36); \
  __ret; \
})
#else
#define vsliq_n_p8(__p0, __p1, __p2) __extension__ ({ \
  poly8x16_t __s0 = __p0; \
  poly8x16_t __s1 = __p1; \
  poly8x16_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  poly8x16_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  poly8x16_t __ret; \
  __ret = (poly8x16_t) __builtin_neon_vsliq_n_v((int8x16_t)__rev0, (int8x16_t)__rev1, __p2, 36); \
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vsliq_n_p16(__p0, __p1, __p2) __extension__ ({ \
  poly16x8_t __s0 = __p0; \
  poly16x8_t __s1 = __p1; \
  poly16x8_t __ret; \
  __ret = (poly16x8_t) __builtin_neon_vsliq_n_v((int8x16_t)__s0, (int8x16_t)__s1, __p2, 37); \
  __ret; \
})
#else
#define vsliq_n_p16(__p0, __p1, __p2) __extension__ ({ \
  poly16x8_t __s0 = __p0; \
  poly16x8_t __s1 = __p1; \
  poly16x8_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 7, 6, 5, 4, 3, 2, 1, 0); \
  poly16x8_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 7, 6, 5, 4, 3, 2, 1, 0); \
  poly16x8_t __ret; \
  __ret = (poly16x8_t) __builtin_neon_vsliq_n_v((int8x16_t)__rev0, (int8x16_t)__rev1, __p2, 37); \
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vsliq_n_u8(__p0, __p1, __p2) __extension__ ({ \
  uint8x16_t __s0 = __p0; \
  uint8x16_t __s1 = __p1; \
  uint8x16_t __ret; \
  __ret = (uint8x16_t) __builtin_neon_vsliq_n_v((int8x16_t)__s0, (int8x16_t)__s1, __p2, 48); \
  __ret; \
})
#else
#define vsliq_n_u8(__p0, __p1, __p2) __extension__ ({ \
  uint8x16_t __s0 = __p0; \
  uint8x16_t __s1 = __p1; \
  uint8x16_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint8x16_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint8x16_t __ret; \
  __ret = (uint8x16_t) __builtin_neon_vsliq_n_v((int8x16_t)__rev0, (int8x16_t)__rev1, __p2, 48); \
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vsliq_n_u32(__p0, __p1, __p2) __extension__ ({ \
  uint32x4_t __s0 = __p0; \
  uint32x4_t __s1 = __p1; \
  uint32x4_t __ret; \
  __ret = (uint32x4_t) __builtin_neon_vsliq_n_v((int8x16_t)__s0, (int8x16_t)__s1, __p2, 50); \
  __ret; \
})
#else
#define vsliq_n_u32(__p0, __p1, __p2) __extension__ ({ \
  uint32x4_t __s0 = __p0; \
  uint32x4_t __s1 = __p1; \
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  uint32x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  uint32x4_t __ret; \
  __ret = (uint32x4_t) __builtin_neon_vsliq_n_v((int8x16_t)__rev0, (int8x16_t)__rev1, __p2, 50); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vsliq_n_u64(__p0, __p1, __p2) __extension__ ({ \
  uint64x2_t __s0 = __p0; \
  uint64x2_t __s1 = __p1; \
  uint64x2_t __ret; \
  __ret = (uint64x2_t) __builtin_neon_vsliq_n_v((int8x16_t)__s0, (int8x16_t)__s1, __p2, 51); \
  __ret; \
})
#else
#define vsliq_n_u64(__p0, __p1, __p2) __extension__ ({ \
  uint64x2_t __s0 = __p0; \
  uint64x2_t __s1 = __p1; \
  uint64x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  uint64x2_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 1, 0); \
  uint64x2_t __ret; \
  __ret = (uint64x2_t) __builtin_neon_vsliq_n_v((int8x16_t)__rev0, (int8x16_t)__rev1, __p2, 51); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vsliq_n_u16(__p0, __p1, __p2) __extension__ ({ \
  uint16x8_t __s0 = __p0; \
  uint16x8_t __s1 = __p1; \
  uint16x8_t __ret; \
  __ret = (uint16x8_t) __builtin_neon_vsliq_n_v((int8x16_t)__s0, (int8x16_t)__s1, __p2, 49); \
  __ret; \
})
#else
#define vsliq_n_u16(__p0, __p1, __p2) __extension__ ({ \
  uint16x8_t __s0 = __p0; \
  uint16x8_t __s1 = __p1; \
  uint16x8_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint16x8_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint16x8_t __ret; \
  __ret = (uint16x8_t) __builtin_neon_vsliq_n_v((int8x16_t)__rev0, (int8x16_t)__rev1, __p2, 49); \
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vsliq_n_s8(__p0, __p1, __p2) __extension__ ({ \
  int8x16_t __s0 = __p0; \
  int8x16_t __s1 = __p1; \
  int8x16_t __ret; \
  __ret = (int8x16_t) __builtin_neon_vsliq_n_v((int8x16_t)__s0, (int8x16_t)__s1, __p2, 32); \
  __ret; \
})
#else
#define vsliq_n_s8(__p0, __p1, __p2) __extension__ ({ \
  int8x16_t __s0 = __p0; \
  int8x16_t __s1 = __p1; \
  int8x16_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  int8x16_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  int8x16_t __ret; \
  __ret = (int8x16_t) __builtin_neon_vsliq_n_v((int8x16_t)__rev0, (int8x16_t)__rev1, __p2, 32); \
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vsliq_n_s32(__p0, __p1, __p2) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  int32x4_t __s1 = __p1; \
  int32x4_t __ret; \
  __ret = (int32x4_t) __builtin_neon_vsliq_n_v((int8x16_t)__s0, (int8x16_t)__s1, __p2, 34); \
  __ret; \
})
#else
#define vsliq_n_s32(__p0, __p1, __p2) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  int32x4_t __s1 = __p1; \
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  int32x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  int32x4_t __ret; \
  __ret = (int32x4_t) __builtin_neon_vsliq_n_v((int8x16_t)__rev0, (int8x16_t)__rev1, __p2, 34); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vsliq_n_s64(__p0, __p1, __p2) __extension__ ({ \
  int64x2_t __s0 = __p0; \
  int64x2_t __s1 = __p1; \
  int64x2_t __ret; \
  __ret = (int64x2_t) __builtin_neon_vsliq_n_v((int8x16_t)__s0, (int8x16_t)__s1, __p2, 35); \
  __ret; \
})
#else
#define vsliq_n_s64(__p0, __p1, __p2) __extension__ ({ \
  int64x2_t __s0 = __p0; \
  int64x2_t __s1 = __p1; \
  int64x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  int64x2_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 1, 0); \
  int64x2_t __ret; \
  __ret = (int64x2_t) __builtin_neon_vsliq_n_v((int8x16_t)__rev0, (int8x16_t)__rev1, __p2, 35); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vsliq_n_s16(__p0, __p1, __p2) __extension__ ({ \
  int16x8_t __s0 = __p0; \
  int16x8_t __s1 = __p1; \
  int16x8_t __ret; \
  __ret = (int16x8_t) __builtin_neon_vsliq_n_v((int8x16_t)__s0, (int8x16_t)__s1, __p2, 33); \
  __ret; \
})
#else
#define vsliq_n_s16(__p0, __p1, __p2) __extension__ ({ \
  int16x8_t __s0 = __p0; \
  int16x8_t __s1 = __p1; \
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 7, 6, 5, 4, 3, 2, 1, 0); \
  int16x8_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 7, 6, 5, 4, 3, 2, 1, 0); \
  int16x8_t __ret; \
  __ret = (int16x8_t) __builtin_neon_vsliq_n_v((int8x16_t)__rev0, (int8x16_t)__rev1, __p2, 33); \
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vsli_n_u8(__p0, __p1, __p2) __extension__ ({ \
  uint8x8_t __s0 = __p0; \
  uint8x8_t __s1 = __p1; \
  uint8x8_t __ret; \
  __ret = (uint8x8_t) __builtin_neon_vsli_n_v((int8x8_t)__s0, (int8x8_t)__s1, __p2, 16); \
  __ret; \
})
#else
#define vsli_n_u8(__p0, __p1, __p2) __extension__ ({ \
  uint8x8_t __s0 = __p0; \
  uint8x8_t __s1 = __p1; \
  uint8x8_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint8x8_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint8x8_t __ret; \
  __ret = (uint8x8_t) __builtin_neon_vsli_n_v((int8x8_t)__rev0, (int8x8_t)__rev1, __p2, 16); \
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vsli_n_u32(__p0, __p1, __p2) __extension__ ({ \
  uint32x2_t __s0 = __p0; \
  uint32x2_t __s1 = __p1; \
  uint32x2_t __ret; \
  __ret = (uint32x2_t) __builtin_neon_vsli_n_v((int8x8_t)__s0, (int8x8_t)__s1, __p2, 18); \
  __ret; \
})
#else
#define vsli_n_u32(__p0, __p1, __p2) __extension__ ({ \
  uint32x2_t __s0 = __p0; \
  uint32x2_t __s1 = __p1; \
  uint32x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  uint32x2_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 1, 0); \
  uint32x2_t __ret; \
  __ret = (uint32x2_t) __builtin_neon_vsli_n_v((int8x8_t)__rev0, (int8x8_t)__rev1, __p2, 18); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vsli_n_u64(__p0, __p1, __p2) __extension__ ({ \
  uint64x1_t __s0 = __p0; \
  uint64x1_t __s1 = __p1; \
  uint64x1_t __ret; \
  __ret = (uint64x1_t) __builtin_neon_vsli_n_v((int8x8_t)__s0, (int8x8_t)__s1, __p2, 19); \
  __ret; \
})
#else
#define vsli_n_u64(__p0, __p1, __p2) __extension__ ({ \
  uint64x1_t __s0 = __p0; \
  uint64x1_t __s1 = __p1; \
  uint64x1_t __ret; \
  __ret = (uint64x1_t) __builtin_neon_vsli_n_v((int8x8_t)__s0, (int8x8_t)__s1, __p2, 19); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vsli_n_u16(__p0, __p1, __p2) __extension__ ({ \
  uint16x4_t __s0 = __p0; \
  uint16x4_t __s1 = __p1; \
  uint16x4_t __ret; \
  __ret = (uint16x4_t) __builtin_neon_vsli_n_v((int8x8_t)__s0, (int8x8_t)__s1, __p2, 17); \
  __ret; \
})
#else
#define vsli_n_u16(__p0, __p1, __p2) __extension__ ({ \
  uint16x4_t __s0 = __p0; \
  uint16x4_t __s1 = __p1; \
  uint16x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  uint16x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  uint16x4_t __ret; \
  __ret = (uint16x4_t) __builtin_neon_vsli_n_v((int8x8_t)__rev0, (int8x8_t)__rev1, __p2, 17); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vsli_n_s8(__p0, __p1, __p2) __extension__ ({ \
  int8x8_t __s0 = __p0; \
  int8x8_t __s1 = __p1; \
  int8x8_t __ret; \
  __ret = (int8x8_t) __builtin_neon_vsli_n_v((int8x8_t)__s0, (int8x8_t)__s1, __p2, 0); \
  __ret; \
})
#else
#define vsli_n_s8(__p0, __p1, __p2) __extension__ ({ \
  int8x8_t __s0 = __p0; \
  int8x8_t __s1 = __p1; \
  int8x8_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 7, 6, 5, 4, 3, 2, 1, 0); \
  int8x8_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 7, 6, 5, 4, 3, 2, 1, 0); \
  int8x8_t __ret; \
  __ret = (int8x8_t) __builtin_neon_vsli_n_v((int8x8_t)__rev0, (int8x8_t)__rev1, __p2, 0); \
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vsli_n_s32(__p0, __p1, __p2) __extension__ ({ \
  int32x2_t __s0 = __p0; \
  int32x2_t __s1 = __p1; \
  int32x2_t __ret; \
  __ret = (int32x2_t) __builtin_neon_vsli_n_v((int8x8_t)__s0, (int8x8_t)__s1, __p2, 2); \
  __ret; \
})
#else
#define vsli_n_s32(__p0, __p1, __p2) __extension__ ({ \
  int32x2_t __s0 = __p0; \
  int32x2_t __s1 = __p1; \
  int32x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  int32x2_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 1, 0); \
  int32x2_t __ret; \
  __ret = (int32x2_t) __builtin_neon_vsli_n_v((int8x8_t)__rev0, (int8x8_t)__rev1, __p2, 2); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vsli_n_s64(__p0, __p1, __p2) __extension__ ({ \
  int64x1_t __s0 = __p0; \
  int64x1_t __s1 = __p1; \
  int64x1_t __ret; \
  __ret = (int64x1_t) __builtin_neon_vsli_n_v((int8x8_t)__s0, (int8x8_t)__s1, __p2, 3); \
  __ret; \
})
#else
#define vsli_n_s64(__p0, __p1, __p2) __extension__ ({ \
  int64x1_t __s0 = __p0; \
  int64x1_t __s1 = __p1; \
  int64x1_t __ret; \
  __ret = (int64x1_t) __builtin_neon_vsli_n_v((int8x8_t)__s0, (int8x8_t)__s1, __p2, 3); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vsli_n_s16(__p0, __p1, __p2) __extension__ ({ \
  int16x4_t __s0 = __p0; \
  int16x4_t __s1 = __p1; \
  int16x4_t __ret; \
  __ret = (int16x4_t) __builtin_neon_vsli_n_v((int8x8_t)__s0, (int8x8_t)__s1, __p2, 1); \
  __ret; \
})
#else
#define vsli_n_s16(__p0, __p1, __p2) __extension__ ({ \
  int16x4_t __s0 = __p0; \
  int16x4_t __s1 = __p1; \
  int16x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  int16x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  int16x4_t __ret; \
  __ret = (int16x4_t) __builtin_neon_vsli_n_v((int8x8_t)__rev0, (int8x8_t)__rev1, __p2, 1); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vsraq_n_u8(__p0, __p1, __p2) __extension__ ({ \
  uint8x16_t __s0 = __p0; \
  uint8x16_t __s1 = __p1; \
  uint8x16_t __ret; \
  __ret = (uint8x16_t) __builtin_neon_vsraq_n_v((int8x16_t)__s0, (int8x16_t)__s1, __p2, 48); \
  __ret; \
})
#else
#define vsraq_n_u8(__p0, __p1, __p2) __extension__ ({ \
  uint8x16_t __s0 = __p0; \
  uint8x16_t __s1 = __p1; \
  uint8x16_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint8x16_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint8x16_t __ret; \
  __ret = (uint8x16_t) __builtin_neon_vsraq_n_v((int8x16_t)__rev0, (int8x16_t)__rev1, __p2, 48); \
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vsraq_n_u32(__p0, __p1, __p2) __extension__ ({ \
  uint32x4_t __s0 = __p0; \
  uint32x4_t __s1 = __p1; \
  uint32x4_t __ret; \
  __ret = (uint32x4_t) __builtin_neon_vsraq_n_v((int8x16_t)__s0, (int8x16_t)__s1, __p2, 50); \
  __ret; \
})
#else
#define vsraq_n_u32(__p0, __p1, __p2) __extension__ ({ \
  uint32x4_t __s0 = __p0; \
  uint32x4_t __s1 = __p1; \
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  uint32x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  uint32x4_t __ret; \
  __ret = (uint32x4_t) __builtin_neon_vsraq_n_v((int8x16_t)__rev0, (int8x16_t)__rev1, __p2, 50); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vsraq_n_u64(__p0, __p1, __p2) __extension__ ({ \
  uint64x2_t __s0 = __p0; \
  uint64x2_t __s1 = __p1; \
  uint64x2_t __ret; \
  __ret = (uint64x2_t) __builtin_neon_vsraq_n_v((int8x16_t)__s0, (int8x16_t)__s1, __p2, 51); \
  __ret; \
})
#else
#define vsraq_n_u64(__p0, __p1, __p2) __extension__ ({ \
  uint64x2_t __s0 = __p0; \
  uint64x2_t __s1 = __p1; \
  uint64x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  uint64x2_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 1, 0); \
  uint64x2_t __ret; \
  __ret = (uint64x2_t) __builtin_neon_vsraq_n_v((int8x16_t)__rev0, (int8x16_t)__rev1, __p2, 51); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vsraq_n_u16(__p0, __p1, __p2) __extension__ ({ \
  uint16x8_t __s0 = __p0; \
  uint16x8_t __s1 = __p1; \
  uint16x8_t __ret; \
  __ret = (uint16x8_t) __builtin_neon_vsraq_n_v((int8x16_t)__s0, (int8x16_t)__s1, __p2, 49); \
  __ret; \
})
#else
#define vsraq_n_u16(__p0, __p1, __p2) __extension__ ({ \
  uint16x8_t __s0 = __p0; \
  uint16x8_t __s1 = __p1; \
  uint16x8_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint16x8_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint16x8_t __ret; \
  __ret = (uint16x8_t) __builtin_neon_vsraq_n_v((int8x16_t)__rev0, (int8x16_t)__rev1, __p2, 49); \
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vsraq_n_s8(__p0, __p1, __p2) __extension__ ({ \
  int8x16_t __s0 = __p0; \
  int8x16_t __s1 = __p1; \
  int8x16_t __ret; \
  __ret = (int8x16_t) __builtin_neon_vsraq_n_v((int8x16_t)__s0, (int8x16_t)__s1, __p2, 32); \
  __ret; \
})
#else
#define vsraq_n_s8(__p0, __p1, __p2) __extension__ ({ \
  int8x16_t __s0 = __p0; \
  int8x16_t __s1 = __p1; \
  int8x16_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  int8x16_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  int8x16_t __ret; \
  __ret = (int8x16_t) __builtin_neon_vsraq_n_v((int8x16_t)__rev0, (int8x16_t)__rev1, __p2, 32); \
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vsraq_n_s32(__p0, __p1, __p2) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  int32x4_t __s1 = __p1; \
  int32x4_t __ret; \
  __ret = (int32x4_t) __builtin_neon_vsraq_n_v((int8x16_t)__s0, (int8x16_t)__s1, __p2, 34); \
  __ret; \
})
#else
#define vsraq_n_s32(__p0, __p1, __p2) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  int32x4_t __s1 = __p1; \
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  int32x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  int32x4_t __ret; \
  __ret = (int32x4_t) __builtin_neon_vsraq_n_v((int8x16_t)__rev0, (int8x16_t)__rev1, __p2, 34); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vsraq_n_s64(__p0, __p1, __p2) __extension__ ({ \
  int64x2_t __s0 = __p0; \
  int64x2_t __s1 = __p1; \
  int64x2_t __ret; \
  __ret = (int64x2_t) __builtin_neon_vsraq_n_v((int8x16_t)__s0, (int8x16_t)__s1, __p2, 35); \
  __ret; \
})
#else
#define vsraq_n_s64(__p0, __p1, __p2) __extension__ ({ \
  int64x2_t __s0 = __p0; \
  int64x2_t __s1 = __p1; \
  int64x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  int64x2_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 1, 0); \
  int64x2_t __ret; \
  __ret = (int64x2_t) __builtin_neon_vsraq_n_v((int8x16_t)__rev0, (int8x16_t)__rev1, __p2, 35); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vsraq_n_s16(__p0, __p1, __p2) __extension__ ({ \
  int16x8_t __s0 = __p0; \
  int16x8_t __s1 = __p1; \
  int16x8_t __ret; \
  __ret = (int16x8_t) __builtin_neon_vsraq_n_v((int8x16_t)__s0, (int8x16_t)__s1, __p2, 33); \
  __ret; \
})
#else
#define vsraq_n_s16(__p0, __p1, __p2) __extension__ ({ \
  int16x8_t __s0 = __p0; \
  int16x8_t __s1 = __p1; \
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 7, 6, 5, 4, 3, 2, 1, 0); \
  int16x8_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 7, 6, 5, 4, 3, 2, 1, 0); \
  int16x8_t __ret; \
  __ret = (int16x8_t) __builtin_neon_vsraq_n_v((int8x16_t)__rev0, (int8x16_t)__rev1, __p2, 33); \
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vsra_n_u8(__p0, __p1, __p2) __extension__ ({ \
  uint8x8_t __s0 = __p0; \
  uint8x8_t __s1 = __p1; \
  uint8x8_t __ret; \
  __ret = (uint8x8_t) __builtin_neon_vsra_n_v((int8x8_t)__s0, (int8x8_t)__s1, __p2, 16); \
  __ret; \
})
#else
#define vsra_n_u8(__p0, __p1, __p2) __extension__ ({ \
  uint8x8_t __s0 = __p0; \
  uint8x8_t __s1 = __p1; \
  uint8x8_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint8x8_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint8x8_t __ret; \
  __ret = (uint8x8_t) __builtin_neon_vsra_n_v((int8x8_t)__rev0, (int8x8_t)__rev1, __p2, 16); \
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vsra_n_u32(__p0, __p1, __p2) __extension__ ({ \
  uint32x2_t __s0 = __p0; \
  uint32x2_t __s1 = __p1; \
  uint32x2_t __ret; \
  __ret = (uint32x2_t) __builtin_neon_vsra_n_v((int8x8_t)__s0, (int8x8_t)__s1, __p2, 18); \
  __ret; \
})
#else
#define vsra_n_u32(__p0, __p1, __p2) __extension__ ({ \
  uint32x2_t __s0 = __p0; \
  uint32x2_t __s1 = __p1; \
  uint32x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  uint32x2_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 1, 0); \
  uint32x2_t __ret; \
  __ret = (uint32x2_t) __builtin_neon_vsra_n_v((int8x8_t)__rev0, (int8x8_t)__rev1, __p2, 18); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vsra_n_u64(__p0, __p1, __p2) __extension__ ({ \
  uint64x1_t __s0 = __p0; \
  uint64x1_t __s1 = __p1; \
  uint64x1_t __ret; \
  __ret = (uint64x1_t) __builtin_neon_vsra_n_v((int8x8_t)__s0, (int8x8_t)__s1, __p2, 19); \
  __ret; \
})
#else
#define vsra_n_u64(__p0, __p1, __p2) __extension__ ({ \
  uint64x1_t __s0 = __p0; \
  uint64x1_t __s1 = __p1; \
  uint64x1_t __ret; \
  __ret = (uint64x1_t) __builtin_neon_vsra_n_v((int8x8_t)__s0, (int8x8_t)__s1, __p2, 19); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vsra_n_u16(__p0, __p1, __p2) __extension__ ({ \
  uint16x4_t __s0 = __p0; \
  uint16x4_t __s1 = __p1; \
  uint16x4_t __ret; \
  __ret = (uint16x4_t) __builtin_neon_vsra_n_v((int8x8_t)__s0, (int8x8_t)__s1, __p2, 17); \
  __ret; \
})
#else
#define vsra_n_u16(__p0, __p1, __p2) __extension__ ({ \
  uint16x4_t __s0 = __p0; \
  uint16x4_t __s1 = __p1; \
  uint16x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  uint16x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  uint16x4_t __ret; \
  __ret = (uint16x4_t) __builtin_neon_vsra_n_v((int8x8_t)__rev0, (int8x8_t)__rev1, __p2, 17); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vsra_n_s8(__p0, __p1, __p2) __extension__ ({ \
  int8x8_t __s0 = __p0; \
  int8x8_t __s1 = __p1; \
  int8x8_t __ret; \
  __ret = (int8x8_t) __builtin_neon_vsra_n_v((int8x8_t)__s0, (int8x8_t)__s1, __p2, 0); \
  __ret; \
})
#else
#define vsra_n_s8(__p0, __p1, __p2) __extension__ ({ \
  int8x8_t __s0 = __p0; \
  int8x8_t __s1 = __p1; \
  int8x8_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 7, 6, 5, 4, 3, 2, 1, 0); \
  int8x8_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 7, 6, 5, 4, 3, 2, 1, 0); \
  int8x8_t __ret; \
  __ret = (int8x8_t) __builtin_neon_vsra_n_v((int8x8_t)__rev0, (int8x8_t)__rev1, __p2, 0); \
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vsra_n_s32(__p0, __p1, __p2) __extension__ ({ \
  int32x2_t __s0 = __p0; \
  int32x2_t __s1 = __p1; \
  int32x2_t __ret; \
  __ret = (int32x2_t) __builtin_neon_vsra_n_v((int8x8_t)__s0, (int8x8_t)__s1, __p2, 2); \
  __ret; \
})
#else
#define vsra_n_s32(__p0, __p1, __p2) __extension__ ({ \
  int32x2_t __s0 = __p0; \
  int32x2_t __s1 = __p1; \
  int32x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  int32x2_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 1, 0); \
  int32x2_t __ret; \
  __ret = (int32x2_t) __builtin_neon_vsra_n_v((int8x8_t)__rev0, (int8x8_t)__rev1, __p2, 2); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vsra_n_s64(__p0, __p1, __p2) __extension__ ({ \
  int64x1_t __s0 = __p0; \
  int64x1_t __s1 = __p1; \
  int64x1_t __ret; \
  __ret = (int64x1_t) __builtin_neon_vsra_n_v((int8x8_t)__s0, (int8x8_t)__s1, __p2, 3); \
  __ret; \
})
#else
#define vsra_n_s64(__p0, __p1, __p2) __extension__ ({ \
  int64x1_t __s0 = __p0; \
  int64x1_t __s1 = __p1; \
  int64x1_t __ret; \
  __ret = (int64x1_t) __builtin_neon_vsra_n_v((int8x8_t)__s0, (int8x8_t)__s1, __p2, 3); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vsra_n_s16(__p0, __p1, __p2) __extension__ ({ \
  int16x4_t __s0 = __p0; \
  int16x4_t __s1 = __p1; \
  int16x4_t __ret; \
  __ret = (int16x4_t) __builtin_neon_vsra_n_v((int8x8_t)__s0, (int8x8_t)__s1, __p2, 1); \
  __ret; \
})
#else
#define vsra_n_s16(__p0, __p1, __p2) __extension__ ({ \
  int16x4_t __s0 = __p0; \
  int16x4_t __s1 = __p1; \
  int16x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  int16x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  int16x4_t __ret; \
  __ret = (int16x4_t) __builtin_neon_vsra_n_v((int8x8_t)__rev0, (int8x8_t)__rev1, __p2, 1); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vsri_n_p8(__p0, __p1, __p2) __extension__ ({ \
  poly8x8_t __s0 = __p0; \
  poly8x8_t __s1 = __p1; \
  poly8x8_t __ret; \
  __ret = (poly8x8_t) __builtin_neon_vsri_n_v((int8x8_t)__s0, (int8x8_t)__s1, __p2, 4); \
  __ret; \
})
#else
#define vsri_n_p8(__p0, __p1, __p2) __extension__ ({ \
  poly8x8_t __s0 = __p0; \
  poly8x8_t __s1 = __p1; \
  poly8x8_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 7, 6, 5, 4, 3, 2, 1, 0); \
  poly8x8_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 7, 6, 5, 4, 3, 2, 1, 0); \
  poly8x8_t __ret; \
  __ret = (poly8x8_t) __builtin_neon_vsri_n_v((int8x8_t)__rev0, (int8x8_t)__rev1, __p2, 4); \
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vsri_n_p16(__p0, __p1, __p2) __extension__ ({ \
  poly16x4_t __s0 = __p0; \
  poly16x4_t __s1 = __p1; \
  poly16x4_t __ret; \
  __ret = (poly16x4_t) __builtin_neon_vsri_n_v((int8x8_t)__s0, (int8x8_t)__s1, __p2, 5); \
  __ret; \
})
#else
#define vsri_n_p16(__p0, __p1, __p2) __extension__ ({ \
  poly16x4_t __s0 = __p0; \
  poly16x4_t __s1 = __p1; \
  poly16x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  poly16x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  poly16x4_t __ret; \
  __ret = (poly16x4_t) __builtin_neon_vsri_n_v((int8x8_t)__rev0, (int8x8_t)__rev1, __p2, 5); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vsriq_n_p8(__p0, __p1, __p2) __extension__ ({ \
  poly8x16_t __s0 = __p0; \
  poly8x16_t __s1 = __p1; \
  poly8x16_t __ret; \
  __ret = (poly8x16_t) __builtin_neon_vsriq_n_v((int8x16_t)__s0, (int8x16_t)__s1, __p2, 36); \
  __ret; \
})
#else
#define vsriq_n_p8(__p0, __p1, __p2) __extension__ ({ \
  poly8x16_t __s0 = __p0; \
  poly8x16_t __s1 = __p1; \
  poly8x16_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  poly8x16_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  poly8x16_t __ret; \
  __ret = (poly8x16_t) __builtin_neon_vsriq_n_v((int8x16_t)__rev0, (int8x16_t)__rev1, __p2, 36); \
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vsriq_n_p16(__p0, __p1, __p2) __extension__ ({ \
  poly16x8_t __s0 = __p0; \
  poly16x8_t __s1 = __p1; \
  poly16x8_t __ret; \
  __ret = (poly16x8_t) __builtin_neon_vsriq_n_v((int8x16_t)__s0, (int8x16_t)__s1, __p2, 37); \
  __ret; \
})
#else
#define vsriq_n_p16(__p0, __p1, __p2) __extension__ ({ \
  poly16x8_t __s0 = __p0; \
  poly16x8_t __s1 = __p1; \
  poly16x8_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 7, 6, 5, 4, 3, 2, 1, 0); \
  poly16x8_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 7, 6, 5, 4, 3, 2, 1, 0); \
  poly16x8_t __ret; \
  __ret = (poly16x8_t) __builtin_neon_vsriq_n_v((int8x16_t)__rev0, (int8x16_t)__rev1, __p2, 37); \
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vsriq_n_u8(__p0, __p1, __p2) __extension__ ({ \
  uint8x16_t __s0 = __p0; \
  uint8x16_t __s1 = __p1; \
  uint8x16_t __ret; \
  __ret = (uint8x16_t) __builtin_neon_vsriq_n_v((int8x16_t)__s0, (int8x16_t)__s1, __p2, 48); \
  __ret; \
})
#else
#define vsriq_n_u8(__p0, __p1, __p2) __extension__ ({ \
  uint8x16_t __s0 = __p0; \
  uint8x16_t __s1 = __p1; \
  uint8x16_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint8x16_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint8x16_t __ret; \
  __ret = (uint8x16_t) __builtin_neon_vsriq_n_v((int8x16_t)__rev0, (int8x16_t)__rev1, __p2, 48); \
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vsriq_n_u32(__p0, __p1, __p2) __extension__ ({ \
  uint32x4_t __s0 = __p0; \
  uint32x4_t __s1 = __p1; \
  uint32x4_t __ret; \
  __ret = (uint32x4_t) __builtin_neon_vsriq_n_v((int8x16_t)__s0, (int8x16_t)__s1, __p2, 50); \
  __ret; \
})
#else
#define vsriq_n_u32(__p0, __p1, __p2) __extension__ ({ \
  uint32x4_t __s0 = __p0; \
  uint32x4_t __s1 = __p1; \
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  uint32x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  uint32x4_t __ret; \
  __ret = (uint32x4_t) __builtin_neon_vsriq_n_v((int8x16_t)__rev0, (int8x16_t)__rev1, __p2, 50); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vsriq_n_u64(__p0, __p1, __p2) __extension__ ({ \
  uint64x2_t __s0 = __p0; \
  uint64x2_t __s1 = __p1; \
  uint64x2_t __ret; \
  __ret = (uint64x2_t) __builtin_neon_vsriq_n_v((int8x16_t)__s0, (int8x16_t)__s1, __p2, 51); \
  __ret; \
})
#else
#define vsriq_n_u64(__p0, __p1, __p2) __extension__ ({ \
  uint64x2_t __s0 = __p0; \
  uint64x2_t __s1 = __p1; \
  uint64x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  uint64x2_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 1, 0); \
  uint64x2_t __ret; \
  __ret = (uint64x2_t) __builtin_neon_vsriq_n_v((int8x16_t)__rev0, (int8x16_t)__rev1, __p2, 51); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vsriq_n_u16(__p0, __p1, __p2) __extension__ ({ \
  uint16x8_t __s0 = __p0; \
  uint16x8_t __s1 = __p1; \
  uint16x8_t __ret; \
  __ret = (uint16x8_t) __builtin_neon_vsriq_n_v((int8x16_t)__s0, (int8x16_t)__s1, __p2, 49); \
  __ret; \
})
#else
#define vsriq_n_u16(__p0, __p1, __p2) __extension__ ({ \
  uint16x8_t __s0 = __p0; \
  uint16x8_t __s1 = __p1; \
  uint16x8_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint16x8_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint16x8_t __ret; \
  __ret = (uint16x8_t) __builtin_neon_vsriq_n_v((int8x16_t)__rev0, (int8x16_t)__rev1, __p2, 49); \
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vsriq_n_s8(__p0, __p1, __p2) __extension__ ({ \
  int8x16_t __s0 = __p0; \
  int8x16_t __s1 = __p1; \
  int8x16_t __ret; \
  __ret = (int8x16_t) __builtin_neon_vsriq_n_v((int8x16_t)__s0, (int8x16_t)__s1, __p2, 32); \
  __ret; \
})
#else
#define vsriq_n_s8(__p0, __p1, __p2) __extension__ ({ \
  int8x16_t __s0 = __p0; \
  int8x16_t __s1 = __p1; \
  int8x16_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  int8x16_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  int8x16_t __ret; \
  __ret = (int8x16_t) __builtin_neon_vsriq_n_v((int8x16_t)__rev0, (int8x16_t)__rev1, __p2, 32); \
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vsriq_n_s32(__p0, __p1, __p2) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  int32x4_t __s1 = __p1; \
  int32x4_t __ret; \
  __ret = (int32x4_t) __builtin_neon_vsriq_n_v((int8x16_t)__s0, (int8x16_t)__s1, __p2, 34); \
  __ret; \
})
#else
#define vsriq_n_s32(__p0, __p1, __p2) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  int32x4_t __s1 = __p1; \
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  int32x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  int32x4_t __ret; \
  __ret = (int32x4_t) __builtin_neon_vsriq_n_v((int8x16_t)__rev0, (int8x16_t)__rev1, __p2, 34); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vsriq_n_s64(__p0, __p1, __p2) __extension__ ({ \
  int64x2_t __s0 = __p0; \
  int64x2_t __s1 = __p1; \
  int64x2_t __ret; \
  __ret = (int64x2_t) __builtin_neon_vsriq_n_v((int8x16_t)__s0, (int8x16_t)__s1, __p2, 35); \
  __ret; \
})
#else
#define vsriq_n_s64(__p0, __p1, __p2) __extension__ ({ \
  int64x2_t __s0 = __p0; \
  int64x2_t __s1 = __p1; \
  int64x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  int64x2_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 1, 0); \
  int64x2_t __ret; \
  __ret = (int64x2_t) __builtin_neon_vsriq_n_v((int8x16_t)__rev0, (int8x16_t)__rev1, __p2, 35); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vsriq_n_s16(__p0, __p1, __p2) __extension__ ({ \
  int16x8_t __s0 = __p0; \
  int16x8_t __s1 = __p1; \
  int16x8_t __ret; \
  __ret = (int16x8_t) __builtin_neon_vsriq_n_v((int8x16_t)__s0, (int8x16_t)__s1, __p2, 33); \
  __ret; \
})
#else
#define vsriq_n_s16(__p0, __p1, __p2) __extension__ ({ \
  int16x8_t __s0 = __p0; \
  int16x8_t __s1 = __p1; \
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 7, 6, 5, 4, 3, 2, 1, 0); \
  int16x8_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 7, 6, 5, 4, 3, 2, 1, 0); \
  int16x8_t __ret; \
  __ret = (int16x8_t) __builtin_neon_vsriq_n_v((int8x16_t)__rev0, (int8x16_t)__rev1, __p2, 33); \
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vsri_n_u8(__p0, __p1, __p2) __extension__ ({ \
  uint8x8_t __s0 = __p0; \
  uint8x8_t __s1 = __p1; \
  uint8x8_t __ret; \
  __ret = (uint8x8_t) __builtin_neon_vsri_n_v((int8x8_t)__s0, (int8x8_t)__s1, __p2, 16); \
  __ret; \
})
#else
#define vsri_n_u8(__p0, __p1, __p2) __extension__ ({ \
  uint8x8_t __s0 = __p0; \
  uint8x8_t __s1 = __p1; \
  uint8x8_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint8x8_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint8x8_t __ret; \
  __ret = (uint8x8_t) __builtin_neon_vsri_n_v((int8x8_t)__rev0, (int8x8_t)__rev1, __p2, 16); \
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vsri_n_u32(__p0, __p1, __p2) __extension__ ({ \
  uint32x2_t __s0 = __p0; \
  uint32x2_t __s1 = __p1; \
  uint32x2_t __ret; \
  __ret = (uint32x2_t) __builtin_neon_vsri_n_v((int8x8_t)__s0, (int8x8_t)__s1, __p2, 18); \
  __ret; \
})
#else
#define vsri_n_u32(__p0, __p1, __p2) __extension__ ({ \
  uint32x2_t __s0 = __p0; \
  uint32x2_t __s1 = __p1; \
  uint32x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  uint32x2_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 1, 0); \
  uint32x2_t __ret; \
  __ret = (uint32x2_t) __builtin_neon_vsri_n_v((int8x8_t)__rev0, (int8x8_t)__rev1, __p2, 18); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vsri_n_u64(__p0, __p1, __p2) __extension__ ({ \
  uint64x1_t __s0 = __p0; \
  uint64x1_t __s1 = __p1; \
  uint64x1_t __ret; \
  __ret = (uint64x1_t) __builtin_neon_vsri_n_v((int8x8_t)__s0, (int8x8_t)__s1, __p2, 19); \
  __ret; \
})
#else
#define vsri_n_u64(__p0, __p1, __p2) __extension__ ({ \
  uint64x1_t __s0 = __p0; \
  uint64x1_t __s1 = __p1; \
  uint64x1_t __ret; \
  __ret = (uint64x1_t) __builtin_neon_vsri_n_v((int8x8_t)__s0, (int8x8_t)__s1, __p2, 19); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vsri_n_u16(__p0, __p1, __p2) __extension__ ({ \
  uint16x4_t __s0 = __p0; \
  uint16x4_t __s1 = __p1; \
  uint16x4_t __ret; \
  __ret = (uint16x4_t) __builtin_neon_vsri_n_v((int8x8_t)__s0, (int8x8_t)__s1, __p2, 17); \
  __ret; \
})
#else
#define vsri_n_u16(__p0, __p1, __p2) __extension__ ({ \
  uint16x4_t __s0 = __p0; \
  uint16x4_t __s1 = __p1; \
  uint16x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  uint16x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  uint16x4_t __ret; \
  __ret = (uint16x4_t) __builtin_neon_vsri_n_v((int8x8_t)__rev0, (int8x8_t)__rev1, __p2, 17); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vsri_n_s8(__p0, __p1, __p2) __extension__ ({ \
  int8x8_t __s0 = __p0; \
  int8x8_t __s1 = __p1; \
  int8x8_t __ret; \
  __ret = (int8x8_t) __builtin_neon_vsri_n_v((int8x8_t)__s0, (int8x8_t)__s1, __p2, 0); \
  __ret; \
})
#else
#define vsri_n_s8(__p0, __p1, __p2) __extension__ ({ \
  int8x8_t __s0 = __p0; \
  int8x8_t __s1 = __p1; \
  int8x8_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 7, 6, 5, 4, 3, 2, 1, 0); \
  int8x8_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 7, 6, 5, 4, 3, 2, 1, 0); \
  int8x8_t __ret; \
  __ret = (int8x8_t) __builtin_neon_vsri_n_v((int8x8_t)__rev0, (int8x8_t)__rev1, __p2, 0); \
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vsri_n_s32(__p0, __p1, __p2) __extension__ ({ \
  int32x2_t __s0 = __p0; \
  int32x2_t __s1 = __p1; \
  int32x2_t __ret; \
  __ret = (int32x2_t) __builtin_neon_vsri_n_v((int8x8_t)__s0, (int8x8_t)__s1, __p2, 2); \
  __ret; \
})
#else
#define vsri_n_s32(__p0, __p1, __p2) __extension__ ({ \
  int32x2_t __s0 = __p0; \
  int32x2_t __s1 = __p1; \
  int32x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  int32x2_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 1, 0); \
  int32x2_t __ret; \
  __ret = (int32x2_t) __builtin_neon_vsri_n_v((int8x8_t)__rev0, (int8x8_t)__rev1, __p2, 2); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vsri_n_s64(__p0, __p1, __p2) __extension__ ({ \
  int64x1_t __s0 = __p0; \
  int64x1_t __s1 = __p1; \
  int64x1_t __ret; \
  __ret = (int64x1_t) __builtin_neon_vsri_n_v((int8x8_t)__s0, (int8x8_t)__s1, __p2, 3); \
  __ret; \
})
#else
#define vsri_n_s64(__p0, __p1, __p2) __extension__ ({ \
  int64x1_t __s0 = __p0; \
  int64x1_t __s1 = __p1; \
  int64x1_t __ret; \
  __ret = (int64x1_t) __builtin_neon_vsri_n_v((int8x8_t)__s0, (int8x8_t)__s1, __p2, 3); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vsri_n_s16(__p0, __p1, __p2) __extension__ ({ \
  int16x4_t __s0 = __p0; \
  int16x4_t __s1 = __p1; \
  int16x4_t __ret; \
  __ret = (int16x4_t) __builtin_neon_vsri_n_v((int8x8_t)__s0, (int8x8_t)__s1, __p2, 1); \
  __ret; \
})
#else
#define vsri_n_s16(__p0, __p1, __p2) __extension__ ({ \
  int16x4_t __s0 = __p0; \
  int16x4_t __s1 = __p1; \
  int16x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  int16x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  int16x4_t __ret; \
  __ret = (int16x4_t) __builtin_neon_vsri_n_v((int8x8_t)__rev0, (int8x8_t)__rev1, __p2, 1); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1_p8(__p0, __p1) __extension__ ({ \
  poly8x8_t __s1 = __p1; \
  __builtin_neon_vst1_v(__p0, (int8x8_t)__s1, 4); \
})
#else
#define vst1_p8(__p0, __p1) __extension__ ({ \
  poly8x8_t __s1 = __p1; \
  poly8x8_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst1_v(__p0, (int8x8_t)__rev1, 4); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1_p16(__p0, __p1) __extension__ ({ \
  poly16x4_t __s1 = __p1; \
  __builtin_neon_vst1_v(__p0, (int8x8_t)__s1, 5); \
})
#else
#define vst1_p16(__p0, __p1) __extension__ ({ \
  poly16x4_t __s1 = __p1; \
  poly16x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  __builtin_neon_vst1_v(__p0, (int8x8_t)__rev1, 5); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1q_p8(__p0, __p1) __extension__ ({ \
  poly8x16_t __s1 = __p1; \
  __builtin_neon_vst1q_v(__p0, (int8x16_t)__s1, 36); \
})
#else
#define vst1q_p8(__p0, __p1) __extension__ ({ \
  poly8x16_t __s1 = __p1; \
  poly8x16_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst1q_v(__p0, (int8x16_t)__rev1, 36); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1q_p16(__p0, __p1) __extension__ ({ \
  poly16x8_t __s1 = __p1; \
  __builtin_neon_vst1q_v(__p0, (int8x16_t)__s1, 37); \
})
#else
#define vst1q_p16(__p0, __p1) __extension__ ({ \
  poly16x8_t __s1 = __p1; \
  poly16x8_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst1q_v(__p0, (int8x16_t)__rev1, 37); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1q_u8(__p0, __p1) __extension__ ({ \
  uint8x16_t __s1 = __p1; \
  __builtin_neon_vst1q_v(__p0, (int8x16_t)__s1, 48); \
})
#else
#define vst1q_u8(__p0, __p1) __extension__ ({ \
  uint8x16_t __s1 = __p1; \
  uint8x16_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst1q_v(__p0, (int8x16_t)__rev1, 48); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1q_u32(__p0, __p1) __extension__ ({ \
  uint32x4_t __s1 = __p1; \
  __builtin_neon_vst1q_v(__p0, (int8x16_t)__s1, 50); \
})
#else
#define vst1q_u32(__p0, __p1) __extension__ ({ \
  uint32x4_t __s1 = __p1; \
  uint32x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  __builtin_neon_vst1q_v(__p0, (int8x16_t)__rev1, 50); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1q_u64(__p0, __p1) __extension__ ({ \
  uint64x2_t __s1 = __p1; \
  __builtin_neon_vst1q_v(__p0, (int8x16_t)__s1, 51); \
})
#else
#define vst1q_u64(__p0, __p1) __extension__ ({ \
  uint64x2_t __s1 = __p1; \
  uint64x2_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 1, 0); \
  __builtin_neon_vst1q_v(__p0, (int8x16_t)__rev1, 51); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1q_u16(__p0, __p1) __extension__ ({ \
  uint16x8_t __s1 = __p1; \
  __builtin_neon_vst1q_v(__p0, (int8x16_t)__s1, 49); \
})
#else
#define vst1q_u16(__p0, __p1) __extension__ ({ \
  uint16x8_t __s1 = __p1; \
  uint16x8_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst1q_v(__p0, (int8x16_t)__rev1, 49); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1q_s8(__p0, __p1) __extension__ ({ \
  int8x16_t __s1 = __p1; \
  __builtin_neon_vst1q_v(__p0, (int8x16_t)__s1, 32); \
})
#else
#define vst1q_s8(__p0, __p1) __extension__ ({ \
  int8x16_t __s1 = __p1; \
  int8x16_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst1q_v(__p0, (int8x16_t)__rev1, 32); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1q_f32(__p0, __p1) __extension__ ({ \
  float32x4_t __s1 = __p1; \
  __builtin_neon_vst1q_v(__p0, (int8x16_t)__s1, 41); \
})
#else
#define vst1q_f32(__p0, __p1) __extension__ ({ \
  float32x4_t __s1 = __p1; \
  float32x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  __builtin_neon_vst1q_v(__p0, (int8x16_t)__rev1, 41); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1q_f16(__p0, __p1) __extension__ ({ \
  float16x8_t __s1 = __p1; \
  __builtin_neon_vst1q_v(__p0, (int8x16_t)__s1, 40); \
})
#else
#define vst1q_f16(__p0, __p1) __extension__ ({ \
  float16x8_t __s1 = __p1; \
  float16x8_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst1q_v(__p0, (int8x16_t)__rev1, 40); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1q_s32(__p0, __p1) __extension__ ({ \
  int32x4_t __s1 = __p1; \
  __builtin_neon_vst1q_v(__p0, (int8x16_t)__s1, 34); \
})
#else
#define vst1q_s32(__p0, __p1) __extension__ ({ \
  int32x4_t __s1 = __p1; \
  int32x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  __builtin_neon_vst1q_v(__p0, (int8x16_t)__rev1, 34); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1q_s64(__p0, __p1) __extension__ ({ \
  int64x2_t __s1 = __p1; \
  __builtin_neon_vst1q_v(__p0, (int8x16_t)__s1, 35); \
})
#else
#define vst1q_s64(__p0, __p1) __extension__ ({ \
  int64x2_t __s1 = __p1; \
  int64x2_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 1, 0); \
  __builtin_neon_vst1q_v(__p0, (int8x16_t)__rev1, 35); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1q_s16(__p0, __p1) __extension__ ({ \
  int16x8_t __s1 = __p1; \
  __builtin_neon_vst1q_v(__p0, (int8x16_t)__s1, 33); \
})
#else
#define vst1q_s16(__p0, __p1) __extension__ ({ \
  int16x8_t __s1 = __p1; \
  int16x8_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst1q_v(__p0, (int8x16_t)__rev1, 33); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1_u8(__p0, __p1) __extension__ ({ \
  uint8x8_t __s1 = __p1; \
  __builtin_neon_vst1_v(__p0, (int8x8_t)__s1, 16); \
})
#else
#define vst1_u8(__p0, __p1) __extension__ ({ \
  uint8x8_t __s1 = __p1; \
  uint8x8_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst1_v(__p0, (int8x8_t)__rev1, 16); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1_u32(__p0, __p1) __extension__ ({ \
  uint32x2_t __s1 = __p1; \
  __builtin_neon_vst1_v(__p0, (int8x8_t)__s1, 18); \
})
#else
#define vst1_u32(__p0, __p1) __extension__ ({ \
  uint32x2_t __s1 = __p1; \
  uint32x2_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 1, 0); \
  __builtin_neon_vst1_v(__p0, (int8x8_t)__rev1, 18); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1_u64(__p0, __p1) __extension__ ({ \
  uint64x1_t __s1 = __p1; \
  __builtin_neon_vst1_v(__p0, (int8x8_t)__s1, 19); \
})
#else
#define vst1_u64(__p0, __p1) __extension__ ({ \
  uint64x1_t __s1 = __p1; \
  __builtin_neon_vst1_v(__p0, (int8x8_t)__s1, 19); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1_u16(__p0, __p1) __extension__ ({ \
  uint16x4_t __s1 = __p1; \
  __builtin_neon_vst1_v(__p0, (int8x8_t)__s1, 17); \
})
#else
#define vst1_u16(__p0, __p1) __extension__ ({ \
  uint16x4_t __s1 = __p1; \
  uint16x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  __builtin_neon_vst1_v(__p0, (int8x8_t)__rev1, 17); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1_s8(__p0, __p1) __extension__ ({ \
  int8x8_t __s1 = __p1; \
  __builtin_neon_vst1_v(__p0, (int8x8_t)__s1, 0); \
})
#else
#define vst1_s8(__p0, __p1) __extension__ ({ \
  int8x8_t __s1 = __p1; \
  int8x8_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst1_v(__p0, (int8x8_t)__rev1, 0); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1_f32(__p0, __p1) __extension__ ({ \
  float32x2_t __s1 = __p1; \
  __builtin_neon_vst1_v(__p0, (int8x8_t)__s1, 9); \
})
#else
#define vst1_f32(__p0, __p1) __extension__ ({ \
  float32x2_t __s1 = __p1; \
  float32x2_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 1, 0); \
  __builtin_neon_vst1_v(__p0, (int8x8_t)__rev1, 9); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1_f16(__p0, __p1) __extension__ ({ \
  float16x4_t __s1 = __p1; \
  __builtin_neon_vst1_v(__p0, (int8x8_t)__s1, 8); \
})
#else
#define vst1_f16(__p0, __p1) __extension__ ({ \
  float16x4_t __s1 = __p1; \
  float16x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  __builtin_neon_vst1_v(__p0, (int8x8_t)__rev1, 8); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1_s32(__p0, __p1) __extension__ ({ \
  int32x2_t __s1 = __p1; \
  __builtin_neon_vst1_v(__p0, (int8x8_t)__s1, 2); \
})
#else
#define vst1_s32(__p0, __p1) __extension__ ({ \
  int32x2_t __s1 = __p1; \
  int32x2_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 1, 0); \
  __builtin_neon_vst1_v(__p0, (int8x8_t)__rev1, 2); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1_s64(__p0, __p1) __extension__ ({ \
  int64x1_t __s1 = __p1; \
  __builtin_neon_vst1_v(__p0, (int8x8_t)__s1, 3); \
})
#else
#define vst1_s64(__p0, __p1) __extension__ ({ \
  int64x1_t __s1 = __p1; \
  __builtin_neon_vst1_v(__p0, (int8x8_t)__s1, 3); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1_s16(__p0, __p1) __extension__ ({ \
  int16x4_t __s1 = __p1; \
  __builtin_neon_vst1_v(__p0, (int8x8_t)__s1, 1); \
})
#else
#define vst1_s16(__p0, __p1) __extension__ ({ \
  int16x4_t __s1 = __p1; \
  int16x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  __builtin_neon_vst1_v(__p0, (int8x8_t)__rev1, 1); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1_lane_p8(__p0, __p1, __p2) __extension__ ({ \
  poly8x8_t __s1 = __p1; \
  __builtin_neon_vst1_lane_v(__p0, (int8x8_t)__s1, __p2, 4); \
})
#else
#define vst1_lane_p8(__p0, __p1, __p2) __extension__ ({ \
  poly8x8_t __s1 = __p1; \
  poly8x8_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst1_lane_v(__p0, (int8x8_t)__rev1, __p2, 4); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1_lane_p16(__p0, __p1, __p2) __extension__ ({ \
  poly16x4_t __s1 = __p1; \
  __builtin_neon_vst1_lane_v(__p0, (int8x8_t)__s1, __p2, 5); \
})
#else
#define vst1_lane_p16(__p0, __p1, __p2) __extension__ ({ \
  poly16x4_t __s1 = __p1; \
  poly16x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  __builtin_neon_vst1_lane_v(__p0, (int8x8_t)__rev1, __p2, 5); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1q_lane_p8(__p0, __p1, __p2) __extension__ ({ \
  poly8x16_t __s1 = __p1; \
  __builtin_neon_vst1q_lane_v(__p0, (int8x16_t)__s1, __p2, 36); \
})
#else
#define vst1q_lane_p8(__p0, __p1, __p2) __extension__ ({ \
  poly8x16_t __s1 = __p1; \
  poly8x16_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst1q_lane_v(__p0, (int8x16_t)__rev1, __p2, 36); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1q_lane_p16(__p0, __p1, __p2) __extension__ ({ \
  poly16x8_t __s1 = __p1; \
  __builtin_neon_vst1q_lane_v(__p0, (int8x16_t)__s1, __p2, 37); \
})
#else
#define vst1q_lane_p16(__p0, __p1, __p2) __extension__ ({ \
  poly16x8_t __s1 = __p1; \
  poly16x8_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst1q_lane_v(__p0, (int8x16_t)__rev1, __p2, 37); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1q_lane_u8(__p0, __p1, __p2) __extension__ ({ \
  uint8x16_t __s1 = __p1; \
  __builtin_neon_vst1q_lane_v(__p0, (int8x16_t)__s1, __p2, 48); \
})
#else
#define vst1q_lane_u8(__p0, __p1, __p2) __extension__ ({ \
  uint8x16_t __s1 = __p1; \
  uint8x16_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst1q_lane_v(__p0, (int8x16_t)__rev1, __p2, 48); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1q_lane_u32(__p0, __p1, __p2) __extension__ ({ \
  uint32x4_t __s1 = __p1; \
  __builtin_neon_vst1q_lane_v(__p0, (int8x16_t)__s1, __p2, 50); \
})
#else
#define vst1q_lane_u32(__p0, __p1, __p2) __extension__ ({ \
  uint32x4_t __s1 = __p1; \
  uint32x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  __builtin_neon_vst1q_lane_v(__p0, (int8x16_t)__rev1, __p2, 50); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1q_lane_u64(__p0, __p1, __p2) __extension__ ({ \
  uint64x2_t __s1 = __p1; \
  __builtin_neon_vst1q_lane_v(__p0, (int8x16_t)__s1, __p2, 51); \
})
#else
#define vst1q_lane_u64(__p0, __p1, __p2) __extension__ ({ \
  uint64x2_t __s1 = __p1; \
  uint64x2_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 1, 0); \
  __builtin_neon_vst1q_lane_v(__p0, (int8x16_t)__rev1, __p2, 51); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1q_lane_u16(__p0, __p1, __p2) __extension__ ({ \
  uint16x8_t __s1 = __p1; \
  __builtin_neon_vst1q_lane_v(__p0, (int8x16_t)__s1, __p2, 49); \
})
#else
#define vst1q_lane_u16(__p0, __p1, __p2) __extension__ ({ \
  uint16x8_t __s1 = __p1; \
  uint16x8_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst1q_lane_v(__p0, (int8x16_t)__rev1, __p2, 49); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1q_lane_s8(__p0, __p1, __p2) __extension__ ({ \
  int8x16_t __s1 = __p1; \
  __builtin_neon_vst1q_lane_v(__p0, (int8x16_t)__s1, __p2, 32); \
})
#else
#define vst1q_lane_s8(__p0, __p1, __p2) __extension__ ({ \
  int8x16_t __s1 = __p1; \
  int8x16_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst1q_lane_v(__p0, (int8x16_t)__rev1, __p2, 32); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1q_lane_f32(__p0, __p1, __p2) __extension__ ({ \
  float32x4_t __s1 = __p1; \
  __builtin_neon_vst1q_lane_v(__p0, (int8x16_t)__s1, __p2, 41); \
})
#else
#define vst1q_lane_f32(__p0, __p1, __p2) __extension__ ({ \
  float32x4_t __s1 = __p1; \
  float32x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  __builtin_neon_vst1q_lane_v(__p0, (int8x16_t)__rev1, __p2, 41); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1q_lane_f16(__p0, __p1, __p2) __extension__ ({ \
  float16x8_t __s1 = __p1; \
  __builtin_neon_vst1q_lane_v(__p0, (int8x16_t)__s1, __p2, 40); \
})
#else
#define vst1q_lane_f16(__p0, __p1, __p2) __extension__ ({ \
  float16x8_t __s1 = __p1; \
  float16x8_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst1q_lane_v(__p0, (int8x16_t)__rev1, __p2, 40); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1q_lane_s32(__p0, __p1, __p2) __extension__ ({ \
  int32x4_t __s1 = __p1; \
  __builtin_neon_vst1q_lane_v(__p0, (int8x16_t)__s1, __p2, 34); \
})
#else
#define vst1q_lane_s32(__p0, __p1, __p2) __extension__ ({ \
  int32x4_t __s1 = __p1; \
  int32x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  __builtin_neon_vst1q_lane_v(__p0, (int8x16_t)__rev1, __p2, 34); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1q_lane_s64(__p0, __p1, __p2) __extension__ ({ \
  int64x2_t __s1 = __p1; \
  __builtin_neon_vst1q_lane_v(__p0, (int8x16_t)__s1, __p2, 35); \
})
#else
#define vst1q_lane_s64(__p0, __p1, __p2) __extension__ ({ \
  int64x2_t __s1 = __p1; \
  int64x2_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 1, 0); \
  __builtin_neon_vst1q_lane_v(__p0, (int8x16_t)__rev1, __p2, 35); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1q_lane_s16(__p0, __p1, __p2) __extension__ ({ \
  int16x8_t __s1 = __p1; \
  __builtin_neon_vst1q_lane_v(__p0, (int8x16_t)__s1, __p2, 33); \
})
#else
#define vst1q_lane_s16(__p0, __p1, __p2) __extension__ ({ \
  int16x8_t __s1 = __p1; \
  int16x8_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst1q_lane_v(__p0, (int8x16_t)__rev1, __p2, 33); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1_lane_u8(__p0, __p1, __p2) __extension__ ({ \
  uint8x8_t __s1 = __p1; \
  __builtin_neon_vst1_lane_v(__p0, (int8x8_t)__s1, __p2, 16); \
})
#else
#define vst1_lane_u8(__p0, __p1, __p2) __extension__ ({ \
  uint8x8_t __s1 = __p1; \
  uint8x8_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst1_lane_v(__p0, (int8x8_t)__rev1, __p2, 16); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1_lane_u32(__p0, __p1, __p2) __extension__ ({ \
  uint32x2_t __s1 = __p1; \
  __builtin_neon_vst1_lane_v(__p0, (int8x8_t)__s1, __p2, 18); \
})
#else
#define vst1_lane_u32(__p0, __p1, __p2) __extension__ ({ \
  uint32x2_t __s1 = __p1; \
  uint32x2_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 1, 0); \
  __builtin_neon_vst1_lane_v(__p0, (int8x8_t)__rev1, __p2, 18); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1_lane_u64(__p0, __p1, __p2) __extension__ ({ \
  uint64x1_t __s1 = __p1; \
  __builtin_neon_vst1_lane_v(__p0, (int8x8_t)__s1, __p2, 19); \
})
#else
#define vst1_lane_u64(__p0, __p1, __p2) __extension__ ({ \
  uint64x1_t __s1 = __p1; \
  __builtin_neon_vst1_lane_v(__p0, (int8x8_t)__s1, __p2, 19); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1_lane_u16(__p0, __p1, __p2) __extension__ ({ \
  uint16x4_t __s1 = __p1; \
  __builtin_neon_vst1_lane_v(__p0, (int8x8_t)__s1, __p2, 17); \
})
#else
#define vst1_lane_u16(__p0, __p1, __p2) __extension__ ({ \
  uint16x4_t __s1 = __p1; \
  uint16x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  __builtin_neon_vst1_lane_v(__p0, (int8x8_t)__rev1, __p2, 17); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1_lane_s8(__p0, __p1, __p2) __extension__ ({ \
  int8x8_t __s1 = __p1; \
  __builtin_neon_vst1_lane_v(__p0, (int8x8_t)__s1, __p2, 0); \
})
#else
#define vst1_lane_s8(__p0, __p1, __p2) __extension__ ({ \
  int8x8_t __s1 = __p1; \
  int8x8_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst1_lane_v(__p0, (int8x8_t)__rev1, __p2, 0); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1_lane_f32(__p0, __p1, __p2) __extension__ ({ \
  float32x2_t __s1 = __p1; \
  __builtin_neon_vst1_lane_v(__p0, (int8x8_t)__s1, __p2, 9); \
})
#else
#define vst1_lane_f32(__p0, __p1, __p2) __extension__ ({ \
  float32x2_t __s1 = __p1; \
  float32x2_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 1, 0); \
  __builtin_neon_vst1_lane_v(__p0, (int8x8_t)__rev1, __p2, 9); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1_lane_f16(__p0, __p1, __p2) __extension__ ({ \
  float16x4_t __s1 = __p1; \
  __builtin_neon_vst1_lane_v(__p0, (int8x8_t)__s1, __p2, 8); \
})
#else
#define vst1_lane_f16(__p0, __p1, __p2) __extension__ ({ \
  float16x4_t __s1 = __p1; \
  float16x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  __builtin_neon_vst1_lane_v(__p0, (int8x8_t)__rev1, __p2, 8); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1_lane_s32(__p0, __p1, __p2) __extension__ ({ \
  int32x2_t __s1 = __p1; \
  __builtin_neon_vst1_lane_v(__p0, (int8x8_t)__s1, __p2, 2); \
})
#else
#define vst1_lane_s32(__p0, __p1, __p2) __extension__ ({ \
  int32x2_t __s1 = __p1; \
  int32x2_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 1, 0); \
  __builtin_neon_vst1_lane_v(__p0, (int8x8_t)__rev1, __p2, 2); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1_lane_s64(__p0, __p1, __p2) __extension__ ({ \
  int64x1_t __s1 = __p1; \
  __builtin_neon_vst1_lane_v(__p0, (int8x8_t)__s1, __p2, 3); \
})
#else
#define vst1_lane_s64(__p0, __p1, __p2) __extension__ ({ \
  int64x1_t __s1 = __p1; \
  __builtin_neon_vst1_lane_v(__p0, (int8x8_t)__s1, __p2, 3); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst1_lane_s16(__p0, __p1, __p2) __extension__ ({ \
  int16x4_t __s1 = __p1; \
  __builtin_neon_vst1_lane_v(__p0, (int8x8_t)__s1, __p2, 1); \
})
#else
#define vst1_lane_s16(__p0, __p1, __p2) __extension__ ({ \
  int16x4_t __s1 = __p1; \
  int16x4_t __rev1;  __rev1 = __builtin_shufflevector(__s1, __s1, 3, 2, 1, 0); \
  __builtin_neon_vst1_lane_v(__p0, (int8x8_t)__rev1, __p2, 1); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst2_p8(__p0, __p1) __extension__ ({ \
  poly8x8x2_t __s1 = __p1; \
  __builtin_neon_vst2_v(__p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], 4); \
})
#else
#define vst2_p8(__p0, __p1) __extension__ ({ \
  poly8x8x2_t __s1 = __p1; \
  poly8x8x2_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst2_v(__p0, (int8x8_t)__rev1.val[0], (int8x8_t)__rev1.val[1], 4); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst2_p16(__p0, __p1) __extension__ ({ \
  poly16x4x2_t __s1 = __p1; \
  __builtin_neon_vst2_v(__p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], 5); \
})
#else
#define vst2_p16(__p0, __p1) __extension__ ({ \
  poly16x4x2_t __s1 = __p1; \
  poly16x4x2_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 3, 2, 1, 0); \
  __builtin_neon_vst2_v(__p0, (int8x8_t)__rev1.val[0], (int8x8_t)__rev1.val[1], 5); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst2q_p8(__p0, __p1) __extension__ ({ \
  poly8x16x2_t __s1 = __p1; \
  __builtin_neon_vst2q_v(__p0, (int8x16_t)__s1.val[0], (int8x16_t)__s1.val[1], 36); \
})
#else
#define vst2q_p8(__p0, __p1) __extension__ ({ \
  poly8x16x2_t __s1 = __p1; \
  poly8x16x2_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst2q_v(__p0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], 36); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst2q_p16(__p0, __p1) __extension__ ({ \
  poly16x8x2_t __s1 = __p1; \
  __builtin_neon_vst2q_v(__p0, (int8x16_t)__s1.val[0], (int8x16_t)__s1.val[1], 37); \
})
#else
#define vst2q_p16(__p0, __p1) __extension__ ({ \
  poly16x8x2_t __s1 = __p1; \
  poly16x8x2_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst2q_v(__p0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], 37); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst2q_u8(__p0, __p1) __extension__ ({ \
  uint8x16x2_t __s1 = __p1; \
  __builtin_neon_vst2q_v(__p0, (int8x16_t)__s1.val[0], (int8x16_t)__s1.val[1], 48); \
})
#else
#define vst2q_u8(__p0, __p1) __extension__ ({ \
  uint8x16x2_t __s1 = __p1; \
  uint8x16x2_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst2q_v(__p0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], 48); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst2q_u32(__p0, __p1) __extension__ ({ \
  uint32x4x2_t __s1 = __p1; \
  __builtin_neon_vst2q_v(__p0, (int8x16_t)__s1.val[0], (int8x16_t)__s1.val[1], 50); \
})
#else
#define vst2q_u32(__p0, __p1) __extension__ ({ \
  uint32x4x2_t __s1 = __p1; \
  uint32x4x2_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 3, 2, 1, 0); \
  __builtin_neon_vst2q_v(__p0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], 50); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst2q_u16(__p0, __p1) __extension__ ({ \
  uint16x8x2_t __s1 = __p1; \
  __builtin_neon_vst2q_v(__p0, (int8x16_t)__s1.val[0], (int8x16_t)__s1.val[1], 49); \
})
#else
#define vst2q_u16(__p0, __p1) __extension__ ({ \
  uint16x8x2_t __s1 = __p1; \
  uint16x8x2_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst2q_v(__p0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], 49); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst2q_s8(__p0, __p1) __extension__ ({ \
  int8x16x2_t __s1 = __p1; \
  __builtin_neon_vst2q_v(__p0, (int8x16_t)__s1.val[0], (int8x16_t)__s1.val[1], 32); \
})
#else
#define vst2q_s8(__p0, __p1) __extension__ ({ \
  int8x16x2_t __s1 = __p1; \
  int8x16x2_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst2q_v(__p0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], 32); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst2q_f32(__p0, __p1) __extension__ ({ \
  float32x4x2_t __s1 = __p1; \
  __builtin_neon_vst2q_v(__p0, __s1.val[0], __s1.val[1], 41); \
})
#else
#define vst2q_f32(__p0, __p1) __extension__ ({ \
  float32x4x2_t __s1 = __p1; \
  float32x4x2_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 3, 2, 1, 0); \
  __builtin_neon_vst2q_v(__p0, __rev1.val[0], __rev1.val[1], 41); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst2q_f16(__p0, __p1) __extension__ ({ \
  float16x8x2_t __s1 = __p1; \
  __builtin_neon_vst2q_v(__p0, __s1.val[0], __s1.val[1], 40); \
})
#else
#define vst2q_f16(__p0, __p1) __extension__ ({ \
  float16x8x2_t __s1 = __p1; \
  float16x8x2_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst2q_v(__p0, __rev1.val[0], __rev1.val[1], 40); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst2q_s32(__p0, __p1) __extension__ ({ \
  int32x4x2_t __s1 = __p1; \
  __builtin_neon_vst2q_v(__p0, __s1.val[0], __s1.val[1], 34); \
})
#else
#define vst2q_s32(__p0, __p1) __extension__ ({ \
  int32x4x2_t __s1 = __p1; \
  int32x4x2_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 3, 2, 1, 0); \
  __builtin_neon_vst2q_v(__p0, __rev1.val[0], __rev1.val[1], 34); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst2q_s16(__p0, __p1) __extension__ ({ \
  int16x8x2_t __s1 = __p1; \
  __builtin_neon_vst2q_v(__p0, __s1.val[0], __s1.val[1], 33); \
})
#else
#define vst2q_s16(__p0, __p1) __extension__ ({ \
  int16x8x2_t __s1 = __p1; \
  int16x8x2_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst2q_v(__p0, __rev1.val[0], __rev1.val[1], 33); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst2_u8(__p0, __p1) __extension__ ({ \
  uint8x8x2_t __s1 = __p1; \
  __builtin_neon_vst2_v(__p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], 16); \
})
#else
#define vst2_u8(__p0, __p1) __extension__ ({ \
  uint8x8x2_t __s1 = __p1; \
  uint8x8x2_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst2_v(__p0, (int8x8_t)__rev1.val[0], (int8x8_t)__rev1.val[1], 16); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst2_u32(__p0, __p1) __extension__ ({ \
  uint32x2x2_t __s1 = __p1; \
  __builtin_neon_vst2_v(__p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], 18); \
})
#else
#define vst2_u32(__p0, __p1) __extension__ ({ \
  uint32x2x2_t __s1 = __p1; \
  uint32x2x2_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 1, 0); \
  __builtin_neon_vst2_v(__p0, (int8x8_t)__rev1.val[0], (int8x8_t)__rev1.val[1], 18); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst2_u64(__p0, __p1) __extension__ ({ \
  uint64x1x2_t __s1 = __p1; \
  __builtin_neon_vst2_v(__p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], 19); \
})
#else
#define vst2_u64(__p0, __p1) __extension__ ({ \
  uint64x1x2_t __s1 = __p1; \
  __builtin_neon_vst2_v(__p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], 19); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst2_u16(__p0, __p1) __extension__ ({ \
  uint16x4x2_t __s1 = __p1; \
  __builtin_neon_vst2_v(__p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], 17); \
})
#else
#define vst2_u16(__p0, __p1) __extension__ ({ \
  uint16x4x2_t __s1 = __p1; \
  uint16x4x2_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 3, 2, 1, 0); \
  __builtin_neon_vst2_v(__p0, (int8x8_t)__rev1.val[0], (int8x8_t)__rev1.val[1], 17); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst2_s8(__p0, __p1) __extension__ ({ \
  int8x8x2_t __s1 = __p1; \
  __builtin_neon_vst2_v(__p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], 0); \
})
#else
#define vst2_s8(__p0, __p1) __extension__ ({ \
  int8x8x2_t __s1 = __p1; \
  int8x8x2_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst2_v(__p0, (int8x8_t)__rev1.val[0], (int8x8_t)__rev1.val[1], 0); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst2_f32(__p0, __p1) __extension__ ({ \
  float32x2x2_t __s1 = __p1; \
  __builtin_neon_vst2_v(__p0, __s1.val[0], __s1.val[1], 9); \
})
#else
#define vst2_f32(__p0, __p1) __extension__ ({ \
  float32x2x2_t __s1 = __p1; \
  float32x2x2_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 1, 0); \
  __builtin_neon_vst2_v(__p0, __rev1.val[0], __rev1.val[1], 9); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst2_f16(__p0, __p1) __extension__ ({ \
  float16x4x2_t __s1 = __p1; \
  __builtin_neon_vst2_v(__p0, __s1.val[0], __s1.val[1], 8); \
})
#else
#define vst2_f16(__p0, __p1) __extension__ ({ \
  float16x4x2_t __s1 = __p1; \
  float16x4x2_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 3, 2, 1, 0); \
  __builtin_neon_vst2_v(__p0, __rev1.val[0], __rev1.val[1], 8); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst2_s32(__p0, __p1) __extension__ ({ \
  int32x2x2_t __s1 = __p1; \
  __builtin_neon_vst2_v(__p0, __s1.val[0], __s1.val[1], 2); \
})
#else
#define vst2_s32(__p0, __p1) __extension__ ({ \
  int32x2x2_t __s1 = __p1; \
  int32x2x2_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 1, 0); \
  __builtin_neon_vst2_v(__p0, __rev1.val[0], __rev1.val[1], 2); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst2_s64(__p0, __p1) __extension__ ({ \
  int64x1x2_t __s1 = __p1; \
  __builtin_neon_vst2_v(__p0, __s1.val[0], __s1.val[1], 3); \
})
#else
#define vst2_s64(__p0, __p1) __extension__ ({ \
  int64x1x2_t __s1 = __p1; \
  __builtin_neon_vst2_v(__p0, __s1.val[0], __s1.val[1], 3); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst2_s16(__p0, __p1) __extension__ ({ \
  int16x4x2_t __s1 = __p1; \
  __builtin_neon_vst2_v(__p0, __s1.val[0], __s1.val[1], 1); \
})
#else
#define vst2_s16(__p0, __p1) __extension__ ({ \
  int16x4x2_t __s1 = __p1; \
  int16x4x2_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 3, 2, 1, 0); \
  __builtin_neon_vst2_v(__p0, __rev1.val[0], __rev1.val[1], 1); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst2_lane_p8(__p0, __p1, __p2) __extension__ ({ \
  poly8x8x2_t __s1 = __p1; \
  __builtin_neon_vst2_lane_v(__p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], __p2, 4); \
})
#else
#define vst2_lane_p8(__p0, __p1, __p2) __extension__ ({ \
  poly8x8x2_t __s1 = __p1; \
  poly8x8x2_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst2_lane_v(__p0, (int8x8_t)__rev1.val[0], (int8x8_t)__rev1.val[1], __p2, 4); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst2_lane_p16(__p0, __p1, __p2) __extension__ ({ \
  poly16x4x2_t __s1 = __p1; \
  __builtin_neon_vst2_lane_v(__p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], __p2, 5); \
})
#else
#define vst2_lane_p16(__p0, __p1, __p2) __extension__ ({ \
  poly16x4x2_t __s1 = __p1; \
  poly16x4x2_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 3, 2, 1, 0); \
  __builtin_neon_vst2_lane_v(__p0, (int8x8_t)__rev1.val[0], (int8x8_t)__rev1.val[1], __p2, 5); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst2q_lane_p16(__p0, __p1, __p2) __extension__ ({ \
  poly16x8x2_t __s1 = __p1; \
  __builtin_neon_vst2q_lane_v(__p0, (int8x16_t)__s1.val[0], (int8x16_t)__s1.val[1], __p2, 37); \
})
#else
#define vst2q_lane_p16(__p0, __p1, __p2) __extension__ ({ \
  poly16x8x2_t __s1 = __p1; \
  poly16x8x2_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst2q_lane_v(__p0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], __p2, 37); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst2q_lane_u32(__p0, __p1, __p2) __extension__ ({ \
  uint32x4x2_t __s1 = __p1; \
  __builtin_neon_vst2q_lane_v(__p0, (int8x16_t)__s1.val[0], (int8x16_t)__s1.val[1], __p2, 50); \
})
#else
#define vst2q_lane_u32(__p0, __p1, __p2) __extension__ ({ \
  uint32x4x2_t __s1 = __p1; \
  uint32x4x2_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 3, 2, 1, 0); \
  __builtin_neon_vst2q_lane_v(__p0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], __p2, 50); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst2q_lane_u16(__p0, __p1, __p2) __extension__ ({ \
  uint16x8x2_t __s1 = __p1; \
  __builtin_neon_vst2q_lane_v(__p0, (int8x16_t)__s1.val[0], (int8x16_t)__s1.val[1], __p2, 49); \
})
#else
#define vst2q_lane_u16(__p0, __p1, __p2) __extension__ ({ \
  uint16x8x2_t __s1 = __p1; \
  uint16x8x2_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst2q_lane_v(__p0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], __p2, 49); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst2q_lane_f32(__p0, __p1, __p2) __extension__ ({ \
  float32x4x2_t __s1 = __p1; \
  __builtin_neon_vst2q_lane_v(__p0, __s1.val[0], __s1.val[1], __p2, 41); \
})
#else
#define vst2q_lane_f32(__p0, __p1, __p2) __extension__ ({ \
  float32x4x2_t __s1 = __p1; \
  float32x4x2_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 3, 2, 1, 0); \
  __builtin_neon_vst2q_lane_v(__p0, __rev1.val[0], __rev1.val[1], __p2, 41); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst2q_lane_f16(__p0, __p1, __p2) __extension__ ({ \
  float16x8x2_t __s1 = __p1; \
  __builtin_neon_vst2q_lane_v(__p0, __s1.val[0], __s1.val[1], __p2, 40); \
})
#else
#define vst2q_lane_f16(__p0, __p1, __p2) __extension__ ({ \
  float16x8x2_t __s1 = __p1; \
  float16x8x2_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst2q_lane_v(__p0, __rev1.val[0], __rev1.val[1], __p2, 40); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst2q_lane_s32(__p0, __p1, __p2) __extension__ ({ \
  int32x4x2_t __s1 = __p1; \
  __builtin_neon_vst2q_lane_v(__p0, __s1.val[0], __s1.val[1], __p2, 34); \
})
#else
#define vst2q_lane_s32(__p0, __p1, __p2) __extension__ ({ \
  int32x4x2_t __s1 = __p1; \
  int32x4x2_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 3, 2, 1, 0); \
  __builtin_neon_vst2q_lane_v(__p0, __rev1.val[0], __rev1.val[1], __p2, 34); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst2q_lane_s16(__p0, __p1, __p2) __extension__ ({ \
  int16x8x2_t __s1 = __p1; \
  __builtin_neon_vst2q_lane_v(__p0, __s1.val[0], __s1.val[1], __p2, 33); \
})
#else
#define vst2q_lane_s16(__p0, __p1, __p2) __extension__ ({ \
  int16x8x2_t __s1 = __p1; \
  int16x8x2_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst2q_lane_v(__p0, __rev1.val[0], __rev1.val[1], __p2, 33); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst2_lane_u8(__p0, __p1, __p2) __extension__ ({ \
  uint8x8x2_t __s1 = __p1; \
  __builtin_neon_vst2_lane_v(__p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], __p2, 16); \
})
#else
#define vst2_lane_u8(__p0, __p1, __p2) __extension__ ({ \
  uint8x8x2_t __s1 = __p1; \
  uint8x8x2_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst2_lane_v(__p0, (int8x8_t)__rev1.val[0], (int8x8_t)__rev1.val[1], __p2, 16); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst2_lane_u32(__p0, __p1, __p2) __extension__ ({ \
  uint32x2x2_t __s1 = __p1; \
  __builtin_neon_vst2_lane_v(__p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], __p2, 18); \
})
#else
#define vst2_lane_u32(__p0, __p1, __p2) __extension__ ({ \
  uint32x2x2_t __s1 = __p1; \
  uint32x2x2_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 1, 0); \
  __builtin_neon_vst2_lane_v(__p0, (int8x8_t)__rev1.val[0], (int8x8_t)__rev1.val[1], __p2, 18); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst2_lane_u16(__p0, __p1, __p2) __extension__ ({ \
  uint16x4x2_t __s1 = __p1; \
  __builtin_neon_vst2_lane_v(__p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], __p2, 17); \
})
#else
#define vst2_lane_u16(__p0, __p1, __p2) __extension__ ({ \
  uint16x4x2_t __s1 = __p1; \
  uint16x4x2_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 3, 2, 1, 0); \
  __builtin_neon_vst2_lane_v(__p0, (int8x8_t)__rev1.val[0], (int8x8_t)__rev1.val[1], __p2, 17); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst2_lane_s8(__p0, __p1, __p2) __extension__ ({ \
  int8x8x2_t __s1 = __p1; \
  __builtin_neon_vst2_lane_v(__p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], __p2, 0); \
})
#else
#define vst2_lane_s8(__p0, __p1, __p2) __extension__ ({ \
  int8x8x2_t __s1 = __p1; \
  int8x8x2_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst2_lane_v(__p0, (int8x8_t)__rev1.val[0], (int8x8_t)__rev1.val[1], __p2, 0); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst2_lane_f32(__p0, __p1, __p2) __extension__ ({ \
  float32x2x2_t __s1 = __p1; \
  __builtin_neon_vst2_lane_v(__p0, __s1.val[0], __s1.val[1], __p2, 9); \
})
#else
#define vst2_lane_f32(__p0, __p1, __p2) __extension__ ({ \
  float32x2x2_t __s1 = __p1; \
  float32x2x2_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 1, 0); \
  __builtin_neon_vst2_lane_v(__p0, __rev1.val[0], __rev1.val[1], __p2, 9); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst2_lane_f16(__p0, __p1, __p2) __extension__ ({ \
  float16x4x2_t __s1 = __p1; \
  __builtin_neon_vst2_lane_v(__p0, __s1.val[0], __s1.val[1], __p2, 8); \
})
#else
#define vst2_lane_f16(__p0, __p1, __p2) __extension__ ({ \
  float16x4x2_t __s1 = __p1; \
  float16x4x2_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 3, 2, 1, 0); \
  __builtin_neon_vst2_lane_v(__p0, __rev1.val[0], __rev1.val[1], __p2, 8); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst2_lane_s32(__p0, __p1, __p2) __extension__ ({ \
  int32x2x2_t __s1 = __p1; \
  __builtin_neon_vst2_lane_v(__p0, __s1.val[0], __s1.val[1], __p2, 2); \
})
#else
#define vst2_lane_s32(__p0, __p1, __p2) __extension__ ({ \
  int32x2x2_t __s1 = __p1; \
  int32x2x2_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 1, 0); \
  __builtin_neon_vst2_lane_v(__p0, __rev1.val[0], __rev1.val[1], __p2, 2); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst2_lane_s16(__p0, __p1, __p2) __extension__ ({ \
  int16x4x2_t __s1 = __p1; \
  __builtin_neon_vst2_lane_v(__p0, __s1.val[0], __s1.val[1], __p2, 1); \
})
#else
#define vst2_lane_s16(__p0, __p1, __p2) __extension__ ({ \
  int16x4x2_t __s1 = __p1; \
  int16x4x2_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 3, 2, 1, 0); \
  __builtin_neon_vst2_lane_v(__p0, __rev1.val[0], __rev1.val[1], __p2, 1); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst3_p8(__p0, __p1) __extension__ ({ \
  poly8x8x3_t __s1 = __p1; \
  __builtin_neon_vst3_v(__p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], (int8x8_t)__s1.val[2], 4); \
})
#else
#define vst3_p8(__p0, __p1) __extension__ ({ \
  poly8x8x3_t __s1 = __p1; \
  poly8x8x3_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst3_v(__p0, (int8x8_t)__rev1.val[0], (int8x8_t)__rev1.val[1], (int8x8_t)__rev1.val[2], 4); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst3_p16(__p0, __p1) __extension__ ({ \
  poly16x4x3_t __s1 = __p1; \
  __builtin_neon_vst3_v(__p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], (int8x8_t)__s1.val[2], 5); \
})
#else
#define vst3_p16(__p0, __p1) __extension__ ({ \
  poly16x4x3_t __s1 = __p1; \
  poly16x4x3_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 3, 2, 1, 0); \
  __builtin_neon_vst3_v(__p0, (int8x8_t)__rev1.val[0], (int8x8_t)__rev1.val[1], (int8x8_t)__rev1.val[2], 5); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst3q_p8(__p0, __p1) __extension__ ({ \
  poly8x16x3_t __s1 = __p1; \
  __builtin_neon_vst3q_v(__p0, (int8x16_t)__s1.val[0], (int8x16_t)__s1.val[1], (int8x16_t)__s1.val[2], 36); \
})
#else
#define vst3q_p8(__p0, __p1) __extension__ ({ \
  poly8x16x3_t __s1 = __p1; \
  poly8x16x3_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst3q_v(__p0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], (int8x16_t)__rev1.val[2], 36); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst3q_p16(__p0, __p1) __extension__ ({ \
  poly16x8x3_t __s1 = __p1; \
  __builtin_neon_vst3q_v(__p0, (int8x16_t)__s1.val[0], (int8x16_t)__s1.val[1], (int8x16_t)__s1.val[2], 37); \
})
#else
#define vst3q_p16(__p0, __p1) __extension__ ({ \
  poly16x8x3_t __s1 = __p1; \
  poly16x8x3_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst3q_v(__p0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], (int8x16_t)__rev1.val[2], 37); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst3q_u8(__p0, __p1) __extension__ ({ \
  uint8x16x3_t __s1 = __p1; \
  __builtin_neon_vst3q_v(__p0, (int8x16_t)__s1.val[0], (int8x16_t)__s1.val[1], (int8x16_t)__s1.val[2], 48); \
})
#else
#define vst3q_u8(__p0, __p1) __extension__ ({ \
  uint8x16x3_t __s1 = __p1; \
  uint8x16x3_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst3q_v(__p0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], (int8x16_t)__rev1.val[2], 48); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst3q_u32(__p0, __p1) __extension__ ({ \
  uint32x4x3_t __s1 = __p1; \
  __builtin_neon_vst3q_v(__p0, (int8x16_t)__s1.val[0], (int8x16_t)__s1.val[1], (int8x16_t)__s1.val[2], 50); \
})
#else
#define vst3q_u32(__p0, __p1) __extension__ ({ \
  uint32x4x3_t __s1 = __p1; \
  uint32x4x3_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 3, 2, 1, 0); \
  __builtin_neon_vst3q_v(__p0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], (int8x16_t)__rev1.val[2], 50); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst3q_u16(__p0, __p1) __extension__ ({ \
  uint16x8x3_t __s1 = __p1; \
  __builtin_neon_vst3q_v(__p0, (int8x16_t)__s1.val[0], (int8x16_t)__s1.val[1], (int8x16_t)__s1.val[2], 49); \
})
#else
#define vst3q_u16(__p0, __p1) __extension__ ({ \
  uint16x8x3_t __s1 = __p1; \
  uint16x8x3_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst3q_v(__p0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], (int8x16_t)__rev1.val[2], 49); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst3q_s8(__p0, __p1) __extension__ ({ \
  int8x16x3_t __s1 = __p1; \
  __builtin_neon_vst3q_v(__p0, (int8x16_t)__s1.val[0], (int8x16_t)__s1.val[1], (int8x16_t)__s1.val[2], 32); \
})
#else
#define vst3q_s8(__p0, __p1) __extension__ ({ \
  int8x16x3_t __s1 = __p1; \
  int8x16x3_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst3q_v(__p0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], (int8x16_t)__rev1.val[2], 32); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst3q_f32(__p0, __p1) __extension__ ({ \
  float32x4x3_t __s1 = __p1; \
  __builtin_neon_vst3q_v(__p0, __s1.val[0], __s1.val[1], __s1.val[2], 41); \
})
#else
#define vst3q_f32(__p0, __p1) __extension__ ({ \
  float32x4x3_t __s1 = __p1; \
  float32x4x3_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 3, 2, 1, 0); \
  __builtin_neon_vst3q_v(__p0, __rev1.val[0], __rev1.val[1], __rev1.val[2], 41); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst3q_f16(__p0, __p1) __extension__ ({ \
  float16x8x3_t __s1 = __p1; \
  __builtin_neon_vst3q_v(__p0, __s1.val[0], __s1.val[1], __s1.val[2], 40); \
})
#else
#define vst3q_f16(__p0, __p1) __extension__ ({ \
  float16x8x3_t __s1 = __p1; \
  float16x8x3_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst3q_v(__p0, __rev1.val[0], __rev1.val[1], __rev1.val[2], 40); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst3q_s32(__p0, __p1) __extension__ ({ \
  int32x4x3_t __s1 = __p1; \
  __builtin_neon_vst3q_v(__p0, __s1.val[0], __s1.val[1], __s1.val[2], 34); \
})
#else
#define vst3q_s32(__p0, __p1) __extension__ ({ \
  int32x4x3_t __s1 = __p1; \
  int32x4x3_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 3, 2, 1, 0); \
  __builtin_neon_vst3q_v(__p0, __rev1.val[0], __rev1.val[1], __rev1.val[2], 34); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst3q_s16(__p0, __p1) __extension__ ({ \
  int16x8x3_t __s1 = __p1; \
  __builtin_neon_vst3q_v(__p0, __s1.val[0], __s1.val[1], __s1.val[2], 33); \
})
#else
#define vst3q_s16(__p0, __p1) __extension__ ({ \
  int16x8x3_t __s1 = __p1; \
  int16x8x3_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst3q_v(__p0, __rev1.val[0], __rev1.val[1], __rev1.val[2], 33); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst3_u8(__p0, __p1) __extension__ ({ \
  uint8x8x3_t __s1 = __p1; \
  __builtin_neon_vst3_v(__p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], (int8x8_t)__s1.val[2], 16); \
})
#else
#define vst3_u8(__p0, __p1) __extension__ ({ \
  uint8x8x3_t __s1 = __p1; \
  uint8x8x3_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst3_v(__p0, (int8x8_t)__rev1.val[0], (int8x8_t)__rev1.val[1], (int8x8_t)__rev1.val[2], 16); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst3_u32(__p0, __p1) __extension__ ({ \
  uint32x2x3_t __s1 = __p1; \
  __builtin_neon_vst3_v(__p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], (int8x8_t)__s1.val[2], 18); \
})
#else
#define vst3_u32(__p0, __p1) __extension__ ({ \
  uint32x2x3_t __s1 = __p1; \
  uint32x2x3_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 1, 0); \
  __builtin_neon_vst3_v(__p0, (int8x8_t)__rev1.val[0], (int8x8_t)__rev1.val[1], (int8x8_t)__rev1.val[2], 18); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst3_u64(__p0, __p1) __extension__ ({ \
  uint64x1x3_t __s1 = __p1; \
  __builtin_neon_vst3_v(__p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], (int8x8_t)__s1.val[2], 19); \
})
#else
#define vst3_u64(__p0, __p1) __extension__ ({ \
  uint64x1x3_t __s1 = __p1; \
  __builtin_neon_vst3_v(__p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], (int8x8_t)__s1.val[2], 19); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst3_u16(__p0, __p1) __extension__ ({ \
  uint16x4x3_t __s1 = __p1; \
  __builtin_neon_vst3_v(__p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], (int8x8_t)__s1.val[2], 17); \
})
#else
#define vst3_u16(__p0, __p1) __extension__ ({ \
  uint16x4x3_t __s1 = __p1; \
  uint16x4x3_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 3, 2, 1, 0); \
  __builtin_neon_vst3_v(__p0, (int8x8_t)__rev1.val[0], (int8x8_t)__rev1.val[1], (int8x8_t)__rev1.val[2], 17); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst3_s8(__p0, __p1) __extension__ ({ \
  int8x8x3_t __s1 = __p1; \
  __builtin_neon_vst3_v(__p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], (int8x8_t)__s1.val[2], 0); \
})
#else
#define vst3_s8(__p0, __p1) __extension__ ({ \
  int8x8x3_t __s1 = __p1; \
  int8x8x3_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst3_v(__p0, (int8x8_t)__rev1.val[0], (int8x8_t)__rev1.val[1], (int8x8_t)__rev1.val[2], 0); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst3_f32(__p0, __p1) __extension__ ({ \
  float32x2x3_t __s1 = __p1; \
  __builtin_neon_vst3_v(__p0, __s1.val[0], __s1.val[1], __s1.val[2], 9); \
})
#else
#define vst3_f32(__p0, __p1) __extension__ ({ \
  float32x2x3_t __s1 = __p1; \
  float32x2x3_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 1, 0); \
  __builtin_neon_vst3_v(__p0, __rev1.val[0], __rev1.val[1], __rev1.val[2], 9); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst3_f16(__p0, __p1) __extension__ ({ \
  float16x4x3_t __s1 = __p1; \
  __builtin_neon_vst3_v(__p0, __s1.val[0], __s1.val[1], __s1.val[2], 8); \
})
#else
#define vst3_f16(__p0, __p1) __extension__ ({ \
  float16x4x3_t __s1 = __p1; \
  float16x4x3_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 3, 2, 1, 0); \
  __builtin_neon_vst3_v(__p0, __rev1.val[0], __rev1.val[1], __rev1.val[2], 8); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst3_s32(__p0, __p1) __extension__ ({ \
  int32x2x3_t __s1 = __p1; \
  __builtin_neon_vst3_v(__p0, __s1.val[0], __s1.val[1], __s1.val[2], 2); \
})
#else
#define vst3_s32(__p0, __p1) __extension__ ({ \
  int32x2x3_t __s1 = __p1; \
  int32x2x3_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 1, 0); \
  __builtin_neon_vst3_v(__p0, __rev1.val[0], __rev1.val[1], __rev1.val[2], 2); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst3_s64(__p0, __p1) __extension__ ({ \
  int64x1x3_t __s1 = __p1; \
  __builtin_neon_vst3_v(__p0, __s1.val[0], __s1.val[1], __s1.val[2], 3); \
})
#else
#define vst3_s64(__p0, __p1) __extension__ ({ \
  int64x1x3_t __s1 = __p1; \
  __builtin_neon_vst3_v(__p0, __s1.val[0], __s1.val[1], __s1.val[2], 3); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst3_s16(__p0, __p1) __extension__ ({ \
  int16x4x3_t __s1 = __p1; \
  __builtin_neon_vst3_v(__p0, __s1.val[0], __s1.val[1], __s1.val[2], 1); \
})
#else
#define vst3_s16(__p0, __p1) __extension__ ({ \
  int16x4x3_t __s1 = __p1; \
  int16x4x3_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 3, 2, 1, 0); \
  __builtin_neon_vst3_v(__p0, __rev1.val[0], __rev1.val[1], __rev1.val[2], 1); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst3_lane_p8(__p0, __p1, __p2) __extension__ ({ \
  poly8x8x3_t __s1 = __p1; \
  __builtin_neon_vst3_lane_v(__p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], (int8x8_t)__s1.val[2], __p2, 4); \
})
#else
#define vst3_lane_p8(__p0, __p1, __p2) __extension__ ({ \
  poly8x8x3_t __s1 = __p1; \
  poly8x8x3_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst3_lane_v(__p0, (int8x8_t)__rev1.val[0], (int8x8_t)__rev1.val[1], (int8x8_t)__rev1.val[2], __p2, 4); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst3_lane_p16(__p0, __p1, __p2) __extension__ ({ \
  poly16x4x3_t __s1 = __p1; \
  __builtin_neon_vst3_lane_v(__p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], (int8x8_t)__s1.val[2], __p2, 5); \
})
#else
#define vst3_lane_p16(__p0, __p1, __p2) __extension__ ({ \
  poly16x4x3_t __s1 = __p1; \
  poly16x4x3_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 3, 2, 1, 0); \
  __builtin_neon_vst3_lane_v(__p0, (int8x8_t)__rev1.val[0], (int8x8_t)__rev1.val[1], (int8x8_t)__rev1.val[2], __p2, 5); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst3q_lane_p16(__p0, __p1, __p2) __extension__ ({ \
  poly16x8x3_t __s1 = __p1; \
  __builtin_neon_vst3q_lane_v(__p0, (int8x16_t)__s1.val[0], (int8x16_t)__s1.val[1], (int8x16_t)__s1.val[2], __p2, 37); \
})
#else
#define vst3q_lane_p16(__p0, __p1, __p2) __extension__ ({ \
  poly16x8x3_t __s1 = __p1; \
  poly16x8x3_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst3q_lane_v(__p0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], (int8x16_t)__rev1.val[2], __p2, 37); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst3q_lane_u32(__p0, __p1, __p2) __extension__ ({ \
  uint32x4x3_t __s1 = __p1; \
  __builtin_neon_vst3q_lane_v(__p0, (int8x16_t)__s1.val[0], (int8x16_t)__s1.val[1], (int8x16_t)__s1.val[2], __p2, 50); \
})
#else
#define vst3q_lane_u32(__p0, __p1, __p2) __extension__ ({ \
  uint32x4x3_t __s1 = __p1; \
  uint32x4x3_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 3, 2, 1, 0); \
  __builtin_neon_vst3q_lane_v(__p0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], (int8x16_t)__rev1.val[2], __p2, 50); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst3q_lane_u16(__p0, __p1, __p2) __extension__ ({ \
  uint16x8x3_t __s1 = __p1; \
  __builtin_neon_vst3q_lane_v(__p0, (int8x16_t)__s1.val[0], (int8x16_t)__s1.val[1], (int8x16_t)__s1.val[2], __p2, 49); \
})
#else
#define vst3q_lane_u16(__p0, __p1, __p2) __extension__ ({ \
  uint16x8x3_t __s1 = __p1; \
  uint16x8x3_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst3q_lane_v(__p0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], (int8x16_t)__rev1.val[2], __p2, 49); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst3q_lane_f32(__p0, __p1, __p2) __extension__ ({ \
  float32x4x3_t __s1 = __p1; \
  __builtin_neon_vst3q_lane_v(__p0, __s1.val[0], __s1.val[1], __s1.val[2], __p2, 41); \
})
#else
#define vst3q_lane_f32(__p0, __p1, __p2) __extension__ ({ \
  float32x4x3_t __s1 = __p1; \
  float32x4x3_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 3, 2, 1, 0); \
  __builtin_neon_vst3q_lane_v(__p0, __rev1.val[0], __rev1.val[1], __rev1.val[2], __p2, 41); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst3q_lane_f16(__p0, __p1, __p2) __extension__ ({ \
  float16x8x3_t __s1 = __p1; \
  __builtin_neon_vst3q_lane_v(__p0, __s1.val[0], __s1.val[1], __s1.val[2], __p2, 40); \
})
#else
#define vst3q_lane_f16(__p0, __p1, __p2) __extension__ ({ \
  float16x8x3_t __s1 = __p1; \
  float16x8x3_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst3q_lane_v(__p0, __rev1.val[0], __rev1.val[1], __rev1.val[2], __p2, 40); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst3q_lane_s32(__p0, __p1, __p2) __extension__ ({ \
  int32x4x3_t __s1 = __p1; \
  __builtin_neon_vst3q_lane_v(__p0, __s1.val[0], __s1.val[1], __s1.val[2], __p2, 34); \
})
#else
#define vst3q_lane_s32(__p0, __p1, __p2) __extension__ ({ \
  int32x4x3_t __s1 = __p1; \
  int32x4x3_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 3, 2, 1, 0); \
  __builtin_neon_vst3q_lane_v(__p0, __rev1.val[0], __rev1.val[1], __rev1.val[2], __p2, 34); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst3q_lane_s16(__p0, __p1, __p2) __extension__ ({ \
  int16x8x3_t __s1 = __p1; \
  __builtin_neon_vst3q_lane_v(__p0, __s1.val[0], __s1.val[1], __s1.val[2], __p2, 33); \
})
#else
#define vst3q_lane_s16(__p0, __p1, __p2) __extension__ ({ \
  int16x8x3_t __s1 = __p1; \
  int16x8x3_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst3q_lane_v(__p0, __rev1.val[0], __rev1.val[1], __rev1.val[2], __p2, 33); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst3_lane_u8(__p0, __p1, __p2) __extension__ ({ \
  uint8x8x3_t __s1 = __p1; \
  __builtin_neon_vst3_lane_v(__p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], (int8x8_t)__s1.val[2], __p2, 16); \
})
#else
#define vst3_lane_u8(__p0, __p1, __p2) __extension__ ({ \
  uint8x8x3_t __s1 = __p1; \
  uint8x8x3_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst3_lane_v(__p0, (int8x8_t)__rev1.val[0], (int8x8_t)__rev1.val[1], (int8x8_t)__rev1.val[2], __p2, 16); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst3_lane_u32(__p0, __p1, __p2) __extension__ ({ \
  uint32x2x3_t __s1 = __p1; \
  __builtin_neon_vst3_lane_v(__p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], (int8x8_t)__s1.val[2], __p2, 18); \
})
#else
#define vst3_lane_u32(__p0, __p1, __p2) __extension__ ({ \
  uint32x2x3_t __s1 = __p1; \
  uint32x2x3_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 1, 0); \
  __builtin_neon_vst3_lane_v(__p0, (int8x8_t)__rev1.val[0], (int8x8_t)__rev1.val[1], (int8x8_t)__rev1.val[2], __p2, 18); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst3_lane_u16(__p0, __p1, __p2) __extension__ ({ \
  uint16x4x3_t __s1 = __p1; \
  __builtin_neon_vst3_lane_v(__p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], (int8x8_t)__s1.val[2], __p2, 17); \
})
#else
#define vst3_lane_u16(__p0, __p1, __p2) __extension__ ({ \
  uint16x4x3_t __s1 = __p1; \
  uint16x4x3_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 3, 2, 1, 0); \
  __builtin_neon_vst3_lane_v(__p0, (int8x8_t)__rev1.val[0], (int8x8_t)__rev1.val[1], (int8x8_t)__rev1.val[2], __p2, 17); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst3_lane_s8(__p0, __p1, __p2) __extension__ ({ \
  int8x8x3_t __s1 = __p1; \
  __builtin_neon_vst3_lane_v(__p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], (int8x8_t)__s1.val[2], __p2, 0); \
})
#else
#define vst3_lane_s8(__p0, __p1, __p2) __extension__ ({ \
  int8x8x3_t __s1 = __p1; \
  int8x8x3_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst3_lane_v(__p0, (int8x8_t)__rev1.val[0], (int8x8_t)__rev1.val[1], (int8x8_t)__rev1.val[2], __p2, 0); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst3_lane_f32(__p0, __p1, __p2) __extension__ ({ \
  float32x2x3_t __s1 = __p1; \
  __builtin_neon_vst3_lane_v(__p0, __s1.val[0], __s1.val[1], __s1.val[2], __p2, 9); \
})
#else
#define vst3_lane_f32(__p0, __p1, __p2) __extension__ ({ \
  float32x2x3_t __s1 = __p1; \
  float32x2x3_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 1, 0); \
  __builtin_neon_vst3_lane_v(__p0, __rev1.val[0], __rev1.val[1], __rev1.val[2], __p2, 9); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst3_lane_f16(__p0, __p1, __p2) __extension__ ({ \
  float16x4x3_t __s1 = __p1; \
  __builtin_neon_vst3_lane_v(__p0, __s1.val[0], __s1.val[1], __s1.val[2], __p2, 8); \
})
#else
#define vst3_lane_f16(__p0, __p1, __p2) __extension__ ({ \
  float16x4x3_t __s1 = __p1; \
  float16x4x3_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 3, 2, 1, 0); \
  __builtin_neon_vst3_lane_v(__p0, __rev1.val[0], __rev1.val[1], __rev1.val[2], __p2, 8); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst3_lane_s32(__p0, __p1, __p2) __extension__ ({ \
  int32x2x3_t __s1 = __p1; \
  __builtin_neon_vst3_lane_v(__p0, __s1.val[0], __s1.val[1], __s1.val[2], __p2, 2); \
})
#else
#define vst3_lane_s32(__p0, __p1, __p2) __extension__ ({ \
  int32x2x3_t __s1 = __p1; \
  int32x2x3_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 1, 0); \
  __builtin_neon_vst3_lane_v(__p0, __rev1.val[0], __rev1.val[1], __rev1.val[2], __p2, 2); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst3_lane_s16(__p0, __p1, __p2) __extension__ ({ \
  int16x4x3_t __s1 = __p1; \
  __builtin_neon_vst3_lane_v(__p0, __s1.val[0], __s1.val[1], __s1.val[2], __p2, 1); \
})
#else
#define vst3_lane_s16(__p0, __p1, __p2) __extension__ ({ \
  int16x4x3_t __s1 = __p1; \
  int16x4x3_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 3, 2, 1, 0); \
  __builtin_neon_vst3_lane_v(__p0, __rev1.val[0], __rev1.val[1], __rev1.val[2], __p2, 1); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst4_p8(__p0, __p1) __extension__ ({ \
  poly8x8x4_t __s1 = __p1; \
  __builtin_neon_vst4_v(__p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], (int8x8_t)__s1.val[2], (int8x8_t)__s1.val[3], 4); \
})
#else
#define vst4_p8(__p0, __p1) __extension__ ({ \
  poly8x8x4_t __s1 = __p1; \
  poly8x8x4_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[3] = __builtin_shufflevector(__s1.val[3], __s1.val[3], 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst4_v(__p0, (int8x8_t)__rev1.val[0], (int8x8_t)__rev1.val[1], (int8x8_t)__rev1.val[2], (int8x8_t)__rev1.val[3], 4); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst4_p16(__p0, __p1) __extension__ ({ \
  poly16x4x4_t __s1 = __p1; \
  __builtin_neon_vst4_v(__p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], (int8x8_t)__s1.val[2], (int8x8_t)__s1.val[3], 5); \
})
#else
#define vst4_p16(__p0, __p1) __extension__ ({ \
  poly16x4x4_t __s1 = __p1; \
  poly16x4x4_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 3, 2, 1, 0); \
  __rev1.val[3] = __builtin_shufflevector(__s1.val[3], __s1.val[3], 3, 2, 1, 0); \
  __builtin_neon_vst4_v(__p0, (int8x8_t)__rev1.val[0], (int8x8_t)__rev1.val[1], (int8x8_t)__rev1.val[2], (int8x8_t)__rev1.val[3], 5); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst4q_p8(__p0, __p1) __extension__ ({ \
  poly8x16x4_t __s1 = __p1; \
  __builtin_neon_vst4q_v(__p0, (int8x16_t)__s1.val[0], (int8x16_t)__s1.val[1], (int8x16_t)__s1.val[2], (int8x16_t)__s1.val[3], 36); \
})
#else
#define vst4q_p8(__p0, __p1) __extension__ ({ \
  poly8x16x4_t __s1 = __p1; \
  poly8x16x4_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[3] = __builtin_shufflevector(__s1.val[3], __s1.val[3], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst4q_v(__p0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], (int8x16_t)__rev1.val[2], (int8x16_t)__rev1.val[3], 36); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst4q_p16(__p0, __p1) __extension__ ({ \
  poly16x8x4_t __s1 = __p1; \
  __builtin_neon_vst4q_v(__p0, (int8x16_t)__s1.val[0], (int8x16_t)__s1.val[1], (int8x16_t)__s1.val[2], (int8x16_t)__s1.val[3], 37); \
})
#else
#define vst4q_p16(__p0, __p1) __extension__ ({ \
  poly16x8x4_t __s1 = __p1; \
  poly16x8x4_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[3] = __builtin_shufflevector(__s1.val[3], __s1.val[3], 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst4q_v(__p0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], (int8x16_t)__rev1.val[2], (int8x16_t)__rev1.val[3], 37); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst4q_u8(__p0, __p1) __extension__ ({ \
  uint8x16x4_t __s1 = __p1; \
  __builtin_neon_vst4q_v(__p0, (int8x16_t)__s1.val[0], (int8x16_t)__s1.val[1], (int8x16_t)__s1.val[2], (int8x16_t)__s1.val[3], 48); \
})
#else
#define vst4q_u8(__p0, __p1) __extension__ ({ \
  uint8x16x4_t __s1 = __p1; \
  uint8x16x4_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[3] = __builtin_shufflevector(__s1.val[3], __s1.val[3], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst4q_v(__p0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], (int8x16_t)__rev1.val[2], (int8x16_t)__rev1.val[3], 48); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst4q_u32(__p0, __p1) __extension__ ({ \
  uint32x4x4_t __s1 = __p1; \
  __builtin_neon_vst4q_v(__p0, (int8x16_t)__s1.val[0], (int8x16_t)__s1.val[1], (int8x16_t)__s1.val[2], (int8x16_t)__s1.val[3], 50); \
})
#else
#define vst4q_u32(__p0, __p1) __extension__ ({ \
  uint32x4x4_t __s1 = __p1; \
  uint32x4x4_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 3, 2, 1, 0); \
  __rev1.val[3] = __builtin_shufflevector(__s1.val[3], __s1.val[3], 3, 2, 1, 0); \
  __builtin_neon_vst4q_v(__p0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], (int8x16_t)__rev1.val[2], (int8x16_t)__rev1.val[3], 50); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst4q_u16(__p0, __p1) __extension__ ({ \
  uint16x8x4_t __s1 = __p1; \
  __builtin_neon_vst4q_v(__p0, (int8x16_t)__s1.val[0], (int8x16_t)__s1.val[1], (int8x16_t)__s1.val[2], (int8x16_t)__s1.val[3], 49); \
})
#else
#define vst4q_u16(__p0, __p1) __extension__ ({ \
  uint16x8x4_t __s1 = __p1; \
  uint16x8x4_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[3] = __builtin_shufflevector(__s1.val[3], __s1.val[3], 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst4q_v(__p0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], (int8x16_t)__rev1.val[2], (int8x16_t)__rev1.val[3], 49); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst4q_s8(__p0, __p1) __extension__ ({ \
  int8x16x4_t __s1 = __p1; \
  __builtin_neon_vst4q_v(__p0, (int8x16_t)__s1.val[0], (int8x16_t)__s1.val[1], (int8x16_t)__s1.val[2], (int8x16_t)__s1.val[3], 32); \
})
#else
#define vst4q_s8(__p0, __p1) __extension__ ({ \
  int8x16x4_t __s1 = __p1; \
  int8x16x4_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[3] = __builtin_shufflevector(__s1.val[3], __s1.val[3], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst4q_v(__p0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], (int8x16_t)__rev1.val[2], (int8x16_t)__rev1.val[3], 32); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst4q_f32(__p0, __p1) __extension__ ({ \
  float32x4x4_t __s1 = __p1; \
  __builtin_neon_vst4q_v(__p0, __s1.val[0], __s1.val[1], __s1.val[2], __s1.val[3], 41); \
})
#else
#define vst4q_f32(__p0, __p1) __extension__ ({ \
  float32x4x4_t __s1 = __p1; \
  float32x4x4_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 3, 2, 1, 0); \
  __rev1.val[3] = __builtin_shufflevector(__s1.val[3], __s1.val[3], 3, 2, 1, 0); \
  __builtin_neon_vst4q_v(__p0, __rev1.val[0], __rev1.val[1], __rev1.val[2], __rev1.val[3], 41); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst4q_f16(__p0, __p1) __extension__ ({ \
  float16x8x4_t __s1 = __p1; \
  __builtin_neon_vst4q_v(__p0, __s1.val[0], __s1.val[1], __s1.val[2], __s1.val[3], 40); \
})
#else
#define vst4q_f16(__p0, __p1) __extension__ ({ \
  float16x8x4_t __s1 = __p1; \
  float16x8x4_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[3] = __builtin_shufflevector(__s1.val[3], __s1.val[3], 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst4q_v(__p0, __rev1.val[0], __rev1.val[1], __rev1.val[2], __rev1.val[3], 40); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst4q_s32(__p0, __p1) __extension__ ({ \
  int32x4x4_t __s1 = __p1; \
  __builtin_neon_vst4q_v(__p0, __s1.val[0], __s1.val[1], __s1.val[2], __s1.val[3], 34); \
})
#else
#define vst4q_s32(__p0, __p1) __extension__ ({ \
  int32x4x4_t __s1 = __p1; \
  int32x4x4_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 3, 2, 1, 0); \
  __rev1.val[3] = __builtin_shufflevector(__s1.val[3], __s1.val[3], 3, 2, 1, 0); \
  __builtin_neon_vst4q_v(__p0, __rev1.val[0], __rev1.val[1], __rev1.val[2], __rev1.val[3], 34); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst4q_s16(__p0, __p1) __extension__ ({ \
  int16x8x4_t __s1 = __p1; \
  __builtin_neon_vst4q_v(__p0, __s1.val[0], __s1.val[1], __s1.val[2], __s1.val[3], 33); \
})
#else
#define vst4q_s16(__p0, __p1) __extension__ ({ \
  int16x8x4_t __s1 = __p1; \
  int16x8x4_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[3] = __builtin_shufflevector(__s1.val[3], __s1.val[3], 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst4q_v(__p0, __rev1.val[0], __rev1.val[1], __rev1.val[2], __rev1.val[3], 33); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst4_u8(__p0, __p1) __extension__ ({ \
  uint8x8x4_t __s1 = __p1; \
  __builtin_neon_vst4_v(__p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], (int8x8_t)__s1.val[2], (int8x8_t)__s1.val[3], 16); \
})
#else
#define vst4_u8(__p0, __p1) __extension__ ({ \
  uint8x8x4_t __s1 = __p1; \
  uint8x8x4_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[3] = __builtin_shufflevector(__s1.val[3], __s1.val[3], 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst4_v(__p0, (int8x8_t)__rev1.val[0], (int8x8_t)__rev1.val[1], (int8x8_t)__rev1.val[2], (int8x8_t)__rev1.val[3], 16); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst4_u32(__p0, __p1) __extension__ ({ \
  uint32x2x4_t __s1 = __p1; \
  __builtin_neon_vst4_v(__p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], (int8x8_t)__s1.val[2], (int8x8_t)__s1.val[3], 18); \
})
#else
#define vst4_u32(__p0, __p1) __extension__ ({ \
  uint32x2x4_t __s1 = __p1; \
  uint32x2x4_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 1, 0); \
  __rev1.val[3] = __builtin_shufflevector(__s1.val[3], __s1.val[3], 1, 0); \
  __builtin_neon_vst4_v(__p0, (int8x8_t)__rev1.val[0], (int8x8_t)__rev1.val[1], (int8x8_t)__rev1.val[2], (int8x8_t)__rev1.val[3], 18); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst4_u64(__p0, __p1) __extension__ ({ \
  uint64x1x4_t __s1 = __p1; \
  __builtin_neon_vst4_v(__p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], (int8x8_t)__s1.val[2], (int8x8_t)__s1.val[3], 19); \
})
#else
#define vst4_u64(__p0, __p1) __extension__ ({ \
  uint64x1x4_t __s1 = __p1; \
  __builtin_neon_vst4_v(__p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], (int8x8_t)__s1.val[2], (int8x8_t)__s1.val[3], 19); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst4_u16(__p0, __p1) __extension__ ({ \
  uint16x4x4_t __s1 = __p1; \
  __builtin_neon_vst4_v(__p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], (int8x8_t)__s1.val[2], (int8x8_t)__s1.val[3], 17); \
})
#else
#define vst4_u16(__p0, __p1) __extension__ ({ \
  uint16x4x4_t __s1 = __p1; \
  uint16x4x4_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 3, 2, 1, 0); \
  __rev1.val[3] = __builtin_shufflevector(__s1.val[3], __s1.val[3], 3, 2, 1, 0); \
  __builtin_neon_vst4_v(__p0, (int8x8_t)__rev1.val[0], (int8x8_t)__rev1.val[1], (int8x8_t)__rev1.val[2], (int8x8_t)__rev1.val[3], 17); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst4_s8(__p0, __p1) __extension__ ({ \
  int8x8x4_t __s1 = __p1; \
  __builtin_neon_vst4_v(__p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], (int8x8_t)__s1.val[2], (int8x8_t)__s1.val[3], 0); \
})
#else
#define vst4_s8(__p0, __p1) __extension__ ({ \
  int8x8x4_t __s1 = __p1; \
  int8x8x4_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[3] = __builtin_shufflevector(__s1.val[3], __s1.val[3], 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst4_v(__p0, (int8x8_t)__rev1.val[0], (int8x8_t)__rev1.val[1], (int8x8_t)__rev1.val[2], (int8x8_t)__rev1.val[3], 0); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst4_f32(__p0, __p1) __extension__ ({ \
  float32x2x4_t __s1 = __p1; \
  __builtin_neon_vst4_v(__p0, __s1.val[0], __s1.val[1], __s1.val[2], __s1.val[3], 9); \
})
#else
#define vst4_f32(__p0, __p1) __extension__ ({ \
  float32x2x4_t __s1 = __p1; \
  float32x2x4_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 1, 0); \
  __rev1.val[3] = __builtin_shufflevector(__s1.val[3], __s1.val[3], 1, 0); \
  __builtin_neon_vst4_v(__p0, __rev1.val[0], __rev1.val[1], __rev1.val[2], __rev1.val[3], 9); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst4_f16(__p0, __p1) __extension__ ({ \
  float16x4x4_t __s1 = __p1; \
  __builtin_neon_vst4_v(__p0, __s1.val[0], __s1.val[1], __s1.val[2], __s1.val[3], 8); \
})
#else
#define vst4_f16(__p0, __p1) __extension__ ({ \
  float16x4x4_t __s1 = __p1; \
  float16x4x4_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 3, 2, 1, 0); \
  __rev1.val[3] = __builtin_shufflevector(__s1.val[3], __s1.val[3], 3, 2, 1, 0); \
  __builtin_neon_vst4_v(__p0, __rev1.val[0], __rev1.val[1], __rev1.val[2], __rev1.val[3], 8); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst4_s32(__p0, __p1) __extension__ ({ \
  int32x2x4_t __s1 = __p1; \
  __builtin_neon_vst4_v(__p0, __s1.val[0], __s1.val[1], __s1.val[2], __s1.val[3], 2); \
})
#else
#define vst4_s32(__p0, __p1) __extension__ ({ \
  int32x2x4_t __s1 = __p1; \
  int32x2x4_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 1, 0); \
  __rev1.val[3] = __builtin_shufflevector(__s1.val[3], __s1.val[3], 1, 0); \
  __builtin_neon_vst4_v(__p0, __rev1.val[0], __rev1.val[1], __rev1.val[2], __rev1.val[3], 2); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst4_s64(__p0, __p1) __extension__ ({ \
  int64x1x4_t __s1 = __p1; \
  __builtin_neon_vst4_v(__p0, __s1.val[0], __s1.val[1], __s1.val[2], __s1.val[3], 3); \
})
#else
#define vst4_s64(__p0, __p1) __extension__ ({ \
  int64x1x4_t __s1 = __p1; \
  __builtin_neon_vst4_v(__p0, __s1.val[0], __s1.val[1], __s1.val[2], __s1.val[3], 3); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst4_s16(__p0, __p1) __extension__ ({ \
  int16x4x4_t __s1 = __p1; \
  __builtin_neon_vst4_v(__p0, __s1.val[0], __s1.val[1], __s1.val[2], __s1.val[3], 1); \
})
#else
#define vst4_s16(__p0, __p1) __extension__ ({ \
  int16x4x4_t __s1 = __p1; \
  int16x4x4_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 3, 2, 1, 0); \
  __rev1.val[3] = __builtin_shufflevector(__s1.val[3], __s1.val[3], 3, 2, 1, 0); \
  __builtin_neon_vst4_v(__p0, __rev1.val[0], __rev1.val[1], __rev1.val[2], __rev1.val[3], 1); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst4_lane_p8(__p0, __p1, __p2) __extension__ ({ \
  poly8x8x4_t __s1 = __p1; \
  __builtin_neon_vst4_lane_v(__p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], (int8x8_t)__s1.val[2], (int8x8_t)__s1.val[3], __p2, 4); \
})
#else
#define vst4_lane_p8(__p0, __p1, __p2) __extension__ ({ \
  poly8x8x4_t __s1 = __p1; \
  poly8x8x4_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[3] = __builtin_shufflevector(__s1.val[3], __s1.val[3], 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst4_lane_v(__p0, (int8x8_t)__rev1.val[0], (int8x8_t)__rev1.val[1], (int8x8_t)__rev1.val[2], (int8x8_t)__rev1.val[3], __p2, 4); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst4_lane_p16(__p0, __p1, __p2) __extension__ ({ \
  poly16x4x4_t __s1 = __p1; \
  __builtin_neon_vst4_lane_v(__p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], (int8x8_t)__s1.val[2], (int8x8_t)__s1.val[3], __p2, 5); \
})
#else
#define vst4_lane_p16(__p0, __p1, __p2) __extension__ ({ \
  poly16x4x4_t __s1 = __p1; \
  poly16x4x4_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 3, 2, 1, 0); \
  __rev1.val[3] = __builtin_shufflevector(__s1.val[3], __s1.val[3], 3, 2, 1, 0); \
  __builtin_neon_vst4_lane_v(__p0, (int8x8_t)__rev1.val[0], (int8x8_t)__rev1.val[1], (int8x8_t)__rev1.val[2], (int8x8_t)__rev1.val[3], __p2, 5); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst4q_lane_p16(__p0, __p1, __p2) __extension__ ({ \
  poly16x8x4_t __s1 = __p1; \
  __builtin_neon_vst4q_lane_v(__p0, (int8x16_t)__s1.val[0], (int8x16_t)__s1.val[1], (int8x16_t)__s1.val[2], (int8x16_t)__s1.val[3], __p2, 37); \
})
#else
#define vst4q_lane_p16(__p0, __p1, __p2) __extension__ ({ \
  poly16x8x4_t __s1 = __p1; \
  poly16x8x4_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[3] = __builtin_shufflevector(__s1.val[3], __s1.val[3], 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst4q_lane_v(__p0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], (int8x16_t)__rev1.val[2], (int8x16_t)__rev1.val[3], __p2, 37); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst4q_lane_u32(__p0, __p1, __p2) __extension__ ({ \
  uint32x4x4_t __s1 = __p1; \
  __builtin_neon_vst4q_lane_v(__p0, (int8x16_t)__s1.val[0], (int8x16_t)__s1.val[1], (int8x16_t)__s1.val[2], (int8x16_t)__s1.val[3], __p2, 50); \
})
#else
#define vst4q_lane_u32(__p0, __p1, __p2) __extension__ ({ \
  uint32x4x4_t __s1 = __p1; \
  uint32x4x4_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 3, 2, 1, 0); \
  __rev1.val[3] = __builtin_shufflevector(__s1.val[3], __s1.val[3], 3, 2, 1, 0); \
  __builtin_neon_vst4q_lane_v(__p0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], (int8x16_t)__rev1.val[2], (int8x16_t)__rev1.val[3], __p2, 50); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst4q_lane_u16(__p0, __p1, __p2) __extension__ ({ \
  uint16x8x4_t __s1 = __p1; \
  __builtin_neon_vst4q_lane_v(__p0, (int8x16_t)__s1.val[0], (int8x16_t)__s1.val[1], (int8x16_t)__s1.val[2], (int8x16_t)__s1.val[3], __p2, 49); \
})
#else
#define vst4q_lane_u16(__p0, __p1, __p2) __extension__ ({ \
  uint16x8x4_t __s1 = __p1; \
  uint16x8x4_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[3] = __builtin_shufflevector(__s1.val[3], __s1.val[3], 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst4q_lane_v(__p0, (int8x16_t)__rev1.val[0], (int8x16_t)__rev1.val[1], (int8x16_t)__rev1.val[2], (int8x16_t)__rev1.val[3], __p2, 49); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst4q_lane_f32(__p0, __p1, __p2) __extension__ ({ \
  float32x4x4_t __s1 = __p1; \
  __builtin_neon_vst4q_lane_v(__p0, __s1.val[0], __s1.val[1], __s1.val[2], __s1.val[3], __p2, 41); \
})
#else
#define vst4q_lane_f32(__p0, __p1, __p2) __extension__ ({ \
  float32x4x4_t __s1 = __p1; \
  float32x4x4_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 3, 2, 1, 0); \
  __rev1.val[3] = __builtin_shufflevector(__s1.val[3], __s1.val[3], 3, 2, 1, 0); \
  __builtin_neon_vst4q_lane_v(__p0, __rev1.val[0], __rev1.val[1], __rev1.val[2], __rev1.val[3], __p2, 41); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst4q_lane_f16(__p0, __p1, __p2) __extension__ ({ \
  float16x8x4_t __s1 = __p1; \
  __builtin_neon_vst4q_lane_v(__p0, __s1.val[0], __s1.val[1], __s1.val[2], __s1.val[3], __p2, 40); \
})
#else
#define vst4q_lane_f16(__p0, __p1, __p2) __extension__ ({ \
  float16x8x4_t __s1 = __p1; \
  float16x8x4_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[3] = __builtin_shufflevector(__s1.val[3], __s1.val[3], 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst4q_lane_v(__p0, __rev1.val[0], __rev1.val[1], __rev1.val[2], __rev1.val[3], __p2, 40); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst4q_lane_s32(__p0, __p1, __p2) __extension__ ({ \
  int32x4x4_t __s1 = __p1; \
  __builtin_neon_vst4q_lane_v(__p0, __s1.val[0], __s1.val[1], __s1.val[2], __s1.val[3], __p2, 34); \
})
#else
#define vst4q_lane_s32(__p0, __p1, __p2) __extension__ ({ \
  int32x4x4_t __s1 = __p1; \
  int32x4x4_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 3, 2, 1, 0); \
  __rev1.val[3] = __builtin_shufflevector(__s1.val[3], __s1.val[3], 3, 2, 1, 0); \
  __builtin_neon_vst4q_lane_v(__p0, __rev1.val[0], __rev1.val[1], __rev1.val[2], __rev1.val[3], __p2, 34); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst4q_lane_s16(__p0, __p1, __p2) __extension__ ({ \
  int16x8x4_t __s1 = __p1; \
  __builtin_neon_vst4q_lane_v(__p0, __s1.val[0], __s1.val[1], __s1.val[2], __s1.val[3], __p2, 33); \
})
#else
#define vst4q_lane_s16(__p0, __p1, __p2) __extension__ ({ \
  int16x8x4_t __s1 = __p1; \
  int16x8x4_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[3] = __builtin_shufflevector(__s1.val[3], __s1.val[3], 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst4q_lane_v(__p0, __rev1.val[0], __rev1.val[1], __rev1.val[2], __rev1.val[3], __p2, 33); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst4_lane_u8(__p0, __p1, __p2) __extension__ ({ \
  uint8x8x4_t __s1 = __p1; \
  __builtin_neon_vst4_lane_v(__p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], (int8x8_t)__s1.val[2], (int8x8_t)__s1.val[3], __p2, 16); \
})
#else
#define vst4_lane_u8(__p0, __p1, __p2) __extension__ ({ \
  uint8x8x4_t __s1 = __p1; \
  uint8x8x4_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[3] = __builtin_shufflevector(__s1.val[3], __s1.val[3], 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst4_lane_v(__p0, (int8x8_t)__rev1.val[0], (int8x8_t)__rev1.val[1], (int8x8_t)__rev1.val[2], (int8x8_t)__rev1.val[3], __p2, 16); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst4_lane_u32(__p0, __p1, __p2) __extension__ ({ \
  uint32x2x4_t __s1 = __p1; \
  __builtin_neon_vst4_lane_v(__p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], (int8x8_t)__s1.val[2], (int8x8_t)__s1.val[3], __p2, 18); \
})
#else
#define vst4_lane_u32(__p0, __p1, __p2) __extension__ ({ \
  uint32x2x4_t __s1 = __p1; \
  uint32x2x4_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 1, 0); \
  __rev1.val[3] = __builtin_shufflevector(__s1.val[3], __s1.val[3], 1, 0); \
  __builtin_neon_vst4_lane_v(__p0, (int8x8_t)__rev1.val[0], (int8x8_t)__rev1.val[1], (int8x8_t)__rev1.val[2], (int8x8_t)__rev1.val[3], __p2, 18); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst4_lane_u16(__p0, __p1, __p2) __extension__ ({ \
  uint16x4x4_t __s1 = __p1; \
  __builtin_neon_vst4_lane_v(__p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], (int8x8_t)__s1.val[2], (int8x8_t)__s1.val[3], __p2, 17); \
})
#else
#define vst4_lane_u16(__p0, __p1, __p2) __extension__ ({ \
  uint16x4x4_t __s1 = __p1; \
  uint16x4x4_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 3, 2, 1, 0); \
  __rev1.val[3] = __builtin_shufflevector(__s1.val[3], __s1.val[3], 3, 2, 1, 0); \
  __builtin_neon_vst4_lane_v(__p0, (int8x8_t)__rev1.val[0], (int8x8_t)__rev1.val[1], (int8x8_t)__rev1.val[2], (int8x8_t)__rev1.val[3], __p2, 17); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst4_lane_s8(__p0, __p1, __p2) __extension__ ({ \
  int8x8x4_t __s1 = __p1; \
  __builtin_neon_vst4_lane_v(__p0, (int8x8_t)__s1.val[0], (int8x8_t)__s1.val[1], (int8x8_t)__s1.val[2], (int8x8_t)__s1.val[3], __p2, 0); \
})
#else
#define vst4_lane_s8(__p0, __p1, __p2) __extension__ ({ \
  int8x8x4_t __s1 = __p1; \
  int8x8x4_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 7, 6, 5, 4, 3, 2, 1, 0); \
  __rev1.val[3] = __builtin_shufflevector(__s1.val[3], __s1.val[3], 7, 6, 5, 4, 3, 2, 1, 0); \
  __builtin_neon_vst4_lane_v(__p0, (int8x8_t)__rev1.val[0], (int8x8_t)__rev1.val[1], (int8x8_t)__rev1.val[2], (int8x8_t)__rev1.val[3], __p2, 0); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst4_lane_f32(__p0, __p1, __p2) __extension__ ({ \
  float32x2x4_t __s1 = __p1; \
  __builtin_neon_vst4_lane_v(__p0, __s1.val[0], __s1.val[1], __s1.val[2], __s1.val[3], __p2, 9); \
})
#else
#define vst4_lane_f32(__p0, __p1, __p2) __extension__ ({ \
  float32x2x4_t __s1 = __p1; \
  float32x2x4_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 1, 0); \
  __rev1.val[3] = __builtin_shufflevector(__s1.val[3], __s1.val[3], 1, 0); \
  __builtin_neon_vst4_lane_v(__p0, __rev1.val[0], __rev1.val[1], __rev1.val[2], __rev1.val[3], __p2, 9); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst4_lane_f16(__p0, __p1, __p2) __extension__ ({ \
  float16x4x4_t __s1 = __p1; \
  __builtin_neon_vst4_lane_v(__p0, __s1.val[0], __s1.val[1], __s1.val[2], __s1.val[3], __p2, 8); \
})
#else
#define vst4_lane_f16(__p0, __p1, __p2) __extension__ ({ \
  float16x4x4_t __s1 = __p1; \
  float16x4x4_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 3, 2, 1, 0); \
  __rev1.val[3] = __builtin_shufflevector(__s1.val[3], __s1.val[3], 3, 2, 1, 0); \
  __builtin_neon_vst4_lane_v(__p0, __rev1.val[0], __rev1.val[1], __rev1.val[2], __rev1.val[3], __p2, 8); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst4_lane_s32(__p0, __p1, __p2) __extension__ ({ \
  int32x2x4_t __s1 = __p1; \
  __builtin_neon_vst4_lane_v(__p0, __s1.val[0], __s1.val[1], __s1.val[2], __s1.val[3], __p2, 2); \
})
#else
#define vst4_lane_s32(__p0, __p1, __p2) __extension__ ({ \
  int32x2x4_t __s1 = __p1; \
  int32x2x4_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 1, 0); \
  __rev1.val[3] = __builtin_shufflevector(__s1.val[3], __s1.val[3], 1, 0); \
  __builtin_neon_vst4_lane_v(__p0, __rev1.val[0], __rev1.val[1], __rev1.val[2], __rev1.val[3], __p2, 2); \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vst4_lane_s16(__p0, __p1, __p2) __extension__ ({ \
  int16x4x4_t __s1 = __p1; \
  __builtin_neon_vst4_lane_v(__p0, __s1.val[0], __s1.val[1], __s1.val[2], __s1.val[3], __p2, 1); \
})
#else
#define vst4_lane_s16(__p0, __p1, __p2) __extension__ ({ \
  int16x4x4_t __s1 = __p1; \
  int16x4x4_t __rev1; \
  __rev1.val[0] = __builtin_shufflevector(__s1.val[0], __s1.val[0], 3, 2, 1, 0); \
  __rev1.val[1] = __builtin_shufflevector(__s1.val[1], __s1.val[1], 3, 2, 1, 0); \
  __rev1.val[2] = __builtin_shufflevector(__s1.val[2], __s1.val[2], 3, 2, 1, 0); \
  __rev1.val[3] = __builtin_shufflevector(__s1.val[3], __s1.val[3], 3, 2, 1, 0); \
  __builtin_neon_vst4_lane_v(__p0, __rev1.val[0], __rev1.val[1], __rev1.val[2], __rev1.val[3], __p2, 1); \
})
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x16_t vsubq_u8(uint8x16_t __p0, uint8x16_t __p1) {
  uint8x16_t __ret;
  __ret = __p0 - __p1;
  return __ret;
}
#else
__ai uint8x16_t vsubq_u8(uint8x16_t __p0, uint8x16_t __p1) {
  uint8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __ret;
  __ret = __rev0 - __rev1;
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vsubq_u32(uint32x4_t __p0, uint32x4_t __p1) {
  uint32x4_t __ret;
  __ret = __p0 - __p1;
  return __ret;
}
#else
__ai uint32x4_t vsubq_u32(uint32x4_t __p0, uint32x4_t __p1) {
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint32x4_t __ret;
  __ret = __rev0 - __rev1;
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vsubq_u64(uint64x2_t __p0, uint64x2_t __p1) {
  uint64x2_t __ret;
  __ret = __p0 - __p1;
  return __ret;
}
#else
__ai uint64x2_t vsubq_u64(uint64x2_t __p0, uint64x2_t __p1) {
  uint64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  uint64x2_t __ret;
  __ret = __rev0 - __rev1;
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x8_t vsubq_u16(uint16x8_t __p0, uint16x8_t __p1) {
  uint16x8_t __ret;
  __ret = __p0 - __p1;
  return __ret;
}
#else
__ai uint16x8_t vsubq_u16(uint16x8_t __p0, uint16x8_t __p1) {
  uint16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __ret;
  __ret = __rev0 - __rev1;
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x16_t vsubq_s8(int8x16_t __p0, int8x16_t __p1) {
  int8x16_t __ret;
  __ret = __p0 - __p1;
  return __ret;
}
#else
__ai int8x16_t vsubq_s8(int8x16_t __p0, int8x16_t __p1) {
  int8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __ret;
  __ret = __rev0 - __rev1;
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x4_t vsubq_f32(float32x4_t __p0, float32x4_t __p1) {
  float32x4_t __ret;
  __ret = __p0 - __p1;
  return __ret;
}
#else
__ai float32x4_t vsubq_f32(float32x4_t __p0, float32x4_t __p1) {
  float32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  float32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  float32x4_t __ret;
  __ret = __rev0 - __rev1;
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vsubq_s32(int32x4_t __p0, int32x4_t __p1) {
  int32x4_t __ret;
  __ret = __p0 - __p1;
  return __ret;
}
#else
__ai int32x4_t vsubq_s32(int32x4_t __p0, int32x4_t __p1) {
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int32x4_t __ret;
  __ret = __rev0 - __rev1;
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x2_t vsubq_s64(int64x2_t __p0, int64x2_t __p1) {
  int64x2_t __ret;
  __ret = __p0 - __p1;
  return __ret;
}
#else
__ai int64x2_t vsubq_s64(int64x2_t __p0, int64x2_t __p1) {
  int64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  int64x2_t __ret;
  __ret = __rev0 - __rev1;
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x8_t vsubq_s16(int16x8_t __p0, int16x8_t __p1) {
  int16x8_t __ret;
  __ret = __p0 - __p1;
  return __ret;
}
#else
__ai int16x8_t vsubq_s16(int16x8_t __p0, int16x8_t __p1) {
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __ret;
  __ret = __rev0 - __rev1;
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x8_t vsub_u8(uint8x8_t __p0, uint8x8_t __p1) {
  uint8x8_t __ret;
  __ret = __p0 - __p1;
  return __ret;
}
#else
__ai uint8x8_t vsub_u8(uint8x8_t __p0, uint8x8_t __p1) {
  uint8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __ret;
  __ret = __rev0 - __rev1;
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x2_t vsub_u32(uint32x2_t __p0, uint32x2_t __p1) {
  uint32x2_t __ret;
  __ret = __p0 - __p1;
  return __ret;
}
#else
__ai uint32x2_t vsub_u32(uint32x2_t __p0, uint32x2_t __p1) {
  uint32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  uint32x2_t __ret;
  __ret = __rev0 - __rev1;
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x1_t vsub_u64(uint64x1_t __p0, uint64x1_t __p1) {
  uint64x1_t __ret;
  __ret = __p0 - __p1;
  return __ret;
}
#else
__ai uint64x1_t vsub_u64(uint64x1_t __p0, uint64x1_t __p1) {
  uint64x1_t __ret;
  __ret = __p0 - __p1;
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x4_t vsub_u16(uint16x4_t __p0, uint16x4_t __p1) {
  uint16x4_t __ret;
  __ret = __p0 - __p1;
  return __ret;
}
#else
__ai uint16x4_t vsub_u16(uint16x4_t __p0, uint16x4_t __p1) {
  uint16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint16x4_t __ret;
  __ret = __rev0 - __rev1;
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x8_t vsub_s8(int8x8_t __p0, int8x8_t __p1) {
  int8x8_t __ret;
  __ret = __p0 - __p1;
  return __ret;
}
#else
__ai int8x8_t vsub_s8(int8x8_t __p0, int8x8_t __p1) {
  int8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __ret;
  __ret = __rev0 - __rev1;
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x2_t vsub_f32(float32x2_t __p0, float32x2_t __p1) {
  float32x2_t __ret;
  __ret = __p0 - __p1;
  return __ret;
}
#else
__ai float32x2_t vsub_f32(float32x2_t __p0, float32x2_t __p1) {
  float32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  float32x2_t __ret;
  __ret = __rev0 - __rev1;
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x2_t vsub_s32(int32x2_t __p0, int32x2_t __p1) {
  int32x2_t __ret;
  __ret = __p0 - __p1;
  return __ret;
}
#else
__ai int32x2_t vsub_s32(int32x2_t __p0, int32x2_t __p1) {
  int32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  int32x2_t __ret;
  __ret = __rev0 - __rev1;
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x1_t vsub_s64(int64x1_t __p0, int64x1_t __p1) {
  int64x1_t __ret;
  __ret = __p0 - __p1;
  return __ret;
}
#else
__ai int64x1_t vsub_s64(int64x1_t __p0, int64x1_t __p1) {
  int64x1_t __ret;
  __ret = __p0 - __p1;
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x4_t vsub_s16(int16x4_t __p0, int16x4_t __p1) {
  int16x4_t __ret;
  __ret = __p0 - __p1;
  return __ret;
}
#else
__ai int16x4_t vsub_s16(int16x4_t __p0, int16x4_t __p1) {
  int16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int16x4_t __ret;
  __ret = __rev0 - __rev1;
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x4_t vsubhn_u32(uint32x4_t __p0, uint32x4_t __p1) {
  uint16x4_t __ret;
  __ret = (uint16x4_t) __builtin_neon_vsubhn_v((int8x16_t)__p0, (int8x16_t)__p1, 17);
  return __ret;
}
#else
__ai uint16x4_t vsubhn_u32(uint32x4_t __p0, uint32x4_t __p1) {
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint16x4_t __ret;
  __ret = (uint16x4_t) __builtin_neon_vsubhn_v((int8x16_t)__rev0, (int8x16_t)__rev1, 17);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
__ai uint16x4_t __noswap_vsubhn_u32(uint32x4_t __p0, uint32x4_t __p1) {
  uint16x4_t __ret;
  __ret = (uint16x4_t) __builtin_neon_vsubhn_v((int8x16_t)__p0, (int8x16_t)__p1, 17);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x2_t vsubhn_u64(uint64x2_t __p0, uint64x2_t __p1) {
  uint32x2_t __ret;
  __ret = (uint32x2_t) __builtin_neon_vsubhn_v((int8x16_t)__p0, (int8x16_t)__p1, 18);
  return __ret;
}
#else
__ai uint32x2_t vsubhn_u64(uint64x2_t __p0, uint64x2_t __p1) {
  uint64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  uint32x2_t __ret;
  __ret = (uint32x2_t) __builtin_neon_vsubhn_v((int8x16_t)__rev0, (int8x16_t)__rev1, 18);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
__ai uint32x2_t __noswap_vsubhn_u64(uint64x2_t __p0, uint64x2_t __p1) {
  uint32x2_t __ret;
  __ret = (uint32x2_t) __builtin_neon_vsubhn_v((int8x16_t)__p0, (int8x16_t)__p1, 18);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x8_t vsubhn_u16(uint16x8_t __p0, uint16x8_t __p1) {
  uint8x8_t __ret;
  __ret = (uint8x8_t) __builtin_neon_vsubhn_v((int8x16_t)__p0, (int8x16_t)__p1, 16);
  return __ret;
}
#else
__ai uint8x8_t vsubhn_u16(uint16x8_t __p0, uint16x8_t __p1) {
  uint16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __ret;
  __ret = (uint8x8_t) __builtin_neon_vsubhn_v((int8x16_t)__rev0, (int8x16_t)__rev1, 16);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
__ai uint8x8_t __noswap_vsubhn_u16(uint16x8_t __p0, uint16x8_t __p1) {
  uint8x8_t __ret;
  __ret = (uint8x8_t) __builtin_neon_vsubhn_v((int8x16_t)__p0, (int8x16_t)__p1, 16);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x4_t vsubhn_s32(int32x4_t __p0, int32x4_t __p1) {
  int16x4_t __ret;
  __ret = (int16x4_t) __builtin_neon_vsubhn_v((int8x16_t)__p0, (int8x16_t)__p1, 1);
  return __ret;
}
#else
__ai int16x4_t vsubhn_s32(int32x4_t __p0, int32x4_t __p1) {
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int16x4_t __ret;
  __ret = (int16x4_t) __builtin_neon_vsubhn_v((int8x16_t)__rev0, (int8x16_t)__rev1, 1);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
__ai int16x4_t __noswap_vsubhn_s32(int32x4_t __p0, int32x4_t __p1) {
  int16x4_t __ret;
  __ret = (int16x4_t) __builtin_neon_vsubhn_v((int8x16_t)__p0, (int8x16_t)__p1, 1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x2_t vsubhn_s64(int64x2_t __p0, int64x2_t __p1) {
  int32x2_t __ret;
  __ret = (int32x2_t) __builtin_neon_vsubhn_v((int8x16_t)__p0, (int8x16_t)__p1, 2);
  return __ret;
}
#else
__ai int32x2_t vsubhn_s64(int64x2_t __p0, int64x2_t __p1) {
  int64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  int32x2_t __ret;
  __ret = (int32x2_t) __builtin_neon_vsubhn_v((int8x16_t)__rev0, (int8x16_t)__rev1, 2);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
__ai int32x2_t __noswap_vsubhn_s64(int64x2_t __p0, int64x2_t __p1) {
  int32x2_t __ret;
  __ret = (int32x2_t) __builtin_neon_vsubhn_v((int8x16_t)__p0, (int8x16_t)__p1, 2);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x8_t vsubhn_s16(int16x8_t __p0, int16x8_t __p1) {
  int8x8_t __ret;
  __ret = (int8x8_t) __builtin_neon_vsubhn_v((int8x16_t)__p0, (int8x16_t)__p1, 0);
  return __ret;
}
#else
__ai int8x8_t vsubhn_s16(int16x8_t __p0, int16x8_t __p1) {
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __ret;
  __ret = (int8x8_t) __builtin_neon_vsubhn_v((int8x16_t)__rev0, (int8x16_t)__rev1, 0);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
__ai int8x8_t __noswap_vsubhn_s16(int16x8_t __p0, int16x8_t __p1) {
  int8x8_t __ret;
  __ret = (int8x8_t) __builtin_neon_vsubhn_v((int8x16_t)__p0, (int8x16_t)__p1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x8_t vsubl_u8(uint8x8_t __p0, uint8x8_t __p1) {
  uint16x8_t __ret;
  __ret = vmovl_u8(__p0) - vmovl_u8(__p1);
  return __ret;
}
#else
__ai uint16x8_t vsubl_u8(uint8x8_t __p0, uint8x8_t __p1) {
  uint8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __ret;
  __ret = __noswap_vmovl_u8(__rev0) - __noswap_vmovl_u8(__rev1);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vsubl_u32(uint32x2_t __p0, uint32x2_t __p1) {
  uint64x2_t __ret;
  __ret = vmovl_u32(__p0) - vmovl_u32(__p1);
  return __ret;
}
#else
__ai uint64x2_t vsubl_u32(uint32x2_t __p0, uint32x2_t __p1) {
  uint32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  uint64x2_t __ret;
  __ret = __noswap_vmovl_u32(__rev0) - __noswap_vmovl_u32(__rev1);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vsubl_u16(uint16x4_t __p0, uint16x4_t __p1) {
  uint32x4_t __ret;
  __ret = vmovl_u16(__p0) - vmovl_u16(__p1);
  return __ret;
}
#else
__ai uint32x4_t vsubl_u16(uint16x4_t __p0, uint16x4_t __p1) {
  uint16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint32x4_t __ret;
  __ret = __noswap_vmovl_u16(__rev0) - __noswap_vmovl_u16(__rev1);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x8_t vsubl_s8(int8x8_t __p0, int8x8_t __p1) {
  int16x8_t __ret;
  __ret = vmovl_s8(__p0) - vmovl_s8(__p1);
  return __ret;
}
#else
__ai int16x8_t vsubl_s8(int8x8_t __p0, int8x8_t __p1) {
  int8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __ret;
  __ret = __noswap_vmovl_s8(__rev0) - __noswap_vmovl_s8(__rev1);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x2_t vsubl_s32(int32x2_t __p0, int32x2_t __p1) {
  int64x2_t __ret;
  __ret = vmovl_s32(__p0) - vmovl_s32(__p1);
  return __ret;
}
#else
__ai int64x2_t vsubl_s32(int32x2_t __p0, int32x2_t __p1) {
  int32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  int64x2_t __ret;
  __ret = __noswap_vmovl_s32(__rev0) - __noswap_vmovl_s32(__rev1);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vsubl_s16(int16x4_t __p0, int16x4_t __p1) {
  int32x4_t __ret;
  __ret = vmovl_s16(__p0) - vmovl_s16(__p1);
  return __ret;
}
#else
__ai int32x4_t vsubl_s16(int16x4_t __p0, int16x4_t __p1) {
  int16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int32x4_t __ret;
  __ret = __noswap_vmovl_s16(__rev0) - __noswap_vmovl_s16(__rev1);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x8_t vsubw_u8(uint16x8_t __p0, uint8x8_t __p1) {
  uint16x8_t __ret;
  __ret = __p0 - vmovl_u8(__p1);
  return __ret;
}
#else
__ai uint16x8_t vsubw_u8(uint16x8_t __p0, uint8x8_t __p1) {
  uint16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __ret;
  __ret = __rev0 - __noswap_vmovl_u8(__rev1);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vsubw_u32(uint64x2_t __p0, uint32x2_t __p1) {
  uint64x2_t __ret;
  __ret = __p0 - vmovl_u32(__p1);
  return __ret;
}
#else
__ai uint64x2_t vsubw_u32(uint64x2_t __p0, uint32x2_t __p1) {
  uint64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  uint64x2_t __ret;
  __ret = __rev0 - __noswap_vmovl_u32(__rev1);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vsubw_u16(uint32x4_t __p0, uint16x4_t __p1) {
  uint32x4_t __ret;
  __ret = __p0 - vmovl_u16(__p1);
  return __ret;
}
#else
__ai uint32x4_t vsubw_u16(uint32x4_t __p0, uint16x4_t __p1) {
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint32x4_t __ret;
  __ret = __rev0 - __noswap_vmovl_u16(__rev1);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x8_t vsubw_s8(int16x8_t __p0, int8x8_t __p1) {
  int16x8_t __ret;
  __ret = __p0 - vmovl_s8(__p1);
  return __ret;
}
#else
__ai int16x8_t vsubw_s8(int16x8_t __p0, int8x8_t __p1) {
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __ret;
  __ret = __rev0 - __noswap_vmovl_s8(__rev1);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x2_t vsubw_s32(int64x2_t __p0, int32x2_t __p1) {
  int64x2_t __ret;
  __ret = __p0 - vmovl_s32(__p1);
  return __ret;
}
#else
__ai int64x2_t vsubw_s32(int64x2_t __p0, int32x2_t __p1) {
  int64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  int64x2_t __ret;
  __ret = __rev0 - __noswap_vmovl_s32(__rev1);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vsubw_s16(int32x4_t __p0, int16x4_t __p1) {
  int32x4_t __ret;
  __ret = __p0 - vmovl_s16(__p1);
  return __ret;
}
#else
__ai int32x4_t vsubw_s16(int32x4_t __p0, int16x4_t __p1) {
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int32x4_t __ret;
  __ret = __rev0 - __noswap_vmovl_s16(__rev1);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x8_t vtbl1_p8(poly8x8_t __p0, uint8x8_t __p1) {
  poly8x8_t __ret;
  __ret = (poly8x8_t) __builtin_neon_vtbl1_v((int8x8_t)__p0, (int8x8_t)__p1, 4);
  return __ret;
}
#else
__ai poly8x8_t vtbl1_p8(poly8x8_t __p0, uint8x8_t __p1) {
  poly8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  poly8x8_t __ret;
  __ret = (poly8x8_t) __builtin_neon_vtbl1_v((int8x8_t)__rev0, (int8x8_t)__rev1, 4);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x8_t vtbl1_u8(uint8x8_t __p0, uint8x8_t __p1) {
  uint8x8_t __ret;
  __ret = (uint8x8_t) __builtin_neon_vtbl1_v((int8x8_t)__p0, (int8x8_t)__p1, 16);
  return __ret;
}
#else
__ai uint8x8_t vtbl1_u8(uint8x8_t __p0, uint8x8_t __p1) {
  uint8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __ret;
  __ret = (uint8x8_t) __builtin_neon_vtbl1_v((int8x8_t)__rev0, (int8x8_t)__rev1, 16);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x8_t vtbl1_s8(int8x8_t __p0, int8x8_t __p1) {
  int8x8_t __ret;
  __ret = (int8x8_t) __builtin_neon_vtbl1_v((int8x8_t)__p0, (int8x8_t)__p1, 0);
  return __ret;
}
#else
__ai int8x8_t vtbl1_s8(int8x8_t __p0, int8x8_t __p1) {
  int8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __ret;
  __ret = (int8x8_t) __builtin_neon_vtbl1_v((int8x8_t)__rev0, (int8x8_t)__rev1, 0);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x8_t vtbl2_p8(poly8x8x2_t __p0, uint8x8_t __p1) {
  poly8x8_t __ret;
  __ret = (poly8x8_t) __builtin_neon_vtbl2_v((int8x8_t)__p0.val[0], (int8x8_t)__p0.val[1], (int8x8_t)__p1, 4);
  return __ret;
}
#else
__ai poly8x8_t vtbl2_p8(poly8x8x2_t __p0, uint8x8_t __p1) {
  poly8x8x2_t __rev0;
  __rev0.val[0] = __builtin_shufflevector(__p0.val[0], __p0.val[0], 7, 6, 5, 4, 3, 2, 1, 0);
  __rev0.val[1] = __builtin_shufflevector(__p0.val[1], __p0.val[1], 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  poly8x8_t __ret;
  __ret = (poly8x8_t) __builtin_neon_vtbl2_v((int8x8_t)__rev0.val[0], (int8x8_t)__rev0.val[1], (int8x8_t)__rev1, 4);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x8_t vtbl2_u8(uint8x8x2_t __p0, uint8x8_t __p1) {
  uint8x8_t __ret;
  __ret = (uint8x8_t) __builtin_neon_vtbl2_v((int8x8_t)__p0.val[0], (int8x8_t)__p0.val[1], (int8x8_t)__p1, 16);
  return __ret;
}
#else
__ai uint8x8_t vtbl2_u8(uint8x8x2_t __p0, uint8x8_t __p1) {
  uint8x8x2_t __rev0;
  __rev0.val[0] = __builtin_shufflevector(__p0.val[0], __p0.val[0], 7, 6, 5, 4, 3, 2, 1, 0);
  __rev0.val[1] = __builtin_shufflevector(__p0.val[1], __p0.val[1], 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __ret;
  __ret = (uint8x8_t) __builtin_neon_vtbl2_v((int8x8_t)__rev0.val[0], (int8x8_t)__rev0.val[1], (int8x8_t)__rev1, 16);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x8_t vtbl2_s8(int8x8x2_t __p0, int8x8_t __p1) {
  int8x8_t __ret;
  __ret = (int8x8_t) __builtin_neon_vtbl2_v((int8x8_t)__p0.val[0], (int8x8_t)__p0.val[1], (int8x8_t)__p1, 0);
  return __ret;
}
#else
__ai int8x8_t vtbl2_s8(int8x8x2_t __p0, int8x8_t __p1) {
  int8x8x2_t __rev0;
  __rev0.val[0] = __builtin_shufflevector(__p0.val[0], __p0.val[0], 7, 6, 5, 4, 3, 2, 1, 0);
  __rev0.val[1] = __builtin_shufflevector(__p0.val[1], __p0.val[1], 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __ret;
  __ret = (int8x8_t) __builtin_neon_vtbl2_v((int8x8_t)__rev0.val[0], (int8x8_t)__rev0.val[1], (int8x8_t)__rev1, 0);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x8_t vtbl3_p8(poly8x8x3_t __p0, uint8x8_t __p1) {
  poly8x8_t __ret;
  __ret = (poly8x8_t) __builtin_neon_vtbl3_v((int8x8_t)__p0.val[0], (int8x8_t)__p0.val[1], (int8x8_t)__p0.val[2], (int8x8_t)__p1, 4);
  return __ret;
}
#else
__ai poly8x8_t vtbl3_p8(poly8x8x3_t __p0, uint8x8_t __p1) {
  poly8x8x3_t __rev0;
  __rev0.val[0] = __builtin_shufflevector(__p0.val[0], __p0.val[0], 7, 6, 5, 4, 3, 2, 1, 0);
  __rev0.val[1] = __builtin_shufflevector(__p0.val[1], __p0.val[1], 7, 6, 5, 4, 3, 2, 1, 0);
  __rev0.val[2] = __builtin_shufflevector(__p0.val[2], __p0.val[2], 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  poly8x8_t __ret;
  __ret = (poly8x8_t) __builtin_neon_vtbl3_v((int8x8_t)__rev0.val[0], (int8x8_t)__rev0.val[1], (int8x8_t)__rev0.val[2], (int8x8_t)__rev1, 4);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x8_t vtbl3_u8(uint8x8x3_t __p0, uint8x8_t __p1) {
  uint8x8_t __ret;
  __ret = (uint8x8_t) __builtin_neon_vtbl3_v((int8x8_t)__p0.val[0], (int8x8_t)__p0.val[1], (int8x8_t)__p0.val[2], (int8x8_t)__p1, 16);
  return __ret;
}
#else
__ai uint8x8_t vtbl3_u8(uint8x8x3_t __p0, uint8x8_t __p1) {
  uint8x8x3_t __rev0;
  __rev0.val[0] = __builtin_shufflevector(__p0.val[0], __p0.val[0], 7, 6, 5, 4, 3, 2, 1, 0);
  __rev0.val[1] = __builtin_shufflevector(__p0.val[1], __p0.val[1], 7, 6, 5, 4, 3, 2, 1, 0);
  __rev0.val[2] = __builtin_shufflevector(__p0.val[2], __p0.val[2], 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __ret;
  __ret = (uint8x8_t) __builtin_neon_vtbl3_v((int8x8_t)__rev0.val[0], (int8x8_t)__rev0.val[1], (int8x8_t)__rev0.val[2], (int8x8_t)__rev1, 16);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x8_t vtbl3_s8(int8x8x3_t __p0, int8x8_t __p1) {
  int8x8_t __ret;
  __ret = (int8x8_t) __builtin_neon_vtbl3_v((int8x8_t)__p0.val[0], (int8x8_t)__p0.val[1], (int8x8_t)__p0.val[2], (int8x8_t)__p1, 0);
  return __ret;
}
#else
__ai int8x8_t vtbl3_s8(int8x8x3_t __p0, int8x8_t __p1) {
  int8x8x3_t __rev0;
  __rev0.val[0] = __builtin_shufflevector(__p0.val[0], __p0.val[0], 7, 6, 5, 4, 3, 2, 1, 0);
  __rev0.val[1] = __builtin_shufflevector(__p0.val[1], __p0.val[1], 7, 6, 5, 4, 3, 2, 1, 0);
  __rev0.val[2] = __builtin_shufflevector(__p0.val[2], __p0.val[2], 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __ret;
  __ret = (int8x8_t) __builtin_neon_vtbl3_v((int8x8_t)__rev0.val[0], (int8x8_t)__rev0.val[1], (int8x8_t)__rev0.val[2], (int8x8_t)__rev1, 0);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x8_t vtbl4_p8(poly8x8x4_t __p0, uint8x8_t __p1) {
  poly8x8_t __ret;
  __ret = (poly8x8_t) __builtin_neon_vtbl4_v((int8x8_t)__p0.val[0], (int8x8_t)__p0.val[1], (int8x8_t)__p0.val[2], (int8x8_t)__p0.val[3], (int8x8_t)__p1, 4);
  return __ret;
}
#else
__ai poly8x8_t vtbl4_p8(poly8x8x4_t __p0, uint8x8_t __p1) {
  poly8x8x4_t __rev0;
  __rev0.val[0] = __builtin_shufflevector(__p0.val[0], __p0.val[0], 7, 6, 5, 4, 3, 2, 1, 0);
  __rev0.val[1] = __builtin_shufflevector(__p0.val[1], __p0.val[1], 7, 6, 5, 4, 3, 2, 1, 0);
  __rev0.val[2] = __builtin_shufflevector(__p0.val[2], __p0.val[2], 7, 6, 5, 4, 3, 2, 1, 0);
  __rev0.val[3] = __builtin_shufflevector(__p0.val[3], __p0.val[3], 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  poly8x8_t __ret;
  __ret = (poly8x8_t) __builtin_neon_vtbl4_v((int8x8_t)__rev0.val[0], (int8x8_t)__rev0.val[1], (int8x8_t)__rev0.val[2], (int8x8_t)__rev0.val[3], (int8x8_t)__rev1, 4);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x8_t vtbl4_u8(uint8x8x4_t __p0, uint8x8_t __p1) {
  uint8x8_t __ret;
  __ret = (uint8x8_t) __builtin_neon_vtbl4_v((int8x8_t)__p0.val[0], (int8x8_t)__p0.val[1], (int8x8_t)__p0.val[2], (int8x8_t)__p0.val[3], (int8x8_t)__p1, 16);
  return __ret;
}
#else
__ai uint8x8_t vtbl4_u8(uint8x8x4_t __p0, uint8x8_t __p1) {
  uint8x8x4_t __rev0;
  __rev0.val[0] = __builtin_shufflevector(__p0.val[0], __p0.val[0], 7, 6, 5, 4, 3, 2, 1, 0);
  __rev0.val[1] = __builtin_shufflevector(__p0.val[1], __p0.val[1], 7, 6, 5, 4, 3, 2, 1, 0);
  __rev0.val[2] = __builtin_shufflevector(__p0.val[2], __p0.val[2], 7, 6, 5, 4, 3, 2, 1, 0);
  __rev0.val[3] = __builtin_shufflevector(__p0.val[3], __p0.val[3], 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __ret;
  __ret = (uint8x8_t) __builtin_neon_vtbl4_v((int8x8_t)__rev0.val[0], (int8x8_t)__rev0.val[1], (int8x8_t)__rev0.val[2], (int8x8_t)__rev0.val[3], (int8x8_t)__rev1, 16);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x8_t vtbl4_s8(int8x8x4_t __p0, int8x8_t __p1) {
  int8x8_t __ret;
  __ret = (int8x8_t) __builtin_neon_vtbl4_v((int8x8_t)__p0.val[0], (int8x8_t)__p0.val[1], (int8x8_t)__p0.val[2], (int8x8_t)__p0.val[3], (int8x8_t)__p1, 0);
  return __ret;
}
#else
__ai int8x8_t vtbl4_s8(int8x8x4_t __p0, int8x8_t __p1) {
  int8x8x4_t __rev0;
  __rev0.val[0] = __builtin_shufflevector(__p0.val[0], __p0.val[0], 7, 6, 5, 4, 3, 2, 1, 0);
  __rev0.val[1] = __builtin_shufflevector(__p0.val[1], __p0.val[1], 7, 6, 5, 4, 3, 2, 1, 0);
  __rev0.val[2] = __builtin_shufflevector(__p0.val[2], __p0.val[2], 7, 6, 5, 4, 3, 2, 1, 0);
  __rev0.val[3] = __builtin_shufflevector(__p0.val[3], __p0.val[3], 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __ret;
  __ret = (int8x8_t) __builtin_neon_vtbl4_v((int8x8_t)__rev0.val[0], (int8x8_t)__rev0.val[1], (int8x8_t)__rev0.val[2], (int8x8_t)__rev0.val[3], (int8x8_t)__rev1, 0);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x8_t vtbx1_p8(poly8x8_t __p0, poly8x8_t __p1, uint8x8_t __p2) {
  poly8x8_t __ret;
  __ret = (poly8x8_t) __builtin_neon_vtbx1_v((int8x8_t)__p0, (int8x8_t)__p1, (int8x8_t)__p2, 4);
  return __ret;
}
#else
__ai poly8x8_t vtbx1_p8(poly8x8_t __p0, poly8x8_t __p1, uint8x8_t __p2) {
  poly8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  poly8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 7, 6, 5, 4, 3, 2, 1, 0);
  poly8x8_t __ret;
  __ret = (poly8x8_t) __builtin_neon_vtbx1_v((int8x8_t)__rev0, (int8x8_t)__rev1, (int8x8_t)__rev2, 4);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x8_t vtbx1_u8(uint8x8_t __p0, uint8x8_t __p1, uint8x8_t __p2) {
  uint8x8_t __ret;
  __ret = (uint8x8_t) __builtin_neon_vtbx1_v((int8x8_t)__p0, (int8x8_t)__p1, (int8x8_t)__p2, 16);
  return __ret;
}
#else
__ai uint8x8_t vtbx1_u8(uint8x8_t __p0, uint8x8_t __p1, uint8x8_t __p2) {
  uint8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __ret;
  __ret = (uint8x8_t) __builtin_neon_vtbx1_v((int8x8_t)__rev0, (int8x8_t)__rev1, (int8x8_t)__rev2, 16);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x8_t vtbx1_s8(int8x8_t __p0, int8x8_t __p1, int8x8_t __p2) {
  int8x8_t __ret;
  __ret = (int8x8_t) __builtin_neon_vtbx1_v((int8x8_t)__p0, (int8x8_t)__p1, (int8x8_t)__p2, 0);
  return __ret;
}
#else
__ai int8x8_t vtbx1_s8(int8x8_t __p0, int8x8_t __p1, int8x8_t __p2) {
  int8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __ret;
  __ret = (int8x8_t) __builtin_neon_vtbx1_v((int8x8_t)__rev0, (int8x8_t)__rev1, (int8x8_t)__rev2, 0);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x8_t vtbx2_p8(poly8x8_t __p0, poly8x8x2_t __p1, uint8x8_t __p2) {
  poly8x8_t __ret;
  __ret = (poly8x8_t) __builtin_neon_vtbx2_v((int8x8_t)__p0, (int8x8_t)__p1.val[0], (int8x8_t)__p1.val[1], (int8x8_t)__p2, 4);
  return __ret;
}
#else
__ai poly8x8_t vtbx2_p8(poly8x8_t __p0, poly8x8x2_t __p1, uint8x8_t __p2) {
  poly8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  poly8x8x2_t __rev1;
  __rev1.val[0] = __builtin_shufflevector(__p1.val[0], __p1.val[0], 7, 6, 5, 4, 3, 2, 1, 0);
  __rev1.val[1] = __builtin_shufflevector(__p1.val[1], __p1.val[1], 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 7, 6, 5, 4, 3, 2, 1, 0);
  poly8x8_t __ret;
  __ret = (poly8x8_t) __builtin_neon_vtbx2_v((int8x8_t)__rev0, (int8x8_t)__rev1.val[0], (int8x8_t)__rev1.val[1], (int8x8_t)__rev2, 4);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x8_t vtbx2_u8(uint8x8_t __p0, uint8x8x2_t __p1, uint8x8_t __p2) {
  uint8x8_t __ret;
  __ret = (uint8x8_t) __builtin_neon_vtbx2_v((int8x8_t)__p0, (int8x8_t)__p1.val[0], (int8x8_t)__p1.val[1], (int8x8_t)__p2, 16);
  return __ret;
}
#else
__ai uint8x8_t vtbx2_u8(uint8x8_t __p0, uint8x8x2_t __p1, uint8x8_t __p2) {
  uint8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8x2_t __rev1;
  __rev1.val[0] = __builtin_shufflevector(__p1.val[0], __p1.val[0], 7, 6, 5, 4, 3, 2, 1, 0);
  __rev1.val[1] = __builtin_shufflevector(__p1.val[1], __p1.val[1], 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __ret;
  __ret = (uint8x8_t) __builtin_neon_vtbx2_v((int8x8_t)__rev0, (int8x8_t)__rev1.val[0], (int8x8_t)__rev1.val[1], (int8x8_t)__rev2, 16);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x8_t vtbx2_s8(int8x8_t __p0, int8x8x2_t __p1, int8x8_t __p2) {
  int8x8_t __ret;
  __ret = (int8x8_t) __builtin_neon_vtbx2_v((int8x8_t)__p0, (int8x8_t)__p1.val[0], (int8x8_t)__p1.val[1], (int8x8_t)__p2, 0);
  return __ret;
}
#else
__ai int8x8_t vtbx2_s8(int8x8_t __p0, int8x8x2_t __p1, int8x8_t __p2) {
  int8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8x2_t __rev1;
  __rev1.val[0] = __builtin_shufflevector(__p1.val[0], __p1.val[0], 7, 6, 5, 4, 3, 2, 1, 0);
  __rev1.val[1] = __builtin_shufflevector(__p1.val[1], __p1.val[1], 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __ret;
  __ret = (int8x8_t) __builtin_neon_vtbx2_v((int8x8_t)__rev0, (int8x8_t)__rev1.val[0], (int8x8_t)__rev1.val[1], (int8x8_t)__rev2, 0);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x8_t vtbx3_p8(poly8x8_t __p0, poly8x8x3_t __p1, uint8x8_t __p2) {
  poly8x8_t __ret;
  __ret = (poly8x8_t) __builtin_neon_vtbx3_v((int8x8_t)__p0, (int8x8_t)__p1.val[0], (int8x8_t)__p1.val[1], (int8x8_t)__p1.val[2], (int8x8_t)__p2, 4);
  return __ret;
}
#else
__ai poly8x8_t vtbx3_p8(poly8x8_t __p0, poly8x8x3_t __p1, uint8x8_t __p2) {
  poly8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  poly8x8x3_t __rev1;
  __rev1.val[0] = __builtin_shufflevector(__p1.val[0], __p1.val[0], 7, 6, 5, 4, 3, 2, 1, 0);
  __rev1.val[1] = __builtin_shufflevector(__p1.val[1], __p1.val[1], 7, 6, 5, 4, 3, 2, 1, 0);
  __rev1.val[2] = __builtin_shufflevector(__p1.val[2], __p1.val[2], 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 7, 6, 5, 4, 3, 2, 1, 0);
  poly8x8_t __ret;
  __ret = (poly8x8_t) __builtin_neon_vtbx3_v((int8x8_t)__rev0, (int8x8_t)__rev1.val[0], (int8x8_t)__rev1.val[1], (int8x8_t)__rev1.val[2], (int8x8_t)__rev2, 4);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x8_t vtbx3_u8(uint8x8_t __p0, uint8x8x3_t __p1, uint8x8_t __p2) {
  uint8x8_t __ret;
  __ret = (uint8x8_t) __builtin_neon_vtbx3_v((int8x8_t)__p0, (int8x8_t)__p1.val[0], (int8x8_t)__p1.val[1], (int8x8_t)__p1.val[2], (int8x8_t)__p2, 16);
  return __ret;
}
#else
__ai uint8x8_t vtbx3_u8(uint8x8_t __p0, uint8x8x3_t __p1, uint8x8_t __p2) {
  uint8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8x3_t __rev1;
  __rev1.val[0] = __builtin_shufflevector(__p1.val[0], __p1.val[0], 7, 6, 5, 4, 3, 2, 1, 0);
  __rev1.val[1] = __builtin_shufflevector(__p1.val[1], __p1.val[1], 7, 6, 5, 4, 3, 2, 1, 0);
  __rev1.val[2] = __builtin_shufflevector(__p1.val[2], __p1.val[2], 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __ret;
  __ret = (uint8x8_t) __builtin_neon_vtbx3_v((int8x8_t)__rev0, (int8x8_t)__rev1.val[0], (int8x8_t)__rev1.val[1], (int8x8_t)__rev1.val[2], (int8x8_t)__rev2, 16);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x8_t vtbx3_s8(int8x8_t __p0, int8x8x3_t __p1, int8x8_t __p2) {
  int8x8_t __ret;
  __ret = (int8x8_t) __builtin_neon_vtbx3_v((int8x8_t)__p0, (int8x8_t)__p1.val[0], (int8x8_t)__p1.val[1], (int8x8_t)__p1.val[2], (int8x8_t)__p2, 0);
  return __ret;
}
#else
__ai int8x8_t vtbx3_s8(int8x8_t __p0, int8x8x3_t __p1, int8x8_t __p2) {
  int8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8x3_t __rev1;
  __rev1.val[0] = __builtin_shufflevector(__p1.val[0], __p1.val[0], 7, 6, 5, 4, 3, 2, 1, 0);
  __rev1.val[1] = __builtin_shufflevector(__p1.val[1], __p1.val[1], 7, 6, 5, 4, 3, 2, 1, 0);
  __rev1.val[2] = __builtin_shufflevector(__p1.val[2], __p1.val[2], 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __ret;
  __ret = (int8x8_t) __builtin_neon_vtbx3_v((int8x8_t)__rev0, (int8x8_t)__rev1.val[0], (int8x8_t)__rev1.val[1], (int8x8_t)__rev1.val[2], (int8x8_t)__rev2, 0);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x8_t vtbx4_p8(poly8x8_t __p0, poly8x8x4_t __p1, uint8x8_t __p2) {
  poly8x8_t __ret;
  __ret = (poly8x8_t) __builtin_neon_vtbx4_v((int8x8_t)__p0, (int8x8_t)__p1.val[0], (int8x8_t)__p1.val[1], (int8x8_t)__p1.val[2], (int8x8_t)__p1.val[3], (int8x8_t)__p2, 4);
  return __ret;
}
#else
__ai poly8x8_t vtbx4_p8(poly8x8_t __p0, poly8x8x4_t __p1, uint8x8_t __p2) {
  poly8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  poly8x8x4_t __rev1;
  __rev1.val[0] = __builtin_shufflevector(__p1.val[0], __p1.val[0], 7, 6, 5, 4, 3, 2, 1, 0);
  __rev1.val[1] = __builtin_shufflevector(__p1.val[1], __p1.val[1], 7, 6, 5, 4, 3, 2, 1, 0);
  __rev1.val[2] = __builtin_shufflevector(__p1.val[2], __p1.val[2], 7, 6, 5, 4, 3, 2, 1, 0);
  __rev1.val[3] = __builtin_shufflevector(__p1.val[3], __p1.val[3], 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 7, 6, 5, 4, 3, 2, 1, 0);
  poly8x8_t __ret;
  __ret = (poly8x8_t) __builtin_neon_vtbx4_v((int8x8_t)__rev0, (int8x8_t)__rev1.val[0], (int8x8_t)__rev1.val[1], (int8x8_t)__rev1.val[2], (int8x8_t)__rev1.val[3], (int8x8_t)__rev2, 4);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x8_t vtbx4_u8(uint8x8_t __p0, uint8x8x4_t __p1, uint8x8_t __p2) {
  uint8x8_t __ret;
  __ret = (uint8x8_t) __builtin_neon_vtbx4_v((int8x8_t)__p0, (int8x8_t)__p1.val[0], (int8x8_t)__p1.val[1], (int8x8_t)__p1.val[2], (int8x8_t)__p1.val[3], (int8x8_t)__p2, 16);
  return __ret;
}
#else
__ai uint8x8_t vtbx4_u8(uint8x8_t __p0, uint8x8x4_t __p1, uint8x8_t __p2) {
  uint8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8x4_t __rev1;
  __rev1.val[0] = __builtin_shufflevector(__p1.val[0], __p1.val[0], 7, 6, 5, 4, 3, 2, 1, 0);
  __rev1.val[1] = __builtin_shufflevector(__p1.val[1], __p1.val[1], 7, 6, 5, 4, 3, 2, 1, 0);
  __rev1.val[2] = __builtin_shufflevector(__p1.val[2], __p1.val[2], 7, 6, 5, 4, 3, 2, 1, 0);
  __rev1.val[3] = __builtin_shufflevector(__p1.val[3], __p1.val[3], 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __ret;
  __ret = (uint8x8_t) __builtin_neon_vtbx4_v((int8x8_t)__rev0, (int8x8_t)__rev1.val[0], (int8x8_t)__rev1.val[1], (int8x8_t)__rev1.val[2], (int8x8_t)__rev1.val[3], (int8x8_t)__rev2, 16);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x8_t vtbx4_s8(int8x8_t __p0, int8x8x4_t __p1, int8x8_t __p2) {
  int8x8_t __ret;
  __ret = (int8x8_t) __builtin_neon_vtbx4_v((int8x8_t)__p0, (int8x8_t)__p1.val[0], (int8x8_t)__p1.val[1], (int8x8_t)__p1.val[2], (int8x8_t)__p1.val[3], (int8x8_t)__p2, 0);
  return __ret;
}
#else
__ai int8x8_t vtbx4_s8(int8x8_t __p0, int8x8x4_t __p1, int8x8_t __p2) {
  int8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8x4_t __rev1;
  __rev1.val[0] = __builtin_shufflevector(__p1.val[0], __p1.val[0], 7, 6, 5, 4, 3, 2, 1, 0);
  __rev1.val[1] = __builtin_shufflevector(__p1.val[1], __p1.val[1], 7, 6, 5, 4, 3, 2, 1, 0);
  __rev1.val[2] = __builtin_shufflevector(__p1.val[2], __p1.val[2], 7, 6, 5, 4, 3, 2, 1, 0);
  __rev1.val[3] = __builtin_shufflevector(__p1.val[3], __p1.val[3], 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __ret;
  __ret = (int8x8_t) __builtin_neon_vtbx4_v((int8x8_t)__rev0, (int8x8_t)__rev1.val[0], (int8x8_t)__rev1.val[1], (int8x8_t)__rev1.val[2], (int8x8_t)__rev1.val[3], (int8x8_t)__rev2, 0);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x8x2_t vtrn_p8(poly8x8_t __p0, poly8x8_t __p1) {
  poly8x8x2_t __ret;
  __builtin_neon_vtrn_v(&__ret, (int8x8_t)__p0, (int8x8_t)__p1, 4);
  return __ret;
}
#else
__ai poly8x8x2_t vtrn_p8(poly8x8_t __p0, poly8x8_t __p1) {
  poly8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  poly8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  poly8x8x2_t __ret;
  __builtin_neon_vtrn_v(&__ret, (int8x8_t)__rev0, (int8x8_t)__rev1, 4);

  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 7, 6, 5, 4, 3, 2, 1, 0);
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly16x4x2_t vtrn_p16(poly16x4_t __p0, poly16x4_t __p1) {
  poly16x4x2_t __ret;
  __builtin_neon_vtrn_v(&__ret, (int8x8_t)__p0, (int8x8_t)__p1, 5);
  return __ret;
}
#else
__ai poly16x4x2_t vtrn_p16(poly16x4_t __p0, poly16x4_t __p1) {
  poly16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  poly16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  poly16x4x2_t __ret;
  __builtin_neon_vtrn_v(&__ret, (int8x8_t)__rev0, (int8x8_t)__rev1, 5);

  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 3, 2, 1, 0);
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x16x2_t vtrnq_p8(poly8x16_t __p0, poly8x16_t __p1) {
  poly8x16x2_t __ret;
  __builtin_neon_vtrnq_v(&__ret, (int8x16_t)__p0, (int8x16_t)__p1, 36);
  return __ret;
}
#else
__ai poly8x16x2_t vtrnq_p8(poly8x16_t __p0, poly8x16_t __p1) {
  poly8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  poly8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  poly8x16x2_t __ret;
  __builtin_neon_vtrnq_v(&__ret, (int8x16_t)__rev0, (int8x16_t)__rev1, 36);

  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly16x8x2_t vtrnq_p16(poly16x8_t __p0, poly16x8_t __p1) {
  poly16x8x2_t __ret;
  __builtin_neon_vtrnq_v(&__ret, (int8x16_t)__p0, (int8x16_t)__p1, 37);
  return __ret;
}
#else
__ai poly16x8x2_t vtrnq_p16(poly16x8_t __p0, poly16x8_t __p1) {
  poly16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  poly16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  poly16x8x2_t __ret;
  __builtin_neon_vtrnq_v(&__ret, (int8x16_t)__rev0, (int8x16_t)__rev1, 37);

  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 7, 6, 5, 4, 3, 2, 1, 0);
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x16x2_t vtrnq_u8(uint8x16_t __p0, uint8x16_t __p1) {
  uint8x16x2_t __ret;
  __builtin_neon_vtrnq_v(&__ret, (int8x16_t)__p0, (int8x16_t)__p1, 48);
  return __ret;
}
#else
__ai uint8x16x2_t vtrnq_u8(uint8x16_t __p0, uint8x16_t __p1) {
  uint8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16x2_t __ret;
  __builtin_neon_vtrnq_v(&__ret, (int8x16_t)__rev0, (int8x16_t)__rev1, 48);

  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4x2_t vtrnq_u32(uint32x4_t __p0, uint32x4_t __p1) {
  uint32x4x2_t __ret;
  __builtin_neon_vtrnq_v(&__ret, (int8x16_t)__p0, (int8x16_t)__p1, 50);
  return __ret;
}
#else
__ai uint32x4x2_t vtrnq_u32(uint32x4_t __p0, uint32x4_t __p1) {
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint32x4x2_t __ret;
  __builtin_neon_vtrnq_v(&__ret, (int8x16_t)__rev0, (int8x16_t)__rev1, 50);

  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 3, 2, 1, 0);
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x8x2_t vtrnq_u16(uint16x8_t __p0, uint16x8_t __p1) {
  uint16x8x2_t __ret;
  __builtin_neon_vtrnq_v(&__ret, (int8x16_t)__p0, (int8x16_t)__p1, 49);
  return __ret;
}
#else
__ai uint16x8x2_t vtrnq_u16(uint16x8_t __p0, uint16x8_t __p1) {
  uint16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8x2_t __ret;
  __builtin_neon_vtrnq_v(&__ret, (int8x16_t)__rev0, (int8x16_t)__rev1, 49);

  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 7, 6, 5, 4, 3, 2, 1, 0);
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x16x2_t vtrnq_s8(int8x16_t __p0, int8x16_t __p1) {
  int8x16x2_t __ret;
  __builtin_neon_vtrnq_v(&__ret, (int8x16_t)__p0, (int8x16_t)__p1, 32);
  return __ret;
}
#else
__ai int8x16x2_t vtrnq_s8(int8x16_t __p0, int8x16_t __p1) {
  int8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16x2_t __ret;
  __builtin_neon_vtrnq_v(&__ret, (int8x16_t)__rev0, (int8x16_t)__rev1, 32);

  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x4x2_t vtrnq_f32(float32x4_t __p0, float32x4_t __p1) {
  float32x4x2_t __ret;
  __builtin_neon_vtrnq_v(&__ret, (int8x16_t)__p0, (int8x16_t)__p1, 41);
  return __ret;
}
#else
__ai float32x4x2_t vtrnq_f32(float32x4_t __p0, float32x4_t __p1) {
  float32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  float32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  float32x4x2_t __ret;
  __builtin_neon_vtrnq_v(&__ret, (int8x16_t)__rev0, (int8x16_t)__rev1, 41);

  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 3, 2, 1, 0);
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4x2_t vtrnq_s32(int32x4_t __p0, int32x4_t __p1) {
  int32x4x2_t __ret;
  __builtin_neon_vtrnq_v(&__ret, (int8x16_t)__p0, (int8x16_t)__p1, 34);
  return __ret;
}
#else
__ai int32x4x2_t vtrnq_s32(int32x4_t __p0, int32x4_t __p1) {
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int32x4x2_t __ret;
  __builtin_neon_vtrnq_v(&__ret, (int8x16_t)__rev0, (int8x16_t)__rev1, 34);

  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 3, 2, 1, 0);
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x8x2_t vtrnq_s16(int16x8_t __p0, int16x8_t __p1) {
  int16x8x2_t __ret;
  __builtin_neon_vtrnq_v(&__ret, (int8x16_t)__p0, (int8x16_t)__p1, 33);
  return __ret;
}
#else
__ai int16x8x2_t vtrnq_s16(int16x8_t __p0, int16x8_t __p1) {
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8x2_t __ret;
  __builtin_neon_vtrnq_v(&__ret, (int8x16_t)__rev0, (int8x16_t)__rev1, 33);

  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 7, 6, 5, 4, 3, 2, 1, 0);
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x8x2_t vtrn_u8(uint8x8_t __p0, uint8x8_t __p1) {
  uint8x8x2_t __ret;
  __builtin_neon_vtrn_v(&__ret, (int8x8_t)__p0, (int8x8_t)__p1, 16);
  return __ret;
}
#else
__ai uint8x8x2_t vtrn_u8(uint8x8_t __p0, uint8x8_t __p1) {
  uint8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8x2_t __ret;
  __builtin_neon_vtrn_v(&__ret, (int8x8_t)__rev0, (int8x8_t)__rev1, 16);

  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 7, 6, 5, 4, 3, 2, 1, 0);
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x2x2_t vtrn_u32(uint32x2_t __p0, uint32x2_t __p1) {
  uint32x2x2_t __ret;
  __builtin_neon_vtrn_v(&__ret, (int8x8_t)__p0, (int8x8_t)__p1, 18);
  return __ret;
}
#else
__ai uint32x2x2_t vtrn_u32(uint32x2_t __p0, uint32x2_t __p1) {
  uint32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  uint32x2x2_t __ret;
  __builtin_neon_vtrn_v(&__ret, (int8x8_t)__rev0, (int8x8_t)__rev1, 18);

  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 1, 0);
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x4x2_t vtrn_u16(uint16x4_t __p0, uint16x4_t __p1) {
  uint16x4x2_t __ret;
  __builtin_neon_vtrn_v(&__ret, (int8x8_t)__p0, (int8x8_t)__p1, 17);
  return __ret;
}
#else
__ai uint16x4x2_t vtrn_u16(uint16x4_t __p0, uint16x4_t __p1) {
  uint16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint16x4x2_t __ret;
  __builtin_neon_vtrn_v(&__ret, (int8x8_t)__rev0, (int8x8_t)__rev1, 17);

  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 3, 2, 1, 0);
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x8x2_t vtrn_s8(int8x8_t __p0, int8x8_t __p1) {
  int8x8x2_t __ret;
  __builtin_neon_vtrn_v(&__ret, (int8x8_t)__p0, (int8x8_t)__p1, 0);
  return __ret;
}
#else
__ai int8x8x2_t vtrn_s8(int8x8_t __p0, int8x8_t __p1) {
  int8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8x2_t __ret;
  __builtin_neon_vtrn_v(&__ret, (int8x8_t)__rev0, (int8x8_t)__rev1, 0);

  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 7, 6, 5, 4, 3, 2, 1, 0);
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x2x2_t vtrn_f32(float32x2_t __p0, float32x2_t __p1) {
  float32x2x2_t __ret;
  __builtin_neon_vtrn_v(&__ret, (int8x8_t)__p0, (int8x8_t)__p1, 9);
  return __ret;
}
#else
__ai float32x2x2_t vtrn_f32(float32x2_t __p0, float32x2_t __p1) {
  float32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  float32x2x2_t __ret;
  __builtin_neon_vtrn_v(&__ret, (int8x8_t)__rev0, (int8x8_t)__rev1, 9);

  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 1, 0);
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x2x2_t vtrn_s32(int32x2_t __p0, int32x2_t __p1) {
  int32x2x2_t __ret;
  __builtin_neon_vtrn_v(&__ret, (int8x8_t)__p0, (int8x8_t)__p1, 2);
  return __ret;
}
#else
__ai int32x2x2_t vtrn_s32(int32x2_t __p0, int32x2_t __p1) {
  int32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  int32x2x2_t __ret;
  __builtin_neon_vtrn_v(&__ret, (int8x8_t)__rev0, (int8x8_t)__rev1, 2);

  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 1, 0);
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x4x2_t vtrn_s16(int16x4_t __p0, int16x4_t __p1) {
  int16x4x2_t __ret;
  __builtin_neon_vtrn_v(&__ret, (int8x8_t)__p0, (int8x8_t)__p1, 1);
  return __ret;
}
#else
__ai int16x4x2_t vtrn_s16(int16x4_t __p0, int16x4_t __p1) {
  int16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int16x4x2_t __ret;
  __builtin_neon_vtrn_v(&__ret, (int8x8_t)__rev0, (int8x8_t)__rev1, 1);

  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 3, 2, 1, 0);
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x8_t vtst_p8(poly8x8_t __p0, poly8x8_t __p1) {
  uint8x8_t __ret;
  __ret = (uint8x8_t) __builtin_neon_vtst_v((int8x8_t)__p0, (int8x8_t)__p1, 16);
  return __ret;
}
#else
__ai uint8x8_t vtst_p8(poly8x8_t __p0, poly8x8_t __p1) {
  poly8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  poly8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __ret;
  __ret = (uint8x8_t) __builtin_neon_vtst_v((int8x8_t)__rev0, (int8x8_t)__rev1, 16);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x4_t vtst_p16(poly16x4_t __p0, poly16x4_t __p1) {
  uint16x4_t __ret;
  __ret = (uint16x4_t) __builtin_neon_vtst_v((int8x8_t)__p0, (int8x8_t)__p1, 17);
  return __ret;
}
#else
__ai uint16x4_t vtst_p16(poly16x4_t __p0, poly16x4_t __p1) {
  poly16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  poly16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint16x4_t __ret;
  __ret = (uint16x4_t) __builtin_neon_vtst_v((int8x8_t)__rev0, (int8x8_t)__rev1, 17);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x16_t vtstq_p8(poly8x16_t __p0, poly8x16_t __p1) {
  uint8x16_t __ret;
  __ret = (uint8x16_t) __builtin_neon_vtstq_v((int8x16_t)__p0, (int8x16_t)__p1, 48);
  return __ret;
}
#else
__ai uint8x16_t vtstq_p8(poly8x16_t __p0, poly8x16_t __p1) {
  poly8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  poly8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __ret;
  __ret = (uint8x16_t) __builtin_neon_vtstq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 48);
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x8_t vtstq_p16(poly16x8_t __p0, poly16x8_t __p1) {
  uint16x8_t __ret;
  __ret = (uint16x8_t) __builtin_neon_vtstq_v((int8x16_t)__p0, (int8x16_t)__p1, 49);
  return __ret;
}
#else
__ai uint16x8_t vtstq_p16(poly16x8_t __p0, poly16x8_t __p1) {
  poly16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  poly16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __ret;
  __ret = (uint16x8_t) __builtin_neon_vtstq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 49);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x16_t vtstq_u8(uint8x16_t __p0, uint8x16_t __p1) {
  uint8x16_t __ret;
  __ret = (uint8x16_t) __builtin_neon_vtstq_v((int8x16_t)__p0, (int8x16_t)__p1, 48);
  return __ret;
}
#else
__ai uint8x16_t vtstq_u8(uint8x16_t __p0, uint8x16_t __p1) {
  uint8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __ret;
  __ret = (uint8x16_t) __builtin_neon_vtstq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 48);
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vtstq_u32(uint32x4_t __p0, uint32x4_t __p1) {
  uint32x4_t __ret;
  __ret = (uint32x4_t) __builtin_neon_vtstq_v((int8x16_t)__p0, (int8x16_t)__p1, 50);
  return __ret;
}
#else
__ai uint32x4_t vtstq_u32(uint32x4_t __p0, uint32x4_t __p1) {
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint32x4_t __ret;
  __ret = (uint32x4_t) __builtin_neon_vtstq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 50);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x8_t vtstq_u16(uint16x8_t __p0, uint16x8_t __p1) {
  uint16x8_t __ret;
  __ret = (uint16x8_t) __builtin_neon_vtstq_v((int8x16_t)__p0, (int8x16_t)__p1, 49);
  return __ret;
}
#else
__ai uint16x8_t vtstq_u16(uint16x8_t __p0, uint16x8_t __p1) {
  uint16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __ret;
  __ret = (uint16x8_t) __builtin_neon_vtstq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 49);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x16_t vtstq_s8(int8x16_t __p0, int8x16_t __p1) {
  uint8x16_t __ret;
  __ret = (uint8x16_t) __builtin_neon_vtstq_v((int8x16_t)__p0, (int8x16_t)__p1, 48);
  return __ret;
}
#else
__ai uint8x16_t vtstq_s8(int8x16_t __p0, int8x16_t __p1) {
  int8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __ret;
  __ret = (uint8x16_t) __builtin_neon_vtstq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 48);
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vtstq_s32(int32x4_t __p0, int32x4_t __p1) {
  uint32x4_t __ret;
  __ret = (uint32x4_t) __builtin_neon_vtstq_v((int8x16_t)__p0, (int8x16_t)__p1, 50);
  return __ret;
}
#else
__ai uint32x4_t vtstq_s32(int32x4_t __p0, int32x4_t __p1) {
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint32x4_t __ret;
  __ret = (uint32x4_t) __builtin_neon_vtstq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 50);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x8_t vtstq_s16(int16x8_t __p0, int16x8_t __p1) {
  uint16x8_t __ret;
  __ret = (uint16x8_t) __builtin_neon_vtstq_v((int8x16_t)__p0, (int8x16_t)__p1, 49);
  return __ret;
}
#else
__ai uint16x8_t vtstq_s16(int16x8_t __p0, int16x8_t __p1) {
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __ret;
  __ret = (uint16x8_t) __builtin_neon_vtstq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 49);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x8_t vtst_u8(uint8x8_t __p0, uint8x8_t __p1) {
  uint8x8_t __ret;
  __ret = (uint8x8_t) __builtin_neon_vtst_v((int8x8_t)__p0, (int8x8_t)__p1, 16);
  return __ret;
}
#else
__ai uint8x8_t vtst_u8(uint8x8_t __p0, uint8x8_t __p1) {
  uint8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __ret;
  __ret = (uint8x8_t) __builtin_neon_vtst_v((int8x8_t)__rev0, (int8x8_t)__rev1, 16);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x2_t vtst_u32(uint32x2_t __p0, uint32x2_t __p1) {
  uint32x2_t __ret;
  __ret = (uint32x2_t) __builtin_neon_vtst_v((int8x8_t)__p0, (int8x8_t)__p1, 18);
  return __ret;
}
#else
__ai uint32x2_t vtst_u32(uint32x2_t __p0, uint32x2_t __p1) {
  uint32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  uint32x2_t __ret;
  __ret = (uint32x2_t) __builtin_neon_vtst_v((int8x8_t)__rev0, (int8x8_t)__rev1, 18);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x4_t vtst_u16(uint16x4_t __p0, uint16x4_t __p1) {
  uint16x4_t __ret;
  __ret = (uint16x4_t) __builtin_neon_vtst_v((int8x8_t)__p0, (int8x8_t)__p1, 17);
  return __ret;
}
#else
__ai uint16x4_t vtst_u16(uint16x4_t __p0, uint16x4_t __p1) {
  uint16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint16x4_t __ret;
  __ret = (uint16x4_t) __builtin_neon_vtst_v((int8x8_t)__rev0, (int8x8_t)__rev1, 17);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x8_t vtst_s8(int8x8_t __p0, int8x8_t __p1) {
  uint8x8_t __ret;
  __ret = (uint8x8_t) __builtin_neon_vtst_v((int8x8_t)__p0, (int8x8_t)__p1, 16);
  return __ret;
}
#else
__ai uint8x8_t vtst_s8(int8x8_t __p0, int8x8_t __p1) {
  int8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __ret;
  __ret = (uint8x8_t) __builtin_neon_vtst_v((int8x8_t)__rev0, (int8x8_t)__rev1, 16);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x2_t vtst_s32(int32x2_t __p0, int32x2_t __p1) {
  uint32x2_t __ret;
  __ret = (uint32x2_t) __builtin_neon_vtst_v((int8x8_t)__p0, (int8x8_t)__p1, 18);
  return __ret;
}
#else
__ai uint32x2_t vtst_s32(int32x2_t __p0, int32x2_t __p1) {
  int32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  uint32x2_t __ret;
  __ret = (uint32x2_t) __builtin_neon_vtst_v((int8x8_t)__rev0, (int8x8_t)__rev1, 18);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x4_t vtst_s16(int16x4_t __p0, int16x4_t __p1) {
  uint16x4_t __ret;
  __ret = (uint16x4_t) __builtin_neon_vtst_v((int8x8_t)__p0, (int8x8_t)__p1, 17);
  return __ret;
}
#else
__ai uint16x4_t vtst_s16(int16x4_t __p0, int16x4_t __p1) {
  int16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint16x4_t __ret;
  __ret = (uint16x4_t) __builtin_neon_vtst_v((int8x8_t)__rev0, (int8x8_t)__rev1, 17);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x8x2_t vuzp_p8(poly8x8_t __p0, poly8x8_t __p1) {
  poly8x8x2_t __ret;
  __builtin_neon_vuzp_v(&__ret, (int8x8_t)__p0, (int8x8_t)__p1, 4);
  return __ret;
}
#else
__ai poly8x8x2_t vuzp_p8(poly8x8_t __p0, poly8x8_t __p1) {
  poly8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  poly8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  poly8x8x2_t __ret;
  __builtin_neon_vuzp_v(&__ret, (int8x8_t)__rev0, (int8x8_t)__rev1, 4);

  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 7, 6, 5, 4, 3, 2, 1, 0);
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly16x4x2_t vuzp_p16(poly16x4_t __p0, poly16x4_t __p1) {
  poly16x4x2_t __ret;
  __builtin_neon_vuzp_v(&__ret, (int8x8_t)__p0, (int8x8_t)__p1, 5);
  return __ret;
}
#else
__ai poly16x4x2_t vuzp_p16(poly16x4_t __p0, poly16x4_t __p1) {
  poly16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  poly16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  poly16x4x2_t __ret;
  __builtin_neon_vuzp_v(&__ret, (int8x8_t)__rev0, (int8x8_t)__rev1, 5);

  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 3, 2, 1, 0);
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x16x2_t vuzpq_p8(poly8x16_t __p0, poly8x16_t __p1) {
  poly8x16x2_t __ret;
  __builtin_neon_vuzpq_v(&__ret, (int8x16_t)__p0, (int8x16_t)__p1, 36);
  return __ret;
}
#else
__ai poly8x16x2_t vuzpq_p8(poly8x16_t __p0, poly8x16_t __p1) {
  poly8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  poly8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  poly8x16x2_t __ret;
  __builtin_neon_vuzpq_v(&__ret, (int8x16_t)__rev0, (int8x16_t)__rev1, 36);

  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly16x8x2_t vuzpq_p16(poly16x8_t __p0, poly16x8_t __p1) {
  poly16x8x2_t __ret;
  __builtin_neon_vuzpq_v(&__ret, (int8x16_t)__p0, (int8x16_t)__p1, 37);
  return __ret;
}
#else
__ai poly16x8x2_t vuzpq_p16(poly16x8_t __p0, poly16x8_t __p1) {
  poly16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  poly16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  poly16x8x2_t __ret;
  __builtin_neon_vuzpq_v(&__ret, (int8x16_t)__rev0, (int8x16_t)__rev1, 37);

  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 7, 6, 5, 4, 3, 2, 1, 0);
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x16x2_t vuzpq_u8(uint8x16_t __p0, uint8x16_t __p1) {
  uint8x16x2_t __ret;
  __builtin_neon_vuzpq_v(&__ret, (int8x16_t)__p0, (int8x16_t)__p1, 48);
  return __ret;
}
#else
__ai uint8x16x2_t vuzpq_u8(uint8x16_t __p0, uint8x16_t __p1) {
  uint8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16x2_t __ret;
  __builtin_neon_vuzpq_v(&__ret, (int8x16_t)__rev0, (int8x16_t)__rev1, 48);

  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4x2_t vuzpq_u32(uint32x4_t __p0, uint32x4_t __p1) {
  uint32x4x2_t __ret;
  __builtin_neon_vuzpq_v(&__ret, (int8x16_t)__p0, (int8x16_t)__p1, 50);
  return __ret;
}
#else
__ai uint32x4x2_t vuzpq_u32(uint32x4_t __p0, uint32x4_t __p1) {
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint32x4x2_t __ret;
  __builtin_neon_vuzpq_v(&__ret, (int8x16_t)__rev0, (int8x16_t)__rev1, 50);

  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 3, 2, 1, 0);
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x8x2_t vuzpq_u16(uint16x8_t __p0, uint16x8_t __p1) {
  uint16x8x2_t __ret;
  __builtin_neon_vuzpq_v(&__ret, (int8x16_t)__p0, (int8x16_t)__p1, 49);
  return __ret;
}
#else
__ai uint16x8x2_t vuzpq_u16(uint16x8_t __p0, uint16x8_t __p1) {
  uint16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8x2_t __ret;
  __builtin_neon_vuzpq_v(&__ret, (int8x16_t)__rev0, (int8x16_t)__rev1, 49);

  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 7, 6, 5, 4, 3, 2, 1, 0);
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x16x2_t vuzpq_s8(int8x16_t __p0, int8x16_t __p1) {
  int8x16x2_t __ret;
  __builtin_neon_vuzpq_v(&__ret, (int8x16_t)__p0, (int8x16_t)__p1, 32);
  return __ret;
}
#else
__ai int8x16x2_t vuzpq_s8(int8x16_t __p0, int8x16_t __p1) {
  int8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16x2_t __ret;
  __builtin_neon_vuzpq_v(&__ret, (int8x16_t)__rev0, (int8x16_t)__rev1, 32);

  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x4x2_t vuzpq_f32(float32x4_t __p0, float32x4_t __p1) {
  float32x4x2_t __ret;
  __builtin_neon_vuzpq_v(&__ret, (int8x16_t)__p0, (int8x16_t)__p1, 41);
  return __ret;
}
#else
__ai float32x4x2_t vuzpq_f32(float32x4_t __p0, float32x4_t __p1) {
  float32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  float32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  float32x4x2_t __ret;
  __builtin_neon_vuzpq_v(&__ret, (int8x16_t)__rev0, (int8x16_t)__rev1, 41);

  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 3, 2, 1, 0);
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4x2_t vuzpq_s32(int32x4_t __p0, int32x4_t __p1) {
  int32x4x2_t __ret;
  __builtin_neon_vuzpq_v(&__ret, (int8x16_t)__p0, (int8x16_t)__p1, 34);
  return __ret;
}
#else
__ai int32x4x2_t vuzpq_s32(int32x4_t __p0, int32x4_t __p1) {
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int32x4x2_t __ret;
  __builtin_neon_vuzpq_v(&__ret, (int8x16_t)__rev0, (int8x16_t)__rev1, 34);

  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 3, 2, 1, 0);
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x8x2_t vuzpq_s16(int16x8_t __p0, int16x8_t __p1) {
  int16x8x2_t __ret;
  __builtin_neon_vuzpq_v(&__ret, (int8x16_t)__p0, (int8x16_t)__p1, 33);
  return __ret;
}
#else
__ai int16x8x2_t vuzpq_s16(int16x8_t __p0, int16x8_t __p1) {
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8x2_t __ret;
  __builtin_neon_vuzpq_v(&__ret, (int8x16_t)__rev0, (int8x16_t)__rev1, 33);

  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 7, 6, 5, 4, 3, 2, 1, 0);
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x8x2_t vuzp_u8(uint8x8_t __p0, uint8x8_t __p1) {
  uint8x8x2_t __ret;
  __builtin_neon_vuzp_v(&__ret, (int8x8_t)__p0, (int8x8_t)__p1, 16);
  return __ret;
}
#else
__ai uint8x8x2_t vuzp_u8(uint8x8_t __p0, uint8x8_t __p1) {
  uint8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8x2_t __ret;
  __builtin_neon_vuzp_v(&__ret, (int8x8_t)__rev0, (int8x8_t)__rev1, 16);

  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 7, 6, 5, 4, 3, 2, 1, 0);
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x2x2_t vuzp_u32(uint32x2_t __p0, uint32x2_t __p1) {
  uint32x2x2_t __ret;
  __builtin_neon_vuzp_v(&__ret, (int8x8_t)__p0, (int8x8_t)__p1, 18);
  return __ret;
}
#else
__ai uint32x2x2_t vuzp_u32(uint32x2_t __p0, uint32x2_t __p1) {
  uint32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  uint32x2x2_t __ret;
  __builtin_neon_vuzp_v(&__ret, (int8x8_t)__rev0, (int8x8_t)__rev1, 18);

  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 1, 0);
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x4x2_t vuzp_u16(uint16x4_t __p0, uint16x4_t __p1) {
  uint16x4x2_t __ret;
  __builtin_neon_vuzp_v(&__ret, (int8x8_t)__p0, (int8x8_t)__p1, 17);
  return __ret;
}
#else
__ai uint16x4x2_t vuzp_u16(uint16x4_t __p0, uint16x4_t __p1) {
  uint16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint16x4x2_t __ret;
  __builtin_neon_vuzp_v(&__ret, (int8x8_t)__rev0, (int8x8_t)__rev1, 17);

  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 3, 2, 1, 0);
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x8x2_t vuzp_s8(int8x8_t __p0, int8x8_t __p1) {
  int8x8x2_t __ret;
  __builtin_neon_vuzp_v(&__ret, (int8x8_t)__p0, (int8x8_t)__p1, 0);
  return __ret;
}
#else
__ai int8x8x2_t vuzp_s8(int8x8_t __p0, int8x8_t __p1) {
  int8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8x2_t __ret;
  __builtin_neon_vuzp_v(&__ret, (int8x8_t)__rev0, (int8x8_t)__rev1, 0);

  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 7, 6, 5, 4, 3, 2, 1, 0);
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x2x2_t vuzp_f32(float32x2_t __p0, float32x2_t __p1) {
  float32x2x2_t __ret;
  __builtin_neon_vuzp_v(&__ret, (int8x8_t)__p0, (int8x8_t)__p1, 9);
  return __ret;
}
#else
__ai float32x2x2_t vuzp_f32(float32x2_t __p0, float32x2_t __p1) {
  float32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  float32x2x2_t __ret;
  __builtin_neon_vuzp_v(&__ret, (int8x8_t)__rev0, (int8x8_t)__rev1, 9);

  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 1, 0);
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x2x2_t vuzp_s32(int32x2_t __p0, int32x2_t __p1) {
  int32x2x2_t __ret;
  __builtin_neon_vuzp_v(&__ret, (int8x8_t)__p0, (int8x8_t)__p1, 2);
  return __ret;
}
#else
__ai int32x2x2_t vuzp_s32(int32x2_t __p0, int32x2_t __p1) {
  int32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  int32x2x2_t __ret;
  __builtin_neon_vuzp_v(&__ret, (int8x8_t)__rev0, (int8x8_t)__rev1, 2);

  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 1, 0);
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x4x2_t vuzp_s16(int16x4_t __p0, int16x4_t __p1) {
  int16x4x2_t __ret;
  __builtin_neon_vuzp_v(&__ret, (int8x8_t)__p0, (int8x8_t)__p1, 1);
  return __ret;
}
#else
__ai int16x4x2_t vuzp_s16(int16x4_t __p0, int16x4_t __p1) {
  int16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int16x4x2_t __ret;
  __builtin_neon_vuzp_v(&__ret, (int8x8_t)__rev0, (int8x8_t)__rev1, 1);

  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 3, 2, 1, 0);
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x8x2_t vzip_p8(poly8x8_t __p0, poly8x8_t __p1) {
  poly8x8x2_t __ret;
  __builtin_neon_vzip_v(&__ret, (int8x8_t)__p0, (int8x8_t)__p1, 4);
  return __ret;
}
#else
__ai poly8x8x2_t vzip_p8(poly8x8_t __p0, poly8x8_t __p1) {
  poly8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  poly8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  poly8x8x2_t __ret;
  __builtin_neon_vzip_v(&__ret, (int8x8_t)__rev0, (int8x8_t)__rev1, 4);

  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 7, 6, 5, 4, 3, 2, 1, 0);
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly16x4x2_t vzip_p16(poly16x4_t __p0, poly16x4_t __p1) {
  poly16x4x2_t __ret;
  __builtin_neon_vzip_v(&__ret, (int8x8_t)__p0, (int8x8_t)__p1, 5);
  return __ret;
}
#else
__ai poly16x4x2_t vzip_p16(poly16x4_t __p0, poly16x4_t __p1) {
  poly16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  poly16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  poly16x4x2_t __ret;
  __builtin_neon_vzip_v(&__ret, (int8x8_t)__rev0, (int8x8_t)__rev1, 5);

  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 3, 2, 1, 0);
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x16x2_t vzipq_p8(poly8x16_t __p0, poly8x16_t __p1) {
  poly8x16x2_t __ret;
  __builtin_neon_vzipq_v(&__ret, (int8x16_t)__p0, (int8x16_t)__p1, 36);
  return __ret;
}
#else
__ai poly8x16x2_t vzipq_p8(poly8x16_t __p0, poly8x16_t __p1) {
  poly8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  poly8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  poly8x16x2_t __ret;
  __builtin_neon_vzipq_v(&__ret, (int8x16_t)__rev0, (int8x16_t)__rev1, 36);

  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly16x8x2_t vzipq_p16(poly16x8_t __p0, poly16x8_t __p1) {
  poly16x8x2_t __ret;
  __builtin_neon_vzipq_v(&__ret, (int8x16_t)__p0, (int8x16_t)__p1, 37);
  return __ret;
}
#else
__ai poly16x8x2_t vzipq_p16(poly16x8_t __p0, poly16x8_t __p1) {
  poly16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  poly16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  poly16x8x2_t __ret;
  __builtin_neon_vzipq_v(&__ret, (int8x16_t)__rev0, (int8x16_t)__rev1, 37);

  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 7, 6, 5, 4, 3, 2, 1, 0);
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x16x2_t vzipq_u8(uint8x16_t __p0, uint8x16_t __p1) {
  uint8x16x2_t __ret;
  __builtin_neon_vzipq_v(&__ret, (int8x16_t)__p0, (int8x16_t)__p1, 48);
  return __ret;
}
#else
__ai uint8x16x2_t vzipq_u8(uint8x16_t __p0, uint8x16_t __p1) {
  uint8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16x2_t __ret;
  __builtin_neon_vzipq_v(&__ret, (int8x16_t)__rev0, (int8x16_t)__rev1, 48);

  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4x2_t vzipq_u32(uint32x4_t __p0, uint32x4_t __p1) {
  uint32x4x2_t __ret;
  __builtin_neon_vzipq_v(&__ret, (int8x16_t)__p0, (int8x16_t)__p1, 50);
  return __ret;
}
#else
__ai uint32x4x2_t vzipq_u32(uint32x4_t __p0, uint32x4_t __p1) {
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint32x4x2_t __ret;
  __builtin_neon_vzipq_v(&__ret, (int8x16_t)__rev0, (int8x16_t)__rev1, 50);

  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 3, 2, 1, 0);
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x8x2_t vzipq_u16(uint16x8_t __p0, uint16x8_t __p1) {
  uint16x8x2_t __ret;
  __builtin_neon_vzipq_v(&__ret, (int8x16_t)__p0, (int8x16_t)__p1, 49);
  return __ret;
}
#else
__ai uint16x8x2_t vzipq_u16(uint16x8_t __p0, uint16x8_t __p1) {
  uint16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8x2_t __ret;
  __builtin_neon_vzipq_v(&__ret, (int8x16_t)__rev0, (int8x16_t)__rev1, 49);

  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 7, 6, 5, 4, 3, 2, 1, 0);
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x16x2_t vzipq_s8(int8x16_t __p0, int8x16_t __p1) {
  int8x16x2_t __ret;
  __builtin_neon_vzipq_v(&__ret, (int8x16_t)__p0, (int8x16_t)__p1, 32);
  return __ret;
}
#else
__ai int8x16x2_t vzipq_s8(int8x16_t __p0, int8x16_t __p1) {
  int8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16x2_t __ret;
  __builtin_neon_vzipq_v(&__ret, (int8x16_t)__rev0, (int8x16_t)__rev1, 32);

  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x4x2_t vzipq_f32(float32x4_t __p0, float32x4_t __p1) {
  float32x4x2_t __ret;
  __builtin_neon_vzipq_v(&__ret, (int8x16_t)__p0, (int8x16_t)__p1, 41);
  return __ret;
}
#else
__ai float32x4x2_t vzipq_f32(float32x4_t __p0, float32x4_t __p1) {
  float32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  float32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  float32x4x2_t __ret;
  __builtin_neon_vzipq_v(&__ret, (int8x16_t)__rev0, (int8x16_t)__rev1, 41);

  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 3, 2, 1, 0);
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4x2_t vzipq_s32(int32x4_t __p0, int32x4_t __p1) {
  int32x4x2_t __ret;
  __builtin_neon_vzipq_v(&__ret, (int8x16_t)__p0, (int8x16_t)__p1, 34);
  return __ret;
}
#else
__ai int32x4x2_t vzipq_s32(int32x4_t __p0, int32x4_t __p1) {
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int32x4x2_t __ret;
  __builtin_neon_vzipq_v(&__ret, (int8x16_t)__rev0, (int8x16_t)__rev1, 34);

  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 3, 2, 1, 0);
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x8x2_t vzipq_s16(int16x8_t __p0, int16x8_t __p1) {
  int16x8x2_t __ret;
  __builtin_neon_vzipq_v(&__ret, (int8x16_t)__p0, (int8x16_t)__p1, 33);
  return __ret;
}
#else
__ai int16x8x2_t vzipq_s16(int16x8_t __p0, int16x8_t __p1) {
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8x2_t __ret;
  __builtin_neon_vzipq_v(&__ret, (int8x16_t)__rev0, (int8x16_t)__rev1, 33);

  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 7, 6, 5, 4, 3, 2, 1, 0);
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x8x2_t vzip_u8(uint8x8_t __p0, uint8x8_t __p1) {
  uint8x8x2_t __ret;
  __builtin_neon_vzip_v(&__ret, (int8x8_t)__p0, (int8x8_t)__p1, 16);
  return __ret;
}
#else
__ai uint8x8x2_t vzip_u8(uint8x8_t __p0, uint8x8_t __p1) {
  uint8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8x2_t __ret;
  __builtin_neon_vzip_v(&__ret, (int8x8_t)__rev0, (int8x8_t)__rev1, 16);

  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 7, 6, 5, 4, 3, 2, 1, 0);
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x2x2_t vzip_u32(uint32x2_t __p0, uint32x2_t __p1) {
  uint32x2x2_t __ret;
  __builtin_neon_vzip_v(&__ret, (int8x8_t)__p0, (int8x8_t)__p1, 18);
  return __ret;
}
#else
__ai uint32x2x2_t vzip_u32(uint32x2_t __p0, uint32x2_t __p1) {
  uint32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  uint32x2x2_t __ret;
  __builtin_neon_vzip_v(&__ret, (int8x8_t)__rev0, (int8x8_t)__rev1, 18);

  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 1, 0);
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x4x2_t vzip_u16(uint16x4_t __p0, uint16x4_t __p1) {
  uint16x4x2_t __ret;
  __builtin_neon_vzip_v(&__ret, (int8x8_t)__p0, (int8x8_t)__p1, 17);
  return __ret;
}
#else
__ai uint16x4x2_t vzip_u16(uint16x4_t __p0, uint16x4_t __p1) {
  uint16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint16x4x2_t __ret;
  __builtin_neon_vzip_v(&__ret, (int8x8_t)__rev0, (int8x8_t)__rev1, 17);

  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 3, 2, 1, 0);
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x8x2_t vzip_s8(int8x8_t __p0, int8x8_t __p1) {
  int8x8x2_t __ret;
  __builtin_neon_vzip_v(&__ret, (int8x8_t)__p0, (int8x8_t)__p1, 0);
  return __ret;
}
#else
__ai int8x8x2_t vzip_s8(int8x8_t __p0, int8x8_t __p1) {
  int8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x8x2_t __ret;
  __builtin_neon_vzip_v(&__ret, (int8x8_t)__rev0, (int8x8_t)__rev1, 0);

  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 7, 6, 5, 4, 3, 2, 1, 0);
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x2x2_t vzip_f32(float32x2_t __p0, float32x2_t __p1) {
  float32x2x2_t __ret;
  __builtin_neon_vzip_v(&__ret, (int8x8_t)__p0, (int8x8_t)__p1, 9);
  return __ret;
}
#else
__ai float32x2x2_t vzip_f32(float32x2_t __p0, float32x2_t __p1) {
  float32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  float32x2x2_t __ret;
  __builtin_neon_vzip_v(&__ret, (int8x8_t)__rev0, (int8x8_t)__rev1, 9);

  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 1, 0);
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x2x2_t vzip_s32(int32x2_t __p0, int32x2_t __p1) {
  int32x2x2_t __ret;
  __builtin_neon_vzip_v(&__ret, (int8x8_t)__p0, (int8x8_t)__p1, 2);
  return __ret;
}
#else
__ai int32x2x2_t vzip_s32(int32x2_t __p0, int32x2_t __p1) {
  int32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  int32x2x2_t __ret;
  __builtin_neon_vzip_v(&__ret, (int8x8_t)__rev0, (int8x8_t)__rev1, 2);

  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 1, 0);
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x4x2_t vzip_s16(int16x4_t __p0, int16x4_t __p1) {
  int16x4x2_t __ret;
  __builtin_neon_vzip_v(&__ret, (int8x8_t)__p0, (int8x8_t)__p1, 1);
  return __ret;
}
#else
__ai int16x4x2_t vzip_s16(int16x4_t __p0, int16x4_t __p1) {
  int16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int16x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int16x4x2_t __ret;
  __builtin_neon_vzip_v(&__ret, (int8x8_t)__rev0, (int8x8_t)__rev1, 1);

  __ret.val[0] = __builtin_shufflevector(__ret.val[0], __ret.val[0], 3, 2, 1, 0);
  __ret.val[1] = __builtin_shufflevector(__ret.val[1], __ret.val[1], 3, 2, 1, 0);
  return __ret;
}
#endif

#if !defined(__aarch64__)
#ifdef __LITTLE_ENDIAN__
__ai poly8x8_t vreinterpret_p8_p16(poly16x4_t __p0) {
  poly8x8_t __ret;
  __ret = (poly8x8_t)(__p0);
  return __ret;
}
#else
__ai poly8x8_t vreinterpret_p8_p16(poly16x4_t __p0) {
  poly8x8_t __ret;
  __ret = (poly8x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x8_t vreinterpret_p8_u8(uint8x8_t __p0) {
  poly8x8_t __ret;
  __ret = (poly8x8_t)(__p0);
  return __ret;
}
#else
__ai poly8x8_t vreinterpret_p8_u8(uint8x8_t __p0) {
  poly8x8_t __ret;
  __ret = (poly8x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x8_t vreinterpret_p8_u32(uint32x2_t __p0) {
  poly8x8_t __ret;
  __ret = (poly8x8_t)(__p0);
  return __ret;
}
#else
__ai poly8x8_t vreinterpret_p8_u32(uint32x2_t __p0) {
  poly8x8_t __ret;
  __ret = (poly8x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x8_t vreinterpret_p8_u64(uint64x1_t __p0) {
  poly8x8_t __ret;
  __ret = (poly8x8_t)(__p0);
  return __ret;
}
#else
__ai poly8x8_t vreinterpret_p8_u64(uint64x1_t __p0) {
  poly8x8_t __ret;
  __ret = (poly8x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x8_t vreinterpret_p8_u16(uint16x4_t __p0) {
  poly8x8_t __ret;
  __ret = (poly8x8_t)(__p0);
  return __ret;
}
#else
__ai poly8x8_t vreinterpret_p8_u16(uint16x4_t __p0) {
  poly8x8_t __ret;
  __ret = (poly8x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x8_t vreinterpret_p8_s8(int8x8_t __p0) {
  poly8x8_t __ret;
  __ret = (poly8x8_t)(__p0);
  return __ret;
}
#else
__ai poly8x8_t vreinterpret_p8_s8(int8x8_t __p0) {
  poly8x8_t __ret;
  __ret = (poly8x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x8_t vreinterpret_p8_f32(float32x2_t __p0) {
  poly8x8_t __ret;
  __ret = (poly8x8_t)(__p0);
  return __ret;
}
#else
__ai poly8x8_t vreinterpret_p8_f32(float32x2_t __p0) {
  poly8x8_t __ret;
  __ret = (poly8x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x8_t vreinterpret_p8_f16(float16x4_t __p0) {
  poly8x8_t __ret;
  __ret = (poly8x8_t)(__p0);
  return __ret;
}
#else
__ai poly8x8_t vreinterpret_p8_f16(float16x4_t __p0) {
  poly8x8_t __ret;
  __ret = (poly8x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x8_t vreinterpret_p8_s32(int32x2_t __p0) {
  poly8x8_t __ret;
  __ret = (poly8x8_t)(__p0);
  return __ret;
}
#else
__ai poly8x8_t vreinterpret_p8_s32(int32x2_t __p0) {
  poly8x8_t __ret;
  __ret = (poly8x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x8_t vreinterpret_p8_s64(int64x1_t __p0) {
  poly8x8_t __ret;
  __ret = (poly8x8_t)(__p0);
  return __ret;
}
#else
__ai poly8x8_t vreinterpret_p8_s64(int64x1_t __p0) {
  poly8x8_t __ret;
  __ret = (poly8x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x8_t vreinterpret_p8_s16(int16x4_t __p0) {
  poly8x8_t __ret;
  __ret = (poly8x8_t)(__p0);
  return __ret;
}
#else
__ai poly8x8_t vreinterpret_p8_s16(int16x4_t __p0) {
  poly8x8_t __ret;
  __ret = (poly8x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly16x4_t vreinterpret_p16_p8(poly8x8_t __p0) {
  poly16x4_t __ret;
  __ret = (poly16x4_t)(__p0);
  return __ret;
}
#else
__ai poly16x4_t vreinterpret_p16_p8(poly8x8_t __p0) {
  poly16x4_t __ret;
  __ret = (poly16x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly16x4_t vreinterpret_p16_u8(uint8x8_t __p0) {
  poly16x4_t __ret;
  __ret = (poly16x4_t)(__p0);
  return __ret;
}
#else
__ai poly16x4_t vreinterpret_p16_u8(uint8x8_t __p0) {
  poly16x4_t __ret;
  __ret = (poly16x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly16x4_t vreinterpret_p16_u32(uint32x2_t __p0) {
  poly16x4_t __ret;
  __ret = (poly16x4_t)(__p0);
  return __ret;
}
#else
__ai poly16x4_t vreinterpret_p16_u32(uint32x2_t __p0) {
  poly16x4_t __ret;
  __ret = (poly16x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly16x4_t vreinterpret_p16_u64(uint64x1_t __p0) {
  poly16x4_t __ret;
  __ret = (poly16x4_t)(__p0);
  return __ret;
}
#else
__ai poly16x4_t vreinterpret_p16_u64(uint64x1_t __p0) {
  poly16x4_t __ret;
  __ret = (poly16x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly16x4_t vreinterpret_p16_u16(uint16x4_t __p0) {
  poly16x4_t __ret;
  __ret = (poly16x4_t)(__p0);
  return __ret;
}
#else
__ai poly16x4_t vreinterpret_p16_u16(uint16x4_t __p0) {
  poly16x4_t __ret;
  __ret = (poly16x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly16x4_t vreinterpret_p16_s8(int8x8_t __p0) {
  poly16x4_t __ret;
  __ret = (poly16x4_t)(__p0);
  return __ret;
}
#else
__ai poly16x4_t vreinterpret_p16_s8(int8x8_t __p0) {
  poly16x4_t __ret;
  __ret = (poly16x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly16x4_t vreinterpret_p16_f32(float32x2_t __p0) {
  poly16x4_t __ret;
  __ret = (poly16x4_t)(__p0);
  return __ret;
}
#else
__ai poly16x4_t vreinterpret_p16_f32(float32x2_t __p0) {
  poly16x4_t __ret;
  __ret = (poly16x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly16x4_t vreinterpret_p16_f16(float16x4_t __p0) {
  poly16x4_t __ret;
  __ret = (poly16x4_t)(__p0);
  return __ret;
}
#else
__ai poly16x4_t vreinterpret_p16_f16(float16x4_t __p0) {
  poly16x4_t __ret;
  __ret = (poly16x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly16x4_t vreinterpret_p16_s32(int32x2_t __p0) {
  poly16x4_t __ret;
  __ret = (poly16x4_t)(__p0);
  return __ret;
}
#else
__ai poly16x4_t vreinterpret_p16_s32(int32x2_t __p0) {
  poly16x4_t __ret;
  __ret = (poly16x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly16x4_t vreinterpret_p16_s64(int64x1_t __p0) {
  poly16x4_t __ret;
  __ret = (poly16x4_t)(__p0);
  return __ret;
}
#else
__ai poly16x4_t vreinterpret_p16_s64(int64x1_t __p0) {
  poly16x4_t __ret;
  __ret = (poly16x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly16x4_t vreinterpret_p16_s16(int16x4_t __p0) {
  poly16x4_t __ret;
  __ret = (poly16x4_t)(__p0);
  return __ret;
}
#else
__ai poly16x4_t vreinterpret_p16_s16(int16x4_t __p0) {
  poly16x4_t __ret;
  __ret = (poly16x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x16_t vreinterpretq_p8_p16(poly16x8_t __p0) {
  poly8x16_t __ret;
  __ret = (poly8x16_t)(__p0);
  return __ret;
}
#else
__ai poly8x16_t vreinterpretq_p8_p16(poly16x8_t __p0) {
  poly8x16_t __ret;
  __ret = (poly8x16_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x16_t vreinterpretq_p8_u8(uint8x16_t __p0) {
  poly8x16_t __ret;
  __ret = (poly8x16_t)(__p0);
  return __ret;
}
#else
__ai poly8x16_t vreinterpretq_p8_u8(uint8x16_t __p0) {
  poly8x16_t __ret;
  __ret = (poly8x16_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x16_t vreinterpretq_p8_u32(uint32x4_t __p0) {
  poly8x16_t __ret;
  __ret = (poly8x16_t)(__p0);
  return __ret;
}
#else
__ai poly8x16_t vreinterpretq_p8_u32(uint32x4_t __p0) {
  poly8x16_t __ret;
  __ret = (poly8x16_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x16_t vreinterpretq_p8_u64(uint64x2_t __p0) {
  poly8x16_t __ret;
  __ret = (poly8x16_t)(__p0);
  return __ret;
}
#else
__ai poly8x16_t vreinterpretq_p8_u64(uint64x2_t __p0) {
  poly8x16_t __ret;
  __ret = (poly8x16_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x16_t vreinterpretq_p8_u16(uint16x8_t __p0) {
  poly8x16_t __ret;
  __ret = (poly8x16_t)(__p0);
  return __ret;
}
#else
__ai poly8x16_t vreinterpretq_p8_u16(uint16x8_t __p0) {
  poly8x16_t __ret;
  __ret = (poly8x16_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x16_t vreinterpretq_p8_s8(int8x16_t __p0) {
  poly8x16_t __ret;
  __ret = (poly8x16_t)(__p0);
  return __ret;
}
#else
__ai poly8x16_t vreinterpretq_p8_s8(int8x16_t __p0) {
  poly8x16_t __ret;
  __ret = (poly8x16_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x16_t vreinterpretq_p8_f32(float32x4_t __p0) {
  poly8x16_t __ret;
  __ret = (poly8x16_t)(__p0);
  return __ret;
}
#else
__ai poly8x16_t vreinterpretq_p8_f32(float32x4_t __p0) {
  poly8x16_t __ret;
  __ret = (poly8x16_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x16_t vreinterpretq_p8_f16(float16x8_t __p0) {
  poly8x16_t __ret;
  __ret = (poly8x16_t)(__p0);
  return __ret;
}
#else
__ai poly8x16_t vreinterpretq_p8_f16(float16x8_t __p0) {
  poly8x16_t __ret;
  __ret = (poly8x16_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x16_t vreinterpretq_p8_s32(int32x4_t __p0) {
  poly8x16_t __ret;
  __ret = (poly8x16_t)(__p0);
  return __ret;
}
#else
__ai poly8x16_t vreinterpretq_p8_s32(int32x4_t __p0) {
  poly8x16_t __ret;
  __ret = (poly8x16_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x16_t vreinterpretq_p8_s64(int64x2_t __p0) {
  poly8x16_t __ret;
  __ret = (poly8x16_t)(__p0);
  return __ret;
}
#else
__ai poly8x16_t vreinterpretq_p8_s64(int64x2_t __p0) {
  poly8x16_t __ret;
  __ret = (poly8x16_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x16_t vreinterpretq_p8_s16(int16x8_t __p0) {
  poly8x16_t __ret;
  __ret = (poly8x16_t)(__p0);
  return __ret;
}
#else
__ai poly8x16_t vreinterpretq_p8_s16(int16x8_t __p0) {
  poly8x16_t __ret;
  __ret = (poly8x16_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly16x8_t vreinterpretq_p16_p8(poly8x16_t __p0) {
  poly16x8_t __ret;
  __ret = (poly16x8_t)(__p0);
  return __ret;
}
#else
__ai poly16x8_t vreinterpretq_p16_p8(poly8x16_t __p0) {
  poly16x8_t __ret;
  __ret = (poly16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly16x8_t vreinterpretq_p16_u8(uint8x16_t __p0) {
  poly16x8_t __ret;
  __ret = (poly16x8_t)(__p0);
  return __ret;
}
#else
__ai poly16x8_t vreinterpretq_p16_u8(uint8x16_t __p0) {
  poly16x8_t __ret;
  __ret = (poly16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly16x8_t vreinterpretq_p16_u32(uint32x4_t __p0) {
  poly16x8_t __ret;
  __ret = (poly16x8_t)(__p0);
  return __ret;
}
#else
__ai poly16x8_t vreinterpretq_p16_u32(uint32x4_t __p0) {
  poly16x8_t __ret;
  __ret = (poly16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly16x8_t vreinterpretq_p16_u64(uint64x2_t __p0) {
  poly16x8_t __ret;
  __ret = (poly16x8_t)(__p0);
  return __ret;
}
#else
__ai poly16x8_t vreinterpretq_p16_u64(uint64x2_t __p0) {
  poly16x8_t __ret;
  __ret = (poly16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly16x8_t vreinterpretq_p16_u16(uint16x8_t __p0) {
  poly16x8_t __ret;
  __ret = (poly16x8_t)(__p0);
  return __ret;
}
#else
__ai poly16x8_t vreinterpretq_p16_u16(uint16x8_t __p0) {
  poly16x8_t __ret;
  __ret = (poly16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly16x8_t vreinterpretq_p16_s8(int8x16_t __p0) {
  poly16x8_t __ret;
  __ret = (poly16x8_t)(__p0);
  return __ret;
}
#else
__ai poly16x8_t vreinterpretq_p16_s8(int8x16_t __p0) {
  poly16x8_t __ret;
  __ret = (poly16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly16x8_t vreinterpretq_p16_f32(float32x4_t __p0) {
  poly16x8_t __ret;
  __ret = (poly16x8_t)(__p0);
  return __ret;
}
#else
__ai poly16x8_t vreinterpretq_p16_f32(float32x4_t __p0) {
  poly16x8_t __ret;
  __ret = (poly16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly16x8_t vreinterpretq_p16_f16(float16x8_t __p0) {
  poly16x8_t __ret;
  __ret = (poly16x8_t)(__p0);
  return __ret;
}
#else
__ai poly16x8_t vreinterpretq_p16_f16(float16x8_t __p0) {
  poly16x8_t __ret;
  __ret = (poly16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly16x8_t vreinterpretq_p16_s32(int32x4_t __p0) {
  poly16x8_t __ret;
  __ret = (poly16x8_t)(__p0);
  return __ret;
}
#else
__ai poly16x8_t vreinterpretq_p16_s32(int32x4_t __p0) {
  poly16x8_t __ret;
  __ret = (poly16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly16x8_t vreinterpretq_p16_s64(int64x2_t __p0) {
  poly16x8_t __ret;
  __ret = (poly16x8_t)(__p0);
  return __ret;
}
#else
__ai poly16x8_t vreinterpretq_p16_s64(int64x2_t __p0) {
  poly16x8_t __ret;
  __ret = (poly16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly16x8_t vreinterpretq_p16_s16(int16x8_t __p0) {
  poly16x8_t __ret;
  __ret = (poly16x8_t)(__p0);
  return __ret;
}
#else
__ai poly16x8_t vreinterpretq_p16_s16(int16x8_t __p0) {
  poly16x8_t __ret;
  __ret = (poly16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x16_t vreinterpretq_u8_p8(poly8x16_t __p0) {
  uint8x16_t __ret;
  __ret = (uint8x16_t)(__p0);
  return __ret;
}
#else
__ai uint8x16_t vreinterpretq_u8_p8(poly8x16_t __p0) {
  uint8x16_t __ret;
  __ret = (uint8x16_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x16_t vreinterpretq_u8_p16(poly16x8_t __p0) {
  uint8x16_t __ret;
  __ret = (uint8x16_t)(__p0);
  return __ret;
}
#else
__ai uint8x16_t vreinterpretq_u8_p16(poly16x8_t __p0) {
  uint8x16_t __ret;
  __ret = (uint8x16_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x16_t vreinterpretq_u8_u32(uint32x4_t __p0) {
  uint8x16_t __ret;
  __ret = (uint8x16_t)(__p0);
  return __ret;
}
#else
__ai uint8x16_t vreinterpretq_u8_u32(uint32x4_t __p0) {
  uint8x16_t __ret;
  __ret = (uint8x16_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x16_t vreinterpretq_u8_u64(uint64x2_t __p0) {
  uint8x16_t __ret;
  __ret = (uint8x16_t)(__p0);
  return __ret;
}
#else
__ai uint8x16_t vreinterpretq_u8_u64(uint64x2_t __p0) {
  uint8x16_t __ret;
  __ret = (uint8x16_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x16_t vreinterpretq_u8_u16(uint16x8_t __p0) {
  uint8x16_t __ret;
  __ret = (uint8x16_t)(__p0);
  return __ret;
}
#else
__ai uint8x16_t vreinterpretq_u8_u16(uint16x8_t __p0) {
  uint8x16_t __ret;
  __ret = (uint8x16_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x16_t vreinterpretq_u8_s8(int8x16_t __p0) {
  uint8x16_t __ret;
  __ret = (uint8x16_t)(__p0);
  return __ret;
}
#else
__ai uint8x16_t vreinterpretq_u8_s8(int8x16_t __p0) {
  uint8x16_t __ret;
  __ret = (uint8x16_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x16_t vreinterpretq_u8_f32(float32x4_t __p0) {
  uint8x16_t __ret;
  __ret = (uint8x16_t)(__p0);
  return __ret;
}
#else
__ai uint8x16_t vreinterpretq_u8_f32(float32x4_t __p0) {
  uint8x16_t __ret;
  __ret = (uint8x16_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x16_t vreinterpretq_u8_f16(float16x8_t __p0) {
  uint8x16_t __ret;
  __ret = (uint8x16_t)(__p0);
  return __ret;
}
#else
__ai uint8x16_t vreinterpretq_u8_f16(float16x8_t __p0) {
  uint8x16_t __ret;
  __ret = (uint8x16_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x16_t vreinterpretq_u8_s32(int32x4_t __p0) {
  uint8x16_t __ret;
  __ret = (uint8x16_t)(__p0);
  return __ret;
}
#else
__ai uint8x16_t vreinterpretq_u8_s32(int32x4_t __p0) {
  uint8x16_t __ret;
  __ret = (uint8x16_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x16_t vreinterpretq_u8_s64(int64x2_t __p0) {
  uint8x16_t __ret;
  __ret = (uint8x16_t)(__p0);
  return __ret;
}
#else
__ai uint8x16_t vreinterpretq_u8_s64(int64x2_t __p0) {
  uint8x16_t __ret;
  __ret = (uint8x16_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x16_t vreinterpretq_u8_s16(int16x8_t __p0) {
  uint8x16_t __ret;
  __ret = (uint8x16_t)(__p0);
  return __ret;
}
#else
__ai uint8x16_t vreinterpretq_u8_s16(int16x8_t __p0) {
  uint8x16_t __ret;
  __ret = (uint8x16_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vreinterpretq_u32_p8(poly8x16_t __p0) {
  uint32x4_t __ret;
  __ret = (uint32x4_t)(__p0);
  return __ret;
}
#else
__ai uint32x4_t vreinterpretq_u32_p8(poly8x16_t __p0) {
  uint32x4_t __ret;
  __ret = (uint32x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vreinterpretq_u32_p16(poly16x8_t __p0) {
  uint32x4_t __ret;
  __ret = (uint32x4_t)(__p0);
  return __ret;
}
#else
__ai uint32x4_t vreinterpretq_u32_p16(poly16x8_t __p0) {
  uint32x4_t __ret;
  __ret = (uint32x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vreinterpretq_u32_u8(uint8x16_t __p0) {
  uint32x4_t __ret;
  __ret = (uint32x4_t)(__p0);
  return __ret;
}
#else
__ai uint32x4_t vreinterpretq_u32_u8(uint8x16_t __p0) {
  uint32x4_t __ret;
  __ret = (uint32x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vreinterpretq_u32_u64(uint64x2_t __p0) {
  uint32x4_t __ret;
  __ret = (uint32x4_t)(__p0);
  return __ret;
}
#else
__ai uint32x4_t vreinterpretq_u32_u64(uint64x2_t __p0) {
  uint32x4_t __ret;
  __ret = (uint32x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vreinterpretq_u32_u16(uint16x8_t __p0) {
  uint32x4_t __ret;
  __ret = (uint32x4_t)(__p0);
  return __ret;
}
#else
__ai uint32x4_t vreinterpretq_u32_u16(uint16x8_t __p0) {
  uint32x4_t __ret;
  __ret = (uint32x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vreinterpretq_u32_s8(int8x16_t __p0) {
  uint32x4_t __ret;
  __ret = (uint32x4_t)(__p0);
  return __ret;
}
#else
__ai uint32x4_t vreinterpretq_u32_s8(int8x16_t __p0) {
  uint32x4_t __ret;
  __ret = (uint32x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vreinterpretq_u32_f32(float32x4_t __p0) {
  uint32x4_t __ret;
  __ret = (uint32x4_t)(__p0);
  return __ret;
}
#else
__ai uint32x4_t vreinterpretq_u32_f32(float32x4_t __p0) {
  uint32x4_t __ret;
  __ret = (uint32x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vreinterpretq_u32_f16(float16x8_t __p0) {
  uint32x4_t __ret;
  __ret = (uint32x4_t)(__p0);
  return __ret;
}
#else
__ai uint32x4_t vreinterpretq_u32_f16(float16x8_t __p0) {
  uint32x4_t __ret;
  __ret = (uint32x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vreinterpretq_u32_s32(int32x4_t __p0) {
  uint32x4_t __ret;
  __ret = (uint32x4_t)(__p0);
  return __ret;
}
#else
__ai uint32x4_t vreinterpretq_u32_s32(int32x4_t __p0) {
  uint32x4_t __ret;
  __ret = (uint32x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vreinterpretq_u32_s64(int64x2_t __p0) {
  uint32x4_t __ret;
  __ret = (uint32x4_t)(__p0);
  return __ret;
}
#else
__ai uint32x4_t vreinterpretq_u32_s64(int64x2_t __p0) {
  uint32x4_t __ret;
  __ret = (uint32x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vreinterpretq_u32_s16(int16x8_t __p0) {
  uint32x4_t __ret;
  __ret = (uint32x4_t)(__p0);
  return __ret;
}
#else
__ai uint32x4_t vreinterpretq_u32_s16(int16x8_t __p0) {
  uint32x4_t __ret;
  __ret = (uint32x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vreinterpretq_u64_p8(poly8x16_t __p0) {
  uint64x2_t __ret;
  __ret = (uint64x2_t)(__p0);
  return __ret;
}
#else
__ai uint64x2_t vreinterpretq_u64_p8(poly8x16_t __p0) {
  uint64x2_t __ret;
  __ret = (uint64x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vreinterpretq_u64_p16(poly16x8_t __p0) {
  uint64x2_t __ret;
  __ret = (uint64x2_t)(__p0);
  return __ret;
}
#else
__ai uint64x2_t vreinterpretq_u64_p16(poly16x8_t __p0) {
  uint64x2_t __ret;
  __ret = (uint64x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vreinterpretq_u64_u8(uint8x16_t __p0) {
  uint64x2_t __ret;
  __ret = (uint64x2_t)(__p0);
  return __ret;
}
#else
__ai uint64x2_t vreinterpretq_u64_u8(uint8x16_t __p0) {
  uint64x2_t __ret;
  __ret = (uint64x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vreinterpretq_u64_u32(uint32x4_t __p0) {
  uint64x2_t __ret;
  __ret = (uint64x2_t)(__p0);
  return __ret;
}
#else
__ai uint64x2_t vreinterpretq_u64_u32(uint32x4_t __p0) {
  uint64x2_t __ret;
  __ret = (uint64x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vreinterpretq_u64_u16(uint16x8_t __p0) {
  uint64x2_t __ret;
  __ret = (uint64x2_t)(__p0);
  return __ret;
}
#else
__ai uint64x2_t vreinterpretq_u64_u16(uint16x8_t __p0) {
  uint64x2_t __ret;
  __ret = (uint64x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vreinterpretq_u64_s8(int8x16_t __p0) {
  uint64x2_t __ret;
  __ret = (uint64x2_t)(__p0);
  return __ret;
}
#else
__ai uint64x2_t vreinterpretq_u64_s8(int8x16_t __p0) {
  uint64x2_t __ret;
  __ret = (uint64x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vreinterpretq_u64_f32(float32x4_t __p0) {
  uint64x2_t __ret;
  __ret = (uint64x2_t)(__p0);
  return __ret;
}
#else
__ai uint64x2_t vreinterpretq_u64_f32(float32x4_t __p0) {
  uint64x2_t __ret;
  __ret = (uint64x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vreinterpretq_u64_f16(float16x8_t __p0) {
  uint64x2_t __ret;
  __ret = (uint64x2_t)(__p0);
  return __ret;
}
#else
__ai uint64x2_t vreinterpretq_u64_f16(float16x8_t __p0) {
  uint64x2_t __ret;
  __ret = (uint64x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vreinterpretq_u64_s32(int32x4_t __p0) {
  uint64x2_t __ret;
  __ret = (uint64x2_t)(__p0);
  return __ret;
}
#else
__ai uint64x2_t vreinterpretq_u64_s32(int32x4_t __p0) {
  uint64x2_t __ret;
  __ret = (uint64x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vreinterpretq_u64_s64(int64x2_t __p0) {
  uint64x2_t __ret;
  __ret = (uint64x2_t)(__p0);
  return __ret;
}
#else
__ai uint64x2_t vreinterpretq_u64_s64(int64x2_t __p0) {
  uint64x2_t __ret;
  __ret = (uint64x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vreinterpretq_u64_s16(int16x8_t __p0) {
  uint64x2_t __ret;
  __ret = (uint64x2_t)(__p0);
  return __ret;
}
#else
__ai uint64x2_t vreinterpretq_u64_s16(int16x8_t __p0) {
  uint64x2_t __ret;
  __ret = (uint64x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x8_t vreinterpretq_u16_p8(poly8x16_t __p0) {
  uint16x8_t __ret;
  __ret = (uint16x8_t)(__p0);
  return __ret;
}
#else
__ai uint16x8_t vreinterpretq_u16_p8(poly8x16_t __p0) {
  uint16x8_t __ret;
  __ret = (uint16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x8_t vreinterpretq_u16_p16(poly16x8_t __p0) {
  uint16x8_t __ret;
  __ret = (uint16x8_t)(__p0);
  return __ret;
}
#else
__ai uint16x8_t vreinterpretq_u16_p16(poly16x8_t __p0) {
  uint16x8_t __ret;
  __ret = (uint16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x8_t vreinterpretq_u16_u8(uint8x16_t __p0) {
  uint16x8_t __ret;
  __ret = (uint16x8_t)(__p0);
  return __ret;
}
#else
__ai uint16x8_t vreinterpretq_u16_u8(uint8x16_t __p0) {
  uint16x8_t __ret;
  __ret = (uint16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x8_t vreinterpretq_u16_u32(uint32x4_t __p0) {
  uint16x8_t __ret;
  __ret = (uint16x8_t)(__p0);
  return __ret;
}
#else
__ai uint16x8_t vreinterpretq_u16_u32(uint32x4_t __p0) {
  uint16x8_t __ret;
  __ret = (uint16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x8_t vreinterpretq_u16_u64(uint64x2_t __p0) {
  uint16x8_t __ret;
  __ret = (uint16x8_t)(__p0);
  return __ret;
}
#else
__ai uint16x8_t vreinterpretq_u16_u64(uint64x2_t __p0) {
  uint16x8_t __ret;
  __ret = (uint16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x8_t vreinterpretq_u16_s8(int8x16_t __p0) {
  uint16x8_t __ret;
  __ret = (uint16x8_t)(__p0);
  return __ret;
}
#else
__ai uint16x8_t vreinterpretq_u16_s8(int8x16_t __p0) {
  uint16x8_t __ret;
  __ret = (uint16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x8_t vreinterpretq_u16_f32(float32x4_t __p0) {
  uint16x8_t __ret;
  __ret = (uint16x8_t)(__p0);
  return __ret;
}
#else
__ai uint16x8_t vreinterpretq_u16_f32(float32x4_t __p0) {
  uint16x8_t __ret;
  __ret = (uint16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x8_t vreinterpretq_u16_f16(float16x8_t __p0) {
  uint16x8_t __ret;
  __ret = (uint16x8_t)(__p0);
  return __ret;
}
#else
__ai uint16x8_t vreinterpretq_u16_f16(float16x8_t __p0) {
  uint16x8_t __ret;
  __ret = (uint16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x8_t vreinterpretq_u16_s32(int32x4_t __p0) {
  uint16x8_t __ret;
  __ret = (uint16x8_t)(__p0);
  return __ret;
}
#else
__ai uint16x8_t vreinterpretq_u16_s32(int32x4_t __p0) {
  uint16x8_t __ret;
  __ret = (uint16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x8_t vreinterpretq_u16_s64(int64x2_t __p0) {
  uint16x8_t __ret;
  __ret = (uint16x8_t)(__p0);
  return __ret;
}
#else
__ai uint16x8_t vreinterpretq_u16_s64(int64x2_t __p0) {
  uint16x8_t __ret;
  __ret = (uint16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x8_t vreinterpretq_u16_s16(int16x8_t __p0) {
  uint16x8_t __ret;
  __ret = (uint16x8_t)(__p0);
  return __ret;
}
#else
__ai uint16x8_t vreinterpretq_u16_s16(int16x8_t __p0) {
  uint16x8_t __ret;
  __ret = (uint16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x16_t vreinterpretq_s8_p8(poly8x16_t __p0) {
  int8x16_t __ret;
  __ret = (int8x16_t)(__p0);
  return __ret;
}
#else
__ai int8x16_t vreinterpretq_s8_p8(poly8x16_t __p0) {
  int8x16_t __ret;
  __ret = (int8x16_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x16_t vreinterpretq_s8_p16(poly16x8_t __p0) {
  int8x16_t __ret;
  __ret = (int8x16_t)(__p0);
  return __ret;
}
#else
__ai int8x16_t vreinterpretq_s8_p16(poly16x8_t __p0) {
  int8x16_t __ret;
  __ret = (int8x16_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x16_t vreinterpretq_s8_u8(uint8x16_t __p0) {
  int8x16_t __ret;
  __ret = (int8x16_t)(__p0);
  return __ret;
}
#else
__ai int8x16_t vreinterpretq_s8_u8(uint8x16_t __p0) {
  int8x16_t __ret;
  __ret = (int8x16_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x16_t vreinterpretq_s8_u32(uint32x4_t __p0) {
  int8x16_t __ret;
  __ret = (int8x16_t)(__p0);
  return __ret;
}
#else
__ai int8x16_t vreinterpretq_s8_u32(uint32x4_t __p0) {
  int8x16_t __ret;
  __ret = (int8x16_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x16_t vreinterpretq_s8_u64(uint64x2_t __p0) {
  int8x16_t __ret;
  __ret = (int8x16_t)(__p0);
  return __ret;
}
#else
__ai int8x16_t vreinterpretq_s8_u64(uint64x2_t __p0) {
  int8x16_t __ret;
  __ret = (int8x16_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x16_t vreinterpretq_s8_u16(uint16x8_t __p0) {
  int8x16_t __ret;
  __ret = (int8x16_t)(__p0);
  return __ret;
}
#else
__ai int8x16_t vreinterpretq_s8_u16(uint16x8_t __p0) {
  int8x16_t __ret;
  __ret = (int8x16_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x16_t vreinterpretq_s8_f32(float32x4_t __p0) {
  int8x16_t __ret;
  __ret = (int8x16_t)(__p0);
  return __ret;
}
#else
__ai int8x16_t vreinterpretq_s8_f32(float32x4_t __p0) {
  int8x16_t __ret;
  __ret = (int8x16_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x16_t vreinterpretq_s8_f16(float16x8_t __p0) {
  int8x16_t __ret;
  __ret = (int8x16_t)(__p0);
  return __ret;
}
#else
__ai int8x16_t vreinterpretq_s8_f16(float16x8_t __p0) {
  int8x16_t __ret;
  __ret = (int8x16_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x16_t vreinterpretq_s8_s32(int32x4_t __p0) {
  int8x16_t __ret;
  __ret = (int8x16_t)(__p0);
  return __ret;
}
#else
__ai int8x16_t vreinterpretq_s8_s32(int32x4_t __p0) {
  int8x16_t __ret;
  __ret = (int8x16_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x16_t vreinterpretq_s8_s64(int64x2_t __p0) {
  int8x16_t __ret;
  __ret = (int8x16_t)(__p0);
  return __ret;
}
#else
__ai int8x16_t vreinterpretq_s8_s64(int64x2_t __p0) {
  int8x16_t __ret;
  __ret = (int8x16_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x16_t vreinterpretq_s8_s16(int16x8_t __p0) {
  int8x16_t __ret;
  __ret = (int8x16_t)(__p0);
  return __ret;
}
#else
__ai int8x16_t vreinterpretq_s8_s16(int16x8_t __p0) {
  int8x16_t __ret;
  __ret = (int8x16_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x4_t vreinterpretq_f32_p8(poly8x16_t __p0) {
  float32x4_t __ret;
  __ret = (float32x4_t)(__p0);
  return __ret;
}
#else
__ai float32x4_t vreinterpretq_f32_p8(poly8x16_t __p0) {
  float32x4_t __ret;
  __ret = (float32x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x4_t vreinterpretq_f32_p16(poly16x8_t __p0) {
  float32x4_t __ret;
  __ret = (float32x4_t)(__p0);
  return __ret;
}
#else
__ai float32x4_t vreinterpretq_f32_p16(poly16x8_t __p0) {
  float32x4_t __ret;
  __ret = (float32x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x4_t vreinterpretq_f32_u8(uint8x16_t __p0) {
  float32x4_t __ret;
  __ret = (float32x4_t)(__p0);
  return __ret;
}
#else
__ai float32x4_t vreinterpretq_f32_u8(uint8x16_t __p0) {
  float32x4_t __ret;
  __ret = (float32x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x4_t vreinterpretq_f32_u32(uint32x4_t __p0) {
  float32x4_t __ret;
  __ret = (float32x4_t)(__p0);
  return __ret;
}
#else
__ai float32x4_t vreinterpretq_f32_u32(uint32x4_t __p0) {
  float32x4_t __ret;
  __ret = (float32x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x4_t vreinterpretq_f32_u64(uint64x2_t __p0) {
  float32x4_t __ret;
  __ret = (float32x4_t)(__p0);
  return __ret;
}
#else
__ai float32x4_t vreinterpretq_f32_u64(uint64x2_t __p0) {
  float32x4_t __ret;
  __ret = (float32x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x4_t vreinterpretq_f32_u16(uint16x8_t __p0) {
  float32x4_t __ret;
  __ret = (float32x4_t)(__p0);
  return __ret;
}
#else
__ai float32x4_t vreinterpretq_f32_u16(uint16x8_t __p0) {
  float32x4_t __ret;
  __ret = (float32x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x4_t vreinterpretq_f32_s8(int8x16_t __p0) {
  float32x4_t __ret;
  __ret = (float32x4_t)(__p0);
  return __ret;
}
#else
__ai float32x4_t vreinterpretq_f32_s8(int8x16_t __p0) {
  float32x4_t __ret;
  __ret = (float32x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x4_t vreinterpretq_f32_f16(float16x8_t __p0) {
  float32x4_t __ret;
  __ret = (float32x4_t)(__p0);
  return __ret;
}
#else
__ai float32x4_t vreinterpretq_f32_f16(float16x8_t __p0) {
  float32x4_t __ret;
  __ret = (float32x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x4_t vreinterpretq_f32_s32(int32x4_t __p0) {
  float32x4_t __ret;
  __ret = (float32x4_t)(__p0);
  return __ret;
}
#else
__ai float32x4_t vreinterpretq_f32_s32(int32x4_t __p0) {
  float32x4_t __ret;
  __ret = (float32x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x4_t vreinterpretq_f32_s64(int64x2_t __p0) {
  float32x4_t __ret;
  __ret = (float32x4_t)(__p0);
  return __ret;
}
#else
__ai float32x4_t vreinterpretq_f32_s64(int64x2_t __p0) {
  float32x4_t __ret;
  __ret = (float32x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x4_t vreinterpretq_f32_s16(int16x8_t __p0) {
  float32x4_t __ret;
  __ret = (float32x4_t)(__p0);
  return __ret;
}
#else
__ai float32x4_t vreinterpretq_f32_s16(int16x8_t __p0) {
  float32x4_t __ret;
  __ret = (float32x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float16x8_t vreinterpretq_f16_p8(poly8x16_t __p0) {
  float16x8_t __ret;
  __ret = (float16x8_t)(__p0);
  return __ret;
}
#else
__ai float16x8_t vreinterpretq_f16_p8(poly8x16_t __p0) {
  float16x8_t __ret;
  __ret = (float16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float16x8_t vreinterpretq_f16_p16(poly16x8_t __p0) {
  float16x8_t __ret;
  __ret = (float16x8_t)(__p0);
  return __ret;
}
#else
__ai float16x8_t vreinterpretq_f16_p16(poly16x8_t __p0) {
  float16x8_t __ret;
  __ret = (float16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float16x8_t vreinterpretq_f16_u8(uint8x16_t __p0) {
  float16x8_t __ret;
  __ret = (float16x8_t)(__p0);
  return __ret;
}
#else
__ai float16x8_t vreinterpretq_f16_u8(uint8x16_t __p0) {
  float16x8_t __ret;
  __ret = (float16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float16x8_t vreinterpretq_f16_u32(uint32x4_t __p0) {
  float16x8_t __ret;
  __ret = (float16x8_t)(__p0);
  return __ret;
}
#else
__ai float16x8_t vreinterpretq_f16_u32(uint32x4_t __p0) {
  float16x8_t __ret;
  __ret = (float16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float16x8_t vreinterpretq_f16_u64(uint64x2_t __p0) {
  float16x8_t __ret;
  __ret = (float16x8_t)(__p0);
  return __ret;
}
#else
__ai float16x8_t vreinterpretq_f16_u64(uint64x2_t __p0) {
  float16x8_t __ret;
  __ret = (float16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float16x8_t vreinterpretq_f16_u16(uint16x8_t __p0) {
  float16x8_t __ret;
  __ret = (float16x8_t)(__p0);
  return __ret;
}
#else
__ai float16x8_t vreinterpretq_f16_u16(uint16x8_t __p0) {
  float16x8_t __ret;
  __ret = (float16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float16x8_t vreinterpretq_f16_s8(int8x16_t __p0) {
  float16x8_t __ret;
  __ret = (float16x8_t)(__p0);
  return __ret;
}
#else
__ai float16x8_t vreinterpretq_f16_s8(int8x16_t __p0) {
  float16x8_t __ret;
  __ret = (float16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float16x8_t vreinterpretq_f16_f32(float32x4_t __p0) {
  float16x8_t __ret;
  __ret = (float16x8_t)(__p0);
  return __ret;
}
#else
__ai float16x8_t vreinterpretq_f16_f32(float32x4_t __p0) {
  float16x8_t __ret;
  __ret = (float16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float16x8_t vreinterpretq_f16_s32(int32x4_t __p0) {
  float16x8_t __ret;
  __ret = (float16x8_t)(__p0);
  return __ret;
}
#else
__ai float16x8_t vreinterpretq_f16_s32(int32x4_t __p0) {
  float16x8_t __ret;
  __ret = (float16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float16x8_t vreinterpretq_f16_s64(int64x2_t __p0) {
  float16x8_t __ret;
  __ret = (float16x8_t)(__p0);
  return __ret;
}
#else
__ai float16x8_t vreinterpretq_f16_s64(int64x2_t __p0) {
  float16x8_t __ret;
  __ret = (float16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float16x8_t vreinterpretq_f16_s16(int16x8_t __p0) {
  float16x8_t __ret;
  __ret = (float16x8_t)(__p0);
  return __ret;
}
#else
__ai float16x8_t vreinterpretq_f16_s16(int16x8_t __p0) {
  float16x8_t __ret;
  __ret = (float16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vreinterpretq_s32_p8(poly8x16_t __p0) {
  int32x4_t __ret;
  __ret = (int32x4_t)(__p0);
  return __ret;
}
#else
__ai int32x4_t vreinterpretq_s32_p8(poly8x16_t __p0) {
  int32x4_t __ret;
  __ret = (int32x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vreinterpretq_s32_p16(poly16x8_t __p0) {
  int32x4_t __ret;
  __ret = (int32x4_t)(__p0);
  return __ret;
}
#else
__ai int32x4_t vreinterpretq_s32_p16(poly16x8_t __p0) {
  int32x4_t __ret;
  __ret = (int32x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vreinterpretq_s32_u8(uint8x16_t __p0) {
  int32x4_t __ret;
  __ret = (int32x4_t)(__p0);
  return __ret;
}
#else
__ai int32x4_t vreinterpretq_s32_u8(uint8x16_t __p0) {
  int32x4_t __ret;
  __ret = (int32x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vreinterpretq_s32_u32(uint32x4_t __p0) {
  int32x4_t __ret;
  __ret = (int32x4_t)(__p0);
  return __ret;
}
#else
__ai int32x4_t vreinterpretq_s32_u32(uint32x4_t __p0) {
  int32x4_t __ret;
  __ret = (int32x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vreinterpretq_s32_u64(uint64x2_t __p0) {
  int32x4_t __ret;
  __ret = (int32x4_t)(__p0);
  return __ret;
}
#else
__ai int32x4_t vreinterpretq_s32_u64(uint64x2_t __p0) {
  int32x4_t __ret;
  __ret = (int32x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vreinterpretq_s32_u16(uint16x8_t __p0) {
  int32x4_t __ret;
  __ret = (int32x4_t)(__p0);
  return __ret;
}
#else
__ai int32x4_t vreinterpretq_s32_u16(uint16x8_t __p0) {
  int32x4_t __ret;
  __ret = (int32x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vreinterpretq_s32_s8(int8x16_t __p0) {
  int32x4_t __ret;
  __ret = (int32x4_t)(__p0);
  return __ret;
}
#else
__ai int32x4_t vreinterpretq_s32_s8(int8x16_t __p0) {
  int32x4_t __ret;
  __ret = (int32x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vreinterpretq_s32_f32(float32x4_t __p0) {
  int32x4_t __ret;
  __ret = (int32x4_t)(__p0);
  return __ret;
}
#else
__ai int32x4_t vreinterpretq_s32_f32(float32x4_t __p0) {
  int32x4_t __ret;
  __ret = (int32x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vreinterpretq_s32_f16(float16x8_t __p0) {
  int32x4_t __ret;
  __ret = (int32x4_t)(__p0);
  return __ret;
}
#else
__ai int32x4_t vreinterpretq_s32_f16(float16x8_t __p0) {
  int32x4_t __ret;
  __ret = (int32x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vreinterpretq_s32_s64(int64x2_t __p0) {
  int32x4_t __ret;
  __ret = (int32x4_t)(__p0);
  return __ret;
}
#else
__ai int32x4_t vreinterpretq_s32_s64(int64x2_t __p0) {
  int32x4_t __ret;
  __ret = (int32x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vreinterpretq_s32_s16(int16x8_t __p0) {
  int32x4_t __ret;
  __ret = (int32x4_t)(__p0);
  return __ret;
}
#else
__ai int32x4_t vreinterpretq_s32_s16(int16x8_t __p0) {
  int32x4_t __ret;
  __ret = (int32x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x2_t vreinterpretq_s64_p8(poly8x16_t __p0) {
  int64x2_t __ret;
  __ret = (int64x2_t)(__p0);
  return __ret;
}
#else
__ai int64x2_t vreinterpretq_s64_p8(poly8x16_t __p0) {
  int64x2_t __ret;
  __ret = (int64x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x2_t vreinterpretq_s64_p16(poly16x8_t __p0) {
  int64x2_t __ret;
  __ret = (int64x2_t)(__p0);
  return __ret;
}
#else
__ai int64x2_t vreinterpretq_s64_p16(poly16x8_t __p0) {
  int64x2_t __ret;
  __ret = (int64x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x2_t vreinterpretq_s64_u8(uint8x16_t __p0) {
  int64x2_t __ret;
  __ret = (int64x2_t)(__p0);
  return __ret;
}
#else
__ai int64x2_t vreinterpretq_s64_u8(uint8x16_t __p0) {
  int64x2_t __ret;
  __ret = (int64x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x2_t vreinterpretq_s64_u32(uint32x4_t __p0) {
  int64x2_t __ret;
  __ret = (int64x2_t)(__p0);
  return __ret;
}
#else
__ai int64x2_t vreinterpretq_s64_u32(uint32x4_t __p0) {
  int64x2_t __ret;
  __ret = (int64x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x2_t vreinterpretq_s64_u64(uint64x2_t __p0) {
  int64x2_t __ret;
  __ret = (int64x2_t)(__p0);
  return __ret;
}
#else
__ai int64x2_t vreinterpretq_s64_u64(uint64x2_t __p0) {
  int64x2_t __ret;
  __ret = (int64x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x2_t vreinterpretq_s64_u16(uint16x8_t __p0) {
  int64x2_t __ret;
  __ret = (int64x2_t)(__p0);
  return __ret;
}
#else
__ai int64x2_t vreinterpretq_s64_u16(uint16x8_t __p0) {
  int64x2_t __ret;
  __ret = (int64x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x2_t vreinterpretq_s64_s8(int8x16_t __p0) {
  int64x2_t __ret;
  __ret = (int64x2_t)(__p0);
  return __ret;
}
#else
__ai int64x2_t vreinterpretq_s64_s8(int8x16_t __p0) {
  int64x2_t __ret;
  __ret = (int64x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x2_t vreinterpretq_s64_f32(float32x4_t __p0) {
  int64x2_t __ret;
  __ret = (int64x2_t)(__p0);
  return __ret;
}
#else
__ai int64x2_t vreinterpretq_s64_f32(float32x4_t __p0) {
  int64x2_t __ret;
  __ret = (int64x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x2_t vreinterpretq_s64_f16(float16x8_t __p0) {
  int64x2_t __ret;
  __ret = (int64x2_t)(__p0);
  return __ret;
}
#else
__ai int64x2_t vreinterpretq_s64_f16(float16x8_t __p0) {
  int64x2_t __ret;
  __ret = (int64x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x2_t vreinterpretq_s64_s32(int32x4_t __p0) {
  int64x2_t __ret;
  __ret = (int64x2_t)(__p0);
  return __ret;
}
#else
__ai int64x2_t vreinterpretq_s64_s32(int32x4_t __p0) {
  int64x2_t __ret;
  __ret = (int64x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x2_t vreinterpretq_s64_s16(int16x8_t __p0) {
  int64x2_t __ret;
  __ret = (int64x2_t)(__p0);
  return __ret;
}
#else
__ai int64x2_t vreinterpretq_s64_s16(int16x8_t __p0) {
  int64x2_t __ret;
  __ret = (int64x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x8_t vreinterpretq_s16_p8(poly8x16_t __p0) {
  int16x8_t __ret;
  __ret = (int16x8_t)(__p0);
  return __ret;
}
#else
__ai int16x8_t vreinterpretq_s16_p8(poly8x16_t __p0) {
  int16x8_t __ret;
  __ret = (int16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x8_t vreinterpretq_s16_p16(poly16x8_t __p0) {
  int16x8_t __ret;
  __ret = (int16x8_t)(__p0);
  return __ret;
}
#else
__ai int16x8_t vreinterpretq_s16_p16(poly16x8_t __p0) {
  int16x8_t __ret;
  __ret = (int16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x8_t vreinterpretq_s16_u8(uint8x16_t __p0) {
  int16x8_t __ret;
  __ret = (int16x8_t)(__p0);
  return __ret;
}
#else
__ai int16x8_t vreinterpretq_s16_u8(uint8x16_t __p0) {
  int16x8_t __ret;
  __ret = (int16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x8_t vreinterpretq_s16_u32(uint32x4_t __p0) {
  int16x8_t __ret;
  __ret = (int16x8_t)(__p0);
  return __ret;
}
#else
__ai int16x8_t vreinterpretq_s16_u32(uint32x4_t __p0) {
  int16x8_t __ret;
  __ret = (int16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x8_t vreinterpretq_s16_u64(uint64x2_t __p0) {
  int16x8_t __ret;
  __ret = (int16x8_t)(__p0);
  return __ret;
}
#else
__ai int16x8_t vreinterpretq_s16_u64(uint64x2_t __p0) {
  int16x8_t __ret;
  __ret = (int16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x8_t vreinterpretq_s16_u16(uint16x8_t __p0) {
  int16x8_t __ret;
  __ret = (int16x8_t)(__p0);
  return __ret;
}
#else
__ai int16x8_t vreinterpretq_s16_u16(uint16x8_t __p0) {
  int16x8_t __ret;
  __ret = (int16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x8_t vreinterpretq_s16_s8(int8x16_t __p0) {
  int16x8_t __ret;
  __ret = (int16x8_t)(__p0);
  return __ret;
}
#else
__ai int16x8_t vreinterpretq_s16_s8(int8x16_t __p0) {
  int16x8_t __ret;
  __ret = (int16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x8_t vreinterpretq_s16_f32(float32x4_t __p0) {
  int16x8_t __ret;
  __ret = (int16x8_t)(__p0);
  return __ret;
}
#else
__ai int16x8_t vreinterpretq_s16_f32(float32x4_t __p0) {
  int16x8_t __ret;
  __ret = (int16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x8_t vreinterpretq_s16_f16(float16x8_t __p0) {
  int16x8_t __ret;
  __ret = (int16x8_t)(__p0);
  return __ret;
}
#else
__ai int16x8_t vreinterpretq_s16_f16(float16x8_t __p0) {
  int16x8_t __ret;
  __ret = (int16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x8_t vreinterpretq_s16_s32(int32x4_t __p0) {
  int16x8_t __ret;
  __ret = (int16x8_t)(__p0);
  return __ret;
}
#else
__ai int16x8_t vreinterpretq_s16_s32(int32x4_t __p0) {
  int16x8_t __ret;
  __ret = (int16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x8_t vreinterpretq_s16_s64(int64x2_t __p0) {
  int16x8_t __ret;
  __ret = (int16x8_t)(__p0);
  return __ret;
}
#else
__ai int16x8_t vreinterpretq_s16_s64(int64x2_t __p0) {
  int16x8_t __ret;
  __ret = (int16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x8_t vreinterpret_u8_p8(poly8x8_t __p0) {
  uint8x8_t __ret;
  __ret = (uint8x8_t)(__p0);
  return __ret;
}
#else
__ai uint8x8_t vreinterpret_u8_p8(poly8x8_t __p0) {
  uint8x8_t __ret;
  __ret = (uint8x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x8_t vreinterpret_u8_p16(poly16x4_t __p0) {
  uint8x8_t __ret;
  __ret = (uint8x8_t)(__p0);
  return __ret;
}
#else
__ai uint8x8_t vreinterpret_u8_p16(poly16x4_t __p0) {
  uint8x8_t __ret;
  __ret = (uint8x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x8_t vreinterpret_u8_u32(uint32x2_t __p0) {
  uint8x8_t __ret;
  __ret = (uint8x8_t)(__p0);
  return __ret;
}
#else
__ai uint8x8_t vreinterpret_u8_u32(uint32x2_t __p0) {
  uint8x8_t __ret;
  __ret = (uint8x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x8_t vreinterpret_u8_u64(uint64x1_t __p0) {
  uint8x8_t __ret;
  __ret = (uint8x8_t)(__p0);
  return __ret;
}
#else
__ai uint8x8_t vreinterpret_u8_u64(uint64x1_t __p0) {
  uint8x8_t __ret;
  __ret = (uint8x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x8_t vreinterpret_u8_u16(uint16x4_t __p0) {
  uint8x8_t __ret;
  __ret = (uint8x8_t)(__p0);
  return __ret;
}
#else
__ai uint8x8_t vreinterpret_u8_u16(uint16x4_t __p0) {
  uint8x8_t __ret;
  __ret = (uint8x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x8_t vreinterpret_u8_s8(int8x8_t __p0) {
  uint8x8_t __ret;
  __ret = (uint8x8_t)(__p0);
  return __ret;
}
#else
__ai uint8x8_t vreinterpret_u8_s8(int8x8_t __p0) {
  uint8x8_t __ret;
  __ret = (uint8x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x8_t vreinterpret_u8_f32(float32x2_t __p0) {
  uint8x8_t __ret;
  __ret = (uint8x8_t)(__p0);
  return __ret;
}
#else
__ai uint8x8_t vreinterpret_u8_f32(float32x2_t __p0) {
  uint8x8_t __ret;
  __ret = (uint8x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x8_t vreinterpret_u8_f16(float16x4_t __p0) {
  uint8x8_t __ret;
  __ret = (uint8x8_t)(__p0);
  return __ret;
}
#else
__ai uint8x8_t vreinterpret_u8_f16(float16x4_t __p0) {
  uint8x8_t __ret;
  __ret = (uint8x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x8_t vreinterpret_u8_s32(int32x2_t __p0) {
  uint8x8_t __ret;
  __ret = (uint8x8_t)(__p0);
  return __ret;
}
#else
__ai uint8x8_t vreinterpret_u8_s32(int32x2_t __p0) {
  uint8x8_t __ret;
  __ret = (uint8x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x8_t vreinterpret_u8_s64(int64x1_t __p0) {
  uint8x8_t __ret;
  __ret = (uint8x8_t)(__p0);
  return __ret;
}
#else
__ai uint8x8_t vreinterpret_u8_s64(int64x1_t __p0) {
  uint8x8_t __ret;
  __ret = (uint8x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x8_t vreinterpret_u8_s16(int16x4_t __p0) {
  uint8x8_t __ret;
  __ret = (uint8x8_t)(__p0);
  return __ret;
}
#else
__ai uint8x8_t vreinterpret_u8_s16(int16x4_t __p0) {
  uint8x8_t __ret;
  __ret = (uint8x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x2_t vreinterpret_u32_p8(poly8x8_t __p0) {
  uint32x2_t __ret;
  __ret = (uint32x2_t)(__p0);
  return __ret;
}
#else
__ai uint32x2_t vreinterpret_u32_p8(poly8x8_t __p0) {
  uint32x2_t __ret;
  __ret = (uint32x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x2_t vreinterpret_u32_p16(poly16x4_t __p0) {
  uint32x2_t __ret;
  __ret = (uint32x2_t)(__p0);
  return __ret;
}
#else
__ai uint32x2_t vreinterpret_u32_p16(poly16x4_t __p0) {
  uint32x2_t __ret;
  __ret = (uint32x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x2_t vreinterpret_u32_u8(uint8x8_t __p0) {
  uint32x2_t __ret;
  __ret = (uint32x2_t)(__p0);
  return __ret;
}
#else
__ai uint32x2_t vreinterpret_u32_u8(uint8x8_t __p0) {
  uint32x2_t __ret;
  __ret = (uint32x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x2_t vreinterpret_u32_u64(uint64x1_t __p0) {
  uint32x2_t __ret;
  __ret = (uint32x2_t)(__p0);
  return __ret;
}
#else
__ai uint32x2_t vreinterpret_u32_u64(uint64x1_t __p0) {
  uint32x2_t __ret;
  __ret = (uint32x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x2_t vreinterpret_u32_u16(uint16x4_t __p0) {
  uint32x2_t __ret;
  __ret = (uint32x2_t)(__p0);
  return __ret;
}
#else
__ai uint32x2_t vreinterpret_u32_u16(uint16x4_t __p0) {
  uint32x2_t __ret;
  __ret = (uint32x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x2_t vreinterpret_u32_s8(int8x8_t __p0) {
  uint32x2_t __ret;
  __ret = (uint32x2_t)(__p0);
  return __ret;
}
#else
__ai uint32x2_t vreinterpret_u32_s8(int8x8_t __p0) {
  uint32x2_t __ret;
  __ret = (uint32x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x2_t vreinterpret_u32_f32(float32x2_t __p0) {
  uint32x2_t __ret;
  __ret = (uint32x2_t)(__p0);
  return __ret;
}
#else
__ai uint32x2_t vreinterpret_u32_f32(float32x2_t __p0) {
  uint32x2_t __ret;
  __ret = (uint32x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x2_t vreinterpret_u32_f16(float16x4_t __p0) {
  uint32x2_t __ret;
  __ret = (uint32x2_t)(__p0);
  return __ret;
}
#else
__ai uint32x2_t vreinterpret_u32_f16(float16x4_t __p0) {
  uint32x2_t __ret;
  __ret = (uint32x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x2_t vreinterpret_u32_s32(int32x2_t __p0) {
  uint32x2_t __ret;
  __ret = (uint32x2_t)(__p0);
  return __ret;
}
#else
__ai uint32x2_t vreinterpret_u32_s32(int32x2_t __p0) {
  uint32x2_t __ret;
  __ret = (uint32x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x2_t vreinterpret_u32_s64(int64x1_t __p0) {
  uint32x2_t __ret;
  __ret = (uint32x2_t)(__p0);
  return __ret;
}
#else
__ai uint32x2_t vreinterpret_u32_s64(int64x1_t __p0) {
  uint32x2_t __ret;
  __ret = (uint32x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x2_t vreinterpret_u32_s16(int16x4_t __p0) {
  uint32x2_t __ret;
  __ret = (uint32x2_t)(__p0);
  return __ret;
}
#else
__ai uint32x2_t vreinterpret_u32_s16(int16x4_t __p0) {
  uint32x2_t __ret;
  __ret = (uint32x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x1_t vreinterpret_u64_p8(poly8x8_t __p0) {
  uint64x1_t __ret;
  __ret = (uint64x1_t)(__p0);
  return __ret;
}
#else
__ai uint64x1_t vreinterpret_u64_p8(poly8x8_t __p0) {
  uint64x1_t __ret;
  __ret = (uint64x1_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x1_t vreinterpret_u64_p16(poly16x4_t __p0) {
  uint64x1_t __ret;
  __ret = (uint64x1_t)(__p0);
  return __ret;
}
#else
__ai uint64x1_t vreinterpret_u64_p16(poly16x4_t __p0) {
  uint64x1_t __ret;
  __ret = (uint64x1_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x1_t vreinterpret_u64_u8(uint8x8_t __p0) {
  uint64x1_t __ret;
  __ret = (uint64x1_t)(__p0);
  return __ret;
}
#else
__ai uint64x1_t vreinterpret_u64_u8(uint8x8_t __p0) {
  uint64x1_t __ret;
  __ret = (uint64x1_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x1_t vreinterpret_u64_u32(uint32x2_t __p0) {
  uint64x1_t __ret;
  __ret = (uint64x1_t)(__p0);
  return __ret;
}
#else
__ai uint64x1_t vreinterpret_u64_u32(uint32x2_t __p0) {
  uint64x1_t __ret;
  __ret = (uint64x1_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x1_t vreinterpret_u64_u16(uint16x4_t __p0) {
  uint64x1_t __ret;
  __ret = (uint64x1_t)(__p0);
  return __ret;
}
#else
__ai uint64x1_t vreinterpret_u64_u16(uint16x4_t __p0) {
  uint64x1_t __ret;
  __ret = (uint64x1_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x1_t vreinterpret_u64_s8(int8x8_t __p0) {
  uint64x1_t __ret;
  __ret = (uint64x1_t)(__p0);
  return __ret;
}
#else
__ai uint64x1_t vreinterpret_u64_s8(int8x8_t __p0) {
  uint64x1_t __ret;
  __ret = (uint64x1_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x1_t vreinterpret_u64_f32(float32x2_t __p0) {
  uint64x1_t __ret;
  __ret = (uint64x1_t)(__p0);
  return __ret;
}
#else
__ai uint64x1_t vreinterpret_u64_f32(float32x2_t __p0) {
  uint64x1_t __ret;
  __ret = (uint64x1_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x1_t vreinterpret_u64_f16(float16x4_t __p0) {
  uint64x1_t __ret;
  __ret = (uint64x1_t)(__p0);
  return __ret;
}
#else
__ai uint64x1_t vreinterpret_u64_f16(float16x4_t __p0) {
  uint64x1_t __ret;
  __ret = (uint64x1_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x1_t vreinterpret_u64_s32(int32x2_t __p0) {
  uint64x1_t __ret;
  __ret = (uint64x1_t)(__p0);
  return __ret;
}
#else
__ai uint64x1_t vreinterpret_u64_s32(int32x2_t __p0) {
  uint64x1_t __ret;
  __ret = (uint64x1_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x1_t vreinterpret_u64_s64(int64x1_t __p0) {
  uint64x1_t __ret;
  __ret = (uint64x1_t)(__p0);
  return __ret;
}
#else
__ai uint64x1_t vreinterpret_u64_s64(int64x1_t __p0) {
  uint64x1_t __ret;
  __ret = (uint64x1_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x1_t vreinterpret_u64_s16(int16x4_t __p0) {
  uint64x1_t __ret;
  __ret = (uint64x1_t)(__p0);
  return __ret;
}
#else
__ai uint64x1_t vreinterpret_u64_s16(int16x4_t __p0) {
  uint64x1_t __ret;
  __ret = (uint64x1_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x4_t vreinterpret_u16_p8(poly8x8_t __p0) {
  uint16x4_t __ret;
  __ret = (uint16x4_t)(__p0);
  return __ret;
}
#else
__ai uint16x4_t vreinterpret_u16_p8(poly8x8_t __p0) {
  uint16x4_t __ret;
  __ret = (uint16x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x4_t vreinterpret_u16_p16(poly16x4_t __p0) {
  uint16x4_t __ret;
  __ret = (uint16x4_t)(__p0);
  return __ret;
}
#else
__ai uint16x4_t vreinterpret_u16_p16(poly16x4_t __p0) {
  uint16x4_t __ret;
  __ret = (uint16x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x4_t vreinterpret_u16_u8(uint8x8_t __p0) {
  uint16x4_t __ret;
  __ret = (uint16x4_t)(__p0);
  return __ret;
}
#else
__ai uint16x4_t vreinterpret_u16_u8(uint8x8_t __p0) {
  uint16x4_t __ret;
  __ret = (uint16x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x4_t vreinterpret_u16_u32(uint32x2_t __p0) {
  uint16x4_t __ret;
  __ret = (uint16x4_t)(__p0);
  return __ret;
}
#else
__ai uint16x4_t vreinterpret_u16_u32(uint32x2_t __p0) {
  uint16x4_t __ret;
  __ret = (uint16x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x4_t vreinterpret_u16_u64(uint64x1_t __p0) {
  uint16x4_t __ret;
  __ret = (uint16x4_t)(__p0);
  return __ret;
}
#else
__ai uint16x4_t vreinterpret_u16_u64(uint64x1_t __p0) {
  uint16x4_t __ret;
  __ret = (uint16x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x4_t vreinterpret_u16_s8(int8x8_t __p0) {
  uint16x4_t __ret;
  __ret = (uint16x4_t)(__p0);
  return __ret;
}
#else
__ai uint16x4_t vreinterpret_u16_s8(int8x8_t __p0) {
  uint16x4_t __ret;
  __ret = (uint16x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x4_t vreinterpret_u16_f32(float32x2_t __p0) {
  uint16x4_t __ret;
  __ret = (uint16x4_t)(__p0);
  return __ret;
}
#else
__ai uint16x4_t vreinterpret_u16_f32(float32x2_t __p0) {
  uint16x4_t __ret;
  __ret = (uint16x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x4_t vreinterpret_u16_f16(float16x4_t __p0) {
  uint16x4_t __ret;
  __ret = (uint16x4_t)(__p0);
  return __ret;
}
#else
__ai uint16x4_t vreinterpret_u16_f16(float16x4_t __p0) {
  uint16x4_t __ret;
  __ret = (uint16x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x4_t vreinterpret_u16_s32(int32x2_t __p0) {
  uint16x4_t __ret;
  __ret = (uint16x4_t)(__p0);
  return __ret;
}
#else
__ai uint16x4_t vreinterpret_u16_s32(int32x2_t __p0) {
  uint16x4_t __ret;
  __ret = (uint16x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x4_t vreinterpret_u16_s64(int64x1_t __p0) {
  uint16x4_t __ret;
  __ret = (uint16x4_t)(__p0);
  return __ret;
}
#else
__ai uint16x4_t vreinterpret_u16_s64(int64x1_t __p0) {
  uint16x4_t __ret;
  __ret = (uint16x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x4_t vreinterpret_u16_s16(int16x4_t __p0) {
  uint16x4_t __ret;
  __ret = (uint16x4_t)(__p0);
  return __ret;
}
#else
__ai uint16x4_t vreinterpret_u16_s16(int16x4_t __p0) {
  uint16x4_t __ret;
  __ret = (uint16x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x8_t vreinterpret_s8_p8(poly8x8_t __p0) {
  int8x8_t __ret;
  __ret = (int8x8_t)(__p0);
  return __ret;
}
#else
__ai int8x8_t vreinterpret_s8_p8(poly8x8_t __p0) {
  int8x8_t __ret;
  __ret = (int8x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x8_t vreinterpret_s8_p16(poly16x4_t __p0) {
  int8x8_t __ret;
  __ret = (int8x8_t)(__p0);
  return __ret;
}
#else
__ai int8x8_t vreinterpret_s8_p16(poly16x4_t __p0) {
  int8x8_t __ret;
  __ret = (int8x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x8_t vreinterpret_s8_u8(uint8x8_t __p0) {
  int8x8_t __ret;
  __ret = (int8x8_t)(__p0);
  return __ret;
}
#else
__ai int8x8_t vreinterpret_s8_u8(uint8x8_t __p0) {
  int8x8_t __ret;
  __ret = (int8x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x8_t vreinterpret_s8_u32(uint32x2_t __p0) {
  int8x8_t __ret;
  __ret = (int8x8_t)(__p0);
  return __ret;
}
#else
__ai int8x8_t vreinterpret_s8_u32(uint32x2_t __p0) {
  int8x8_t __ret;
  __ret = (int8x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x8_t vreinterpret_s8_u64(uint64x1_t __p0) {
  int8x8_t __ret;
  __ret = (int8x8_t)(__p0);
  return __ret;
}
#else
__ai int8x8_t vreinterpret_s8_u64(uint64x1_t __p0) {
  int8x8_t __ret;
  __ret = (int8x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x8_t vreinterpret_s8_u16(uint16x4_t __p0) {
  int8x8_t __ret;
  __ret = (int8x8_t)(__p0);
  return __ret;
}
#else
__ai int8x8_t vreinterpret_s8_u16(uint16x4_t __p0) {
  int8x8_t __ret;
  __ret = (int8x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x8_t vreinterpret_s8_f32(float32x2_t __p0) {
  int8x8_t __ret;
  __ret = (int8x8_t)(__p0);
  return __ret;
}
#else
__ai int8x8_t vreinterpret_s8_f32(float32x2_t __p0) {
  int8x8_t __ret;
  __ret = (int8x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x8_t vreinterpret_s8_f16(float16x4_t __p0) {
  int8x8_t __ret;
  __ret = (int8x8_t)(__p0);
  return __ret;
}
#else
__ai int8x8_t vreinterpret_s8_f16(float16x4_t __p0) {
  int8x8_t __ret;
  __ret = (int8x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x8_t vreinterpret_s8_s32(int32x2_t __p0) {
  int8x8_t __ret;
  __ret = (int8x8_t)(__p0);
  return __ret;
}
#else
__ai int8x8_t vreinterpret_s8_s32(int32x2_t __p0) {
  int8x8_t __ret;
  __ret = (int8x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x8_t vreinterpret_s8_s64(int64x1_t __p0) {
  int8x8_t __ret;
  __ret = (int8x8_t)(__p0);
  return __ret;
}
#else
__ai int8x8_t vreinterpret_s8_s64(int64x1_t __p0) {
  int8x8_t __ret;
  __ret = (int8x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x8_t vreinterpret_s8_s16(int16x4_t __p0) {
  int8x8_t __ret;
  __ret = (int8x8_t)(__p0);
  return __ret;
}
#else
__ai int8x8_t vreinterpret_s8_s16(int16x4_t __p0) {
  int8x8_t __ret;
  __ret = (int8x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x2_t vreinterpret_f32_p8(poly8x8_t __p0) {
  float32x2_t __ret;
  __ret = (float32x2_t)(__p0);
  return __ret;
}
#else
__ai float32x2_t vreinterpret_f32_p8(poly8x8_t __p0) {
  float32x2_t __ret;
  __ret = (float32x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x2_t vreinterpret_f32_p16(poly16x4_t __p0) {
  float32x2_t __ret;
  __ret = (float32x2_t)(__p0);
  return __ret;
}
#else
__ai float32x2_t vreinterpret_f32_p16(poly16x4_t __p0) {
  float32x2_t __ret;
  __ret = (float32x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x2_t vreinterpret_f32_u8(uint8x8_t __p0) {
  float32x2_t __ret;
  __ret = (float32x2_t)(__p0);
  return __ret;
}
#else
__ai float32x2_t vreinterpret_f32_u8(uint8x8_t __p0) {
  float32x2_t __ret;
  __ret = (float32x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x2_t vreinterpret_f32_u32(uint32x2_t __p0) {
  float32x2_t __ret;
  __ret = (float32x2_t)(__p0);
  return __ret;
}
#else
__ai float32x2_t vreinterpret_f32_u32(uint32x2_t __p0) {
  float32x2_t __ret;
  __ret = (float32x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x2_t vreinterpret_f32_u64(uint64x1_t __p0) {
  float32x2_t __ret;
  __ret = (float32x2_t)(__p0);
  return __ret;
}
#else
__ai float32x2_t vreinterpret_f32_u64(uint64x1_t __p0) {
  float32x2_t __ret;
  __ret = (float32x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x2_t vreinterpret_f32_u16(uint16x4_t __p0) {
  float32x2_t __ret;
  __ret = (float32x2_t)(__p0);
  return __ret;
}
#else
__ai float32x2_t vreinterpret_f32_u16(uint16x4_t __p0) {
  float32x2_t __ret;
  __ret = (float32x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x2_t vreinterpret_f32_s8(int8x8_t __p0) {
  float32x2_t __ret;
  __ret = (float32x2_t)(__p0);
  return __ret;
}
#else
__ai float32x2_t vreinterpret_f32_s8(int8x8_t __p0) {
  float32x2_t __ret;
  __ret = (float32x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x2_t vreinterpret_f32_f16(float16x4_t __p0) {
  float32x2_t __ret;
  __ret = (float32x2_t)(__p0);
  return __ret;
}
#else
__ai float32x2_t vreinterpret_f32_f16(float16x4_t __p0) {
  float32x2_t __ret;
  __ret = (float32x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x2_t vreinterpret_f32_s32(int32x2_t __p0) {
  float32x2_t __ret;
  __ret = (float32x2_t)(__p0);
  return __ret;
}
#else
__ai float32x2_t vreinterpret_f32_s32(int32x2_t __p0) {
  float32x2_t __ret;
  __ret = (float32x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x2_t vreinterpret_f32_s64(int64x1_t __p0) {
  float32x2_t __ret;
  __ret = (float32x2_t)(__p0);
  return __ret;
}
#else
__ai float32x2_t vreinterpret_f32_s64(int64x1_t __p0) {
  float32x2_t __ret;
  __ret = (float32x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x2_t vreinterpret_f32_s16(int16x4_t __p0) {
  float32x2_t __ret;
  __ret = (float32x2_t)(__p0);
  return __ret;
}
#else
__ai float32x2_t vreinterpret_f32_s16(int16x4_t __p0) {
  float32x2_t __ret;
  __ret = (float32x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float16x4_t vreinterpret_f16_p8(poly8x8_t __p0) {
  float16x4_t __ret;
  __ret = (float16x4_t)(__p0);
  return __ret;
}
#else
__ai float16x4_t vreinterpret_f16_p8(poly8x8_t __p0) {
  float16x4_t __ret;
  __ret = (float16x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float16x4_t vreinterpret_f16_p16(poly16x4_t __p0) {
  float16x4_t __ret;
  __ret = (float16x4_t)(__p0);
  return __ret;
}
#else
__ai float16x4_t vreinterpret_f16_p16(poly16x4_t __p0) {
  float16x4_t __ret;
  __ret = (float16x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float16x4_t vreinterpret_f16_u8(uint8x8_t __p0) {
  float16x4_t __ret;
  __ret = (float16x4_t)(__p0);
  return __ret;
}
#else
__ai float16x4_t vreinterpret_f16_u8(uint8x8_t __p0) {
  float16x4_t __ret;
  __ret = (float16x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float16x4_t vreinterpret_f16_u32(uint32x2_t __p0) {
  float16x4_t __ret;
  __ret = (float16x4_t)(__p0);
  return __ret;
}
#else
__ai float16x4_t vreinterpret_f16_u32(uint32x2_t __p0) {
  float16x4_t __ret;
  __ret = (float16x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float16x4_t vreinterpret_f16_u64(uint64x1_t __p0) {
  float16x4_t __ret;
  __ret = (float16x4_t)(__p0);
  return __ret;
}
#else
__ai float16x4_t vreinterpret_f16_u64(uint64x1_t __p0) {
  float16x4_t __ret;
  __ret = (float16x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float16x4_t vreinterpret_f16_u16(uint16x4_t __p0) {
  float16x4_t __ret;
  __ret = (float16x4_t)(__p0);
  return __ret;
}
#else
__ai float16x4_t vreinterpret_f16_u16(uint16x4_t __p0) {
  float16x4_t __ret;
  __ret = (float16x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float16x4_t vreinterpret_f16_s8(int8x8_t __p0) {
  float16x4_t __ret;
  __ret = (float16x4_t)(__p0);
  return __ret;
}
#else
__ai float16x4_t vreinterpret_f16_s8(int8x8_t __p0) {
  float16x4_t __ret;
  __ret = (float16x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float16x4_t vreinterpret_f16_f32(float32x2_t __p0) {
  float16x4_t __ret;
  __ret = (float16x4_t)(__p0);
  return __ret;
}
#else
__ai float16x4_t vreinterpret_f16_f32(float32x2_t __p0) {
  float16x4_t __ret;
  __ret = (float16x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float16x4_t vreinterpret_f16_s32(int32x2_t __p0) {
  float16x4_t __ret;
  __ret = (float16x4_t)(__p0);
  return __ret;
}
#else
__ai float16x4_t vreinterpret_f16_s32(int32x2_t __p0) {
  float16x4_t __ret;
  __ret = (float16x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float16x4_t vreinterpret_f16_s64(int64x1_t __p0) {
  float16x4_t __ret;
  __ret = (float16x4_t)(__p0);
  return __ret;
}
#else
__ai float16x4_t vreinterpret_f16_s64(int64x1_t __p0) {
  float16x4_t __ret;
  __ret = (float16x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float16x4_t vreinterpret_f16_s16(int16x4_t __p0) {
  float16x4_t __ret;
  __ret = (float16x4_t)(__p0);
  return __ret;
}
#else
__ai float16x4_t vreinterpret_f16_s16(int16x4_t __p0) {
  float16x4_t __ret;
  __ret = (float16x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x2_t vreinterpret_s32_p8(poly8x8_t __p0) {
  int32x2_t __ret;
  __ret = (int32x2_t)(__p0);
  return __ret;
}
#else
__ai int32x2_t vreinterpret_s32_p8(poly8x8_t __p0) {
  int32x2_t __ret;
  __ret = (int32x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x2_t vreinterpret_s32_p16(poly16x4_t __p0) {
  int32x2_t __ret;
  __ret = (int32x2_t)(__p0);
  return __ret;
}
#else
__ai int32x2_t vreinterpret_s32_p16(poly16x4_t __p0) {
  int32x2_t __ret;
  __ret = (int32x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x2_t vreinterpret_s32_u8(uint8x8_t __p0) {
  int32x2_t __ret;
  __ret = (int32x2_t)(__p0);
  return __ret;
}
#else
__ai int32x2_t vreinterpret_s32_u8(uint8x8_t __p0) {
  int32x2_t __ret;
  __ret = (int32x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x2_t vreinterpret_s32_u32(uint32x2_t __p0) {
  int32x2_t __ret;
  __ret = (int32x2_t)(__p0);
  return __ret;
}
#else
__ai int32x2_t vreinterpret_s32_u32(uint32x2_t __p0) {
  int32x2_t __ret;
  __ret = (int32x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x2_t vreinterpret_s32_u64(uint64x1_t __p0) {
  int32x2_t __ret;
  __ret = (int32x2_t)(__p0);
  return __ret;
}
#else
__ai int32x2_t vreinterpret_s32_u64(uint64x1_t __p0) {
  int32x2_t __ret;
  __ret = (int32x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x2_t vreinterpret_s32_u16(uint16x4_t __p0) {
  int32x2_t __ret;
  __ret = (int32x2_t)(__p0);
  return __ret;
}
#else
__ai int32x2_t vreinterpret_s32_u16(uint16x4_t __p0) {
  int32x2_t __ret;
  __ret = (int32x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x2_t vreinterpret_s32_s8(int8x8_t __p0) {
  int32x2_t __ret;
  __ret = (int32x2_t)(__p0);
  return __ret;
}
#else
__ai int32x2_t vreinterpret_s32_s8(int8x8_t __p0) {
  int32x2_t __ret;
  __ret = (int32x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x2_t vreinterpret_s32_f32(float32x2_t __p0) {
  int32x2_t __ret;
  __ret = (int32x2_t)(__p0);
  return __ret;
}
#else
__ai int32x2_t vreinterpret_s32_f32(float32x2_t __p0) {
  int32x2_t __ret;
  __ret = (int32x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x2_t vreinterpret_s32_f16(float16x4_t __p0) {
  int32x2_t __ret;
  __ret = (int32x2_t)(__p0);
  return __ret;
}
#else
__ai int32x2_t vreinterpret_s32_f16(float16x4_t __p0) {
  int32x2_t __ret;
  __ret = (int32x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x2_t vreinterpret_s32_s64(int64x1_t __p0) {
  int32x2_t __ret;
  __ret = (int32x2_t)(__p0);
  return __ret;
}
#else
__ai int32x2_t vreinterpret_s32_s64(int64x1_t __p0) {
  int32x2_t __ret;
  __ret = (int32x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x2_t vreinterpret_s32_s16(int16x4_t __p0) {
  int32x2_t __ret;
  __ret = (int32x2_t)(__p0);
  return __ret;
}
#else
__ai int32x2_t vreinterpret_s32_s16(int16x4_t __p0) {
  int32x2_t __ret;
  __ret = (int32x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x1_t vreinterpret_s64_p8(poly8x8_t __p0) {
  int64x1_t __ret;
  __ret = (int64x1_t)(__p0);
  return __ret;
}
#else
__ai int64x1_t vreinterpret_s64_p8(poly8x8_t __p0) {
  int64x1_t __ret;
  __ret = (int64x1_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x1_t vreinterpret_s64_p16(poly16x4_t __p0) {
  int64x1_t __ret;
  __ret = (int64x1_t)(__p0);
  return __ret;
}
#else
__ai int64x1_t vreinterpret_s64_p16(poly16x4_t __p0) {
  int64x1_t __ret;
  __ret = (int64x1_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x1_t vreinterpret_s64_u8(uint8x8_t __p0) {
  int64x1_t __ret;
  __ret = (int64x1_t)(__p0);
  return __ret;
}
#else
__ai int64x1_t vreinterpret_s64_u8(uint8x8_t __p0) {
  int64x1_t __ret;
  __ret = (int64x1_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x1_t vreinterpret_s64_u32(uint32x2_t __p0) {
  int64x1_t __ret;
  __ret = (int64x1_t)(__p0);
  return __ret;
}
#else
__ai int64x1_t vreinterpret_s64_u32(uint32x2_t __p0) {
  int64x1_t __ret;
  __ret = (int64x1_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x1_t vreinterpret_s64_u64(uint64x1_t __p0) {
  int64x1_t __ret;
  __ret = (int64x1_t)(__p0);
  return __ret;
}
#else
__ai int64x1_t vreinterpret_s64_u64(uint64x1_t __p0) {
  int64x1_t __ret;
  __ret = (int64x1_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x1_t vreinterpret_s64_u16(uint16x4_t __p0) {
  int64x1_t __ret;
  __ret = (int64x1_t)(__p0);
  return __ret;
}
#else
__ai int64x1_t vreinterpret_s64_u16(uint16x4_t __p0) {
  int64x1_t __ret;
  __ret = (int64x1_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x1_t vreinterpret_s64_s8(int8x8_t __p0) {
  int64x1_t __ret;
  __ret = (int64x1_t)(__p0);
  return __ret;
}
#else
__ai int64x1_t vreinterpret_s64_s8(int8x8_t __p0) {
  int64x1_t __ret;
  __ret = (int64x1_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x1_t vreinterpret_s64_f32(float32x2_t __p0) {
  int64x1_t __ret;
  __ret = (int64x1_t)(__p0);
  return __ret;
}
#else
__ai int64x1_t vreinterpret_s64_f32(float32x2_t __p0) {
  int64x1_t __ret;
  __ret = (int64x1_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x1_t vreinterpret_s64_f16(float16x4_t __p0) {
  int64x1_t __ret;
  __ret = (int64x1_t)(__p0);
  return __ret;
}
#else
__ai int64x1_t vreinterpret_s64_f16(float16x4_t __p0) {
  int64x1_t __ret;
  __ret = (int64x1_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x1_t vreinterpret_s64_s32(int32x2_t __p0) {
  int64x1_t __ret;
  __ret = (int64x1_t)(__p0);
  return __ret;
}
#else
__ai int64x1_t vreinterpret_s64_s32(int32x2_t __p0) {
  int64x1_t __ret;
  __ret = (int64x1_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x1_t vreinterpret_s64_s16(int16x4_t __p0) {
  int64x1_t __ret;
  __ret = (int64x1_t)(__p0);
  return __ret;
}
#else
__ai int64x1_t vreinterpret_s64_s16(int16x4_t __p0) {
  int64x1_t __ret;
  __ret = (int64x1_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x4_t vreinterpret_s16_p8(poly8x8_t __p0) {
  int16x4_t __ret;
  __ret = (int16x4_t)(__p0);
  return __ret;
}
#else
__ai int16x4_t vreinterpret_s16_p8(poly8x8_t __p0) {
  int16x4_t __ret;
  __ret = (int16x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x4_t vreinterpret_s16_p16(poly16x4_t __p0) {
  int16x4_t __ret;
  __ret = (int16x4_t)(__p0);
  return __ret;
}
#else
__ai int16x4_t vreinterpret_s16_p16(poly16x4_t __p0) {
  int16x4_t __ret;
  __ret = (int16x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x4_t vreinterpret_s16_u8(uint8x8_t __p0) {
  int16x4_t __ret;
  __ret = (int16x4_t)(__p0);
  return __ret;
}
#else
__ai int16x4_t vreinterpret_s16_u8(uint8x8_t __p0) {
  int16x4_t __ret;
  __ret = (int16x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x4_t vreinterpret_s16_u32(uint32x2_t __p0) {
  int16x4_t __ret;
  __ret = (int16x4_t)(__p0);
  return __ret;
}
#else
__ai int16x4_t vreinterpret_s16_u32(uint32x2_t __p0) {
  int16x4_t __ret;
  __ret = (int16x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x4_t vreinterpret_s16_u64(uint64x1_t __p0) {
  int16x4_t __ret;
  __ret = (int16x4_t)(__p0);
  return __ret;
}
#else
__ai int16x4_t vreinterpret_s16_u64(uint64x1_t __p0) {
  int16x4_t __ret;
  __ret = (int16x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x4_t vreinterpret_s16_u16(uint16x4_t __p0) {
  int16x4_t __ret;
  __ret = (int16x4_t)(__p0);
  return __ret;
}
#else
__ai int16x4_t vreinterpret_s16_u16(uint16x4_t __p0) {
  int16x4_t __ret;
  __ret = (int16x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x4_t vreinterpret_s16_s8(int8x8_t __p0) {
  int16x4_t __ret;
  __ret = (int16x4_t)(__p0);
  return __ret;
}
#else
__ai int16x4_t vreinterpret_s16_s8(int8x8_t __p0) {
  int16x4_t __ret;
  __ret = (int16x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x4_t vreinterpret_s16_f32(float32x2_t __p0) {
  int16x4_t __ret;
  __ret = (int16x4_t)(__p0);
  return __ret;
}
#else
__ai int16x4_t vreinterpret_s16_f32(float32x2_t __p0) {
  int16x4_t __ret;
  __ret = (int16x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x4_t vreinterpret_s16_f16(float16x4_t __p0) {
  int16x4_t __ret;
  __ret = (int16x4_t)(__p0);
  return __ret;
}
#else
__ai int16x4_t vreinterpret_s16_f16(float16x4_t __p0) {
  int16x4_t __ret;
  __ret = (int16x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x4_t vreinterpret_s16_s32(int32x2_t __p0) {
  int16x4_t __ret;
  __ret = (int16x4_t)(__p0);
  return __ret;
}
#else
__ai int16x4_t vreinterpret_s16_s32(int32x2_t __p0) {
  int16x4_t __ret;
  __ret = (int16x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x4_t vreinterpret_s16_s64(int64x1_t __p0) {
  int16x4_t __ret;
  __ret = (int16x4_t)(__p0);
  return __ret;
}
#else
__ai int16x4_t vreinterpret_s16_s64(int64x1_t __p0) {
  int16x4_t __ret;
  __ret = (int16x4_t)(__p0);
  return __ret;
}
#endif

#endif
#if __ARM_ARCH >= 8
#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vcvtaq_s32_f32(float32x4_t __p0) {
  int32x4_t __ret;
  __ret = (int32x4_t) __builtin_neon_vcvtaq_s32_v((int8x16_t)__p0, 34);
  return __ret;
}
#else
__ai int32x4_t vcvtaq_s32_f32(float32x4_t __p0) {
  float32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int32x4_t __ret;
  __ret = (int32x4_t) __builtin_neon_vcvtaq_s32_v((int8x16_t)__rev0, 34);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x2_t vcvta_s32_f32(float32x2_t __p0) {
  int32x2_t __ret;
  __ret = (int32x2_t) __builtin_neon_vcvta_s32_v((int8x8_t)__p0, 2);
  return __ret;
}
#else
__ai int32x2_t vcvta_s32_f32(float32x2_t __p0) {
  float32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int32x2_t __ret;
  __ret = (int32x2_t) __builtin_neon_vcvta_s32_v((int8x8_t)__rev0, 2);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vcvtaq_u32_f32(float32x4_t __p0) {
  uint32x4_t __ret;
  __ret = (uint32x4_t) __builtin_neon_vcvtaq_u32_v((int8x16_t)__p0, 50);
  return __ret;
}
#else
__ai uint32x4_t vcvtaq_u32_f32(float32x4_t __p0) {
  float32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint32x4_t __ret;
  __ret = (uint32x4_t) __builtin_neon_vcvtaq_u32_v((int8x16_t)__rev0, 50);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x2_t vcvta_u32_f32(float32x2_t __p0) {
  uint32x2_t __ret;
  __ret = (uint32x2_t) __builtin_neon_vcvta_u32_v((int8x8_t)__p0, 18);
  return __ret;
}
#else
__ai uint32x2_t vcvta_u32_f32(float32x2_t __p0) {
  float32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint32x2_t __ret;
  __ret = (uint32x2_t) __builtin_neon_vcvta_u32_v((int8x8_t)__rev0, 18);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vcvtmq_s32_f32(float32x4_t __p0) {
  int32x4_t __ret;
  __ret = (int32x4_t) __builtin_neon_vcvtmq_s32_v((int8x16_t)__p0, 34);
  return __ret;
}
#else
__ai int32x4_t vcvtmq_s32_f32(float32x4_t __p0) {
  float32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int32x4_t __ret;
  __ret = (int32x4_t) __builtin_neon_vcvtmq_s32_v((int8x16_t)__rev0, 34);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x2_t vcvtm_s32_f32(float32x2_t __p0) {
  int32x2_t __ret;
  __ret = (int32x2_t) __builtin_neon_vcvtm_s32_v((int8x8_t)__p0, 2);
  return __ret;
}
#else
__ai int32x2_t vcvtm_s32_f32(float32x2_t __p0) {
  float32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int32x2_t __ret;
  __ret = (int32x2_t) __builtin_neon_vcvtm_s32_v((int8x8_t)__rev0, 2);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vcvtmq_u32_f32(float32x4_t __p0) {
  uint32x4_t __ret;
  __ret = (uint32x4_t) __builtin_neon_vcvtmq_u32_v((int8x16_t)__p0, 50);
  return __ret;
}
#else
__ai uint32x4_t vcvtmq_u32_f32(float32x4_t __p0) {
  float32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint32x4_t __ret;
  __ret = (uint32x4_t) __builtin_neon_vcvtmq_u32_v((int8x16_t)__rev0, 50);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x2_t vcvtm_u32_f32(float32x2_t __p0) {
  uint32x2_t __ret;
  __ret = (uint32x2_t) __builtin_neon_vcvtm_u32_v((int8x8_t)__p0, 18);
  return __ret;
}
#else
__ai uint32x2_t vcvtm_u32_f32(float32x2_t __p0) {
  float32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint32x2_t __ret;
  __ret = (uint32x2_t) __builtin_neon_vcvtm_u32_v((int8x8_t)__rev0, 18);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vcvtnq_s32_f32(float32x4_t __p0) {
  int32x4_t __ret;
  __ret = (int32x4_t) __builtin_neon_vcvtnq_s32_v((int8x16_t)__p0, 34);
  return __ret;
}
#else
__ai int32x4_t vcvtnq_s32_f32(float32x4_t __p0) {
  float32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int32x4_t __ret;
  __ret = (int32x4_t) __builtin_neon_vcvtnq_s32_v((int8x16_t)__rev0, 34);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x2_t vcvtn_s32_f32(float32x2_t __p0) {
  int32x2_t __ret;
  __ret = (int32x2_t) __builtin_neon_vcvtn_s32_v((int8x8_t)__p0, 2);
  return __ret;
}
#else
__ai int32x2_t vcvtn_s32_f32(float32x2_t __p0) {
  float32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int32x2_t __ret;
  __ret = (int32x2_t) __builtin_neon_vcvtn_s32_v((int8x8_t)__rev0, 2);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vcvtnq_u32_f32(float32x4_t __p0) {
  uint32x4_t __ret;
  __ret = (uint32x4_t) __builtin_neon_vcvtnq_u32_v((int8x16_t)__p0, 50);
  return __ret;
}
#else
__ai uint32x4_t vcvtnq_u32_f32(float32x4_t __p0) {
  float32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint32x4_t __ret;
  __ret = (uint32x4_t) __builtin_neon_vcvtnq_u32_v((int8x16_t)__rev0, 50);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x2_t vcvtn_u32_f32(float32x2_t __p0) {
  uint32x2_t __ret;
  __ret = (uint32x2_t) __builtin_neon_vcvtn_u32_v((int8x8_t)__p0, 18);
  return __ret;
}
#else
__ai uint32x2_t vcvtn_u32_f32(float32x2_t __p0) {
  float32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint32x2_t __ret;
  __ret = (uint32x2_t) __builtin_neon_vcvtn_u32_v((int8x8_t)__rev0, 18);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vcvtpq_s32_f32(float32x4_t __p0) {
  int32x4_t __ret;
  __ret = (int32x4_t) __builtin_neon_vcvtpq_s32_v((int8x16_t)__p0, 34);
  return __ret;
}
#else
__ai int32x4_t vcvtpq_s32_f32(float32x4_t __p0) {
  float32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int32x4_t __ret;
  __ret = (int32x4_t) __builtin_neon_vcvtpq_s32_v((int8x16_t)__rev0, 34);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x2_t vcvtp_s32_f32(float32x2_t __p0) {
  int32x2_t __ret;
  __ret = (int32x2_t) __builtin_neon_vcvtp_s32_v((int8x8_t)__p0, 2);
  return __ret;
}
#else
__ai int32x2_t vcvtp_s32_f32(float32x2_t __p0) {
  float32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int32x2_t __ret;
  __ret = (int32x2_t) __builtin_neon_vcvtp_s32_v((int8x8_t)__rev0, 2);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vcvtpq_u32_f32(float32x4_t __p0) {
  uint32x4_t __ret;
  __ret = (uint32x4_t) __builtin_neon_vcvtpq_u32_v((int8x16_t)__p0, 50);
  return __ret;
}
#else
__ai uint32x4_t vcvtpq_u32_f32(float32x4_t __p0) {
  float32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint32x4_t __ret;
  __ret = (uint32x4_t) __builtin_neon_vcvtpq_u32_v((int8x16_t)__rev0, 50);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x2_t vcvtp_u32_f32(float32x2_t __p0) {
  uint32x2_t __ret;
  __ret = (uint32x2_t) __builtin_neon_vcvtp_u32_v((int8x8_t)__p0, 18);
  return __ret;
}
#else
__ai uint32x2_t vcvtp_u32_f32(float32x2_t __p0) {
  float32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint32x2_t __ret;
  __ret = (uint32x2_t) __builtin_neon_vcvtp_u32_v((int8x8_t)__rev0, 18);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#endif
#if __ARM_ARCH >= 8 && defined(__aarch64__)
#ifdef __LITTLE_ENDIAN__
__ai int64x2_t vcvtaq_s64_f64(float64x2_t __p0) {
  int64x2_t __ret;
  __ret = (int64x2_t) __builtin_neon_vcvtaq_s64_v((int8x16_t)__p0, 35);
  return __ret;
}
#else
__ai int64x2_t vcvtaq_s64_f64(float64x2_t __p0) {
  float64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int64x2_t __ret;
  __ret = (int64x2_t) __builtin_neon_vcvtaq_s64_v((int8x16_t)__rev0, 35);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x1_t vcvta_s64_f64(float64x1_t __p0) {
  int64x1_t __ret;
  __ret = (int64x1_t) __builtin_neon_vcvta_s64_v((int8x8_t)__p0, 3);
  return __ret;
}
#else
__ai int64x1_t vcvta_s64_f64(float64x1_t __p0) {
  int64x1_t __ret;
  __ret = (int64x1_t) __builtin_neon_vcvta_s64_v((int8x8_t)__p0, 3);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vcvtaq_u64_f64(float64x2_t __p0) {
  uint64x2_t __ret;
  __ret = (uint64x2_t) __builtin_neon_vcvtaq_u64_v((int8x16_t)__p0, 51);
  return __ret;
}
#else
__ai uint64x2_t vcvtaq_u64_f64(float64x2_t __p0) {
  float64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint64x2_t __ret;
  __ret = (uint64x2_t) __builtin_neon_vcvtaq_u64_v((int8x16_t)__rev0, 51);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x1_t vcvta_u64_f64(float64x1_t __p0) {
  uint64x1_t __ret;
  __ret = (uint64x1_t) __builtin_neon_vcvta_u64_v((int8x8_t)__p0, 19);
  return __ret;
}
#else
__ai uint64x1_t vcvta_u64_f64(float64x1_t __p0) {
  uint64x1_t __ret;
  __ret = (uint64x1_t) __builtin_neon_vcvta_u64_v((int8x8_t)__p0, 19);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x2_t vcvtmq_s64_f64(float64x2_t __p0) {
  int64x2_t __ret;
  __ret = (int64x2_t) __builtin_neon_vcvtmq_s64_v((int8x16_t)__p0, 35);
  return __ret;
}
#else
__ai int64x2_t vcvtmq_s64_f64(float64x2_t __p0) {
  float64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int64x2_t __ret;
  __ret = (int64x2_t) __builtin_neon_vcvtmq_s64_v((int8x16_t)__rev0, 35);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x1_t vcvtm_s64_f64(float64x1_t __p0) {
  int64x1_t __ret;
  __ret = (int64x1_t) __builtin_neon_vcvtm_s64_v((int8x8_t)__p0, 3);
  return __ret;
}
#else
__ai int64x1_t vcvtm_s64_f64(float64x1_t __p0) {
  int64x1_t __ret;
  __ret = (int64x1_t) __builtin_neon_vcvtm_s64_v((int8x8_t)__p0, 3);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vcvtmq_u64_f64(float64x2_t __p0) {
  uint64x2_t __ret;
  __ret = (uint64x2_t) __builtin_neon_vcvtmq_u64_v((int8x16_t)__p0, 51);
  return __ret;
}
#else
__ai uint64x2_t vcvtmq_u64_f64(float64x2_t __p0) {
  float64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint64x2_t __ret;
  __ret = (uint64x2_t) __builtin_neon_vcvtmq_u64_v((int8x16_t)__rev0, 51);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x1_t vcvtm_u64_f64(float64x1_t __p0) {
  uint64x1_t __ret;
  __ret = (uint64x1_t) __builtin_neon_vcvtm_u64_v((int8x8_t)__p0, 19);
  return __ret;
}
#else
__ai uint64x1_t vcvtm_u64_f64(float64x1_t __p0) {
  uint64x1_t __ret;
  __ret = (uint64x1_t) __builtin_neon_vcvtm_u64_v((int8x8_t)__p0, 19);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x2_t vcvtnq_s64_f64(float64x2_t __p0) {
  int64x2_t __ret;
  __ret = (int64x2_t) __builtin_neon_vcvtnq_s64_v((int8x16_t)__p0, 35);
  return __ret;
}
#else
__ai int64x2_t vcvtnq_s64_f64(float64x2_t __p0) {
  float64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int64x2_t __ret;
  __ret = (int64x2_t) __builtin_neon_vcvtnq_s64_v((int8x16_t)__rev0, 35);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x1_t vcvtn_s64_f64(float64x1_t __p0) {
  int64x1_t __ret;
  __ret = (int64x1_t) __builtin_neon_vcvtn_s64_v((int8x8_t)__p0, 3);
  return __ret;
}
#else
__ai int64x1_t vcvtn_s64_f64(float64x1_t __p0) {
  int64x1_t __ret;
  __ret = (int64x1_t) __builtin_neon_vcvtn_s64_v((int8x8_t)__p0, 3);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vcvtnq_u64_f64(float64x2_t __p0) {
  uint64x2_t __ret;
  __ret = (uint64x2_t) __builtin_neon_vcvtnq_u64_v((int8x16_t)__p0, 51);
  return __ret;
}
#else
__ai uint64x2_t vcvtnq_u64_f64(float64x2_t __p0) {
  float64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint64x2_t __ret;
  __ret = (uint64x2_t) __builtin_neon_vcvtnq_u64_v((int8x16_t)__rev0, 51);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x1_t vcvtn_u64_f64(float64x1_t __p0) {
  uint64x1_t __ret;
  __ret = (uint64x1_t) __builtin_neon_vcvtn_u64_v((int8x8_t)__p0, 19);
  return __ret;
}
#else
__ai uint64x1_t vcvtn_u64_f64(float64x1_t __p0) {
  uint64x1_t __ret;
  __ret = (uint64x1_t) __builtin_neon_vcvtn_u64_v((int8x8_t)__p0, 19);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x2_t vcvtpq_s64_f64(float64x2_t __p0) {
  int64x2_t __ret;
  __ret = (int64x2_t) __builtin_neon_vcvtpq_s64_v((int8x16_t)__p0, 35);
  return __ret;
}
#else
__ai int64x2_t vcvtpq_s64_f64(float64x2_t __p0) {
  float64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int64x2_t __ret;
  __ret = (int64x2_t) __builtin_neon_vcvtpq_s64_v((int8x16_t)__rev0, 35);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x1_t vcvtp_s64_f64(float64x1_t __p0) {
  int64x1_t __ret;
  __ret = (int64x1_t) __builtin_neon_vcvtp_s64_v((int8x8_t)__p0, 3);
  return __ret;
}
#else
__ai int64x1_t vcvtp_s64_f64(float64x1_t __p0) {
  int64x1_t __ret;
  __ret = (int64x1_t) __builtin_neon_vcvtp_s64_v((int8x8_t)__p0, 3);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vcvtpq_u64_f64(float64x2_t __p0) {
  uint64x2_t __ret;
  __ret = (uint64x2_t) __builtin_neon_vcvtpq_u64_v((int8x16_t)__p0, 51);
  return __ret;
}
#else
__ai uint64x2_t vcvtpq_u64_f64(float64x2_t __p0) {
  float64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint64x2_t __ret;
  __ret = (uint64x2_t) __builtin_neon_vcvtpq_u64_v((int8x16_t)__rev0, 51);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x1_t vcvtp_u64_f64(float64x1_t __p0) {
  uint64x1_t __ret;
  __ret = (uint64x1_t) __builtin_neon_vcvtp_u64_v((int8x8_t)__p0, 19);
  return __ret;
}
#else
__ai uint64x1_t vcvtp_u64_f64(float64x1_t __p0) {
  uint64x1_t __ret;
  __ret = (uint64x1_t) __builtin_neon_vcvtp_u64_v((int8x8_t)__p0, 19);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x8_t vreinterpret_p8_p64(poly64x1_t __p0) {
  poly8x8_t __ret;
  __ret = (poly8x8_t)(__p0);
  return __ret;
}
#else
__ai poly8x8_t vreinterpret_p8_p64(poly64x1_t __p0) {
  poly8x8_t __ret;
  __ret = (poly8x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x8_t vreinterpret_p8_p16(poly16x4_t __p0) {
  poly8x8_t __ret;
  __ret = (poly8x8_t)(__p0);
  return __ret;
}
#else
__ai poly8x8_t vreinterpret_p8_p16(poly16x4_t __p0) {
  poly8x8_t __ret;
  __ret = (poly8x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x8_t vreinterpret_p8_u8(uint8x8_t __p0) {
  poly8x8_t __ret;
  __ret = (poly8x8_t)(__p0);
  return __ret;
}
#else
__ai poly8x8_t vreinterpret_p8_u8(uint8x8_t __p0) {
  poly8x8_t __ret;
  __ret = (poly8x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x8_t vreinterpret_p8_u32(uint32x2_t __p0) {
  poly8x8_t __ret;
  __ret = (poly8x8_t)(__p0);
  return __ret;
}
#else
__ai poly8x8_t vreinterpret_p8_u32(uint32x2_t __p0) {
  poly8x8_t __ret;
  __ret = (poly8x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x8_t vreinterpret_p8_u64(uint64x1_t __p0) {
  poly8x8_t __ret;
  __ret = (poly8x8_t)(__p0);
  return __ret;
}
#else
__ai poly8x8_t vreinterpret_p8_u64(uint64x1_t __p0) {
  poly8x8_t __ret;
  __ret = (poly8x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x8_t vreinterpret_p8_u16(uint16x4_t __p0) {
  poly8x8_t __ret;
  __ret = (poly8x8_t)(__p0);
  return __ret;
}
#else
__ai poly8x8_t vreinterpret_p8_u16(uint16x4_t __p0) {
  poly8x8_t __ret;
  __ret = (poly8x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x8_t vreinterpret_p8_s8(int8x8_t __p0) {
  poly8x8_t __ret;
  __ret = (poly8x8_t)(__p0);
  return __ret;
}
#else
__ai poly8x8_t vreinterpret_p8_s8(int8x8_t __p0) {
  poly8x8_t __ret;
  __ret = (poly8x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x8_t vreinterpret_p8_f64(float64x1_t __p0) {
  poly8x8_t __ret;
  __ret = (poly8x8_t)(__p0);
  return __ret;
}
#else
__ai poly8x8_t vreinterpret_p8_f64(float64x1_t __p0) {
  poly8x8_t __ret;
  __ret = (poly8x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x8_t vreinterpret_p8_f32(float32x2_t __p0) {
  poly8x8_t __ret;
  __ret = (poly8x8_t)(__p0);
  return __ret;
}
#else
__ai poly8x8_t vreinterpret_p8_f32(float32x2_t __p0) {
  poly8x8_t __ret;
  __ret = (poly8x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x8_t vreinterpret_p8_f16(float16x4_t __p0) {
  poly8x8_t __ret;
  __ret = (poly8x8_t)(__p0);
  return __ret;
}
#else
__ai poly8x8_t vreinterpret_p8_f16(float16x4_t __p0) {
  poly8x8_t __ret;
  __ret = (poly8x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x8_t vreinterpret_p8_s32(int32x2_t __p0) {
  poly8x8_t __ret;
  __ret = (poly8x8_t)(__p0);
  return __ret;
}
#else
__ai poly8x8_t vreinterpret_p8_s32(int32x2_t __p0) {
  poly8x8_t __ret;
  __ret = (poly8x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x8_t vreinterpret_p8_s64(int64x1_t __p0) {
  poly8x8_t __ret;
  __ret = (poly8x8_t)(__p0);
  return __ret;
}
#else
__ai poly8x8_t vreinterpret_p8_s64(int64x1_t __p0) {
  poly8x8_t __ret;
  __ret = (poly8x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x8_t vreinterpret_p8_s16(int16x4_t __p0) {
  poly8x8_t __ret;
  __ret = (poly8x8_t)(__p0);
  return __ret;
}
#else
__ai poly8x8_t vreinterpret_p8_s16(int16x4_t __p0) {
  poly8x8_t __ret;
  __ret = (poly8x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly64x1_t vreinterpret_p64_p8(poly8x8_t __p0) {
  poly64x1_t __ret;
  __ret = (poly64x1_t)(__p0);
  return __ret;
}
#else
__ai poly64x1_t vreinterpret_p64_p8(poly8x8_t __p0) {
  poly64x1_t __ret;
  __ret = (poly64x1_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly64x1_t vreinterpret_p64_p16(poly16x4_t __p0) {
  poly64x1_t __ret;
  __ret = (poly64x1_t)(__p0);
  return __ret;
}
#else
__ai poly64x1_t vreinterpret_p64_p16(poly16x4_t __p0) {
  poly64x1_t __ret;
  __ret = (poly64x1_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly64x1_t vreinterpret_p64_u8(uint8x8_t __p0) {
  poly64x1_t __ret;
  __ret = (poly64x1_t)(__p0);
  return __ret;
}
#else
__ai poly64x1_t vreinterpret_p64_u8(uint8x8_t __p0) {
  poly64x1_t __ret;
  __ret = (poly64x1_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly64x1_t vreinterpret_p64_u32(uint32x2_t __p0) {
  poly64x1_t __ret;
  __ret = (poly64x1_t)(__p0);
  return __ret;
}
#else
__ai poly64x1_t vreinterpret_p64_u32(uint32x2_t __p0) {
  poly64x1_t __ret;
  __ret = (poly64x1_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly64x1_t vreinterpret_p64_u64(uint64x1_t __p0) {
  poly64x1_t __ret;
  __ret = (poly64x1_t)(__p0);
  return __ret;
}
#else
__ai poly64x1_t vreinterpret_p64_u64(uint64x1_t __p0) {
  poly64x1_t __ret;
  __ret = (poly64x1_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly64x1_t vreinterpret_p64_u16(uint16x4_t __p0) {
  poly64x1_t __ret;
  __ret = (poly64x1_t)(__p0);
  return __ret;
}
#else
__ai poly64x1_t vreinterpret_p64_u16(uint16x4_t __p0) {
  poly64x1_t __ret;
  __ret = (poly64x1_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly64x1_t vreinterpret_p64_s8(int8x8_t __p0) {
  poly64x1_t __ret;
  __ret = (poly64x1_t)(__p0);
  return __ret;
}
#else
__ai poly64x1_t vreinterpret_p64_s8(int8x8_t __p0) {
  poly64x1_t __ret;
  __ret = (poly64x1_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly64x1_t vreinterpret_p64_f64(float64x1_t __p0) {
  poly64x1_t __ret;
  __ret = (poly64x1_t)(__p0);
  return __ret;
}
#else
__ai poly64x1_t vreinterpret_p64_f64(float64x1_t __p0) {
  poly64x1_t __ret;
  __ret = (poly64x1_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly64x1_t vreinterpret_p64_f32(float32x2_t __p0) {
  poly64x1_t __ret;
  __ret = (poly64x1_t)(__p0);
  return __ret;
}
#else
__ai poly64x1_t vreinterpret_p64_f32(float32x2_t __p0) {
  poly64x1_t __ret;
  __ret = (poly64x1_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly64x1_t vreinterpret_p64_f16(float16x4_t __p0) {
  poly64x1_t __ret;
  __ret = (poly64x1_t)(__p0);
  return __ret;
}
#else
__ai poly64x1_t vreinterpret_p64_f16(float16x4_t __p0) {
  poly64x1_t __ret;
  __ret = (poly64x1_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly64x1_t vreinterpret_p64_s32(int32x2_t __p0) {
  poly64x1_t __ret;
  __ret = (poly64x1_t)(__p0);
  return __ret;
}
#else
__ai poly64x1_t vreinterpret_p64_s32(int32x2_t __p0) {
  poly64x1_t __ret;
  __ret = (poly64x1_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly64x1_t vreinterpret_p64_s64(int64x1_t __p0) {
  poly64x1_t __ret;
  __ret = (poly64x1_t)(__p0);
  return __ret;
}
#else
__ai poly64x1_t vreinterpret_p64_s64(int64x1_t __p0) {
  poly64x1_t __ret;
  __ret = (poly64x1_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly64x1_t vreinterpret_p64_s16(int16x4_t __p0) {
  poly64x1_t __ret;
  __ret = (poly64x1_t)(__p0);
  return __ret;
}
#else
__ai poly64x1_t vreinterpret_p64_s16(int16x4_t __p0) {
  poly64x1_t __ret;
  __ret = (poly64x1_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly16x4_t vreinterpret_p16_p8(poly8x8_t __p0) {
  poly16x4_t __ret;
  __ret = (poly16x4_t)(__p0);
  return __ret;
}
#else
__ai poly16x4_t vreinterpret_p16_p8(poly8x8_t __p0) {
  poly16x4_t __ret;
  __ret = (poly16x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly16x4_t vreinterpret_p16_p64(poly64x1_t __p0) {
  poly16x4_t __ret;
  __ret = (poly16x4_t)(__p0);
  return __ret;
}
#else
__ai poly16x4_t vreinterpret_p16_p64(poly64x1_t __p0) {
  poly16x4_t __ret;
  __ret = (poly16x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly16x4_t vreinterpret_p16_u8(uint8x8_t __p0) {
  poly16x4_t __ret;
  __ret = (poly16x4_t)(__p0);
  return __ret;
}
#else
__ai poly16x4_t vreinterpret_p16_u8(uint8x8_t __p0) {
  poly16x4_t __ret;
  __ret = (poly16x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly16x4_t vreinterpret_p16_u32(uint32x2_t __p0) {
  poly16x4_t __ret;
  __ret = (poly16x4_t)(__p0);
  return __ret;
}
#else
__ai poly16x4_t vreinterpret_p16_u32(uint32x2_t __p0) {
  poly16x4_t __ret;
  __ret = (poly16x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly16x4_t vreinterpret_p16_u64(uint64x1_t __p0) {
  poly16x4_t __ret;
  __ret = (poly16x4_t)(__p0);
  return __ret;
}
#else
__ai poly16x4_t vreinterpret_p16_u64(uint64x1_t __p0) {
  poly16x4_t __ret;
  __ret = (poly16x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly16x4_t vreinterpret_p16_u16(uint16x4_t __p0) {
  poly16x4_t __ret;
  __ret = (poly16x4_t)(__p0);
  return __ret;
}
#else
__ai poly16x4_t vreinterpret_p16_u16(uint16x4_t __p0) {
  poly16x4_t __ret;
  __ret = (poly16x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly16x4_t vreinterpret_p16_s8(int8x8_t __p0) {
  poly16x4_t __ret;
  __ret = (poly16x4_t)(__p0);
  return __ret;
}
#else
__ai poly16x4_t vreinterpret_p16_s8(int8x8_t __p0) {
  poly16x4_t __ret;
  __ret = (poly16x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly16x4_t vreinterpret_p16_f64(float64x1_t __p0) {
  poly16x4_t __ret;
  __ret = (poly16x4_t)(__p0);
  return __ret;
}
#else
__ai poly16x4_t vreinterpret_p16_f64(float64x1_t __p0) {
  poly16x4_t __ret;
  __ret = (poly16x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly16x4_t vreinterpret_p16_f32(float32x2_t __p0) {
  poly16x4_t __ret;
  __ret = (poly16x4_t)(__p0);
  return __ret;
}
#else
__ai poly16x4_t vreinterpret_p16_f32(float32x2_t __p0) {
  poly16x4_t __ret;
  __ret = (poly16x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly16x4_t vreinterpret_p16_f16(float16x4_t __p0) {
  poly16x4_t __ret;
  __ret = (poly16x4_t)(__p0);
  return __ret;
}
#else
__ai poly16x4_t vreinterpret_p16_f16(float16x4_t __p0) {
  poly16x4_t __ret;
  __ret = (poly16x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly16x4_t vreinterpret_p16_s32(int32x2_t __p0) {
  poly16x4_t __ret;
  __ret = (poly16x4_t)(__p0);
  return __ret;
}
#else
__ai poly16x4_t vreinterpret_p16_s32(int32x2_t __p0) {
  poly16x4_t __ret;
  __ret = (poly16x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly16x4_t vreinterpret_p16_s64(int64x1_t __p0) {
  poly16x4_t __ret;
  __ret = (poly16x4_t)(__p0);
  return __ret;
}
#else
__ai poly16x4_t vreinterpret_p16_s64(int64x1_t __p0) {
  poly16x4_t __ret;
  __ret = (poly16x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly16x4_t vreinterpret_p16_s16(int16x4_t __p0) {
  poly16x4_t __ret;
  __ret = (poly16x4_t)(__p0);
  return __ret;
}
#else
__ai poly16x4_t vreinterpret_p16_s16(int16x4_t __p0) {
  poly16x4_t __ret;
  __ret = (poly16x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x16_t vreinterpretq_p8_p128(poly128_t __p0) {
  poly8x16_t __ret;
  __ret = (poly8x16_t)(__p0);
  return __ret;
}
#else
__ai poly8x16_t vreinterpretq_p8_p128(poly128_t __p0) {
  poly8x16_t __ret;
  __ret = (poly8x16_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x16_t vreinterpretq_p8_p64(poly64x2_t __p0) {
  poly8x16_t __ret;
  __ret = (poly8x16_t)(__p0);
  return __ret;
}
#else
__ai poly8x16_t vreinterpretq_p8_p64(poly64x2_t __p0) {
  poly8x16_t __ret;
  __ret = (poly8x16_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x16_t vreinterpretq_p8_p16(poly16x8_t __p0) {
  poly8x16_t __ret;
  __ret = (poly8x16_t)(__p0);
  return __ret;
}
#else
__ai poly8x16_t vreinterpretq_p8_p16(poly16x8_t __p0) {
  poly8x16_t __ret;
  __ret = (poly8x16_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x16_t vreinterpretq_p8_u8(uint8x16_t __p0) {
  poly8x16_t __ret;
  __ret = (poly8x16_t)(__p0);
  return __ret;
}
#else
__ai poly8x16_t vreinterpretq_p8_u8(uint8x16_t __p0) {
  poly8x16_t __ret;
  __ret = (poly8x16_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x16_t vreinterpretq_p8_u32(uint32x4_t __p0) {
  poly8x16_t __ret;
  __ret = (poly8x16_t)(__p0);
  return __ret;
}
#else
__ai poly8x16_t vreinterpretq_p8_u32(uint32x4_t __p0) {
  poly8x16_t __ret;
  __ret = (poly8x16_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x16_t vreinterpretq_p8_u64(uint64x2_t __p0) {
  poly8x16_t __ret;
  __ret = (poly8x16_t)(__p0);
  return __ret;
}
#else
__ai poly8x16_t vreinterpretq_p8_u64(uint64x2_t __p0) {
  poly8x16_t __ret;
  __ret = (poly8x16_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x16_t vreinterpretq_p8_u16(uint16x8_t __p0) {
  poly8x16_t __ret;
  __ret = (poly8x16_t)(__p0);
  return __ret;
}
#else
__ai poly8x16_t vreinterpretq_p8_u16(uint16x8_t __p0) {
  poly8x16_t __ret;
  __ret = (poly8x16_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x16_t vreinterpretq_p8_s8(int8x16_t __p0) {
  poly8x16_t __ret;
  __ret = (poly8x16_t)(__p0);
  return __ret;
}
#else
__ai poly8x16_t vreinterpretq_p8_s8(int8x16_t __p0) {
  poly8x16_t __ret;
  __ret = (poly8x16_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x16_t vreinterpretq_p8_f64(float64x2_t __p0) {
  poly8x16_t __ret;
  __ret = (poly8x16_t)(__p0);
  return __ret;
}
#else
__ai poly8x16_t vreinterpretq_p8_f64(float64x2_t __p0) {
  poly8x16_t __ret;
  __ret = (poly8x16_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x16_t vreinterpretq_p8_f32(float32x4_t __p0) {
  poly8x16_t __ret;
  __ret = (poly8x16_t)(__p0);
  return __ret;
}
#else
__ai poly8x16_t vreinterpretq_p8_f32(float32x4_t __p0) {
  poly8x16_t __ret;
  __ret = (poly8x16_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x16_t vreinterpretq_p8_f16(float16x8_t __p0) {
  poly8x16_t __ret;
  __ret = (poly8x16_t)(__p0);
  return __ret;
}
#else
__ai poly8x16_t vreinterpretq_p8_f16(float16x8_t __p0) {
  poly8x16_t __ret;
  __ret = (poly8x16_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x16_t vreinterpretq_p8_s32(int32x4_t __p0) {
  poly8x16_t __ret;
  __ret = (poly8x16_t)(__p0);
  return __ret;
}
#else
__ai poly8x16_t vreinterpretq_p8_s32(int32x4_t __p0) {
  poly8x16_t __ret;
  __ret = (poly8x16_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x16_t vreinterpretq_p8_s64(int64x2_t __p0) {
  poly8x16_t __ret;
  __ret = (poly8x16_t)(__p0);
  return __ret;
}
#else
__ai poly8x16_t vreinterpretq_p8_s64(int64x2_t __p0) {
  poly8x16_t __ret;
  __ret = (poly8x16_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly8x16_t vreinterpretq_p8_s16(int16x8_t __p0) {
  poly8x16_t __ret;
  __ret = (poly8x16_t)(__p0);
  return __ret;
}
#else
__ai poly8x16_t vreinterpretq_p8_s16(int16x8_t __p0) {
  poly8x16_t __ret;
  __ret = (poly8x16_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly128_t vreinterpretq_p128_p8(poly8x16_t __p0) {
  poly128_t __ret;
  __ret = (poly128_t)(__p0);
  return __ret;
}
#else
__ai poly128_t vreinterpretq_p128_p8(poly8x16_t __p0) {
  poly128_t __ret;
  __ret = (poly128_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly128_t vreinterpretq_p128_p64(poly64x2_t __p0) {
  poly128_t __ret;
  __ret = (poly128_t)(__p0);
  return __ret;
}
#else
__ai poly128_t vreinterpretq_p128_p64(poly64x2_t __p0) {
  poly128_t __ret;
  __ret = (poly128_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly128_t vreinterpretq_p128_p16(poly16x8_t __p0) {
  poly128_t __ret;
  __ret = (poly128_t)(__p0);
  return __ret;
}
#else
__ai poly128_t vreinterpretq_p128_p16(poly16x8_t __p0) {
  poly128_t __ret;
  __ret = (poly128_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly128_t vreinterpretq_p128_u8(uint8x16_t __p0) {
  poly128_t __ret;
  __ret = (poly128_t)(__p0);
  return __ret;
}
#else
__ai poly128_t vreinterpretq_p128_u8(uint8x16_t __p0) {
  poly128_t __ret;
  __ret = (poly128_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly128_t vreinterpretq_p128_u32(uint32x4_t __p0) {
  poly128_t __ret;
  __ret = (poly128_t)(__p0);
  return __ret;
}
#else
__ai poly128_t vreinterpretq_p128_u32(uint32x4_t __p0) {
  poly128_t __ret;
  __ret = (poly128_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly128_t vreinterpretq_p128_u64(uint64x2_t __p0) {
  poly128_t __ret;
  __ret = (poly128_t)(__p0);
  return __ret;
}
#else
__ai poly128_t vreinterpretq_p128_u64(uint64x2_t __p0) {
  poly128_t __ret;
  __ret = (poly128_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly128_t vreinterpretq_p128_u16(uint16x8_t __p0) {
  poly128_t __ret;
  __ret = (poly128_t)(__p0);
  return __ret;
}
#else
__ai poly128_t vreinterpretq_p128_u16(uint16x8_t __p0) {
  poly128_t __ret;
  __ret = (poly128_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly128_t vreinterpretq_p128_s8(int8x16_t __p0) {
  poly128_t __ret;
  __ret = (poly128_t)(__p0);
  return __ret;
}
#else
__ai poly128_t vreinterpretq_p128_s8(int8x16_t __p0) {
  poly128_t __ret;
  __ret = (poly128_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly128_t vreinterpretq_p128_f64(float64x2_t __p0) {
  poly128_t __ret;
  __ret = (poly128_t)(__p0);
  return __ret;
}
#else
__ai poly128_t vreinterpretq_p128_f64(float64x2_t __p0) {
  poly128_t __ret;
  __ret = (poly128_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly128_t vreinterpretq_p128_f32(float32x4_t __p0) {
  poly128_t __ret;
  __ret = (poly128_t)(__p0);
  return __ret;
}
#else
__ai poly128_t vreinterpretq_p128_f32(float32x4_t __p0) {
  poly128_t __ret;
  __ret = (poly128_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly128_t vreinterpretq_p128_f16(float16x8_t __p0) {
  poly128_t __ret;
  __ret = (poly128_t)(__p0);
  return __ret;
}
#else
__ai poly128_t vreinterpretq_p128_f16(float16x8_t __p0) {
  poly128_t __ret;
  __ret = (poly128_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly128_t vreinterpretq_p128_s32(int32x4_t __p0) {
  poly128_t __ret;
  __ret = (poly128_t)(__p0);
  return __ret;
}
#else
__ai poly128_t vreinterpretq_p128_s32(int32x4_t __p0) {
  poly128_t __ret;
  __ret = (poly128_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly128_t vreinterpretq_p128_s64(int64x2_t __p0) {
  poly128_t __ret;
  __ret = (poly128_t)(__p0);
  return __ret;
}
#else
__ai poly128_t vreinterpretq_p128_s64(int64x2_t __p0) {
  poly128_t __ret;
  __ret = (poly128_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly128_t vreinterpretq_p128_s16(int16x8_t __p0) {
  poly128_t __ret;
  __ret = (poly128_t)(__p0);
  return __ret;
}
#else
__ai poly128_t vreinterpretq_p128_s16(int16x8_t __p0) {
  poly128_t __ret;
  __ret = (poly128_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly64x2_t vreinterpretq_p64_p8(poly8x16_t __p0) {
  poly64x2_t __ret;
  __ret = (poly64x2_t)(__p0);
  return __ret;
}
#else
__ai poly64x2_t vreinterpretq_p64_p8(poly8x16_t __p0) {
  poly64x2_t __ret;
  __ret = (poly64x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly64x2_t vreinterpretq_p64_p128(poly128_t __p0) {
  poly64x2_t __ret;
  __ret = (poly64x2_t)(__p0);
  return __ret;
}
#else
__ai poly64x2_t vreinterpretq_p64_p128(poly128_t __p0) {
  poly64x2_t __ret;
  __ret = (poly64x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly64x2_t vreinterpretq_p64_p16(poly16x8_t __p0) {
  poly64x2_t __ret;
  __ret = (poly64x2_t)(__p0);
  return __ret;
}
#else
__ai poly64x2_t vreinterpretq_p64_p16(poly16x8_t __p0) {
  poly64x2_t __ret;
  __ret = (poly64x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly64x2_t vreinterpretq_p64_u8(uint8x16_t __p0) {
  poly64x2_t __ret;
  __ret = (poly64x2_t)(__p0);
  return __ret;
}
#else
__ai poly64x2_t vreinterpretq_p64_u8(uint8x16_t __p0) {
  poly64x2_t __ret;
  __ret = (poly64x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly64x2_t vreinterpretq_p64_u32(uint32x4_t __p0) {
  poly64x2_t __ret;
  __ret = (poly64x2_t)(__p0);
  return __ret;
}
#else
__ai poly64x2_t vreinterpretq_p64_u32(uint32x4_t __p0) {
  poly64x2_t __ret;
  __ret = (poly64x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly64x2_t vreinterpretq_p64_u64(uint64x2_t __p0) {
  poly64x2_t __ret;
  __ret = (poly64x2_t)(__p0);
  return __ret;
}
#else
__ai poly64x2_t vreinterpretq_p64_u64(uint64x2_t __p0) {
  poly64x2_t __ret;
  __ret = (poly64x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly64x2_t vreinterpretq_p64_u16(uint16x8_t __p0) {
  poly64x2_t __ret;
  __ret = (poly64x2_t)(__p0);
  return __ret;
}
#else
__ai poly64x2_t vreinterpretq_p64_u16(uint16x8_t __p0) {
  poly64x2_t __ret;
  __ret = (poly64x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly64x2_t vreinterpretq_p64_s8(int8x16_t __p0) {
  poly64x2_t __ret;
  __ret = (poly64x2_t)(__p0);
  return __ret;
}
#else
__ai poly64x2_t vreinterpretq_p64_s8(int8x16_t __p0) {
  poly64x2_t __ret;
  __ret = (poly64x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly64x2_t vreinterpretq_p64_f64(float64x2_t __p0) {
  poly64x2_t __ret;
  __ret = (poly64x2_t)(__p0);
  return __ret;
}
#else
__ai poly64x2_t vreinterpretq_p64_f64(float64x2_t __p0) {
  poly64x2_t __ret;
  __ret = (poly64x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly64x2_t vreinterpretq_p64_f32(float32x4_t __p0) {
  poly64x2_t __ret;
  __ret = (poly64x2_t)(__p0);
  return __ret;
}
#else
__ai poly64x2_t vreinterpretq_p64_f32(float32x4_t __p0) {
  poly64x2_t __ret;
  __ret = (poly64x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly64x2_t vreinterpretq_p64_f16(float16x8_t __p0) {
  poly64x2_t __ret;
  __ret = (poly64x2_t)(__p0);
  return __ret;
}
#else
__ai poly64x2_t vreinterpretq_p64_f16(float16x8_t __p0) {
  poly64x2_t __ret;
  __ret = (poly64x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly64x2_t vreinterpretq_p64_s32(int32x4_t __p0) {
  poly64x2_t __ret;
  __ret = (poly64x2_t)(__p0);
  return __ret;
}
#else
__ai poly64x2_t vreinterpretq_p64_s32(int32x4_t __p0) {
  poly64x2_t __ret;
  __ret = (poly64x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly64x2_t vreinterpretq_p64_s64(int64x2_t __p0) {
  poly64x2_t __ret;
  __ret = (poly64x2_t)(__p0);
  return __ret;
}
#else
__ai poly64x2_t vreinterpretq_p64_s64(int64x2_t __p0) {
  poly64x2_t __ret;
  __ret = (poly64x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly64x2_t vreinterpretq_p64_s16(int16x8_t __p0) {
  poly64x2_t __ret;
  __ret = (poly64x2_t)(__p0);
  return __ret;
}
#else
__ai poly64x2_t vreinterpretq_p64_s16(int16x8_t __p0) {
  poly64x2_t __ret;
  __ret = (poly64x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly16x8_t vreinterpretq_p16_p8(poly8x16_t __p0) {
  poly16x8_t __ret;
  __ret = (poly16x8_t)(__p0);
  return __ret;
}
#else
__ai poly16x8_t vreinterpretq_p16_p8(poly8x16_t __p0) {
  poly16x8_t __ret;
  __ret = (poly16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly16x8_t vreinterpretq_p16_p128(poly128_t __p0) {
  poly16x8_t __ret;
  __ret = (poly16x8_t)(__p0);
  return __ret;
}
#else
__ai poly16x8_t vreinterpretq_p16_p128(poly128_t __p0) {
  poly16x8_t __ret;
  __ret = (poly16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly16x8_t vreinterpretq_p16_p64(poly64x2_t __p0) {
  poly16x8_t __ret;
  __ret = (poly16x8_t)(__p0);
  return __ret;
}
#else
__ai poly16x8_t vreinterpretq_p16_p64(poly64x2_t __p0) {
  poly16x8_t __ret;
  __ret = (poly16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly16x8_t vreinterpretq_p16_u8(uint8x16_t __p0) {
  poly16x8_t __ret;
  __ret = (poly16x8_t)(__p0);
  return __ret;
}
#else
__ai poly16x8_t vreinterpretq_p16_u8(uint8x16_t __p0) {
  poly16x8_t __ret;
  __ret = (poly16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly16x8_t vreinterpretq_p16_u32(uint32x4_t __p0) {
  poly16x8_t __ret;
  __ret = (poly16x8_t)(__p0);
  return __ret;
}
#else
__ai poly16x8_t vreinterpretq_p16_u32(uint32x4_t __p0) {
  poly16x8_t __ret;
  __ret = (poly16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly16x8_t vreinterpretq_p16_u64(uint64x2_t __p0) {
  poly16x8_t __ret;
  __ret = (poly16x8_t)(__p0);
  return __ret;
}
#else
__ai poly16x8_t vreinterpretq_p16_u64(uint64x2_t __p0) {
  poly16x8_t __ret;
  __ret = (poly16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly16x8_t vreinterpretq_p16_u16(uint16x8_t __p0) {
  poly16x8_t __ret;
  __ret = (poly16x8_t)(__p0);
  return __ret;
}
#else
__ai poly16x8_t vreinterpretq_p16_u16(uint16x8_t __p0) {
  poly16x8_t __ret;
  __ret = (poly16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly16x8_t vreinterpretq_p16_s8(int8x16_t __p0) {
  poly16x8_t __ret;
  __ret = (poly16x8_t)(__p0);
  return __ret;
}
#else
__ai poly16x8_t vreinterpretq_p16_s8(int8x16_t __p0) {
  poly16x8_t __ret;
  __ret = (poly16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly16x8_t vreinterpretq_p16_f64(float64x2_t __p0) {
  poly16x8_t __ret;
  __ret = (poly16x8_t)(__p0);
  return __ret;
}
#else
__ai poly16x8_t vreinterpretq_p16_f64(float64x2_t __p0) {
  poly16x8_t __ret;
  __ret = (poly16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly16x8_t vreinterpretq_p16_f32(float32x4_t __p0) {
  poly16x8_t __ret;
  __ret = (poly16x8_t)(__p0);
  return __ret;
}
#else
__ai poly16x8_t vreinterpretq_p16_f32(float32x4_t __p0) {
  poly16x8_t __ret;
  __ret = (poly16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly16x8_t vreinterpretq_p16_f16(float16x8_t __p0) {
  poly16x8_t __ret;
  __ret = (poly16x8_t)(__p0);
  return __ret;
}
#else
__ai poly16x8_t vreinterpretq_p16_f16(float16x8_t __p0) {
  poly16x8_t __ret;
  __ret = (poly16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly16x8_t vreinterpretq_p16_s32(int32x4_t __p0) {
  poly16x8_t __ret;
  __ret = (poly16x8_t)(__p0);
  return __ret;
}
#else
__ai poly16x8_t vreinterpretq_p16_s32(int32x4_t __p0) {
  poly16x8_t __ret;
  __ret = (poly16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly16x8_t vreinterpretq_p16_s64(int64x2_t __p0) {
  poly16x8_t __ret;
  __ret = (poly16x8_t)(__p0);
  return __ret;
}
#else
__ai poly16x8_t vreinterpretq_p16_s64(int64x2_t __p0) {
  poly16x8_t __ret;
  __ret = (poly16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly16x8_t vreinterpretq_p16_s16(int16x8_t __p0) {
  poly16x8_t __ret;
  __ret = (poly16x8_t)(__p0);
  return __ret;
}
#else
__ai poly16x8_t vreinterpretq_p16_s16(int16x8_t __p0) {
  poly16x8_t __ret;
  __ret = (poly16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x16_t vreinterpretq_u8_p8(poly8x16_t __p0) {
  uint8x16_t __ret;
  __ret = (uint8x16_t)(__p0);
  return __ret;
}
#else
__ai uint8x16_t vreinterpretq_u8_p8(poly8x16_t __p0) {
  uint8x16_t __ret;
  __ret = (uint8x16_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x16_t vreinterpretq_u8_p128(poly128_t __p0) {
  uint8x16_t __ret;
  __ret = (uint8x16_t)(__p0);
  return __ret;
}
#else
__ai uint8x16_t vreinterpretq_u8_p128(poly128_t __p0) {
  uint8x16_t __ret;
  __ret = (uint8x16_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x16_t vreinterpretq_u8_p64(poly64x2_t __p0) {
  uint8x16_t __ret;
  __ret = (uint8x16_t)(__p0);
  return __ret;
}
#else
__ai uint8x16_t vreinterpretq_u8_p64(poly64x2_t __p0) {
  uint8x16_t __ret;
  __ret = (uint8x16_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x16_t vreinterpretq_u8_p16(poly16x8_t __p0) {
  uint8x16_t __ret;
  __ret = (uint8x16_t)(__p0);
  return __ret;
}
#else
__ai uint8x16_t vreinterpretq_u8_p16(poly16x8_t __p0) {
  uint8x16_t __ret;
  __ret = (uint8x16_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x16_t vreinterpretq_u8_u32(uint32x4_t __p0) {
  uint8x16_t __ret;
  __ret = (uint8x16_t)(__p0);
  return __ret;
}
#else
__ai uint8x16_t vreinterpretq_u8_u32(uint32x4_t __p0) {
  uint8x16_t __ret;
  __ret = (uint8x16_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x16_t vreinterpretq_u8_u64(uint64x2_t __p0) {
  uint8x16_t __ret;
  __ret = (uint8x16_t)(__p0);
  return __ret;
}
#else
__ai uint8x16_t vreinterpretq_u8_u64(uint64x2_t __p0) {
  uint8x16_t __ret;
  __ret = (uint8x16_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x16_t vreinterpretq_u8_u16(uint16x8_t __p0) {
  uint8x16_t __ret;
  __ret = (uint8x16_t)(__p0);
  return __ret;
}
#else
__ai uint8x16_t vreinterpretq_u8_u16(uint16x8_t __p0) {
  uint8x16_t __ret;
  __ret = (uint8x16_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x16_t vreinterpretq_u8_s8(int8x16_t __p0) {
  uint8x16_t __ret;
  __ret = (uint8x16_t)(__p0);
  return __ret;
}
#else
__ai uint8x16_t vreinterpretq_u8_s8(int8x16_t __p0) {
  uint8x16_t __ret;
  __ret = (uint8x16_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x16_t vreinterpretq_u8_f64(float64x2_t __p0) {
  uint8x16_t __ret;
  __ret = (uint8x16_t)(__p0);
  return __ret;
}
#else
__ai uint8x16_t vreinterpretq_u8_f64(float64x2_t __p0) {
  uint8x16_t __ret;
  __ret = (uint8x16_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x16_t vreinterpretq_u8_f32(float32x4_t __p0) {
  uint8x16_t __ret;
  __ret = (uint8x16_t)(__p0);
  return __ret;
}
#else
__ai uint8x16_t vreinterpretq_u8_f32(float32x4_t __p0) {
  uint8x16_t __ret;
  __ret = (uint8x16_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x16_t vreinterpretq_u8_f16(float16x8_t __p0) {
  uint8x16_t __ret;
  __ret = (uint8x16_t)(__p0);
  return __ret;
}
#else
__ai uint8x16_t vreinterpretq_u8_f16(float16x8_t __p0) {
  uint8x16_t __ret;
  __ret = (uint8x16_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x16_t vreinterpretq_u8_s32(int32x4_t __p0) {
  uint8x16_t __ret;
  __ret = (uint8x16_t)(__p0);
  return __ret;
}
#else
__ai uint8x16_t vreinterpretq_u8_s32(int32x4_t __p0) {
  uint8x16_t __ret;
  __ret = (uint8x16_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x16_t vreinterpretq_u8_s64(int64x2_t __p0) {
  uint8x16_t __ret;
  __ret = (uint8x16_t)(__p0);
  return __ret;
}
#else
__ai uint8x16_t vreinterpretq_u8_s64(int64x2_t __p0) {
  uint8x16_t __ret;
  __ret = (uint8x16_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x16_t vreinterpretq_u8_s16(int16x8_t __p0) {
  uint8x16_t __ret;
  __ret = (uint8x16_t)(__p0);
  return __ret;
}
#else
__ai uint8x16_t vreinterpretq_u8_s16(int16x8_t __p0) {
  uint8x16_t __ret;
  __ret = (uint8x16_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vreinterpretq_u32_p8(poly8x16_t __p0) {
  uint32x4_t __ret;
  __ret = (uint32x4_t)(__p0);
  return __ret;
}
#else
__ai uint32x4_t vreinterpretq_u32_p8(poly8x16_t __p0) {
  uint32x4_t __ret;
  __ret = (uint32x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vreinterpretq_u32_p128(poly128_t __p0) {
  uint32x4_t __ret;
  __ret = (uint32x4_t)(__p0);
  return __ret;
}
#else
__ai uint32x4_t vreinterpretq_u32_p128(poly128_t __p0) {
  uint32x4_t __ret;
  __ret = (uint32x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vreinterpretq_u32_p64(poly64x2_t __p0) {
  uint32x4_t __ret;
  __ret = (uint32x4_t)(__p0);
  return __ret;
}
#else
__ai uint32x4_t vreinterpretq_u32_p64(poly64x2_t __p0) {
  uint32x4_t __ret;
  __ret = (uint32x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vreinterpretq_u32_p16(poly16x8_t __p0) {
  uint32x4_t __ret;
  __ret = (uint32x4_t)(__p0);
  return __ret;
}
#else
__ai uint32x4_t vreinterpretq_u32_p16(poly16x8_t __p0) {
  uint32x4_t __ret;
  __ret = (uint32x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vreinterpretq_u32_u8(uint8x16_t __p0) {
  uint32x4_t __ret;
  __ret = (uint32x4_t)(__p0);
  return __ret;
}
#else
__ai uint32x4_t vreinterpretq_u32_u8(uint8x16_t __p0) {
  uint32x4_t __ret;
  __ret = (uint32x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vreinterpretq_u32_u64(uint64x2_t __p0) {
  uint32x4_t __ret;
  __ret = (uint32x4_t)(__p0);
  return __ret;
}
#else
__ai uint32x4_t vreinterpretq_u32_u64(uint64x2_t __p0) {
  uint32x4_t __ret;
  __ret = (uint32x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vreinterpretq_u32_u16(uint16x8_t __p0) {
  uint32x4_t __ret;
  __ret = (uint32x4_t)(__p0);
  return __ret;
}
#else
__ai uint32x4_t vreinterpretq_u32_u16(uint16x8_t __p0) {
  uint32x4_t __ret;
  __ret = (uint32x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vreinterpretq_u32_s8(int8x16_t __p0) {
  uint32x4_t __ret;
  __ret = (uint32x4_t)(__p0);
  return __ret;
}
#else
__ai uint32x4_t vreinterpretq_u32_s8(int8x16_t __p0) {
  uint32x4_t __ret;
  __ret = (uint32x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vreinterpretq_u32_f64(float64x2_t __p0) {
  uint32x4_t __ret;
  __ret = (uint32x4_t)(__p0);
  return __ret;
}
#else
__ai uint32x4_t vreinterpretq_u32_f64(float64x2_t __p0) {
  uint32x4_t __ret;
  __ret = (uint32x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vreinterpretq_u32_f32(float32x4_t __p0) {
  uint32x4_t __ret;
  __ret = (uint32x4_t)(__p0);
  return __ret;
}
#else
__ai uint32x4_t vreinterpretq_u32_f32(float32x4_t __p0) {
  uint32x4_t __ret;
  __ret = (uint32x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vreinterpretq_u32_f16(float16x8_t __p0) {
  uint32x4_t __ret;
  __ret = (uint32x4_t)(__p0);
  return __ret;
}
#else
__ai uint32x4_t vreinterpretq_u32_f16(float16x8_t __p0) {
  uint32x4_t __ret;
  __ret = (uint32x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vreinterpretq_u32_s32(int32x4_t __p0) {
  uint32x4_t __ret;
  __ret = (uint32x4_t)(__p0);
  return __ret;
}
#else
__ai uint32x4_t vreinterpretq_u32_s32(int32x4_t __p0) {
  uint32x4_t __ret;
  __ret = (uint32x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vreinterpretq_u32_s64(int64x2_t __p0) {
  uint32x4_t __ret;
  __ret = (uint32x4_t)(__p0);
  return __ret;
}
#else
__ai uint32x4_t vreinterpretq_u32_s64(int64x2_t __p0) {
  uint32x4_t __ret;
  __ret = (uint32x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vreinterpretq_u32_s16(int16x8_t __p0) {
  uint32x4_t __ret;
  __ret = (uint32x4_t)(__p0);
  return __ret;
}
#else
__ai uint32x4_t vreinterpretq_u32_s16(int16x8_t __p0) {
  uint32x4_t __ret;
  __ret = (uint32x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vreinterpretq_u64_p8(poly8x16_t __p0) {
  uint64x2_t __ret;
  __ret = (uint64x2_t)(__p0);
  return __ret;
}
#else
__ai uint64x2_t vreinterpretq_u64_p8(poly8x16_t __p0) {
  uint64x2_t __ret;
  __ret = (uint64x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vreinterpretq_u64_p128(poly128_t __p0) {
  uint64x2_t __ret;
  __ret = (uint64x2_t)(__p0);
  return __ret;
}
#else
__ai uint64x2_t vreinterpretq_u64_p128(poly128_t __p0) {
  uint64x2_t __ret;
  __ret = (uint64x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vreinterpretq_u64_p64(poly64x2_t __p0) {
  uint64x2_t __ret;
  __ret = (uint64x2_t)(__p0);
  return __ret;
}
#else
__ai uint64x2_t vreinterpretq_u64_p64(poly64x2_t __p0) {
  uint64x2_t __ret;
  __ret = (uint64x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vreinterpretq_u64_p16(poly16x8_t __p0) {
  uint64x2_t __ret;
  __ret = (uint64x2_t)(__p0);
  return __ret;
}
#else
__ai uint64x2_t vreinterpretq_u64_p16(poly16x8_t __p0) {
  uint64x2_t __ret;
  __ret = (uint64x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vreinterpretq_u64_u8(uint8x16_t __p0) {
  uint64x2_t __ret;
  __ret = (uint64x2_t)(__p0);
  return __ret;
}
#else
__ai uint64x2_t vreinterpretq_u64_u8(uint8x16_t __p0) {
  uint64x2_t __ret;
  __ret = (uint64x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vreinterpretq_u64_u32(uint32x4_t __p0) {
  uint64x2_t __ret;
  __ret = (uint64x2_t)(__p0);
  return __ret;
}
#else
__ai uint64x2_t vreinterpretq_u64_u32(uint32x4_t __p0) {
  uint64x2_t __ret;
  __ret = (uint64x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vreinterpretq_u64_u16(uint16x8_t __p0) {
  uint64x2_t __ret;
  __ret = (uint64x2_t)(__p0);
  return __ret;
}
#else
__ai uint64x2_t vreinterpretq_u64_u16(uint16x8_t __p0) {
  uint64x2_t __ret;
  __ret = (uint64x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vreinterpretq_u64_s8(int8x16_t __p0) {
  uint64x2_t __ret;
  __ret = (uint64x2_t)(__p0);
  return __ret;
}
#else
__ai uint64x2_t vreinterpretq_u64_s8(int8x16_t __p0) {
  uint64x2_t __ret;
  __ret = (uint64x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vreinterpretq_u64_f64(float64x2_t __p0) {
  uint64x2_t __ret;
  __ret = (uint64x2_t)(__p0);
  return __ret;
}
#else
__ai uint64x2_t vreinterpretq_u64_f64(float64x2_t __p0) {
  uint64x2_t __ret;
  __ret = (uint64x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vreinterpretq_u64_f32(float32x4_t __p0) {
  uint64x2_t __ret;
  __ret = (uint64x2_t)(__p0);
  return __ret;
}
#else
__ai uint64x2_t vreinterpretq_u64_f32(float32x4_t __p0) {
  uint64x2_t __ret;
  __ret = (uint64x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vreinterpretq_u64_f16(float16x8_t __p0) {
  uint64x2_t __ret;
  __ret = (uint64x2_t)(__p0);
  return __ret;
}
#else
__ai uint64x2_t vreinterpretq_u64_f16(float16x8_t __p0) {
  uint64x2_t __ret;
  __ret = (uint64x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vreinterpretq_u64_s32(int32x4_t __p0) {
  uint64x2_t __ret;
  __ret = (uint64x2_t)(__p0);
  return __ret;
}
#else
__ai uint64x2_t vreinterpretq_u64_s32(int32x4_t __p0) {
  uint64x2_t __ret;
  __ret = (uint64x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vreinterpretq_u64_s64(int64x2_t __p0) {
  uint64x2_t __ret;
  __ret = (uint64x2_t)(__p0);
  return __ret;
}
#else
__ai uint64x2_t vreinterpretq_u64_s64(int64x2_t __p0) {
  uint64x2_t __ret;
  __ret = (uint64x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vreinterpretq_u64_s16(int16x8_t __p0) {
  uint64x2_t __ret;
  __ret = (uint64x2_t)(__p0);
  return __ret;
}
#else
__ai uint64x2_t vreinterpretq_u64_s16(int16x8_t __p0) {
  uint64x2_t __ret;
  __ret = (uint64x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x8_t vreinterpretq_u16_p8(poly8x16_t __p0) {
  uint16x8_t __ret;
  __ret = (uint16x8_t)(__p0);
  return __ret;
}
#else
__ai uint16x8_t vreinterpretq_u16_p8(poly8x16_t __p0) {
  uint16x8_t __ret;
  __ret = (uint16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x8_t vreinterpretq_u16_p128(poly128_t __p0) {
  uint16x8_t __ret;
  __ret = (uint16x8_t)(__p0);
  return __ret;
}
#else
__ai uint16x8_t vreinterpretq_u16_p128(poly128_t __p0) {
  uint16x8_t __ret;
  __ret = (uint16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x8_t vreinterpretq_u16_p64(poly64x2_t __p0) {
  uint16x8_t __ret;
  __ret = (uint16x8_t)(__p0);
  return __ret;
}
#else
__ai uint16x8_t vreinterpretq_u16_p64(poly64x2_t __p0) {
  uint16x8_t __ret;
  __ret = (uint16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x8_t vreinterpretq_u16_p16(poly16x8_t __p0) {
  uint16x8_t __ret;
  __ret = (uint16x8_t)(__p0);
  return __ret;
}
#else
__ai uint16x8_t vreinterpretq_u16_p16(poly16x8_t __p0) {
  uint16x8_t __ret;
  __ret = (uint16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x8_t vreinterpretq_u16_u8(uint8x16_t __p0) {
  uint16x8_t __ret;
  __ret = (uint16x8_t)(__p0);
  return __ret;
}
#else
__ai uint16x8_t vreinterpretq_u16_u8(uint8x16_t __p0) {
  uint16x8_t __ret;
  __ret = (uint16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x8_t vreinterpretq_u16_u32(uint32x4_t __p0) {
  uint16x8_t __ret;
  __ret = (uint16x8_t)(__p0);
  return __ret;
}
#else
__ai uint16x8_t vreinterpretq_u16_u32(uint32x4_t __p0) {
  uint16x8_t __ret;
  __ret = (uint16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x8_t vreinterpretq_u16_u64(uint64x2_t __p0) {
  uint16x8_t __ret;
  __ret = (uint16x8_t)(__p0);
  return __ret;
}
#else
__ai uint16x8_t vreinterpretq_u16_u64(uint64x2_t __p0) {
  uint16x8_t __ret;
  __ret = (uint16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x8_t vreinterpretq_u16_s8(int8x16_t __p0) {
  uint16x8_t __ret;
  __ret = (uint16x8_t)(__p0);
  return __ret;
}
#else
__ai uint16x8_t vreinterpretq_u16_s8(int8x16_t __p0) {
  uint16x8_t __ret;
  __ret = (uint16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x8_t vreinterpretq_u16_f64(float64x2_t __p0) {
  uint16x8_t __ret;
  __ret = (uint16x8_t)(__p0);
  return __ret;
}
#else
__ai uint16x8_t vreinterpretq_u16_f64(float64x2_t __p0) {
  uint16x8_t __ret;
  __ret = (uint16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x8_t vreinterpretq_u16_f32(float32x4_t __p0) {
  uint16x8_t __ret;
  __ret = (uint16x8_t)(__p0);
  return __ret;
}
#else
__ai uint16x8_t vreinterpretq_u16_f32(float32x4_t __p0) {
  uint16x8_t __ret;
  __ret = (uint16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x8_t vreinterpretq_u16_f16(float16x8_t __p0) {
  uint16x8_t __ret;
  __ret = (uint16x8_t)(__p0);
  return __ret;
}
#else
__ai uint16x8_t vreinterpretq_u16_f16(float16x8_t __p0) {
  uint16x8_t __ret;
  __ret = (uint16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x8_t vreinterpretq_u16_s32(int32x4_t __p0) {
  uint16x8_t __ret;
  __ret = (uint16x8_t)(__p0);
  return __ret;
}
#else
__ai uint16x8_t vreinterpretq_u16_s32(int32x4_t __p0) {
  uint16x8_t __ret;
  __ret = (uint16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x8_t vreinterpretq_u16_s64(int64x2_t __p0) {
  uint16x8_t __ret;
  __ret = (uint16x8_t)(__p0);
  return __ret;
}
#else
__ai uint16x8_t vreinterpretq_u16_s64(int64x2_t __p0) {
  uint16x8_t __ret;
  __ret = (uint16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x8_t vreinterpretq_u16_s16(int16x8_t __p0) {
  uint16x8_t __ret;
  __ret = (uint16x8_t)(__p0);
  return __ret;
}
#else
__ai uint16x8_t vreinterpretq_u16_s16(int16x8_t __p0) {
  uint16x8_t __ret;
  __ret = (uint16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x16_t vreinterpretq_s8_p8(poly8x16_t __p0) {
  int8x16_t __ret;
  __ret = (int8x16_t)(__p0);
  return __ret;
}
#else
__ai int8x16_t vreinterpretq_s8_p8(poly8x16_t __p0) {
  int8x16_t __ret;
  __ret = (int8x16_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x16_t vreinterpretq_s8_p128(poly128_t __p0) {
  int8x16_t __ret;
  __ret = (int8x16_t)(__p0);
  return __ret;
}
#else
__ai int8x16_t vreinterpretq_s8_p128(poly128_t __p0) {
  int8x16_t __ret;
  __ret = (int8x16_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x16_t vreinterpretq_s8_p64(poly64x2_t __p0) {
  int8x16_t __ret;
  __ret = (int8x16_t)(__p0);
  return __ret;
}
#else
__ai int8x16_t vreinterpretq_s8_p64(poly64x2_t __p0) {
  int8x16_t __ret;
  __ret = (int8x16_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x16_t vreinterpretq_s8_p16(poly16x8_t __p0) {
  int8x16_t __ret;
  __ret = (int8x16_t)(__p0);
  return __ret;
}
#else
__ai int8x16_t vreinterpretq_s8_p16(poly16x8_t __p0) {
  int8x16_t __ret;
  __ret = (int8x16_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x16_t vreinterpretq_s8_u8(uint8x16_t __p0) {
  int8x16_t __ret;
  __ret = (int8x16_t)(__p0);
  return __ret;
}
#else
__ai int8x16_t vreinterpretq_s8_u8(uint8x16_t __p0) {
  int8x16_t __ret;
  __ret = (int8x16_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x16_t vreinterpretq_s8_u32(uint32x4_t __p0) {
  int8x16_t __ret;
  __ret = (int8x16_t)(__p0);
  return __ret;
}
#else
__ai int8x16_t vreinterpretq_s8_u32(uint32x4_t __p0) {
  int8x16_t __ret;
  __ret = (int8x16_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x16_t vreinterpretq_s8_u64(uint64x2_t __p0) {
  int8x16_t __ret;
  __ret = (int8x16_t)(__p0);
  return __ret;
}
#else
__ai int8x16_t vreinterpretq_s8_u64(uint64x2_t __p0) {
  int8x16_t __ret;
  __ret = (int8x16_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x16_t vreinterpretq_s8_u16(uint16x8_t __p0) {
  int8x16_t __ret;
  __ret = (int8x16_t)(__p0);
  return __ret;
}
#else
__ai int8x16_t vreinterpretq_s8_u16(uint16x8_t __p0) {
  int8x16_t __ret;
  __ret = (int8x16_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x16_t vreinterpretq_s8_f64(float64x2_t __p0) {
  int8x16_t __ret;
  __ret = (int8x16_t)(__p0);
  return __ret;
}
#else
__ai int8x16_t vreinterpretq_s8_f64(float64x2_t __p0) {
  int8x16_t __ret;
  __ret = (int8x16_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x16_t vreinterpretq_s8_f32(float32x4_t __p0) {
  int8x16_t __ret;
  __ret = (int8x16_t)(__p0);
  return __ret;
}
#else
__ai int8x16_t vreinterpretq_s8_f32(float32x4_t __p0) {
  int8x16_t __ret;
  __ret = (int8x16_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x16_t vreinterpretq_s8_f16(float16x8_t __p0) {
  int8x16_t __ret;
  __ret = (int8x16_t)(__p0);
  return __ret;
}
#else
__ai int8x16_t vreinterpretq_s8_f16(float16x8_t __p0) {
  int8x16_t __ret;
  __ret = (int8x16_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x16_t vreinterpretq_s8_s32(int32x4_t __p0) {
  int8x16_t __ret;
  __ret = (int8x16_t)(__p0);
  return __ret;
}
#else
__ai int8x16_t vreinterpretq_s8_s32(int32x4_t __p0) {
  int8x16_t __ret;
  __ret = (int8x16_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x16_t vreinterpretq_s8_s64(int64x2_t __p0) {
  int8x16_t __ret;
  __ret = (int8x16_t)(__p0);
  return __ret;
}
#else
__ai int8x16_t vreinterpretq_s8_s64(int64x2_t __p0) {
  int8x16_t __ret;
  __ret = (int8x16_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x16_t vreinterpretq_s8_s16(int16x8_t __p0) {
  int8x16_t __ret;
  __ret = (int8x16_t)(__p0);
  return __ret;
}
#else
__ai int8x16_t vreinterpretq_s8_s16(int16x8_t __p0) {
  int8x16_t __ret;
  __ret = (int8x16_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x2_t vreinterpretq_f64_p8(poly8x16_t __p0) {
  float64x2_t __ret;
  __ret = (float64x2_t)(__p0);
  return __ret;
}
#else
__ai float64x2_t vreinterpretq_f64_p8(poly8x16_t __p0) {
  float64x2_t __ret;
  __ret = (float64x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x2_t vreinterpretq_f64_p128(poly128_t __p0) {
  float64x2_t __ret;
  __ret = (float64x2_t)(__p0);
  return __ret;
}
#else
__ai float64x2_t vreinterpretq_f64_p128(poly128_t __p0) {
  float64x2_t __ret;
  __ret = (float64x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x2_t vreinterpretq_f64_p64(poly64x2_t __p0) {
  float64x2_t __ret;
  __ret = (float64x2_t)(__p0);
  return __ret;
}
#else
__ai float64x2_t vreinterpretq_f64_p64(poly64x2_t __p0) {
  float64x2_t __ret;
  __ret = (float64x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x2_t vreinterpretq_f64_p16(poly16x8_t __p0) {
  float64x2_t __ret;
  __ret = (float64x2_t)(__p0);
  return __ret;
}
#else
__ai float64x2_t vreinterpretq_f64_p16(poly16x8_t __p0) {
  float64x2_t __ret;
  __ret = (float64x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x2_t vreinterpretq_f64_u8(uint8x16_t __p0) {
  float64x2_t __ret;
  __ret = (float64x2_t)(__p0);
  return __ret;
}
#else
__ai float64x2_t vreinterpretq_f64_u8(uint8x16_t __p0) {
  float64x2_t __ret;
  __ret = (float64x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x2_t vreinterpretq_f64_u32(uint32x4_t __p0) {
  float64x2_t __ret;
  __ret = (float64x2_t)(__p0);
  return __ret;
}
#else
__ai float64x2_t vreinterpretq_f64_u32(uint32x4_t __p0) {
  float64x2_t __ret;
  __ret = (float64x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x2_t vreinterpretq_f64_u64(uint64x2_t __p0) {
  float64x2_t __ret;
  __ret = (float64x2_t)(__p0);
  return __ret;
}
#else
__ai float64x2_t vreinterpretq_f64_u64(uint64x2_t __p0) {
  float64x2_t __ret;
  __ret = (float64x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x2_t vreinterpretq_f64_u16(uint16x8_t __p0) {
  float64x2_t __ret;
  __ret = (float64x2_t)(__p0);
  return __ret;
}
#else
__ai float64x2_t vreinterpretq_f64_u16(uint16x8_t __p0) {
  float64x2_t __ret;
  __ret = (float64x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x2_t vreinterpretq_f64_s8(int8x16_t __p0) {
  float64x2_t __ret;
  __ret = (float64x2_t)(__p0);
  return __ret;
}
#else
__ai float64x2_t vreinterpretq_f64_s8(int8x16_t __p0) {
  float64x2_t __ret;
  __ret = (float64x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x2_t vreinterpretq_f64_f32(float32x4_t __p0) {
  float64x2_t __ret;
  __ret = (float64x2_t)(__p0);
  return __ret;
}
#else
__ai float64x2_t vreinterpretq_f64_f32(float32x4_t __p0) {
  float64x2_t __ret;
  __ret = (float64x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x2_t vreinterpretq_f64_f16(float16x8_t __p0) {
  float64x2_t __ret;
  __ret = (float64x2_t)(__p0);
  return __ret;
}
#else
__ai float64x2_t vreinterpretq_f64_f16(float16x8_t __p0) {
  float64x2_t __ret;
  __ret = (float64x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x2_t vreinterpretq_f64_s32(int32x4_t __p0) {
  float64x2_t __ret;
  __ret = (float64x2_t)(__p0);
  return __ret;
}
#else
__ai float64x2_t vreinterpretq_f64_s32(int32x4_t __p0) {
  float64x2_t __ret;
  __ret = (float64x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x2_t vreinterpretq_f64_s64(int64x2_t __p0) {
  float64x2_t __ret;
  __ret = (float64x2_t)(__p0);
  return __ret;
}
#else
__ai float64x2_t vreinterpretq_f64_s64(int64x2_t __p0) {
  float64x2_t __ret;
  __ret = (float64x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x2_t vreinterpretq_f64_s16(int16x8_t __p0) {
  float64x2_t __ret;
  __ret = (float64x2_t)(__p0);
  return __ret;
}
#else
__ai float64x2_t vreinterpretq_f64_s16(int16x8_t __p0) {
  float64x2_t __ret;
  __ret = (float64x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x4_t vreinterpretq_f32_p8(poly8x16_t __p0) {
  float32x4_t __ret;
  __ret = (float32x4_t)(__p0);
  return __ret;
}
#else
__ai float32x4_t vreinterpretq_f32_p8(poly8x16_t __p0) {
  float32x4_t __ret;
  __ret = (float32x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x4_t vreinterpretq_f32_p128(poly128_t __p0) {
  float32x4_t __ret;
  __ret = (float32x4_t)(__p0);
  return __ret;
}
#else
__ai float32x4_t vreinterpretq_f32_p128(poly128_t __p0) {
  float32x4_t __ret;
  __ret = (float32x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x4_t vreinterpretq_f32_p64(poly64x2_t __p0) {
  float32x4_t __ret;
  __ret = (float32x4_t)(__p0);
  return __ret;
}
#else
__ai float32x4_t vreinterpretq_f32_p64(poly64x2_t __p0) {
  float32x4_t __ret;
  __ret = (float32x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x4_t vreinterpretq_f32_p16(poly16x8_t __p0) {
  float32x4_t __ret;
  __ret = (float32x4_t)(__p0);
  return __ret;
}
#else
__ai float32x4_t vreinterpretq_f32_p16(poly16x8_t __p0) {
  float32x4_t __ret;
  __ret = (float32x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x4_t vreinterpretq_f32_u8(uint8x16_t __p0) {
  float32x4_t __ret;
  __ret = (float32x4_t)(__p0);
  return __ret;
}
#else
__ai float32x4_t vreinterpretq_f32_u8(uint8x16_t __p0) {
  float32x4_t __ret;
  __ret = (float32x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x4_t vreinterpretq_f32_u32(uint32x4_t __p0) {
  float32x4_t __ret;
  __ret = (float32x4_t)(__p0);
  return __ret;
}
#else
__ai float32x4_t vreinterpretq_f32_u32(uint32x4_t __p0) {
  float32x4_t __ret;
  __ret = (float32x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x4_t vreinterpretq_f32_u64(uint64x2_t __p0) {
  float32x4_t __ret;
  __ret = (float32x4_t)(__p0);
  return __ret;
}
#else
__ai float32x4_t vreinterpretq_f32_u64(uint64x2_t __p0) {
  float32x4_t __ret;
  __ret = (float32x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x4_t vreinterpretq_f32_u16(uint16x8_t __p0) {
  float32x4_t __ret;
  __ret = (float32x4_t)(__p0);
  return __ret;
}
#else
__ai float32x4_t vreinterpretq_f32_u16(uint16x8_t __p0) {
  float32x4_t __ret;
  __ret = (float32x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x4_t vreinterpretq_f32_s8(int8x16_t __p0) {
  float32x4_t __ret;
  __ret = (float32x4_t)(__p0);
  return __ret;
}
#else
__ai float32x4_t vreinterpretq_f32_s8(int8x16_t __p0) {
  float32x4_t __ret;
  __ret = (float32x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x4_t vreinterpretq_f32_f64(float64x2_t __p0) {
  float32x4_t __ret;
  __ret = (float32x4_t)(__p0);
  return __ret;
}
#else
__ai float32x4_t vreinterpretq_f32_f64(float64x2_t __p0) {
  float32x4_t __ret;
  __ret = (float32x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x4_t vreinterpretq_f32_f16(float16x8_t __p0) {
  float32x4_t __ret;
  __ret = (float32x4_t)(__p0);
  return __ret;
}
#else
__ai float32x4_t vreinterpretq_f32_f16(float16x8_t __p0) {
  float32x4_t __ret;
  __ret = (float32x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x4_t vreinterpretq_f32_s32(int32x4_t __p0) {
  float32x4_t __ret;
  __ret = (float32x4_t)(__p0);
  return __ret;
}
#else
__ai float32x4_t vreinterpretq_f32_s32(int32x4_t __p0) {
  float32x4_t __ret;
  __ret = (float32x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x4_t vreinterpretq_f32_s64(int64x2_t __p0) {
  float32x4_t __ret;
  __ret = (float32x4_t)(__p0);
  return __ret;
}
#else
__ai float32x4_t vreinterpretq_f32_s64(int64x2_t __p0) {
  float32x4_t __ret;
  __ret = (float32x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x4_t vreinterpretq_f32_s16(int16x8_t __p0) {
  float32x4_t __ret;
  __ret = (float32x4_t)(__p0);
  return __ret;
}
#else
__ai float32x4_t vreinterpretq_f32_s16(int16x8_t __p0) {
  float32x4_t __ret;
  __ret = (float32x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float16x8_t vreinterpretq_f16_p8(poly8x16_t __p0) {
  float16x8_t __ret;
  __ret = (float16x8_t)(__p0);
  return __ret;
}
#else
__ai float16x8_t vreinterpretq_f16_p8(poly8x16_t __p0) {
  float16x8_t __ret;
  __ret = (float16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float16x8_t vreinterpretq_f16_p128(poly128_t __p0) {
  float16x8_t __ret;
  __ret = (float16x8_t)(__p0);
  return __ret;
}
#else
__ai float16x8_t vreinterpretq_f16_p128(poly128_t __p0) {
  float16x8_t __ret;
  __ret = (float16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float16x8_t vreinterpretq_f16_p64(poly64x2_t __p0) {
  float16x8_t __ret;
  __ret = (float16x8_t)(__p0);
  return __ret;
}
#else
__ai float16x8_t vreinterpretq_f16_p64(poly64x2_t __p0) {
  float16x8_t __ret;
  __ret = (float16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float16x8_t vreinterpretq_f16_p16(poly16x8_t __p0) {
  float16x8_t __ret;
  __ret = (float16x8_t)(__p0);
  return __ret;
}
#else
__ai float16x8_t vreinterpretq_f16_p16(poly16x8_t __p0) {
  float16x8_t __ret;
  __ret = (float16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float16x8_t vreinterpretq_f16_u8(uint8x16_t __p0) {
  float16x8_t __ret;
  __ret = (float16x8_t)(__p0);
  return __ret;
}
#else
__ai float16x8_t vreinterpretq_f16_u8(uint8x16_t __p0) {
  float16x8_t __ret;
  __ret = (float16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float16x8_t vreinterpretq_f16_u32(uint32x4_t __p0) {
  float16x8_t __ret;
  __ret = (float16x8_t)(__p0);
  return __ret;
}
#else
__ai float16x8_t vreinterpretq_f16_u32(uint32x4_t __p0) {
  float16x8_t __ret;
  __ret = (float16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float16x8_t vreinterpretq_f16_u64(uint64x2_t __p0) {
  float16x8_t __ret;
  __ret = (float16x8_t)(__p0);
  return __ret;
}
#else
__ai float16x8_t vreinterpretq_f16_u64(uint64x2_t __p0) {
  float16x8_t __ret;
  __ret = (float16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float16x8_t vreinterpretq_f16_u16(uint16x8_t __p0) {
  float16x8_t __ret;
  __ret = (float16x8_t)(__p0);
  return __ret;
}
#else
__ai float16x8_t vreinterpretq_f16_u16(uint16x8_t __p0) {
  float16x8_t __ret;
  __ret = (float16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float16x8_t vreinterpretq_f16_s8(int8x16_t __p0) {
  float16x8_t __ret;
  __ret = (float16x8_t)(__p0);
  return __ret;
}
#else
__ai float16x8_t vreinterpretq_f16_s8(int8x16_t __p0) {
  float16x8_t __ret;
  __ret = (float16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float16x8_t vreinterpretq_f16_f64(float64x2_t __p0) {
  float16x8_t __ret;
  __ret = (float16x8_t)(__p0);
  return __ret;
}
#else
__ai float16x8_t vreinterpretq_f16_f64(float64x2_t __p0) {
  float16x8_t __ret;
  __ret = (float16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float16x8_t vreinterpretq_f16_f32(float32x4_t __p0) {
  float16x8_t __ret;
  __ret = (float16x8_t)(__p0);
  return __ret;
}
#else
__ai float16x8_t vreinterpretq_f16_f32(float32x4_t __p0) {
  float16x8_t __ret;
  __ret = (float16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float16x8_t vreinterpretq_f16_s32(int32x4_t __p0) {
  float16x8_t __ret;
  __ret = (float16x8_t)(__p0);
  return __ret;
}
#else
__ai float16x8_t vreinterpretq_f16_s32(int32x4_t __p0) {
  float16x8_t __ret;
  __ret = (float16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float16x8_t vreinterpretq_f16_s64(int64x2_t __p0) {
  float16x8_t __ret;
  __ret = (float16x8_t)(__p0);
  return __ret;
}
#else
__ai float16x8_t vreinterpretq_f16_s64(int64x2_t __p0) {
  float16x8_t __ret;
  __ret = (float16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float16x8_t vreinterpretq_f16_s16(int16x8_t __p0) {
  float16x8_t __ret;
  __ret = (float16x8_t)(__p0);
  return __ret;
}
#else
__ai float16x8_t vreinterpretq_f16_s16(int16x8_t __p0) {
  float16x8_t __ret;
  __ret = (float16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vreinterpretq_s32_p8(poly8x16_t __p0) {
  int32x4_t __ret;
  __ret = (int32x4_t)(__p0);
  return __ret;
}
#else
__ai int32x4_t vreinterpretq_s32_p8(poly8x16_t __p0) {
  int32x4_t __ret;
  __ret = (int32x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vreinterpretq_s32_p128(poly128_t __p0) {
  int32x4_t __ret;
  __ret = (int32x4_t)(__p0);
  return __ret;
}
#else
__ai int32x4_t vreinterpretq_s32_p128(poly128_t __p0) {
  int32x4_t __ret;
  __ret = (int32x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vreinterpretq_s32_p64(poly64x2_t __p0) {
  int32x4_t __ret;
  __ret = (int32x4_t)(__p0);
  return __ret;
}
#else
__ai int32x4_t vreinterpretq_s32_p64(poly64x2_t __p0) {
  int32x4_t __ret;
  __ret = (int32x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vreinterpretq_s32_p16(poly16x8_t __p0) {
  int32x4_t __ret;
  __ret = (int32x4_t)(__p0);
  return __ret;
}
#else
__ai int32x4_t vreinterpretq_s32_p16(poly16x8_t __p0) {
  int32x4_t __ret;
  __ret = (int32x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vreinterpretq_s32_u8(uint8x16_t __p0) {
  int32x4_t __ret;
  __ret = (int32x4_t)(__p0);
  return __ret;
}
#else
__ai int32x4_t vreinterpretq_s32_u8(uint8x16_t __p0) {
  int32x4_t __ret;
  __ret = (int32x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vreinterpretq_s32_u32(uint32x4_t __p0) {
  int32x4_t __ret;
  __ret = (int32x4_t)(__p0);
  return __ret;
}
#else
__ai int32x4_t vreinterpretq_s32_u32(uint32x4_t __p0) {
  int32x4_t __ret;
  __ret = (int32x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vreinterpretq_s32_u64(uint64x2_t __p0) {
  int32x4_t __ret;
  __ret = (int32x4_t)(__p0);
  return __ret;
}
#else
__ai int32x4_t vreinterpretq_s32_u64(uint64x2_t __p0) {
  int32x4_t __ret;
  __ret = (int32x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vreinterpretq_s32_u16(uint16x8_t __p0) {
  int32x4_t __ret;
  __ret = (int32x4_t)(__p0);
  return __ret;
}
#else
__ai int32x4_t vreinterpretq_s32_u16(uint16x8_t __p0) {
  int32x4_t __ret;
  __ret = (int32x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vreinterpretq_s32_s8(int8x16_t __p0) {
  int32x4_t __ret;
  __ret = (int32x4_t)(__p0);
  return __ret;
}
#else
__ai int32x4_t vreinterpretq_s32_s8(int8x16_t __p0) {
  int32x4_t __ret;
  __ret = (int32x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vreinterpretq_s32_f64(float64x2_t __p0) {
  int32x4_t __ret;
  __ret = (int32x4_t)(__p0);
  return __ret;
}
#else
__ai int32x4_t vreinterpretq_s32_f64(float64x2_t __p0) {
  int32x4_t __ret;
  __ret = (int32x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vreinterpretq_s32_f32(float32x4_t __p0) {
  int32x4_t __ret;
  __ret = (int32x4_t)(__p0);
  return __ret;
}
#else
__ai int32x4_t vreinterpretq_s32_f32(float32x4_t __p0) {
  int32x4_t __ret;
  __ret = (int32x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vreinterpretq_s32_f16(float16x8_t __p0) {
  int32x4_t __ret;
  __ret = (int32x4_t)(__p0);
  return __ret;
}
#else
__ai int32x4_t vreinterpretq_s32_f16(float16x8_t __p0) {
  int32x4_t __ret;
  __ret = (int32x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vreinterpretq_s32_s64(int64x2_t __p0) {
  int32x4_t __ret;
  __ret = (int32x4_t)(__p0);
  return __ret;
}
#else
__ai int32x4_t vreinterpretq_s32_s64(int64x2_t __p0) {
  int32x4_t __ret;
  __ret = (int32x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vreinterpretq_s32_s16(int16x8_t __p0) {
  int32x4_t __ret;
  __ret = (int32x4_t)(__p0);
  return __ret;
}
#else
__ai int32x4_t vreinterpretq_s32_s16(int16x8_t __p0) {
  int32x4_t __ret;
  __ret = (int32x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x2_t vreinterpretq_s64_p8(poly8x16_t __p0) {
  int64x2_t __ret;
  __ret = (int64x2_t)(__p0);
  return __ret;
}
#else
__ai int64x2_t vreinterpretq_s64_p8(poly8x16_t __p0) {
  int64x2_t __ret;
  __ret = (int64x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x2_t vreinterpretq_s64_p128(poly128_t __p0) {
  int64x2_t __ret;
  __ret = (int64x2_t)(__p0);
  return __ret;
}
#else
__ai int64x2_t vreinterpretq_s64_p128(poly128_t __p0) {
  int64x2_t __ret;
  __ret = (int64x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x2_t vreinterpretq_s64_p64(poly64x2_t __p0) {
  int64x2_t __ret;
  __ret = (int64x2_t)(__p0);
  return __ret;
}
#else
__ai int64x2_t vreinterpretq_s64_p64(poly64x2_t __p0) {
  int64x2_t __ret;
  __ret = (int64x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x2_t vreinterpretq_s64_p16(poly16x8_t __p0) {
  int64x2_t __ret;
  __ret = (int64x2_t)(__p0);
  return __ret;
}
#else
__ai int64x2_t vreinterpretq_s64_p16(poly16x8_t __p0) {
  int64x2_t __ret;
  __ret = (int64x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x2_t vreinterpretq_s64_u8(uint8x16_t __p0) {
  int64x2_t __ret;
  __ret = (int64x2_t)(__p0);
  return __ret;
}
#else
__ai int64x2_t vreinterpretq_s64_u8(uint8x16_t __p0) {
  int64x2_t __ret;
  __ret = (int64x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x2_t vreinterpretq_s64_u32(uint32x4_t __p0) {
  int64x2_t __ret;
  __ret = (int64x2_t)(__p0);
  return __ret;
}
#else
__ai int64x2_t vreinterpretq_s64_u32(uint32x4_t __p0) {
  int64x2_t __ret;
  __ret = (int64x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x2_t vreinterpretq_s64_u64(uint64x2_t __p0) {
  int64x2_t __ret;
  __ret = (int64x2_t)(__p0);
  return __ret;
}
#else
__ai int64x2_t vreinterpretq_s64_u64(uint64x2_t __p0) {
  int64x2_t __ret;
  __ret = (int64x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x2_t vreinterpretq_s64_u16(uint16x8_t __p0) {
  int64x2_t __ret;
  __ret = (int64x2_t)(__p0);
  return __ret;
}
#else
__ai int64x2_t vreinterpretq_s64_u16(uint16x8_t __p0) {
  int64x2_t __ret;
  __ret = (int64x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x2_t vreinterpretq_s64_s8(int8x16_t __p0) {
  int64x2_t __ret;
  __ret = (int64x2_t)(__p0);
  return __ret;
}
#else
__ai int64x2_t vreinterpretq_s64_s8(int8x16_t __p0) {
  int64x2_t __ret;
  __ret = (int64x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x2_t vreinterpretq_s64_f64(float64x2_t __p0) {
  int64x2_t __ret;
  __ret = (int64x2_t)(__p0);
  return __ret;
}
#else
__ai int64x2_t vreinterpretq_s64_f64(float64x2_t __p0) {
  int64x2_t __ret;
  __ret = (int64x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x2_t vreinterpretq_s64_f32(float32x4_t __p0) {
  int64x2_t __ret;
  __ret = (int64x2_t)(__p0);
  return __ret;
}
#else
__ai int64x2_t vreinterpretq_s64_f32(float32x4_t __p0) {
  int64x2_t __ret;
  __ret = (int64x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x2_t vreinterpretq_s64_f16(float16x8_t __p0) {
  int64x2_t __ret;
  __ret = (int64x2_t)(__p0);
  return __ret;
}
#else
__ai int64x2_t vreinterpretq_s64_f16(float16x8_t __p0) {
  int64x2_t __ret;
  __ret = (int64x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x2_t vreinterpretq_s64_s32(int32x4_t __p0) {
  int64x2_t __ret;
  __ret = (int64x2_t)(__p0);
  return __ret;
}
#else
__ai int64x2_t vreinterpretq_s64_s32(int32x4_t __p0) {
  int64x2_t __ret;
  __ret = (int64x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x2_t vreinterpretq_s64_s16(int16x8_t __p0) {
  int64x2_t __ret;
  __ret = (int64x2_t)(__p0);
  return __ret;
}
#else
__ai int64x2_t vreinterpretq_s64_s16(int16x8_t __p0) {
  int64x2_t __ret;
  __ret = (int64x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x8_t vreinterpretq_s16_p8(poly8x16_t __p0) {
  int16x8_t __ret;
  __ret = (int16x8_t)(__p0);
  return __ret;
}
#else
__ai int16x8_t vreinterpretq_s16_p8(poly8x16_t __p0) {
  int16x8_t __ret;
  __ret = (int16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x8_t vreinterpretq_s16_p128(poly128_t __p0) {
  int16x8_t __ret;
  __ret = (int16x8_t)(__p0);
  return __ret;
}
#else
__ai int16x8_t vreinterpretq_s16_p128(poly128_t __p0) {
  int16x8_t __ret;
  __ret = (int16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x8_t vreinterpretq_s16_p64(poly64x2_t __p0) {
  int16x8_t __ret;
  __ret = (int16x8_t)(__p0);
  return __ret;
}
#else
__ai int16x8_t vreinterpretq_s16_p64(poly64x2_t __p0) {
  int16x8_t __ret;
  __ret = (int16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x8_t vreinterpretq_s16_p16(poly16x8_t __p0) {
  int16x8_t __ret;
  __ret = (int16x8_t)(__p0);
  return __ret;
}
#else
__ai int16x8_t vreinterpretq_s16_p16(poly16x8_t __p0) {
  int16x8_t __ret;
  __ret = (int16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x8_t vreinterpretq_s16_u8(uint8x16_t __p0) {
  int16x8_t __ret;
  __ret = (int16x8_t)(__p0);
  return __ret;
}
#else
__ai int16x8_t vreinterpretq_s16_u8(uint8x16_t __p0) {
  int16x8_t __ret;
  __ret = (int16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x8_t vreinterpretq_s16_u32(uint32x4_t __p0) {
  int16x8_t __ret;
  __ret = (int16x8_t)(__p0);
  return __ret;
}
#else
__ai int16x8_t vreinterpretq_s16_u32(uint32x4_t __p0) {
  int16x8_t __ret;
  __ret = (int16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x8_t vreinterpretq_s16_u64(uint64x2_t __p0) {
  int16x8_t __ret;
  __ret = (int16x8_t)(__p0);
  return __ret;
}
#else
__ai int16x8_t vreinterpretq_s16_u64(uint64x2_t __p0) {
  int16x8_t __ret;
  __ret = (int16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x8_t vreinterpretq_s16_u16(uint16x8_t __p0) {
  int16x8_t __ret;
  __ret = (int16x8_t)(__p0);
  return __ret;
}
#else
__ai int16x8_t vreinterpretq_s16_u16(uint16x8_t __p0) {
  int16x8_t __ret;
  __ret = (int16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x8_t vreinterpretq_s16_s8(int8x16_t __p0) {
  int16x8_t __ret;
  __ret = (int16x8_t)(__p0);
  return __ret;
}
#else
__ai int16x8_t vreinterpretq_s16_s8(int8x16_t __p0) {
  int16x8_t __ret;
  __ret = (int16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x8_t vreinterpretq_s16_f64(float64x2_t __p0) {
  int16x8_t __ret;
  __ret = (int16x8_t)(__p0);
  return __ret;
}
#else
__ai int16x8_t vreinterpretq_s16_f64(float64x2_t __p0) {
  int16x8_t __ret;
  __ret = (int16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x8_t vreinterpretq_s16_f32(float32x4_t __p0) {
  int16x8_t __ret;
  __ret = (int16x8_t)(__p0);
  return __ret;
}
#else
__ai int16x8_t vreinterpretq_s16_f32(float32x4_t __p0) {
  int16x8_t __ret;
  __ret = (int16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x8_t vreinterpretq_s16_f16(float16x8_t __p0) {
  int16x8_t __ret;
  __ret = (int16x8_t)(__p0);
  return __ret;
}
#else
__ai int16x8_t vreinterpretq_s16_f16(float16x8_t __p0) {
  int16x8_t __ret;
  __ret = (int16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x8_t vreinterpretq_s16_s32(int32x4_t __p0) {
  int16x8_t __ret;
  __ret = (int16x8_t)(__p0);
  return __ret;
}
#else
__ai int16x8_t vreinterpretq_s16_s32(int32x4_t __p0) {
  int16x8_t __ret;
  __ret = (int16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x8_t vreinterpretq_s16_s64(int64x2_t __p0) {
  int16x8_t __ret;
  __ret = (int16x8_t)(__p0);
  return __ret;
}
#else
__ai int16x8_t vreinterpretq_s16_s64(int64x2_t __p0) {
  int16x8_t __ret;
  __ret = (int16x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x8_t vreinterpret_u8_p8(poly8x8_t __p0) {
  uint8x8_t __ret;
  __ret = (uint8x8_t)(__p0);
  return __ret;
}
#else
__ai uint8x8_t vreinterpret_u8_p8(poly8x8_t __p0) {
  uint8x8_t __ret;
  __ret = (uint8x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x8_t vreinterpret_u8_p64(poly64x1_t __p0) {
  uint8x8_t __ret;
  __ret = (uint8x8_t)(__p0);
  return __ret;
}
#else
__ai uint8x8_t vreinterpret_u8_p64(poly64x1_t __p0) {
  uint8x8_t __ret;
  __ret = (uint8x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x8_t vreinterpret_u8_p16(poly16x4_t __p0) {
  uint8x8_t __ret;
  __ret = (uint8x8_t)(__p0);
  return __ret;
}
#else
__ai uint8x8_t vreinterpret_u8_p16(poly16x4_t __p0) {
  uint8x8_t __ret;
  __ret = (uint8x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x8_t vreinterpret_u8_u32(uint32x2_t __p0) {
  uint8x8_t __ret;
  __ret = (uint8x8_t)(__p0);
  return __ret;
}
#else
__ai uint8x8_t vreinterpret_u8_u32(uint32x2_t __p0) {
  uint8x8_t __ret;
  __ret = (uint8x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x8_t vreinterpret_u8_u64(uint64x1_t __p0) {
  uint8x8_t __ret;
  __ret = (uint8x8_t)(__p0);
  return __ret;
}
#else
__ai uint8x8_t vreinterpret_u8_u64(uint64x1_t __p0) {
  uint8x8_t __ret;
  __ret = (uint8x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x8_t vreinterpret_u8_u16(uint16x4_t __p0) {
  uint8x8_t __ret;
  __ret = (uint8x8_t)(__p0);
  return __ret;
}
#else
__ai uint8x8_t vreinterpret_u8_u16(uint16x4_t __p0) {
  uint8x8_t __ret;
  __ret = (uint8x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x8_t vreinterpret_u8_s8(int8x8_t __p0) {
  uint8x8_t __ret;
  __ret = (uint8x8_t)(__p0);
  return __ret;
}
#else
__ai uint8x8_t vreinterpret_u8_s8(int8x8_t __p0) {
  uint8x8_t __ret;
  __ret = (uint8x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x8_t vreinterpret_u8_f64(float64x1_t __p0) {
  uint8x8_t __ret;
  __ret = (uint8x8_t)(__p0);
  return __ret;
}
#else
__ai uint8x8_t vreinterpret_u8_f64(float64x1_t __p0) {
  uint8x8_t __ret;
  __ret = (uint8x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x8_t vreinterpret_u8_f32(float32x2_t __p0) {
  uint8x8_t __ret;
  __ret = (uint8x8_t)(__p0);
  return __ret;
}
#else
__ai uint8x8_t vreinterpret_u8_f32(float32x2_t __p0) {
  uint8x8_t __ret;
  __ret = (uint8x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x8_t vreinterpret_u8_f16(float16x4_t __p0) {
  uint8x8_t __ret;
  __ret = (uint8x8_t)(__p0);
  return __ret;
}
#else
__ai uint8x8_t vreinterpret_u8_f16(float16x4_t __p0) {
  uint8x8_t __ret;
  __ret = (uint8x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x8_t vreinterpret_u8_s32(int32x2_t __p0) {
  uint8x8_t __ret;
  __ret = (uint8x8_t)(__p0);
  return __ret;
}
#else
__ai uint8x8_t vreinterpret_u8_s32(int32x2_t __p0) {
  uint8x8_t __ret;
  __ret = (uint8x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x8_t vreinterpret_u8_s64(int64x1_t __p0) {
  uint8x8_t __ret;
  __ret = (uint8x8_t)(__p0);
  return __ret;
}
#else
__ai uint8x8_t vreinterpret_u8_s64(int64x1_t __p0) {
  uint8x8_t __ret;
  __ret = (uint8x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x8_t vreinterpret_u8_s16(int16x4_t __p0) {
  uint8x8_t __ret;
  __ret = (uint8x8_t)(__p0);
  return __ret;
}
#else
__ai uint8x8_t vreinterpret_u8_s16(int16x4_t __p0) {
  uint8x8_t __ret;
  __ret = (uint8x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x2_t vreinterpret_u32_p8(poly8x8_t __p0) {
  uint32x2_t __ret;
  __ret = (uint32x2_t)(__p0);
  return __ret;
}
#else
__ai uint32x2_t vreinterpret_u32_p8(poly8x8_t __p0) {
  uint32x2_t __ret;
  __ret = (uint32x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x2_t vreinterpret_u32_p64(poly64x1_t __p0) {
  uint32x2_t __ret;
  __ret = (uint32x2_t)(__p0);
  return __ret;
}
#else
__ai uint32x2_t vreinterpret_u32_p64(poly64x1_t __p0) {
  uint32x2_t __ret;
  __ret = (uint32x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x2_t vreinterpret_u32_p16(poly16x4_t __p0) {
  uint32x2_t __ret;
  __ret = (uint32x2_t)(__p0);
  return __ret;
}
#else
__ai uint32x2_t vreinterpret_u32_p16(poly16x4_t __p0) {
  uint32x2_t __ret;
  __ret = (uint32x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x2_t vreinterpret_u32_u8(uint8x8_t __p0) {
  uint32x2_t __ret;
  __ret = (uint32x2_t)(__p0);
  return __ret;
}
#else
__ai uint32x2_t vreinterpret_u32_u8(uint8x8_t __p0) {
  uint32x2_t __ret;
  __ret = (uint32x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x2_t vreinterpret_u32_u64(uint64x1_t __p0) {
  uint32x2_t __ret;
  __ret = (uint32x2_t)(__p0);
  return __ret;
}
#else
__ai uint32x2_t vreinterpret_u32_u64(uint64x1_t __p0) {
  uint32x2_t __ret;
  __ret = (uint32x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x2_t vreinterpret_u32_u16(uint16x4_t __p0) {
  uint32x2_t __ret;
  __ret = (uint32x2_t)(__p0);
  return __ret;
}
#else
__ai uint32x2_t vreinterpret_u32_u16(uint16x4_t __p0) {
  uint32x2_t __ret;
  __ret = (uint32x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x2_t vreinterpret_u32_s8(int8x8_t __p0) {
  uint32x2_t __ret;
  __ret = (uint32x2_t)(__p0);
  return __ret;
}
#else
__ai uint32x2_t vreinterpret_u32_s8(int8x8_t __p0) {
  uint32x2_t __ret;
  __ret = (uint32x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x2_t vreinterpret_u32_f64(float64x1_t __p0) {
  uint32x2_t __ret;
  __ret = (uint32x2_t)(__p0);
  return __ret;
}
#else
__ai uint32x2_t vreinterpret_u32_f64(float64x1_t __p0) {
  uint32x2_t __ret;
  __ret = (uint32x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x2_t vreinterpret_u32_f32(float32x2_t __p0) {
  uint32x2_t __ret;
  __ret = (uint32x2_t)(__p0);
  return __ret;
}
#else
__ai uint32x2_t vreinterpret_u32_f32(float32x2_t __p0) {
  uint32x2_t __ret;
  __ret = (uint32x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x2_t vreinterpret_u32_f16(float16x4_t __p0) {
  uint32x2_t __ret;
  __ret = (uint32x2_t)(__p0);
  return __ret;
}
#else
__ai uint32x2_t vreinterpret_u32_f16(float16x4_t __p0) {
  uint32x2_t __ret;
  __ret = (uint32x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x2_t vreinterpret_u32_s32(int32x2_t __p0) {
  uint32x2_t __ret;
  __ret = (uint32x2_t)(__p0);
  return __ret;
}
#else
__ai uint32x2_t vreinterpret_u32_s32(int32x2_t __p0) {
  uint32x2_t __ret;
  __ret = (uint32x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x2_t vreinterpret_u32_s64(int64x1_t __p0) {
  uint32x2_t __ret;
  __ret = (uint32x2_t)(__p0);
  return __ret;
}
#else
__ai uint32x2_t vreinterpret_u32_s64(int64x1_t __p0) {
  uint32x2_t __ret;
  __ret = (uint32x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x2_t vreinterpret_u32_s16(int16x4_t __p0) {
  uint32x2_t __ret;
  __ret = (uint32x2_t)(__p0);
  return __ret;
}
#else
__ai uint32x2_t vreinterpret_u32_s16(int16x4_t __p0) {
  uint32x2_t __ret;
  __ret = (uint32x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x1_t vreinterpret_u64_p8(poly8x8_t __p0) {
  uint64x1_t __ret;
  __ret = (uint64x1_t)(__p0);
  return __ret;
}
#else
__ai uint64x1_t vreinterpret_u64_p8(poly8x8_t __p0) {
  uint64x1_t __ret;
  __ret = (uint64x1_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x1_t vreinterpret_u64_p64(poly64x1_t __p0) {
  uint64x1_t __ret;
  __ret = (uint64x1_t)(__p0);
  return __ret;
}
#else
__ai uint64x1_t vreinterpret_u64_p64(poly64x1_t __p0) {
  uint64x1_t __ret;
  __ret = (uint64x1_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x1_t vreinterpret_u64_p16(poly16x4_t __p0) {
  uint64x1_t __ret;
  __ret = (uint64x1_t)(__p0);
  return __ret;
}
#else
__ai uint64x1_t vreinterpret_u64_p16(poly16x4_t __p0) {
  uint64x1_t __ret;
  __ret = (uint64x1_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x1_t vreinterpret_u64_u8(uint8x8_t __p0) {
  uint64x1_t __ret;
  __ret = (uint64x1_t)(__p0);
  return __ret;
}
#else
__ai uint64x1_t vreinterpret_u64_u8(uint8x8_t __p0) {
  uint64x1_t __ret;
  __ret = (uint64x1_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x1_t vreinterpret_u64_u32(uint32x2_t __p0) {
  uint64x1_t __ret;
  __ret = (uint64x1_t)(__p0);
  return __ret;
}
#else
__ai uint64x1_t vreinterpret_u64_u32(uint32x2_t __p0) {
  uint64x1_t __ret;
  __ret = (uint64x1_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x1_t vreinterpret_u64_u16(uint16x4_t __p0) {
  uint64x1_t __ret;
  __ret = (uint64x1_t)(__p0);
  return __ret;
}
#else
__ai uint64x1_t vreinterpret_u64_u16(uint16x4_t __p0) {
  uint64x1_t __ret;
  __ret = (uint64x1_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x1_t vreinterpret_u64_s8(int8x8_t __p0) {
  uint64x1_t __ret;
  __ret = (uint64x1_t)(__p0);
  return __ret;
}
#else
__ai uint64x1_t vreinterpret_u64_s8(int8x8_t __p0) {
  uint64x1_t __ret;
  __ret = (uint64x1_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x1_t vreinterpret_u64_f64(float64x1_t __p0) {
  uint64x1_t __ret;
  __ret = (uint64x1_t)(__p0);
  return __ret;
}
#else
__ai uint64x1_t vreinterpret_u64_f64(float64x1_t __p0) {
  uint64x1_t __ret;
  __ret = (uint64x1_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x1_t vreinterpret_u64_f32(float32x2_t __p0) {
  uint64x1_t __ret;
  __ret = (uint64x1_t)(__p0);
  return __ret;
}
#else
__ai uint64x1_t vreinterpret_u64_f32(float32x2_t __p0) {
  uint64x1_t __ret;
  __ret = (uint64x1_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x1_t vreinterpret_u64_f16(float16x4_t __p0) {
  uint64x1_t __ret;
  __ret = (uint64x1_t)(__p0);
  return __ret;
}
#else
__ai uint64x1_t vreinterpret_u64_f16(float16x4_t __p0) {
  uint64x1_t __ret;
  __ret = (uint64x1_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x1_t vreinterpret_u64_s32(int32x2_t __p0) {
  uint64x1_t __ret;
  __ret = (uint64x1_t)(__p0);
  return __ret;
}
#else
__ai uint64x1_t vreinterpret_u64_s32(int32x2_t __p0) {
  uint64x1_t __ret;
  __ret = (uint64x1_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x1_t vreinterpret_u64_s64(int64x1_t __p0) {
  uint64x1_t __ret;
  __ret = (uint64x1_t)(__p0);
  return __ret;
}
#else
__ai uint64x1_t vreinterpret_u64_s64(int64x1_t __p0) {
  uint64x1_t __ret;
  __ret = (uint64x1_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x1_t vreinterpret_u64_s16(int16x4_t __p0) {
  uint64x1_t __ret;
  __ret = (uint64x1_t)(__p0);
  return __ret;
}
#else
__ai uint64x1_t vreinterpret_u64_s16(int16x4_t __p0) {
  uint64x1_t __ret;
  __ret = (uint64x1_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x4_t vreinterpret_u16_p8(poly8x8_t __p0) {
  uint16x4_t __ret;
  __ret = (uint16x4_t)(__p0);
  return __ret;
}
#else
__ai uint16x4_t vreinterpret_u16_p8(poly8x8_t __p0) {
  uint16x4_t __ret;
  __ret = (uint16x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x4_t vreinterpret_u16_p64(poly64x1_t __p0) {
  uint16x4_t __ret;
  __ret = (uint16x4_t)(__p0);
  return __ret;
}
#else
__ai uint16x4_t vreinterpret_u16_p64(poly64x1_t __p0) {
  uint16x4_t __ret;
  __ret = (uint16x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x4_t vreinterpret_u16_p16(poly16x4_t __p0) {
  uint16x4_t __ret;
  __ret = (uint16x4_t)(__p0);
  return __ret;
}
#else
__ai uint16x4_t vreinterpret_u16_p16(poly16x4_t __p0) {
  uint16x4_t __ret;
  __ret = (uint16x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x4_t vreinterpret_u16_u8(uint8x8_t __p0) {
  uint16x4_t __ret;
  __ret = (uint16x4_t)(__p0);
  return __ret;
}
#else
__ai uint16x4_t vreinterpret_u16_u8(uint8x8_t __p0) {
  uint16x4_t __ret;
  __ret = (uint16x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x4_t vreinterpret_u16_u32(uint32x2_t __p0) {
  uint16x4_t __ret;
  __ret = (uint16x4_t)(__p0);
  return __ret;
}
#else
__ai uint16x4_t vreinterpret_u16_u32(uint32x2_t __p0) {
  uint16x4_t __ret;
  __ret = (uint16x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x4_t vreinterpret_u16_u64(uint64x1_t __p0) {
  uint16x4_t __ret;
  __ret = (uint16x4_t)(__p0);
  return __ret;
}
#else
__ai uint16x4_t vreinterpret_u16_u64(uint64x1_t __p0) {
  uint16x4_t __ret;
  __ret = (uint16x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x4_t vreinterpret_u16_s8(int8x8_t __p0) {
  uint16x4_t __ret;
  __ret = (uint16x4_t)(__p0);
  return __ret;
}
#else
__ai uint16x4_t vreinterpret_u16_s8(int8x8_t __p0) {
  uint16x4_t __ret;
  __ret = (uint16x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x4_t vreinterpret_u16_f64(float64x1_t __p0) {
  uint16x4_t __ret;
  __ret = (uint16x4_t)(__p0);
  return __ret;
}
#else
__ai uint16x4_t vreinterpret_u16_f64(float64x1_t __p0) {
  uint16x4_t __ret;
  __ret = (uint16x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x4_t vreinterpret_u16_f32(float32x2_t __p0) {
  uint16x4_t __ret;
  __ret = (uint16x4_t)(__p0);
  return __ret;
}
#else
__ai uint16x4_t vreinterpret_u16_f32(float32x2_t __p0) {
  uint16x4_t __ret;
  __ret = (uint16x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x4_t vreinterpret_u16_f16(float16x4_t __p0) {
  uint16x4_t __ret;
  __ret = (uint16x4_t)(__p0);
  return __ret;
}
#else
__ai uint16x4_t vreinterpret_u16_f16(float16x4_t __p0) {
  uint16x4_t __ret;
  __ret = (uint16x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x4_t vreinterpret_u16_s32(int32x2_t __p0) {
  uint16x4_t __ret;
  __ret = (uint16x4_t)(__p0);
  return __ret;
}
#else
__ai uint16x4_t vreinterpret_u16_s32(int32x2_t __p0) {
  uint16x4_t __ret;
  __ret = (uint16x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x4_t vreinterpret_u16_s64(int64x1_t __p0) {
  uint16x4_t __ret;
  __ret = (uint16x4_t)(__p0);
  return __ret;
}
#else
__ai uint16x4_t vreinterpret_u16_s64(int64x1_t __p0) {
  uint16x4_t __ret;
  __ret = (uint16x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x4_t vreinterpret_u16_s16(int16x4_t __p0) {
  uint16x4_t __ret;
  __ret = (uint16x4_t)(__p0);
  return __ret;
}
#else
__ai uint16x4_t vreinterpret_u16_s16(int16x4_t __p0) {
  uint16x4_t __ret;
  __ret = (uint16x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x8_t vreinterpret_s8_p8(poly8x8_t __p0) {
  int8x8_t __ret;
  __ret = (int8x8_t)(__p0);
  return __ret;
}
#else
__ai int8x8_t vreinterpret_s8_p8(poly8x8_t __p0) {
  int8x8_t __ret;
  __ret = (int8x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x8_t vreinterpret_s8_p64(poly64x1_t __p0) {
  int8x8_t __ret;
  __ret = (int8x8_t)(__p0);
  return __ret;
}
#else
__ai int8x8_t vreinterpret_s8_p64(poly64x1_t __p0) {
  int8x8_t __ret;
  __ret = (int8x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x8_t vreinterpret_s8_p16(poly16x4_t __p0) {
  int8x8_t __ret;
  __ret = (int8x8_t)(__p0);
  return __ret;
}
#else
__ai int8x8_t vreinterpret_s8_p16(poly16x4_t __p0) {
  int8x8_t __ret;
  __ret = (int8x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x8_t vreinterpret_s8_u8(uint8x8_t __p0) {
  int8x8_t __ret;
  __ret = (int8x8_t)(__p0);
  return __ret;
}
#else
__ai int8x8_t vreinterpret_s8_u8(uint8x8_t __p0) {
  int8x8_t __ret;
  __ret = (int8x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x8_t vreinterpret_s8_u32(uint32x2_t __p0) {
  int8x8_t __ret;
  __ret = (int8x8_t)(__p0);
  return __ret;
}
#else
__ai int8x8_t vreinterpret_s8_u32(uint32x2_t __p0) {
  int8x8_t __ret;
  __ret = (int8x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x8_t vreinterpret_s8_u64(uint64x1_t __p0) {
  int8x8_t __ret;
  __ret = (int8x8_t)(__p0);
  return __ret;
}
#else
__ai int8x8_t vreinterpret_s8_u64(uint64x1_t __p0) {
  int8x8_t __ret;
  __ret = (int8x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x8_t vreinterpret_s8_u16(uint16x4_t __p0) {
  int8x8_t __ret;
  __ret = (int8x8_t)(__p0);
  return __ret;
}
#else
__ai int8x8_t vreinterpret_s8_u16(uint16x4_t __p0) {
  int8x8_t __ret;
  __ret = (int8x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x8_t vreinterpret_s8_f64(float64x1_t __p0) {
  int8x8_t __ret;
  __ret = (int8x8_t)(__p0);
  return __ret;
}
#else
__ai int8x8_t vreinterpret_s8_f64(float64x1_t __p0) {
  int8x8_t __ret;
  __ret = (int8x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x8_t vreinterpret_s8_f32(float32x2_t __p0) {
  int8x8_t __ret;
  __ret = (int8x8_t)(__p0);
  return __ret;
}
#else
__ai int8x8_t vreinterpret_s8_f32(float32x2_t __p0) {
  int8x8_t __ret;
  __ret = (int8x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x8_t vreinterpret_s8_f16(float16x4_t __p0) {
  int8x8_t __ret;
  __ret = (int8x8_t)(__p0);
  return __ret;
}
#else
__ai int8x8_t vreinterpret_s8_f16(float16x4_t __p0) {
  int8x8_t __ret;
  __ret = (int8x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x8_t vreinterpret_s8_s32(int32x2_t __p0) {
  int8x8_t __ret;
  __ret = (int8x8_t)(__p0);
  return __ret;
}
#else
__ai int8x8_t vreinterpret_s8_s32(int32x2_t __p0) {
  int8x8_t __ret;
  __ret = (int8x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x8_t vreinterpret_s8_s64(int64x1_t __p0) {
  int8x8_t __ret;
  __ret = (int8x8_t)(__p0);
  return __ret;
}
#else
__ai int8x8_t vreinterpret_s8_s64(int64x1_t __p0) {
  int8x8_t __ret;
  __ret = (int8x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x8_t vreinterpret_s8_s16(int16x4_t __p0) {
  int8x8_t __ret;
  __ret = (int8x8_t)(__p0);
  return __ret;
}
#else
__ai int8x8_t vreinterpret_s8_s16(int16x4_t __p0) {
  int8x8_t __ret;
  __ret = (int8x8_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x1_t vreinterpret_f64_p8(poly8x8_t __p0) {
  float64x1_t __ret;
  __ret = (float64x1_t)(__p0);
  return __ret;
}
#else
__ai float64x1_t vreinterpret_f64_p8(poly8x8_t __p0) {
  float64x1_t __ret;
  __ret = (float64x1_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x1_t vreinterpret_f64_p64(poly64x1_t __p0) {
  float64x1_t __ret;
  __ret = (float64x1_t)(__p0);
  return __ret;
}
#else
__ai float64x1_t vreinterpret_f64_p64(poly64x1_t __p0) {
  float64x1_t __ret;
  __ret = (float64x1_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x1_t vreinterpret_f64_p16(poly16x4_t __p0) {
  float64x1_t __ret;
  __ret = (float64x1_t)(__p0);
  return __ret;
}
#else
__ai float64x1_t vreinterpret_f64_p16(poly16x4_t __p0) {
  float64x1_t __ret;
  __ret = (float64x1_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x1_t vreinterpret_f64_u8(uint8x8_t __p0) {
  float64x1_t __ret;
  __ret = (float64x1_t)(__p0);
  return __ret;
}
#else
__ai float64x1_t vreinterpret_f64_u8(uint8x8_t __p0) {
  float64x1_t __ret;
  __ret = (float64x1_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x1_t vreinterpret_f64_u32(uint32x2_t __p0) {
  float64x1_t __ret;
  __ret = (float64x1_t)(__p0);
  return __ret;
}
#else
__ai float64x1_t vreinterpret_f64_u32(uint32x2_t __p0) {
  float64x1_t __ret;
  __ret = (float64x1_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x1_t vreinterpret_f64_u64(uint64x1_t __p0) {
  float64x1_t __ret;
  __ret = (float64x1_t)(__p0);
  return __ret;
}
#else
__ai float64x1_t vreinterpret_f64_u64(uint64x1_t __p0) {
  float64x1_t __ret;
  __ret = (float64x1_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x1_t vreinterpret_f64_u16(uint16x4_t __p0) {
  float64x1_t __ret;
  __ret = (float64x1_t)(__p0);
  return __ret;
}
#else
__ai float64x1_t vreinterpret_f64_u16(uint16x4_t __p0) {
  float64x1_t __ret;
  __ret = (float64x1_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x1_t vreinterpret_f64_s8(int8x8_t __p0) {
  float64x1_t __ret;
  __ret = (float64x1_t)(__p0);
  return __ret;
}
#else
__ai float64x1_t vreinterpret_f64_s8(int8x8_t __p0) {
  float64x1_t __ret;
  __ret = (float64x1_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x1_t vreinterpret_f64_f32(float32x2_t __p0) {
  float64x1_t __ret;
  __ret = (float64x1_t)(__p0);
  return __ret;
}
#else
__ai float64x1_t vreinterpret_f64_f32(float32x2_t __p0) {
  float64x1_t __ret;
  __ret = (float64x1_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x1_t vreinterpret_f64_f16(float16x4_t __p0) {
  float64x1_t __ret;
  __ret = (float64x1_t)(__p0);
  return __ret;
}
#else
__ai float64x1_t vreinterpret_f64_f16(float16x4_t __p0) {
  float64x1_t __ret;
  __ret = (float64x1_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x1_t vreinterpret_f64_s32(int32x2_t __p0) {
  float64x1_t __ret;
  __ret = (float64x1_t)(__p0);
  return __ret;
}
#else
__ai float64x1_t vreinterpret_f64_s32(int32x2_t __p0) {
  float64x1_t __ret;
  __ret = (float64x1_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x1_t vreinterpret_f64_s64(int64x1_t __p0) {
  float64x1_t __ret;
  __ret = (float64x1_t)(__p0);
  return __ret;
}
#else
__ai float64x1_t vreinterpret_f64_s64(int64x1_t __p0) {
  float64x1_t __ret;
  __ret = (float64x1_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x1_t vreinterpret_f64_s16(int16x4_t __p0) {
  float64x1_t __ret;
  __ret = (float64x1_t)(__p0);
  return __ret;
}
#else
__ai float64x1_t vreinterpret_f64_s16(int16x4_t __p0) {
  float64x1_t __ret;
  __ret = (float64x1_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x2_t vreinterpret_f32_p8(poly8x8_t __p0) {
  float32x2_t __ret;
  __ret = (float32x2_t)(__p0);
  return __ret;
}
#else
__ai float32x2_t vreinterpret_f32_p8(poly8x8_t __p0) {
  float32x2_t __ret;
  __ret = (float32x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x2_t vreinterpret_f32_p64(poly64x1_t __p0) {
  float32x2_t __ret;
  __ret = (float32x2_t)(__p0);
  return __ret;
}
#else
__ai float32x2_t vreinterpret_f32_p64(poly64x1_t __p0) {
  float32x2_t __ret;
  __ret = (float32x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x2_t vreinterpret_f32_p16(poly16x4_t __p0) {
  float32x2_t __ret;
  __ret = (float32x2_t)(__p0);
  return __ret;
}
#else
__ai float32x2_t vreinterpret_f32_p16(poly16x4_t __p0) {
  float32x2_t __ret;
  __ret = (float32x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x2_t vreinterpret_f32_u8(uint8x8_t __p0) {
  float32x2_t __ret;
  __ret = (float32x2_t)(__p0);
  return __ret;
}
#else
__ai float32x2_t vreinterpret_f32_u8(uint8x8_t __p0) {
  float32x2_t __ret;
  __ret = (float32x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x2_t vreinterpret_f32_u32(uint32x2_t __p0) {
  float32x2_t __ret;
  __ret = (float32x2_t)(__p0);
  return __ret;
}
#else
__ai float32x2_t vreinterpret_f32_u32(uint32x2_t __p0) {
  float32x2_t __ret;
  __ret = (float32x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x2_t vreinterpret_f32_u64(uint64x1_t __p0) {
  float32x2_t __ret;
  __ret = (float32x2_t)(__p0);
  return __ret;
}
#else
__ai float32x2_t vreinterpret_f32_u64(uint64x1_t __p0) {
  float32x2_t __ret;
  __ret = (float32x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x2_t vreinterpret_f32_u16(uint16x4_t __p0) {
  float32x2_t __ret;
  __ret = (float32x2_t)(__p0);
  return __ret;
}
#else
__ai float32x2_t vreinterpret_f32_u16(uint16x4_t __p0) {
  float32x2_t __ret;
  __ret = (float32x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x2_t vreinterpret_f32_s8(int8x8_t __p0) {
  float32x2_t __ret;
  __ret = (float32x2_t)(__p0);
  return __ret;
}
#else
__ai float32x2_t vreinterpret_f32_s8(int8x8_t __p0) {
  float32x2_t __ret;
  __ret = (float32x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x2_t vreinterpret_f32_f64(float64x1_t __p0) {
  float32x2_t __ret;
  __ret = (float32x2_t)(__p0);
  return __ret;
}
#else
__ai float32x2_t vreinterpret_f32_f64(float64x1_t __p0) {
  float32x2_t __ret;
  __ret = (float32x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x2_t vreinterpret_f32_f16(float16x4_t __p0) {
  float32x2_t __ret;
  __ret = (float32x2_t)(__p0);
  return __ret;
}
#else
__ai float32x2_t vreinterpret_f32_f16(float16x4_t __p0) {
  float32x2_t __ret;
  __ret = (float32x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x2_t vreinterpret_f32_s32(int32x2_t __p0) {
  float32x2_t __ret;
  __ret = (float32x2_t)(__p0);
  return __ret;
}
#else
__ai float32x2_t vreinterpret_f32_s32(int32x2_t __p0) {
  float32x2_t __ret;
  __ret = (float32x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x2_t vreinterpret_f32_s64(int64x1_t __p0) {
  float32x2_t __ret;
  __ret = (float32x2_t)(__p0);
  return __ret;
}
#else
__ai float32x2_t vreinterpret_f32_s64(int64x1_t __p0) {
  float32x2_t __ret;
  __ret = (float32x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x2_t vreinterpret_f32_s16(int16x4_t __p0) {
  float32x2_t __ret;
  __ret = (float32x2_t)(__p0);
  return __ret;
}
#else
__ai float32x2_t vreinterpret_f32_s16(int16x4_t __p0) {
  float32x2_t __ret;
  __ret = (float32x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float16x4_t vreinterpret_f16_p8(poly8x8_t __p0) {
  float16x4_t __ret;
  __ret = (float16x4_t)(__p0);
  return __ret;
}
#else
__ai float16x4_t vreinterpret_f16_p8(poly8x8_t __p0) {
  float16x4_t __ret;
  __ret = (float16x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float16x4_t vreinterpret_f16_p64(poly64x1_t __p0) {
  float16x4_t __ret;
  __ret = (float16x4_t)(__p0);
  return __ret;
}
#else
__ai float16x4_t vreinterpret_f16_p64(poly64x1_t __p0) {
  float16x4_t __ret;
  __ret = (float16x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float16x4_t vreinterpret_f16_p16(poly16x4_t __p0) {
  float16x4_t __ret;
  __ret = (float16x4_t)(__p0);
  return __ret;
}
#else
__ai float16x4_t vreinterpret_f16_p16(poly16x4_t __p0) {
  float16x4_t __ret;
  __ret = (float16x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float16x4_t vreinterpret_f16_u8(uint8x8_t __p0) {
  float16x4_t __ret;
  __ret = (float16x4_t)(__p0);
  return __ret;
}
#else
__ai float16x4_t vreinterpret_f16_u8(uint8x8_t __p0) {
  float16x4_t __ret;
  __ret = (float16x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float16x4_t vreinterpret_f16_u32(uint32x2_t __p0) {
  float16x4_t __ret;
  __ret = (float16x4_t)(__p0);
  return __ret;
}
#else
__ai float16x4_t vreinterpret_f16_u32(uint32x2_t __p0) {
  float16x4_t __ret;
  __ret = (float16x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float16x4_t vreinterpret_f16_u64(uint64x1_t __p0) {
  float16x4_t __ret;
  __ret = (float16x4_t)(__p0);
  return __ret;
}
#else
__ai float16x4_t vreinterpret_f16_u64(uint64x1_t __p0) {
  float16x4_t __ret;
  __ret = (float16x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float16x4_t vreinterpret_f16_u16(uint16x4_t __p0) {
  float16x4_t __ret;
  __ret = (float16x4_t)(__p0);
  return __ret;
}
#else
__ai float16x4_t vreinterpret_f16_u16(uint16x4_t __p0) {
  float16x4_t __ret;
  __ret = (float16x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float16x4_t vreinterpret_f16_s8(int8x8_t __p0) {
  float16x4_t __ret;
  __ret = (float16x4_t)(__p0);
  return __ret;
}
#else
__ai float16x4_t vreinterpret_f16_s8(int8x8_t __p0) {
  float16x4_t __ret;
  __ret = (float16x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float16x4_t vreinterpret_f16_f64(float64x1_t __p0) {
  float16x4_t __ret;
  __ret = (float16x4_t)(__p0);
  return __ret;
}
#else
__ai float16x4_t vreinterpret_f16_f64(float64x1_t __p0) {
  float16x4_t __ret;
  __ret = (float16x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float16x4_t vreinterpret_f16_f32(float32x2_t __p0) {
  float16x4_t __ret;
  __ret = (float16x4_t)(__p0);
  return __ret;
}
#else
__ai float16x4_t vreinterpret_f16_f32(float32x2_t __p0) {
  float16x4_t __ret;
  __ret = (float16x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float16x4_t vreinterpret_f16_s32(int32x2_t __p0) {
  float16x4_t __ret;
  __ret = (float16x4_t)(__p0);
  return __ret;
}
#else
__ai float16x4_t vreinterpret_f16_s32(int32x2_t __p0) {
  float16x4_t __ret;
  __ret = (float16x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float16x4_t vreinterpret_f16_s64(int64x1_t __p0) {
  float16x4_t __ret;
  __ret = (float16x4_t)(__p0);
  return __ret;
}
#else
__ai float16x4_t vreinterpret_f16_s64(int64x1_t __p0) {
  float16x4_t __ret;
  __ret = (float16x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float16x4_t vreinterpret_f16_s16(int16x4_t __p0) {
  float16x4_t __ret;
  __ret = (float16x4_t)(__p0);
  return __ret;
}
#else
__ai float16x4_t vreinterpret_f16_s16(int16x4_t __p0) {
  float16x4_t __ret;
  __ret = (float16x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x2_t vreinterpret_s32_p8(poly8x8_t __p0) {
  int32x2_t __ret;
  __ret = (int32x2_t)(__p0);
  return __ret;
}
#else
__ai int32x2_t vreinterpret_s32_p8(poly8x8_t __p0) {
  int32x2_t __ret;
  __ret = (int32x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x2_t vreinterpret_s32_p64(poly64x1_t __p0) {
  int32x2_t __ret;
  __ret = (int32x2_t)(__p0);
  return __ret;
}
#else
__ai int32x2_t vreinterpret_s32_p64(poly64x1_t __p0) {
  int32x2_t __ret;
  __ret = (int32x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x2_t vreinterpret_s32_p16(poly16x4_t __p0) {
  int32x2_t __ret;
  __ret = (int32x2_t)(__p0);
  return __ret;
}
#else
__ai int32x2_t vreinterpret_s32_p16(poly16x4_t __p0) {
  int32x2_t __ret;
  __ret = (int32x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x2_t vreinterpret_s32_u8(uint8x8_t __p0) {
  int32x2_t __ret;
  __ret = (int32x2_t)(__p0);
  return __ret;
}
#else
__ai int32x2_t vreinterpret_s32_u8(uint8x8_t __p0) {
  int32x2_t __ret;
  __ret = (int32x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x2_t vreinterpret_s32_u32(uint32x2_t __p0) {
  int32x2_t __ret;
  __ret = (int32x2_t)(__p0);
  return __ret;
}
#else
__ai int32x2_t vreinterpret_s32_u32(uint32x2_t __p0) {
  int32x2_t __ret;
  __ret = (int32x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x2_t vreinterpret_s32_u64(uint64x1_t __p0) {
  int32x2_t __ret;
  __ret = (int32x2_t)(__p0);
  return __ret;
}
#else
__ai int32x2_t vreinterpret_s32_u64(uint64x1_t __p0) {
  int32x2_t __ret;
  __ret = (int32x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x2_t vreinterpret_s32_u16(uint16x4_t __p0) {
  int32x2_t __ret;
  __ret = (int32x2_t)(__p0);
  return __ret;
}
#else
__ai int32x2_t vreinterpret_s32_u16(uint16x4_t __p0) {
  int32x2_t __ret;
  __ret = (int32x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x2_t vreinterpret_s32_s8(int8x8_t __p0) {
  int32x2_t __ret;
  __ret = (int32x2_t)(__p0);
  return __ret;
}
#else
__ai int32x2_t vreinterpret_s32_s8(int8x8_t __p0) {
  int32x2_t __ret;
  __ret = (int32x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x2_t vreinterpret_s32_f64(float64x1_t __p0) {
  int32x2_t __ret;
  __ret = (int32x2_t)(__p0);
  return __ret;
}
#else
__ai int32x2_t vreinterpret_s32_f64(float64x1_t __p0) {
  int32x2_t __ret;
  __ret = (int32x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x2_t vreinterpret_s32_f32(float32x2_t __p0) {
  int32x2_t __ret;
  __ret = (int32x2_t)(__p0);
  return __ret;
}
#else
__ai int32x2_t vreinterpret_s32_f32(float32x2_t __p0) {
  int32x2_t __ret;
  __ret = (int32x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x2_t vreinterpret_s32_f16(float16x4_t __p0) {
  int32x2_t __ret;
  __ret = (int32x2_t)(__p0);
  return __ret;
}
#else
__ai int32x2_t vreinterpret_s32_f16(float16x4_t __p0) {
  int32x2_t __ret;
  __ret = (int32x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x2_t vreinterpret_s32_s64(int64x1_t __p0) {
  int32x2_t __ret;
  __ret = (int32x2_t)(__p0);
  return __ret;
}
#else
__ai int32x2_t vreinterpret_s32_s64(int64x1_t __p0) {
  int32x2_t __ret;
  __ret = (int32x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x2_t vreinterpret_s32_s16(int16x4_t __p0) {
  int32x2_t __ret;
  __ret = (int32x2_t)(__p0);
  return __ret;
}
#else
__ai int32x2_t vreinterpret_s32_s16(int16x4_t __p0) {
  int32x2_t __ret;
  __ret = (int32x2_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x1_t vreinterpret_s64_p8(poly8x8_t __p0) {
  int64x1_t __ret;
  __ret = (int64x1_t)(__p0);
  return __ret;
}
#else
__ai int64x1_t vreinterpret_s64_p8(poly8x8_t __p0) {
  int64x1_t __ret;
  __ret = (int64x1_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x1_t vreinterpret_s64_p64(poly64x1_t __p0) {
  int64x1_t __ret;
  __ret = (int64x1_t)(__p0);
  return __ret;
}
#else
__ai int64x1_t vreinterpret_s64_p64(poly64x1_t __p0) {
  int64x1_t __ret;
  __ret = (int64x1_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x1_t vreinterpret_s64_p16(poly16x4_t __p0) {
  int64x1_t __ret;
  __ret = (int64x1_t)(__p0);
  return __ret;
}
#else
__ai int64x1_t vreinterpret_s64_p16(poly16x4_t __p0) {
  int64x1_t __ret;
  __ret = (int64x1_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x1_t vreinterpret_s64_u8(uint8x8_t __p0) {
  int64x1_t __ret;
  __ret = (int64x1_t)(__p0);
  return __ret;
}
#else
__ai int64x1_t vreinterpret_s64_u8(uint8x8_t __p0) {
  int64x1_t __ret;
  __ret = (int64x1_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x1_t vreinterpret_s64_u32(uint32x2_t __p0) {
  int64x1_t __ret;
  __ret = (int64x1_t)(__p0);
  return __ret;
}
#else
__ai int64x1_t vreinterpret_s64_u32(uint32x2_t __p0) {
  int64x1_t __ret;
  __ret = (int64x1_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x1_t vreinterpret_s64_u64(uint64x1_t __p0) {
  int64x1_t __ret;
  __ret = (int64x1_t)(__p0);
  return __ret;
}
#else
__ai int64x1_t vreinterpret_s64_u64(uint64x1_t __p0) {
  int64x1_t __ret;
  __ret = (int64x1_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x1_t vreinterpret_s64_u16(uint16x4_t __p0) {
  int64x1_t __ret;
  __ret = (int64x1_t)(__p0);
  return __ret;
}
#else
__ai int64x1_t vreinterpret_s64_u16(uint16x4_t __p0) {
  int64x1_t __ret;
  __ret = (int64x1_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x1_t vreinterpret_s64_s8(int8x8_t __p0) {
  int64x1_t __ret;
  __ret = (int64x1_t)(__p0);
  return __ret;
}
#else
__ai int64x1_t vreinterpret_s64_s8(int8x8_t __p0) {
  int64x1_t __ret;
  __ret = (int64x1_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x1_t vreinterpret_s64_f64(float64x1_t __p0) {
  int64x1_t __ret;
  __ret = (int64x1_t)(__p0);
  return __ret;
}
#else
__ai int64x1_t vreinterpret_s64_f64(float64x1_t __p0) {
  int64x1_t __ret;
  __ret = (int64x1_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x1_t vreinterpret_s64_f32(float32x2_t __p0) {
  int64x1_t __ret;
  __ret = (int64x1_t)(__p0);
  return __ret;
}
#else
__ai int64x1_t vreinterpret_s64_f32(float32x2_t __p0) {
  int64x1_t __ret;
  __ret = (int64x1_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x1_t vreinterpret_s64_f16(float16x4_t __p0) {
  int64x1_t __ret;
  __ret = (int64x1_t)(__p0);
  return __ret;
}
#else
__ai int64x1_t vreinterpret_s64_f16(float16x4_t __p0) {
  int64x1_t __ret;
  __ret = (int64x1_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x1_t vreinterpret_s64_s32(int32x2_t __p0) {
  int64x1_t __ret;
  __ret = (int64x1_t)(__p0);
  return __ret;
}
#else
__ai int64x1_t vreinterpret_s64_s32(int32x2_t __p0) {
  int64x1_t __ret;
  __ret = (int64x1_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x1_t vreinterpret_s64_s16(int16x4_t __p0) {
  int64x1_t __ret;
  __ret = (int64x1_t)(__p0);
  return __ret;
}
#else
__ai int64x1_t vreinterpret_s64_s16(int16x4_t __p0) {
  int64x1_t __ret;
  __ret = (int64x1_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x4_t vreinterpret_s16_p8(poly8x8_t __p0) {
  int16x4_t __ret;
  __ret = (int16x4_t)(__p0);
  return __ret;
}
#else
__ai int16x4_t vreinterpret_s16_p8(poly8x8_t __p0) {
  int16x4_t __ret;
  __ret = (int16x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x4_t vreinterpret_s16_p64(poly64x1_t __p0) {
  int16x4_t __ret;
  __ret = (int16x4_t)(__p0);
  return __ret;
}
#else
__ai int16x4_t vreinterpret_s16_p64(poly64x1_t __p0) {
  int16x4_t __ret;
  __ret = (int16x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x4_t vreinterpret_s16_p16(poly16x4_t __p0) {
  int16x4_t __ret;
  __ret = (int16x4_t)(__p0);
  return __ret;
}
#else
__ai int16x4_t vreinterpret_s16_p16(poly16x4_t __p0) {
  int16x4_t __ret;
  __ret = (int16x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x4_t vreinterpret_s16_u8(uint8x8_t __p0) {
  int16x4_t __ret;
  __ret = (int16x4_t)(__p0);
  return __ret;
}
#else
__ai int16x4_t vreinterpret_s16_u8(uint8x8_t __p0) {
  int16x4_t __ret;
  __ret = (int16x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x4_t vreinterpret_s16_u32(uint32x2_t __p0) {
  int16x4_t __ret;
  __ret = (int16x4_t)(__p0);
  return __ret;
}
#else
__ai int16x4_t vreinterpret_s16_u32(uint32x2_t __p0) {
  int16x4_t __ret;
  __ret = (int16x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x4_t vreinterpret_s16_u64(uint64x1_t __p0) {
  int16x4_t __ret;
  __ret = (int16x4_t)(__p0);
  return __ret;
}
#else
__ai int16x4_t vreinterpret_s16_u64(uint64x1_t __p0) {
  int16x4_t __ret;
  __ret = (int16x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x4_t vreinterpret_s16_u16(uint16x4_t __p0) {
  int16x4_t __ret;
  __ret = (int16x4_t)(__p0);
  return __ret;
}
#else
__ai int16x4_t vreinterpret_s16_u16(uint16x4_t __p0) {
  int16x4_t __ret;
  __ret = (int16x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x4_t vreinterpret_s16_s8(int8x8_t __p0) {
  int16x4_t __ret;
  __ret = (int16x4_t)(__p0);
  return __ret;
}
#else
__ai int16x4_t vreinterpret_s16_s8(int8x8_t __p0) {
  int16x4_t __ret;
  __ret = (int16x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x4_t vreinterpret_s16_f64(float64x1_t __p0) {
  int16x4_t __ret;
  __ret = (int16x4_t)(__p0);
  return __ret;
}
#else
__ai int16x4_t vreinterpret_s16_f64(float64x1_t __p0) {
  int16x4_t __ret;
  __ret = (int16x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x4_t vreinterpret_s16_f32(float32x2_t __p0) {
  int16x4_t __ret;
  __ret = (int16x4_t)(__p0);
  return __ret;
}
#else
__ai int16x4_t vreinterpret_s16_f32(float32x2_t __p0) {
  int16x4_t __ret;
  __ret = (int16x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x4_t vreinterpret_s16_f16(float16x4_t __p0) {
  int16x4_t __ret;
  __ret = (int16x4_t)(__p0);
  return __ret;
}
#else
__ai int16x4_t vreinterpret_s16_f16(float16x4_t __p0) {
  int16x4_t __ret;
  __ret = (int16x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x4_t vreinterpret_s16_s32(int32x2_t __p0) {
  int16x4_t __ret;
  __ret = (int16x4_t)(__p0);
  return __ret;
}
#else
__ai int16x4_t vreinterpret_s16_s32(int32x2_t __p0) {
  int16x4_t __ret;
  __ret = (int16x4_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x4_t vreinterpret_s16_s64(int64x1_t __p0) {
  int16x4_t __ret;
  __ret = (int16x4_t)(__p0);
  return __ret;
}
#else
__ai int16x4_t vreinterpret_s16_s64(int64x1_t __p0) {
  int16x4_t __ret;
  __ret = (int16x4_t)(__p0);
  return __ret;
}
#endif

#endif
#if __ARM_FEATURE_CRYPTO
#ifdef __LITTLE_ENDIAN__
__ai uint8x16_t vaesdq_u8(uint8x16_t __p0, uint8x16_t __p1) {
  uint8x16_t __ret;
  __ret = (uint8x16_t) __builtin_neon_vaesdq_v((int8x16_t)__p0, (int8x16_t)__p1, 48);
  return __ret;
}
#else
__ai uint8x16_t vaesdq_u8(uint8x16_t __p0, uint8x16_t __p1) {
  uint8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __ret;
  __ret = (uint8x16_t) __builtin_neon_vaesdq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 48);
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x16_t vaeseq_u8(uint8x16_t __p0, uint8x16_t __p1) {
  uint8x16_t __ret;
  __ret = (uint8x16_t) __builtin_neon_vaeseq_v((int8x16_t)__p0, (int8x16_t)__p1, 48);
  return __ret;
}
#else
__ai uint8x16_t vaeseq_u8(uint8x16_t __p0, uint8x16_t __p1) {
  uint8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __ret;
  __ret = (uint8x16_t) __builtin_neon_vaeseq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 48);
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x16_t vaesimcq_u8(uint8x16_t __p0) {
  uint8x16_t __ret;
  __ret = (uint8x16_t) __builtin_neon_vaesimcq_v((int8x16_t)__p0, 48);
  return __ret;
}
#else
__ai uint8x16_t vaesimcq_u8(uint8x16_t __p0) {
  uint8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __ret;
  __ret = (uint8x16_t) __builtin_neon_vaesimcq_v((int8x16_t)__rev0, 48);
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x16_t vaesmcq_u8(uint8x16_t __p0) {
  uint8x16_t __ret;
  __ret = (uint8x16_t) __builtin_neon_vaesmcq_v((int8x16_t)__p0, 48);
  return __ret;
}
#else
__ai uint8x16_t vaesmcq_u8(uint8x16_t __p0) {
  uint8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __ret;
  __ret = (uint8x16_t) __builtin_neon_vaesmcq_v((int8x16_t)__rev0, 48);
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vsha1cq_u32(uint32x4_t __p0, uint32_t __p1, uint32x4_t __p2) {
  uint32x4_t __ret;
  __ret = (uint32x4_t) __builtin_neon_vsha1cq_u32((int8x16_t)__p0, __p1, (int8x16_t)__p2);
  return __ret;
}
#else
__ai uint32x4_t vsha1cq_u32(uint32x4_t __p0, uint32_t __p1, uint32x4_t __p2) {
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint32x4_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 3, 2, 1, 0);
  uint32x4_t __ret;
  __ret = (uint32x4_t) __builtin_neon_vsha1cq_u32((int8x16_t)__rev0, __p1, (int8x16_t)__rev2);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32_t vsha1h_u32(uint32_t __p0) {
  uint32_t __ret;
  __ret = (uint32_t) __builtin_neon_vsha1h_u32(__p0);
  return __ret;
}
#else
__ai uint32_t vsha1h_u32(uint32_t __p0) {
  uint32_t __ret;
  __ret = (uint32_t) __builtin_neon_vsha1h_u32(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vsha1mq_u32(uint32x4_t __p0, uint32_t __p1, uint32x4_t __p2) {
  uint32x4_t __ret;
  __ret = (uint32x4_t) __builtin_neon_vsha1mq_u32((int8x16_t)__p0, __p1, (int8x16_t)__p2);
  return __ret;
}
#else
__ai uint32x4_t vsha1mq_u32(uint32x4_t __p0, uint32_t __p1, uint32x4_t __p2) {
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint32x4_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 3, 2, 1, 0);
  uint32x4_t __ret;
  __ret = (uint32x4_t) __builtin_neon_vsha1mq_u32((int8x16_t)__rev0, __p1, (int8x16_t)__rev2);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vsha1pq_u32(uint32x4_t __p0, uint32_t __p1, uint32x4_t __p2) {
  uint32x4_t __ret;
  __ret = (uint32x4_t) __builtin_neon_vsha1pq_u32((int8x16_t)__p0, __p1, (int8x16_t)__p2);
  return __ret;
}
#else
__ai uint32x4_t vsha1pq_u32(uint32x4_t __p0, uint32_t __p1, uint32x4_t __p2) {
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint32x4_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 3, 2, 1, 0);
  uint32x4_t __ret;
  __ret = (uint32x4_t) __builtin_neon_vsha1pq_u32((int8x16_t)__rev0, __p1, (int8x16_t)__rev2);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vsha1su0q_u32(uint32x4_t __p0, uint32x4_t __p1, uint32x4_t __p2) {
  uint32x4_t __ret;
  __ret = (uint32x4_t) __builtin_neon_vsha1su0q_v((int8x16_t)__p0, (int8x16_t)__p1, (int8x16_t)__p2, 50);
  return __ret;
}
#else
__ai uint32x4_t vsha1su0q_u32(uint32x4_t __p0, uint32x4_t __p1, uint32x4_t __p2) {
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint32x4_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 3, 2, 1, 0);
  uint32x4_t __ret;
  __ret = (uint32x4_t) __builtin_neon_vsha1su0q_v((int8x16_t)__rev0, (int8x16_t)__rev1, (int8x16_t)__rev2, 50);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vsha1su1q_u32(uint32x4_t __p0, uint32x4_t __p1) {
  uint32x4_t __ret;
  __ret = (uint32x4_t) __builtin_neon_vsha1su1q_v((int8x16_t)__p0, (int8x16_t)__p1, 50);
  return __ret;
}
#else
__ai uint32x4_t vsha1su1q_u32(uint32x4_t __p0, uint32x4_t __p1) {
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint32x4_t __ret;
  __ret = (uint32x4_t) __builtin_neon_vsha1su1q_v((int8x16_t)__rev0, (int8x16_t)__rev1, 50);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vsha256hq_u32(uint32x4_t __p0, uint32x4_t __p1, uint32x4_t __p2) {
  uint32x4_t __ret;
  __ret = (uint32x4_t) __builtin_neon_vsha256hq_v((int8x16_t)__p0, (int8x16_t)__p1, (int8x16_t)__p2, 50);
  return __ret;
}
#else
__ai uint32x4_t vsha256hq_u32(uint32x4_t __p0, uint32x4_t __p1, uint32x4_t __p2) {
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint32x4_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 3, 2, 1, 0);
  uint32x4_t __ret;
  __ret = (uint32x4_t) __builtin_neon_vsha256hq_v((int8x16_t)__rev0, (int8x16_t)__rev1, (int8x16_t)__rev2, 50);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vsha256h2q_u32(uint32x4_t __p0, uint32x4_t __p1, uint32x4_t __p2) {
  uint32x4_t __ret;
  __ret = (uint32x4_t) __builtin_neon_vsha256h2q_v((int8x16_t)__p0, (int8x16_t)__p1, (int8x16_t)__p2, 50);
  return __ret;
}
#else
__ai uint32x4_t vsha256h2q_u32(uint32x4_t __p0, uint32x4_t __p1, uint32x4_t __p2) {
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint32x4_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 3, 2, 1, 0);
  uint32x4_t __ret;
  __ret = (uint32x4_t) __builtin_neon_vsha256h2q_v((int8x16_t)__rev0, (int8x16_t)__rev1, (int8x16_t)__rev2, 50);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vsha256su0q_u32(uint32x4_t __p0, uint32x4_t __p1) {
  uint32x4_t __ret;
  __ret = (uint32x4_t) __builtin_neon_vsha256su0q_v((int8x16_t)__p0, (int8x16_t)__p1, 50);
  return __ret;
}
#else
__ai uint32x4_t vsha256su0q_u32(uint32x4_t __p0, uint32x4_t __p1) {
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint32x4_t __ret;
  __ret = (uint32x4_t) __builtin_neon_vsha256su0q_v((int8x16_t)__rev0, (int8x16_t)__rev1, 50);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vsha256su1q_u32(uint32x4_t __p0, uint32x4_t __p1, uint32x4_t __p2) {
  uint32x4_t __ret;
  __ret = (uint32x4_t) __builtin_neon_vsha256su1q_v((int8x16_t)__p0, (int8x16_t)__p1, (int8x16_t)__p2, 50);
  return __ret;
}
#else
__ai uint32x4_t vsha256su1q_u32(uint32x4_t __p0, uint32x4_t __p1, uint32x4_t __p2) {
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint32x4_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 3, 2, 1, 0);
  uint32x4_t __ret;
  __ret = (uint32x4_t) __builtin_neon_vsha256su1q_v((int8x16_t)__rev0, (int8x16_t)__rev1, (int8x16_t)__rev2, 50);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#endif
#if defined(__aarch64__)
#ifdef __LITTLE_ENDIAN__
__ai float64x2_t vabdq_f64(float64x2_t __p0, float64x2_t __p1) {
  float64x2_t __ret;
  __ret = (float64x2_t) __builtin_neon_vabdq_v((int8x16_t)__p0, (int8x16_t)__p1, 42);
  return __ret;
}
#else
__ai float64x2_t vabdq_f64(float64x2_t __p0, float64x2_t __p1) {
  float64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  float64x2_t __ret;
  __ret = (float64x2_t) __builtin_neon_vabdq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 42);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x1_t vabd_f64(float64x1_t __p0, float64x1_t __p1) {
  float64x1_t __ret;
  __ret = (float64x1_t) __builtin_neon_vabd_v((int8x8_t)__p0, (int8x8_t)__p1, 10);
  return __ret;
}
#else
__ai float64x1_t vabd_f64(float64x1_t __p0, float64x1_t __p1) {
  float64x1_t __ret;
  __ret = (float64x1_t) __builtin_neon_vabd_v((int8x8_t)__p0, (int8x8_t)__p1, 10);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64_t vabdd_f64(float64_t __p0, float64_t __p1) {
  float64_t __ret;
  __ret = (float64_t) __builtin_neon_vabdd_f64(__p0, __p1);
  return __ret;
}
#else
__ai float64_t vabdd_f64(float64_t __p0, float64_t __p1) {
  float64_t __ret;
  __ret = (float64_t) __builtin_neon_vabdd_f64(__p0, __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32_t vabds_f32(float32_t __p0, float32_t __p1) {
  float32_t __ret;
  __ret = (float32_t) __builtin_neon_vabds_f32(__p0, __p1);
  return __ret;
}
#else
__ai float32_t vabds_f32(float32_t __p0, float32_t __p1) {
  float32_t __ret;
  __ret = (float32_t) __builtin_neon_vabds_f32(__p0, __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x2_t vabsq_f64(float64x2_t __p0) {
  float64x2_t __ret;
  __ret = (float64x2_t) __builtin_neon_vabsq_v((int8x16_t)__p0, 42);
  return __ret;
}
#else
__ai float64x2_t vabsq_f64(float64x2_t __p0) {
  float64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float64x2_t __ret;
  __ret = (float64x2_t) __builtin_neon_vabsq_v((int8x16_t)__rev0, 42);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x2_t vabsq_s64(int64x2_t __p0) {
  int64x2_t __ret;
  __ret = (int64x2_t) __builtin_neon_vabsq_v((int8x16_t)__p0, 35);
  return __ret;
}
#else
__ai int64x2_t vabsq_s64(int64x2_t __p0) {
  int64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int64x2_t __ret;
  __ret = (int64x2_t) __builtin_neon_vabsq_v((int8x16_t)__rev0, 35);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x1_t vabs_f64(float64x1_t __p0) {
  float64x1_t __ret;
  __ret = (float64x1_t) __builtin_neon_vabs_v((int8x8_t)__p0, 10);
  return __ret;
}
#else
__ai float64x1_t vabs_f64(float64x1_t __p0) {
  float64x1_t __ret;
  __ret = (float64x1_t) __builtin_neon_vabs_v((int8x8_t)__p0, 10);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x1_t vabs_s64(int64x1_t __p0) {
  int64x1_t __ret;
  __ret = (int64x1_t) __builtin_neon_vabs_v((int8x8_t)__p0, 3);
  return __ret;
}
#else
__ai int64x1_t vabs_s64(int64x1_t __p0) {
  int64x1_t __ret;
  __ret = (int64x1_t) __builtin_neon_vabs_v((int8x8_t)__p0, 3);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64_t vabsd_s64(int64_t __p0) {
  int64_t __ret;
  __ret = (int64_t) __builtin_neon_vabsd_s64(__p0);
  return __ret;
}
#else
__ai int64_t vabsd_s64(int64_t __p0) {
  int64_t __ret;
  __ret = (int64_t) __builtin_neon_vabsd_s64(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x2_t vaddq_f64(float64x2_t __p0, float64x2_t __p1) {
  float64x2_t __ret;
  __ret = __p0 + __p1;
  return __ret;
}
#else
__ai float64x2_t vaddq_f64(float64x2_t __p0, float64x2_t __p1) {
  float64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  float64x2_t __ret;
  __ret = __rev0 + __rev1;
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x1_t vadd_f64(float64x1_t __p0, float64x1_t __p1) {
  float64x1_t __ret;
  __ret = __p0 + __p1;
  return __ret;
}
#else
__ai float64x1_t vadd_f64(float64x1_t __p0, float64x1_t __p1) {
  float64x1_t __ret;
  __ret = __p0 + __p1;
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64_t vaddd_u64(uint64_t __p0, uint64_t __p1) {
  uint64_t __ret;
  __ret = (uint64_t) __builtin_neon_vaddd_u64(__p0, __p1);
  return __ret;
}
#else
__ai uint64_t vaddd_u64(uint64_t __p0, uint64_t __p1) {
  uint64_t __ret;
  __ret = (uint64_t) __builtin_neon_vaddd_u64(__p0, __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64_t vaddd_s64(int64_t __p0, int64_t __p1) {
  int64_t __ret;
  __ret = (int64_t) __builtin_neon_vaddd_s64(__p0, __p1);
  return __ret;
}
#else
__ai int64_t vaddd_s64(int64_t __p0, int64_t __p1) {
  int64_t __ret;
  __ret = (int64_t) __builtin_neon_vaddd_s64(__p0, __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x8_t vaddhn_high_u32(uint16x4_t __p0, uint32x4_t __p1, uint32x4_t __p2) {
  uint16x8_t __ret;
  __ret = vcombine_u16(__p0, vaddhn_u32(__p1, __p2));
  return __ret;
}
#else
__ai uint16x8_t vaddhn_high_u32(uint16x4_t __p0, uint32x4_t __p1, uint32x4_t __p2) {
  uint16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  uint32x4_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 3, 2, 1, 0);
  uint16x8_t __ret;
  __ret = __noswap_vcombine_u16(__rev0, __noswap_vaddhn_u32(__rev1, __rev2));
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vaddhn_high_u64(uint32x2_t __p0, uint64x2_t __p1, uint64x2_t __p2) {
  uint32x4_t __ret;
  __ret = vcombine_u32(__p0, vaddhn_u64(__p1, __p2));
  return __ret;
}
#else
__ai uint32x4_t vaddhn_high_u64(uint32x2_t __p0, uint64x2_t __p1, uint64x2_t __p2) {
  uint32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  uint64x2_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 1, 0);
  uint32x4_t __ret;
  __ret = __noswap_vcombine_u32(__rev0, __noswap_vaddhn_u64(__rev1, __rev2));
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x16_t vaddhn_high_u16(uint8x8_t __p0, uint16x8_t __p1, uint16x8_t __p2) {
  uint8x16_t __ret;
  __ret = vcombine_u8(__p0, vaddhn_u16(__p1, __p2));
  return __ret;
}
#else
__ai uint8x16_t vaddhn_high_u16(uint8x8_t __p0, uint16x8_t __p1, uint16x8_t __p2) {
  uint8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __ret;
  __ret = __noswap_vcombine_u8(__rev0, __noswap_vaddhn_u16(__rev1, __rev2));
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16x8_t vaddhn_high_s32(int16x4_t __p0, int32x4_t __p1, int32x4_t __p2) {
  int16x8_t __ret;
  __ret = vcombine_s16(__p0, vaddhn_s32(__p1, __p2));
  return __ret;
}
#else
__ai int16x8_t vaddhn_high_s32(int16x4_t __p0, int32x4_t __p1, int32x4_t __p2) {
  int16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  int32x4_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 3, 2, 1, 0);
  int16x8_t __ret;
  __ret = __noswap_vcombine_s16(__rev0, __noswap_vaddhn_s32(__rev1, __rev2));
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32x4_t vaddhn_high_s64(int32x2_t __p0, int64x2_t __p1, int64x2_t __p2) {
  int32x4_t __ret;
  __ret = vcombine_s32(__p0, vaddhn_s64(__p1, __p2));
  return __ret;
}
#else
__ai int32x4_t vaddhn_high_s64(int32x2_t __p0, int64x2_t __p1, int64x2_t __p2) {
  int32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  int64x2_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 1, 0);
  int32x4_t __ret;
  __ret = __noswap_vcombine_s32(__rev0, __noswap_vaddhn_s64(__rev1, __rev2));
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8x16_t vaddhn_high_s16(int8x8_t __p0, int16x8_t __p1, int16x8_t __p2) {
  int8x16_t __ret;
  __ret = vcombine_s8(__p0, vaddhn_s16(__p1, __p2));
  return __ret;
}
#else
__ai int8x16_t vaddhn_high_s16(int8x8_t __p0, int16x8_t __p1, int16x8_t __p2) {
  int8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 7, 6, 5, 4, 3, 2, 1, 0);
  int16x8_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 7, 6, 5, 4, 3, 2, 1, 0);
  int8x16_t __ret;
  __ret = __noswap_vcombine_s8(__rev0, __noswap_vaddhn_s16(__rev1, __rev2));
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16_t vaddlvq_u8(uint8x16_t __p0) {
  uint16_t __ret;
  __ret = (uint16_t) __builtin_neon_vaddlvq_u8((int8x16_t)__p0);
  return __ret;
}
#else
__ai uint16_t vaddlvq_u8(uint8x16_t __p0) {
  uint8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16_t __ret;
  __ret = (uint16_t) __builtin_neon_vaddlvq_u8((int8x16_t)__rev0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64_t vaddlvq_u32(uint32x4_t __p0) {
  uint64_t __ret;
  __ret = (uint64_t) __builtin_neon_vaddlvq_u32((int8x16_t)__p0);
  return __ret;
}
#else
__ai uint64_t vaddlvq_u32(uint32x4_t __p0) {
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint64_t __ret;
  __ret = (uint64_t) __builtin_neon_vaddlvq_u32((int8x16_t)__rev0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32_t vaddlvq_u16(uint16x8_t __p0) {
  uint32_t __ret;
  __ret = (uint32_t) __builtin_neon_vaddlvq_u16((int8x16_t)__p0);
  return __ret;
}
#else
__ai uint32_t vaddlvq_u16(uint16x8_t __p0) {
  uint16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint32_t __ret;
  __ret = (uint32_t) __builtin_neon_vaddlvq_u16((int8x16_t)__rev0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16_t vaddlvq_s8(int8x16_t __p0) {
  int16_t __ret;
  __ret = (int16_t) __builtin_neon_vaddlvq_s8((int8x16_t)__p0);
  return __ret;
}
#else
__ai int16_t vaddlvq_s8(int8x16_t __p0) {
  int8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int16_t __ret;
  __ret = (int16_t) __builtin_neon_vaddlvq_s8((int8x16_t)__rev0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64_t vaddlvq_s32(int32x4_t __p0) {
  int64_t __ret;
  __ret = (int64_t) __builtin_neon_vaddlvq_s32((int8x16_t)__p0);
  return __ret;
}
#else
__ai int64_t vaddlvq_s32(int32x4_t __p0) {
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int64_t __ret;
  __ret = (int64_t) __builtin_neon_vaddlvq_s32((int8x16_t)__rev0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32_t vaddlvq_s16(int16x8_t __p0) {
  int32_t __ret;
  __ret = (int32_t) __builtin_neon_vaddlvq_s16((int8x16_t)__p0);
  return __ret;
}
#else
__ai int32_t vaddlvq_s16(int16x8_t __p0) {
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int32_t __ret;
  __ret = (int32_t) __builtin_neon_vaddlvq_s16((int8x16_t)__rev0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16_t vaddlv_u8(uint8x8_t __p0) {
  uint16_t __ret;
  __ret = (uint16_t) __builtin_neon_vaddlv_u8((int8x8_t)__p0);
  return __ret;
}
#else
__ai uint16_t vaddlv_u8(uint8x8_t __p0) {
  uint8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16_t __ret;
  __ret = (uint16_t) __builtin_neon_vaddlv_u8((int8x8_t)__rev0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64_t vaddlv_u32(uint32x2_t __p0) {
  uint64_t __ret;
  __ret = (uint64_t) __builtin_neon_vaddlv_u32((int8x8_t)__p0);
  return __ret;
}
#else
__ai uint64_t vaddlv_u32(uint32x2_t __p0) {
  uint32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint64_t __ret;
  __ret = (uint64_t) __builtin_neon_vaddlv_u32((int8x8_t)__rev0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32_t vaddlv_u16(uint16x4_t __p0) {
  uint32_t __ret;
  __ret = (uint32_t) __builtin_neon_vaddlv_u16((int8x8_t)__p0);
  return __ret;
}
#else
__ai uint32_t vaddlv_u16(uint16x4_t __p0) {
  uint16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint32_t __ret;
  __ret = (uint32_t) __builtin_neon_vaddlv_u16((int8x8_t)__rev0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16_t vaddlv_s8(int8x8_t __p0) {
  int16_t __ret;
  __ret = (int16_t) __builtin_neon_vaddlv_s8((int8x8_t)__p0);
  return __ret;
}
#else
__ai int16_t vaddlv_s8(int8x8_t __p0) {
  int8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int16_t __ret;
  __ret = (int16_t) __builtin_neon_vaddlv_s8((int8x8_t)__rev0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64_t vaddlv_s32(int32x2_t __p0) {
  int64_t __ret;
  __ret = (int64_t) __builtin_neon_vaddlv_s32((int8x8_t)__p0);
  return __ret;
}
#else
__ai int64_t vaddlv_s32(int32x2_t __p0) {
  int32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int64_t __ret;
  __ret = (int64_t) __builtin_neon_vaddlv_s32((int8x8_t)__rev0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32_t vaddlv_s16(int16x4_t __p0) {
  int32_t __ret;
  __ret = (int32_t) __builtin_neon_vaddlv_s16((int8x8_t)__p0);
  return __ret;
}
#else
__ai int32_t vaddlv_s16(int16x4_t __p0) {
  int16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int32_t __ret;
  __ret = (int32_t) __builtin_neon_vaddlv_s16((int8x8_t)__rev0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8_t vaddvq_u8(uint8x16_t __p0) {
  uint8_t __ret;
  __ret = (uint8_t) __builtin_neon_vaddvq_u8((int8x16_t)__p0);
  return __ret;
}
#else
__ai uint8_t vaddvq_u8(uint8x16_t __p0) {
  uint8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8_t __ret;
  __ret = (uint8_t) __builtin_neon_vaddvq_u8((int8x16_t)__rev0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32_t vaddvq_u32(uint32x4_t __p0) {
  uint32_t __ret;
  __ret = (uint32_t) __builtin_neon_vaddvq_u32((int8x16_t)__p0);
  return __ret;
}
#else
__ai uint32_t vaddvq_u32(uint32x4_t __p0) {
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint32_t __ret;
  __ret = (uint32_t) __builtin_neon_vaddvq_u32((int8x16_t)__rev0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64_t vaddvq_u64(uint64x2_t __p0) {
  uint64_t __ret;
  __ret = (uint64_t) __builtin_neon_vaddvq_u64((int8x16_t)__p0);
  return __ret;
}
#else
__ai uint64_t vaddvq_u64(uint64x2_t __p0) {
  uint64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint64_t __ret;
  __ret = (uint64_t) __builtin_neon_vaddvq_u64((int8x16_t)__rev0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16_t vaddvq_u16(uint16x8_t __p0) {
  uint16_t __ret;
  __ret = (uint16_t) __builtin_neon_vaddvq_u16((int8x16_t)__p0);
  return __ret;
}
#else
__ai uint16_t vaddvq_u16(uint16x8_t __p0) {
  uint16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16_t __ret;
  __ret = (uint16_t) __builtin_neon_vaddvq_u16((int8x16_t)__rev0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8_t vaddvq_s8(int8x16_t __p0) {
  int8_t __ret;
  __ret = (int8_t) __builtin_neon_vaddvq_s8((int8x16_t)__p0);
  return __ret;
}
#else
__ai int8_t vaddvq_s8(int8x16_t __p0) {
  int8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  int8_t __ret;
  __ret = (int8_t) __builtin_neon_vaddvq_s8((int8x16_t)__rev0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64_t vaddvq_f64(float64x2_t __p0) {
  float64_t __ret;
  __ret = (float64_t) __builtin_neon_vaddvq_f64((int8x16_t)__p0);
  return __ret;
}
#else
__ai float64_t vaddvq_f64(float64x2_t __p0) {
  float64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float64_t __ret;
  __ret = (float64_t) __builtin_neon_vaddvq_f64((int8x16_t)__rev0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32_t vaddvq_f32(float32x4_t __p0) {
  float32_t __ret;
  __ret = (float32_t) __builtin_neon_vaddvq_f32((int8x16_t)__p0);
  return __ret;
}
#else
__ai float32_t vaddvq_f32(float32x4_t __p0) {
  float32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  float32_t __ret;
  __ret = (float32_t) __builtin_neon_vaddvq_f32((int8x16_t)__rev0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32_t vaddvq_s32(int32x4_t __p0) {
  int32_t __ret;
  __ret = (int32_t) __builtin_neon_vaddvq_s32((int8x16_t)__p0);
  return __ret;
}
#else
__ai int32_t vaddvq_s32(int32x4_t __p0) {
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int32_t __ret;
  __ret = (int32_t) __builtin_neon_vaddvq_s32((int8x16_t)__rev0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64_t vaddvq_s64(int64x2_t __p0) {
  int64_t __ret;
  __ret = (int64_t) __builtin_neon_vaddvq_s64((int8x16_t)__p0);
  return __ret;
}
#else
__ai int64_t vaddvq_s64(int64x2_t __p0) {
  int64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int64_t __ret;
  __ret = (int64_t) __builtin_neon_vaddvq_s64((int8x16_t)__rev0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16_t vaddvq_s16(int16x8_t __p0) {
  int16_t __ret;
  __ret = (int16_t) __builtin_neon_vaddvq_s16((int8x16_t)__p0);
  return __ret;
}
#else
__ai int16_t vaddvq_s16(int16x8_t __p0) {
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int16_t __ret;
  __ret = (int16_t) __builtin_neon_vaddvq_s16((int8x16_t)__rev0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8_t vaddv_u8(uint8x8_t __p0) {
  uint8_t __ret;
  __ret = (uint8_t) __builtin_neon_vaddv_u8((int8x8_t)__p0);
  return __ret;
}
#else
__ai uint8_t vaddv_u8(uint8x8_t __p0) {
  uint8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8_t __ret;
  __ret = (uint8_t) __builtin_neon_vaddv_u8((int8x8_t)__rev0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32_t vaddv_u32(uint32x2_t __p0) {
  uint32_t __ret;
  __ret = (uint32_t) __builtin_neon_vaddv_u32((int8x8_t)__p0);
  return __ret;
}
#else
__ai uint32_t vaddv_u32(uint32x2_t __p0) {
  uint32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint32_t __ret;
  __ret = (uint32_t) __builtin_neon_vaddv_u32((int8x8_t)__rev0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16_t vaddv_u16(uint16x4_t __p0) {
  uint16_t __ret;
  __ret = (uint16_t) __builtin_neon_vaddv_u16((int8x8_t)__p0);
  return __ret;
}
#else
__ai uint16_t vaddv_u16(uint16x4_t __p0) {
  uint16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint16_t __ret;
  __ret = (uint16_t) __builtin_neon_vaddv_u16((int8x8_t)__rev0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int8_t vaddv_s8(int8x8_t __p0) {
  int8_t __ret;
  __ret = (int8_t) __builtin_neon_vaddv_s8((int8x8_t)__p0);
  return __ret;
}
#else
__ai int8_t vaddv_s8(int8x8_t __p0) {
  int8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  int8_t __ret;
  __ret = (int8_t) __builtin_neon_vaddv_s8((int8x8_t)__rev0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32_t vaddv_f32(float32x2_t __p0) {
  float32_t __ret;
  __ret = (float32_t) __builtin_neon_vaddv_f32((int8x8_t)__p0);
  return __ret;
}
#else
__ai float32_t vaddv_f32(float32x2_t __p0) {
  float32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float32_t __ret;
  __ret = (float32_t) __builtin_neon_vaddv_f32((int8x8_t)__rev0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32_t vaddv_s32(int32x2_t __p0) {
  int32_t __ret;
  __ret = (int32_t) __builtin_neon_vaddv_s32((int8x8_t)__p0);
  return __ret;
}
#else
__ai int32_t vaddv_s32(int32x2_t __p0) {
  int32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int32_t __ret;
  __ret = (int32_t) __builtin_neon_vaddv_s32((int8x8_t)__rev0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int16_t vaddv_s16(int16x4_t __p0) {
  int16_t __ret;
  __ret = (int16_t) __builtin_neon_vaddv_s16((int8x8_t)__p0);
  return __ret;
}
#else
__ai int16_t vaddv_s16(int16x4_t __p0) {
  int16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  int16_t __ret;
  __ret = (int16_t) __builtin_neon_vaddv_s16((int8x8_t)__rev0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly64x1_t vbsl_p64(uint64x1_t __p0, poly64x1_t __p1, poly64x1_t __p2) {
  poly64x1_t __ret;
  __ret = (poly64x1_t) __builtin_neon_vbsl_v((int8x8_t)__p0, (int8x8_t)__p1, (int8x8_t)__p2, 6);
  return __ret;
}
#else
__ai poly64x1_t vbsl_p64(uint64x1_t __p0, poly64x1_t __p1, poly64x1_t __p2) {
  poly64x1_t __ret;
  __ret = (poly64x1_t) __builtin_neon_vbsl_v((int8x8_t)__p0, (int8x8_t)__p1, (int8x8_t)__p2, 6);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly64x2_t vbslq_p64(uint64x2_t __p0, poly64x2_t __p1, poly64x2_t __p2) {
  poly64x2_t __ret;
  __ret = (poly64x2_t) __builtin_neon_vbslq_v((int8x16_t)__p0, (int8x16_t)__p1, (int8x16_t)__p2, 38);
  return __ret;
}
#else
__ai poly64x2_t vbslq_p64(uint64x2_t __p0, poly64x2_t __p1, poly64x2_t __p2) {
  uint64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  poly64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  poly64x2_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 1, 0);
  poly64x2_t __ret;
  __ret = (poly64x2_t) __builtin_neon_vbslq_v((int8x16_t)__rev0, (int8x16_t)__rev1, (int8x16_t)__rev2, 38);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x2_t vbslq_f64(uint64x2_t __p0, float64x2_t __p1, float64x2_t __p2) {
  float64x2_t __ret;
  __ret = (float64x2_t) __builtin_neon_vbslq_v((int8x16_t)__p0, (int8x16_t)__p1, (int8x16_t)__p2, 42);
  return __ret;
}
#else
__ai float64x2_t vbslq_f64(uint64x2_t __p0, float64x2_t __p1, float64x2_t __p2) {
  uint64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  float64x2_t __rev2;  __rev2 = __builtin_shufflevector(__p2, __p2, 1, 0);
  float64x2_t __ret;
  __ret = (float64x2_t) __builtin_neon_vbslq_v((int8x16_t)__rev0, (int8x16_t)__rev1, (int8x16_t)__rev2, 42);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x1_t vbsl_f64(uint64x1_t __p0, float64x1_t __p1, float64x1_t __p2) {
  float64x1_t __ret;
  __ret = (float64x1_t) __builtin_neon_vbsl_v((int8x8_t)__p0, (int8x8_t)__p1, (int8x8_t)__p2, 10);
  return __ret;
}
#else
__ai float64x1_t vbsl_f64(uint64x1_t __p0, float64x1_t __p1, float64x1_t __p2) {
  float64x1_t __ret;
  __ret = (float64x1_t) __builtin_neon_vbsl_v((int8x8_t)__p0, (int8x8_t)__p1, (int8x8_t)__p2, 10);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vcageq_f64(float64x2_t __p0, float64x2_t __p1) {
  uint64x2_t __ret;
  __ret = (uint64x2_t) __builtin_neon_vcageq_v((int8x16_t)__p0, (int8x16_t)__p1, 51);
  return __ret;
}
#else
__ai uint64x2_t vcageq_f64(float64x2_t __p0, float64x2_t __p1) {
  float64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  uint64x2_t __ret;
  __ret = (uint64x2_t) __builtin_neon_vcageq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 51);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x1_t vcage_f64(float64x1_t __p0, float64x1_t __p1) {
  uint64x1_t __ret;
  __ret = (uint64x1_t) __builtin_neon_vcage_v((int8x8_t)__p0, (int8x8_t)__p1, 19);
  return __ret;
}
#else
__ai uint64x1_t vcage_f64(float64x1_t __p0, float64x1_t __p1) {
  uint64x1_t __ret;
  __ret = (uint64x1_t) __builtin_neon_vcage_v((int8x8_t)__p0, (int8x8_t)__p1, 19);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64_t vcaged_f64(float64_t __p0, float64_t __p1) {
  uint64_t __ret;
  __ret = (uint64_t) __builtin_neon_vcaged_f64(__p0, __p1);
  return __ret;
}
#else
__ai uint64_t vcaged_f64(float64_t __p0, float64_t __p1) {
  uint64_t __ret;
  __ret = (uint64_t) __builtin_neon_vcaged_f64(__p0, __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32_t vcages_f32(float32_t __p0, float32_t __p1) {
  uint32_t __ret;
  __ret = (uint32_t) __builtin_neon_vcages_f32(__p0, __p1);
  return __ret;
}
#else
__ai uint32_t vcages_f32(float32_t __p0, float32_t __p1) {
  uint32_t __ret;
  __ret = (uint32_t) __builtin_neon_vcages_f32(__p0, __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vcagtq_f64(float64x2_t __p0, float64x2_t __p1) {
  uint64x2_t __ret;
  __ret = (uint64x2_t) __builtin_neon_vcagtq_v((int8x16_t)__p0, (int8x16_t)__p1, 51);
  return __ret;
}
#else
__ai uint64x2_t vcagtq_f64(float64x2_t __p0, float64x2_t __p1) {
  float64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  uint64x2_t __ret;
  __ret = (uint64x2_t) __builtin_neon_vcagtq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 51);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x1_t vcagt_f64(float64x1_t __p0, float64x1_t __p1) {
  uint64x1_t __ret;
  __ret = (uint64x1_t) __builtin_neon_vcagt_v((int8x8_t)__p0, (int8x8_t)__p1, 19);
  return __ret;
}
#else
__ai uint64x1_t vcagt_f64(float64x1_t __p0, float64x1_t __p1) {
  uint64x1_t __ret;
  __ret = (uint64x1_t) __builtin_neon_vcagt_v((int8x8_t)__p0, (int8x8_t)__p1, 19);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64_t vcagtd_f64(float64_t __p0, float64_t __p1) {
  uint64_t __ret;
  __ret = (uint64_t) __builtin_neon_vcagtd_f64(__p0, __p1);
  return __ret;
}
#else
__ai uint64_t vcagtd_f64(float64_t __p0, float64_t __p1) {
  uint64_t __ret;
  __ret = (uint64_t) __builtin_neon_vcagtd_f64(__p0, __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32_t vcagts_f32(float32_t __p0, float32_t __p1) {
  uint32_t __ret;
  __ret = (uint32_t) __builtin_neon_vcagts_f32(__p0, __p1);
  return __ret;
}
#else
__ai uint32_t vcagts_f32(float32_t __p0, float32_t __p1) {
  uint32_t __ret;
  __ret = (uint32_t) __builtin_neon_vcagts_f32(__p0, __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vcaleq_f64(float64x2_t __p0, float64x2_t __p1) {
  uint64x2_t __ret;
  __ret = (uint64x2_t) __builtin_neon_vcaleq_v((int8x16_t)__p0, (int8x16_t)__p1, 51);
  return __ret;
}
#else
__ai uint64x2_t vcaleq_f64(float64x2_t __p0, float64x2_t __p1) {
  float64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  uint64x2_t __ret;
  __ret = (uint64x2_t) __builtin_neon_vcaleq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 51);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x1_t vcale_f64(float64x1_t __p0, float64x1_t __p1) {
  uint64x1_t __ret;
  __ret = (uint64x1_t) __builtin_neon_vcale_v((int8x8_t)__p0, (int8x8_t)__p1, 19);
  return __ret;
}
#else
__ai uint64x1_t vcale_f64(float64x1_t __p0, float64x1_t __p1) {
  uint64x1_t __ret;
  __ret = (uint64x1_t) __builtin_neon_vcale_v((int8x8_t)__p0, (int8x8_t)__p1, 19);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64_t vcaled_f64(float64_t __p0, float64_t __p1) {
  uint64_t __ret;
  __ret = (uint64_t) __builtin_neon_vcaled_f64(__p0, __p1);
  return __ret;
}
#else
__ai uint64_t vcaled_f64(float64_t __p0, float64_t __p1) {
  uint64_t __ret;
  __ret = (uint64_t) __builtin_neon_vcaled_f64(__p0, __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32_t vcales_f32(float32_t __p0, float32_t __p1) {
  uint32_t __ret;
  __ret = (uint32_t) __builtin_neon_vcales_f32(__p0, __p1);
  return __ret;
}
#else
__ai uint32_t vcales_f32(float32_t __p0, float32_t __p1) {
  uint32_t __ret;
  __ret = (uint32_t) __builtin_neon_vcales_f32(__p0, __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vcaltq_f64(float64x2_t __p0, float64x2_t __p1) {
  uint64x2_t __ret;
  __ret = (uint64x2_t) __builtin_neon_vcaltq_v((int8x16_t)__p0, (int8x16_t)__p1, 51);
  return __ret;
}
#else
__ai uint64x2_t vcaltq_f64(float64x2_t __p0, float64x2_t __p1) {
  float64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  uint64x2_t __ret;
  __ret = (uint64x2_t) __builtin_neon_vcaltq_v((int8x16_t)__rev0, (int8x16_t)__rev1, 51);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x1_t vcalt_f64(float64x1_t __p0, float64x1_t __p1) {
  uint64x1_t __ret;
  __ret = (uint64x1_t) __builtin_neon_vcalt_v((int8x8_t)__p0, (int8x8_t)__p1, 19);
  return __ret;
}
#else
__ai uint64x1_t vcalt_f64(float64x1_t __p0, float64x1_t __p1) {
  uint64x1_t __ret;
  __ret = (uint64x1_t) __builtin_neon_vcalt_v((int8x8_t)__p0, (int8x8_t)__p1, 19);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64_t vcaltd_f64(float64_t __p0, float64_t __p1) {
  uint64_t __ret;
  __ret = (uint64_t) __builtin_neon_vcaltd_f64(__p0, __p1);
  return __ret;
}
#else
__ai uint64_t vcaltd_f64(float64_t __p0, float64_t __p1) {
  uint64_t __ret;
  __ret = (uint64_t) __builtin_neon_vcaltd_f64(__p0, __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32_t vcalts_f32(float32_t __p0, float32_t __p1) {
  uint32_t __ret;
  __ret = (uint32_t) __builtin_neon_vcalts_f32(__p0, __p1);
  return __ret;
}
#else
__ai uint32_t vcalts_f32(float32_t __p0, float32_t __p1) {
  uint32_t __ret;
  __ret = (uint32_t) __builtin_neon_vcalts_f32(__p0, __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x1_t vceq_p64(poly64x1_t __p0, poly64x1_t __p1) {
  uint64x1_t __ret;
  __ret = (uint64x1_t)(__p0 == __p1);
  return __ret;
}
#else
__ai uint64x1_t vceq_p64(poly64x1_t __p0, poly64x1_t __p1) {
  uint64x1_t __ret;
  __ret = (uint64x1_t)(__p0 == __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vceqq_p64(poly64x2_t __p0, poly64x2_t __p1) {
  uint64x2_t __ret;
  __ret = (uint64x2_t)(__p0 == __p1);
  return __ret;
}
#else
__ai uint64x2_t vceqq_p64(poly64x2_t __p0, poly64x2_t __p1) {
  poly64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  poly64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  uint64x2_t __ret;
  __ret = (uint64x2_t)(__rev0 == __rev1);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vceqq_u64(uint64x2_t __p0, uint64x2_t __p1) {
  uint64x2_t __ret;
  __ret = (uint64x2_t)(__p0 == __p1);
  return __ret;
}
#else
__ai uint64x2_t vceqq_u64(uint64x2_t __p0, uint64x2_t __p1) {
  uint64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  uint64x2_t __ret;
  __ret = (uint64x2_t)(__rev0 == __rev1);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vceqq_f64(float64x2_t __p0, float64x2_t __p1) {
  uint64x2_t __ret;
  __ret = (uint64x2_t)(__p0 == __p1);
  return __ret;
}
#else
__ai uint64x2_t vceqq_f64(float64x2_t __p0, float64x2_t __p1) {
  float64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  uint64x2_t __ret;
  __ret = (uint64x2_t)(__rev0 == __rev1);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vceqq_s64(int64x2_t __p0, int64x2_t __p1) {
  uint64x2_t __ret;
  __ret = (uint64x2_t)(__p0 == __p1);
  return __ret;
}
#else
__ai uint64x2_t vceqq_s64(int64x2_t __p0, int64x2_t __p1) {
  int64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  uint64x2_t __ret;
  __ret = (uint64x2_t)(__rev0 == __rev1);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x1_t vceq_u64(uint64x1_t __p0, uint64x1_t __p1) {
  uint64x1_t __ret;
  __ret = (uint64x1_t)(__p0 == __p1);
  return __ret;
}
#else
__ai uint64x1_t vceq_u64(uint64x1_t __p0, uint64x1_t __p1) {
  uint64x1_t __ret;
  __ret = (uint64x1_t)(__p0 == __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x1_t vceq_f64(float64x1_t __p0, float64x1_t __p1) {
  uint64x1_t __ret;
  __ret = (uint64x1_t)(__p0 == __p1);
  return __ret;
}
#else
__ai uint64x1_t vceq_f64(float64x1_t __p0, float64x1_t __p1) {
  uint64x1_t __ret;
  __ret = (uint64x1_t)(__p0 == __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x1_t vceq_s64(int64x1_t __p0, int64x1_t __p1) {
  uint64x1_t __ret;
  __ret = (uint64x1_t)(__p0 == __p1);
  return __ret;
}
#else
__ai uint64x1_t vceq_s64(int64x1_t __p0, int64x1_t __p1) {
  uint64x1_t __ret;
  __ret = (uint64x1_t)(__p0 == __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64_t vceqd_u64(uint64_t __p0, uint64_t __p1) {
  uint64_t __ret;
  __ret = (uint64_t) __builtin_neon_vceqd_u64(__p0, __p1);
  return __ret;
}
#else
__ai uint64_t vceqd_u64(uint64_t __p0, uint64_t __p1) {
  uint64_t __ret;
  __ret = (uint64_t) __builtin_neon_vceqd_u64(__p0, __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64_t vceqd_s64(int64_t __p0, int64_t __p1) {
  int64_t __ret;
  __ret = (int64_t) __builtin_neon_vceqd_s64(__p0, __p1);
  return __ret;
}
#else
__ai int64_t vceqd_s64(int64_t __p0, int64_t __p1) {
  int64_t __ret;
  __ret = (int64_t) __builtin_neon_vceqd_s64(__p0, __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64_t vceqd_f64(float64_t __p0, float64_t __p1) {
  uint64_t __ret;
  __ret = (uint64_t) __builtin_neon_vceqd_f64(__p0, __p1);
  return __ret;
}
#else
__ai uint64_t vceqd_f64(float64_t __p0, float64_t __p1) {
  uint64_t __ret;
  __ret = (uint64_t) __builtin_neon_vceqd_f64(__p0, __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32_t vceqs_f32(float32_t __p0, float32_t __p1) {
  uint32_t __ret;
  __ret = (uint32_t) __builtin_neon_vceqs_f32(__p0, __p1);
  return __ret;
}
#else
__ai uint32_t vceqs_f32(float32_t __p0, float32_t __p1) {
  uint32_t __ret;
  __ret = (uint32_t) __builtin_neon_vceqs_f32(__p0, __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x8_t vceqz_p8(poly8x8_t __p0) {
  uint8x8_t __ret;
  __ret = (uint8x8_t) __builtin_neon_vceqz_v((int8x8_t)__p0, 16);
  return __ret;
}
#else
__ai uint8x8_t vceqz_p8(poly8x8_t __p0) {
  poly8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __ret;
  __ret = (uint8x8_t) __builtin_neon_vceqz_v((int8x8_t)__rev0, 16);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x1_t vceqz_p64(poly64x1_t __p0) {
  uint64x1_t __ret;
  __ret = (uint64x1_t) __builtin_neon_vceqz_v((int8x8_t)__p0, 19);
  return __ret;
}
#else
__ai uint64x1_t vceqz_p64(poly64x1_t __p0) {
  uint64x1_t __ret;
  __ret = (uint64x1_t) __builtin_neon_vceqz_v((int8x8_t)__p0, 19);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x4_t vceqz_p16(poly16x4_t __p0) {
  uint16x4_t __ret;
  __ret = (uint16x4_t) __builtin_neon_vceqz_v((int8x8_t)__p0, 17);
  return __ret;
}
#else
__ai uint16x4_t vceqz_p16(poly16x4_t __p0) {
  poly16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint16x4_t __ret;
  __ret = (uint16x4_t) __builtin_neon_vceqz_v((int8x8_t)__rev0, 17);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x16_t vceqzq_p8(poly8x16_t __p0) {
  uint8x16_t __ret;
  __ret = (uint8x16_t) __builtin_neon_vceqzq_v((int8x16_t)__p0, 48);
  return __ret;
}
#else
__ai uint8x16_t vceqzq_p8(poly8x16_t __p0) {
  poly8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __ret;
  __ret = (uint8x16_t) __builtin_neon_vceqzq_v((int8x16_t)__rev0, 48);
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vceqzq_p64(poly64x2_t __p0) {
  uint64x2_t __ret;
  __ret = (uint64x2_t) __builtin_neon_vceqzq_v((int8x16_t)__p0, 51);
  return __ret;
}
#else
__ai uint64x2_t vceqzq_p64(poly64x2_t __p0) {
  poly64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint64x2_t __ret;
  __ret = (uint64x2_t) __builtin_neon_vceqzq_v((int8x16_t)__rev0, 51);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x8_t vceqzq_p16(poly16x8_t __p0) {
  uint16x8_t __ret;
  __ret = (uint16x8_t) __builtin_neon_vceqzq_v((int8x16_t)__p0, 49);
  return __ret;
}
#else
__ai uint16x8_t vceqzq_p16(poly16x8_t __p0) {
  poly16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __ret;
  __ret = (uint16x8_t) __builtin_neon_vceqzq_v((int8x16_t)__rev0, 49);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x16_t vceqzq_u8(uint8x16_t __p0) {
  uint8x16_t __ret;
  __ret = (uint8x16_t) __builtin_neon_vceqzq_v((int8x16_t)__p0, 48);
  return __ret;
}
#else
__ai uint8x16_t vceqzq_u8(uint8x16_t __p0) {
  uint8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __ret;
  __ret = (uint8x16_t) __builtin_neon_vceqzq_v((int8x16_t)__rev0, 48);
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vceqzq_u32(uint32x4_t __p0) {
  uint32x4_t __ret;
  __ret = (uint32x4_t) __builtin_neon_vceqzq_v((int8x16_t)__p0, 50);
  return __ret;
}
#else
__ai uint32x4_t vceqzq_u32(uint32x4_t __p0) {
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint32x4_t __ret;
  __ret = (uint32x4_t) __builtin_neon_vceqzq_v((int8x16_t)__rev0, 50);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vceqzq_u64(uint64x2_t __p0) {
  uint64x2_t __ret;
  __ret = (uint64x2_t) __builtin_neon_vceqzq_v((int8x16_t)__p0, 51);
  return __ret;
}
#else
__ai uint64x2_t vceqzq_u64(uint64x2_t __p0) {
  uint64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint64x2_t __ret;
  __ret = (uint64x2_t) __builtin_neon_vceqzq_v((int8x16_t)__rev0, 51);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x8_t vceqzq_u16(uint16x8_t __p0) {
  uint16x8_t __ret;
  __ret = (uint16x8_t) __builtin_neon_vceqzq_v((int8x16_t)__p0, 49);
  return __ret;
}
#else
__ai uint16x8_t vceqzq_u16(uint16x8_t __p0) {
  uint16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __ret;
  __ret = (uint16x8_t) __builtin_neon_vceqzq_v((int8x16_t)__rev0, 49);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x16_t vceqzq_s8(int8x16_t __p0) {
  uint8x16_t __ret;
  __ret = (uint8x16_t) __builtin_neon_vceqzq_v((int8x16_t)__p0, 48);
  return __ret;
}
#else
__ai uint8x16_t vceqzq_s8(int8x16_t __p0) {
  int8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __ret;
  __ret = (uint8x16_t) __builtin_neon_vceqzq_v((int8x16_t)__rev0, 48);
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vceqzq_f64(float64x2_t __p0) {
  uint64x2_t __ret;
  __ret = (uint64x2_t) __builtin_neon_vceqzq_v((int8x16_t)__p0, 51);
  return __ret;
}
#else
__ai uint64x2_t vceqzq_f64(float64x2_t __p0) {
  float64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint64x2_t __ret;
  __ret = (uint64x2_t) __builtin_neon_vceqzq_v((int8x16_t)__rev0, 51);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vceqzq_f32(float32x4_t __p0) {
  uint32x4_t __ret;
  __ret = (uint32x4_t) __builtin_neon_vceqzq_v((int8x16_t)__p0, 50);
  return __ret;
}
#else
__ai uint32x4_t vceqzq_f32(float32x4_t __p0) {
  float32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint32x4_t __ret;
  __ret = (uint32x4_t) __builtin_neon_vceqzq_v((int8x16_t)__rev0, 50);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vceqzq_s32(int32x4_t __p0) {
  uint32x4_t __ret;
  __ret = (uint32x4_t) __builtin_neon_vceqzq_v((int8x16_t)__p0, 50);
  return __ret;
}
#else
__ai uint32x4_t vceqzq_s32(int32x4_t __p0) {
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint32x4_t __ret;
  __ret = (uint32x4_t) __builtin_neon_vceqzq_v((int8x16_t)__rev0, 50);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vceqzq_s64(int64x2_t __p0) {
  uint64x2_t __ret;
  __ret = (uint64x2_t) __builtin_neon_vceqzq_v((int8x16_t)__p0, 51);
  return __ret;
}
#else
__ai uint64x2_t vceqzq_s64(int64x2_t __p0) {
  int64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint64x2_t __ret;
  __ret = (uint64x2_t) __builtin_neon_vceqzq_v((int8x16_t)__rev0, 51);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x8_t vceqzq_s16(int16x8_t __p0) {
  uint16x8_t __ret;
  __ret = (uint16x8_t) __builtin_neon_vceqzq_v((int8x16_t)__p0, 49);
  return __ret;
}
#else
__ai uint16x8_t vceqzq_s16(int16x8_t __p0) {
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __ret;
  __ret = (uint16x8_t) __builtin_neon_vceqzq_v((int8x16_t)__rev0, 49);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x8_t vceqz_u8(uint8x8_t __p0) {
  uint8x8_t __ret;
  __ret = (uint8x8_t) __builtin_neon_vceqz_v((int8x8_t)__p0, 16);
  return __ret;
}
#else
__ai uint8x8_t vceqz_u8(uint8x8_t __p0) {
  uint8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __ret;
  __ret = (uint8x8_t) __builtin_neon_vceqz_v((int8x8_t)__rev0, 16);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x2_t vceqz_u32(uint32x2_t __p0) {
  uint32x2_t __ret;
  __ret = (uint32x2_t) __builtin_neon_vceqz_v((int8x8_t)__p0, 18);
  return __ret;
}
#else
__ai uint32x2_t vceqz_u32(uint32x2_t __p0) {
  uint32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint32x2_t __ret;
  __ret = (uint32x2_t) __builtin_neon_vceqz_v((int8x8_t)__rev0, 18);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x1_t vceqz_u64(uint64x1_t __p0) {
  uint64x1_t __ret;
  __ret = (uint64x1_t) __builtin_neon_vceqz_v((int8x8_t)__p0, 19);
  return __ret;
}
#else
__ai uint64x1_t vceqz_u64(uint64x1_t __p0) {
  uint64x1_t __ret;
  __ret = (uint64x1_t) __builtin_neon_vceqz_v((int8x8_t)__p0, 19);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x4_t vceqz_u16(uint16x4_t __p0) {
  uint16x4_t __ret;
  __ret = (uint16x4_t) __builtin_neon_vceqz_v((int8x8_t)__p0, 17);
  return __ret;
}
#else
__ai uint16x4_t vceqz_u16(uint16x4_t __p0) {
  uint16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint16x4_t __ret;
  __ret = (uint16x4_t) __builtin_neon_vceqz_v((int8x8_t)__rev0, 17);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x8_t vceqz_s8(int8x8_t __p0) {
  uint8x8_t __ret;
  __ret = (uint8x8_t) __builtin_neon_vceqz_v((int8x8_t)__p0, 16);
  return __ret;
}
#else
__ai uint8x8_t vceqz_s8(int8x8_t __p0) {
  int8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __ret;
  __ret = (uint8x8_t) __builtin_neon_vceqz_v((int8x8_t)__rev0, 16);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x1_t vceqz_f64(float64x1_t __p0) {
  uint64x1_t __ret;
  __ret = (uint64x1_t) __builtin_neon_vceqz_v((int8x8_t)__p0, 19);
  return __ret;
}
#else
__ai uint64x1_t vceqz_f64(float64x1_t __p0) {
  uint64x1_t __ret;
  __ret = (uint64x1_t) __builtin_neon_vceqz_v((int8x8_t)__p0, 19);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x2_t vceqz_f32(float32x2_t __p0) {
  uint32x2_t __ret;
  __ret = (uint32x2_t) __builtin_neon_vceqz_v((int8x8_t)__p0, 18);
  return __ret;
}
#else
__ai uint32x2_t vceqz_f32(float32x2_t __p0) {
  float32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint32x2_t __ret;
  __ret = (uint32x2_t) __builtin_neon_vceqz_v((int8x8_t)__rev0, 18);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x2_t vceqz_s32(int32x2_t __p0) {
  uint32x2_t __ret;
  __ret = (uint32x2_t) __builtin_neon_vceqz_v((int8x8_t)__p0, 18);
  return __ret;
}
#else
__ai uint32x2_t vceqz_s32(int32x2_t __p0) {
  int32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint32x2_t __ret;
  __ret = (uint32x2_t) __builtin_neon_vceqz_v((int8x8_t)__rev0, 18);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x1_t vceqz_s64(int64x1_t __p0) {
  uint64x1_t __ret;
  __ret = (uint64x1_t) __builtin_neon_vceqz_v((int8x8_t)__p0, 19);
  return __ret;
}
#else
__ai uint64x1_t vceqz_s64(int64x1_t __p0) {
  uint64x1_t __ret;
  __ret = (uint64x1_t) __builtin_neon_vceqz_v((int8x8_t)__p0, 19);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x4_t vceqz_s16(int16x4_t __p0) {
  uint16x4_t __ret;
  __ret = (uint16x4_t) __builtin_neon_vceqz_v((int8x8_t)__p0, 17);
  return __ret;
}
#else
__ai uint16x4_t vceqz_s16(int16x4_t __p0) {
  int16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint16x4_t __ret;
  __ret = (uint16x4_t) __builtin_neon_vceqz_v((int8x8_t)__rev0, 17);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64_t vceqzd_u64(uint64_t __p0) {
  uint64_t __ret;
  __ret = (uint64_t) __builtin_neon_vceqzd_u64(__p0);
  return __ret;
}
#else
__ai uint64_t vceqzd_u64(uint64_t __p0) {
  uint64_t __ret;
  __ret = (uint64_t) __builtin_neon_vceqzd_u64(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64_t vceqzd_s64(int64_t __p0) {
  int64_t __ret;
  __ret = (int64_t) __builtin_neon_vceqzd_s64(__p0);
  return __ret;
}
#else
__ai int64_t vceqzd_s64(int64_t __p0) {
  int64_t __ret;
  __ret = (int64_t) __builtin_neon_vceqzd_s64(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64_t vceqzd_f64(float64_t __p0) {
  uint64_t __ret;
  __ret = (uint64_t) __builtin_neon_vceqzd_f64(__p0);
  return __ret;
}
#else
__ai uint64_t vceqzd_f64(float64_t __p0) {
  uint64_t __ret;
  __ret = (uint64_t) __builtin_neon_vceqzd_f64(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32_t vceqzs_f32(float32_t __p0) {
  uint32_t __ret;
  __ret = (uint32_t) __builtin_neon_vceqzs_f32(__p0);
  return __ret;
}
#else
__ai uint32_t vceqzs_f32(float32_t __p0) {
  uint32_t __ret;
  __ret = (uint32_t) __builtin_neon_vceqzs_f32(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vcgeq_u64(uint64x2_t __p0, uint64x2_t __p1) {
  uint64x2_t __ret;
  __ret = (uint64x2_t)(__p0 >= __p1);
  return __ret;
}
#else
__ai uint64x2_t vcgeq_u64(uint64x2_t __p0, uint64x2_t __p1) {
  uint64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  uint64x2_t __ret;
  __ret = (uint64x2_t)(__rev0 >= __rev1);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vcgeq_f64(float64x2_t __p0, float64x2_t __p1) {
  uint64x2_t __ret;
  __ret = (uint64x2_t)(__p0 >= __p1);
  return __ret;
}
#else
__ai uint64x2_t vcgeq_f64(float64x2_t __p0, float64x2_t __p1) {
  float64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  uint64x2_t __ret;
  __ret = (uint64x2_t)(__rev0 >= __rev1);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vcgeq_s64(int64x2_t __p0, int64x2_t __p1) {
  uint64x2_t __ret;
  __ret = (uint64x2_t)(__p0 >= __p1);
  return __ret;
}
#else
__ai uint64x2_t vcgeq_s64(int64x2_t __p0, int64x2_t __p1) {
  int64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  uint64x2_t __ret;
  __ret = (uint64x2_t)(__rev0 >= __rev1);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x1_t vcge_u64(uint64x1_t __p0, uint64x1_t __p1) {
  uint64x1_t __ret;
  __ret = (uint64x1_t)(__p0 >= __p1);
  return __ret;
}
#else
__ai uint64x1_t vcge_u64(uint64x1_t __p0, uint64x1_t __p1) {
  uint64x1_t __ret;
  __ret = (uint64x1_t)(__p0 >= __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x1_t vcge_f64(float64x1_t __p0, float64x1_t __p1) {
  uint64x1_t __ret;
  __ret = (uint64x1_t)(__p0 >= __p1);
  return __ret;
}
#else
__ai uint64x1_t vcge_f64(float64x1_t __p0, float64x1_t __p1) {
  uint64x1_t __ret;
  __ret = (uint64x1_t)(__p0 >= __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x1_t vcge_s64(int64x1_t __p0, int64x1_t __p1) {
  uint64x1_t __ret;
  __ret = (uint64x1_t)(__p0 >= __p1);
  return __ret;
}
#else
__ai uint64x1_t vcge_s64(int64x1_t __p0, int64x1_t __p1) {
  uint64x1_t __ret;
  __ret = (uint64x1_t)(__p0 >= __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64_t vcged_s64(int64_t __p0, int64_t __p1) {
  int64_t __ret;
  __ret = (int64_t) __builtin_neon_vcged_s64(__p0, __p1);
  return __ret;
}
#else
__ai int64_t vcged_s64(int64_t __p0, int64_t __p1) {
  int64_t __ret;
  __ret = (int64_t) __builtin_neon_vcged_s64(__p0, __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64_t vcged_u64(uint64_t __p0, uint64_t __p1) {
  uint64_t __ret;
  __ret = (uint64_t) __builtin_neon_vcged_u64(__p0, __p1);
  return __ret;
}
#else
__ai uint64_t vcged_u64(uint64_t __p0, uint64_t __p1) {
  uint64_t __ret;
  __ret = (uint64_t) __builtin_neon_vcged_u64(__p0, __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64_t vcged_f64(float64_t __p0, float64_t __p1) {
  uint64_t __ret;
  __ret = (uint64_t) __builtin_neon_vcged_f64(__p0, __p1);
  return __ret;
}
#else
__ai uint64_t vcged_f64(float64_t __p0, float64_t __p1) {
  uint64_t __ret;
  __ret = (uint64_t) __builtin_neon_vcged_f64(__p0, __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32_t vcges_f32(float32_t __p0, float32_t __p1) {
  uint32_t __ret;
  __ret = (uint32_t) __builtin_neon_vcges_f32(__p0, __p1);
  return __ret;
}
#else
__ai uint32_t vcges_f32(float32_t __p0, float32_t __p1) {
  uint32_t __ret;
  __ret = (uint32_t) __builtin_neon_vcges_f32(__p0, __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x16_t vcgezq_s8(int8x16_t __p0) {
  uint8x16_t __ret;
  __ret = (uint8x16_t) __builtin_neon_vcgezq_v((int8x16_t)__p0, 48);
  return __ret;
}
#else
__ai uint8x16_t vcgezq_s8(int8x16_t __p0) {
  int8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __ret;
  __ret = (uint8x16_t) __builtin_neon_vcgezq_v((int8x16_t)__rev0, 48);
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vcgezq_f64(float64x2_t __p0) {
  uint64x2_t __ret;
  __ret = (uint64x2_t) __builtin_neon_vcgezq_v((int8x16_t)__p0, 51);
  return __ret;
}
#else
__ai uint64x2_t vcgezq_f64(float64x2_t __p0) {
  float64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint64x2_t __ret;
  __ret = (uint64x2_t) __builtin_neon_vcgezq_v((int8x16_t)__rev0, 51);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vcgezq_f32(float32x4_t __p0) {
  uint32x4_t __ret;
  __ret = (uint32x4_t) __builtin_neon_vcgezq_v((int8x16_t)__p0, 50);
  return __ret;
}
#else
__ai uint32x4_t vcgezq_f32(float32x4_t __p0) {
  float32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint32x4_t __ret;
  __ret = (uint32x4_t) __builtin_neon_vcgezq_v((int8x16_t)__rev0, 50);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vcgezq_s32(int32x4_t __p0) {
  uint32x4_t __ret;
  __ret = (uint32x4_t) __builtin_neon_vcgezq_v((int8x16_t)__p0, 50);
  return __ret;
}
#else
__ai uint32x4_t vcgezq_s32(int32x4_t __p0) {
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint32x4_t __ret;
  __ret = (uint32x4_t) __builtin_neon_vcgezq_v((int8x16_t)__rev0, 50);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vcgezq_s64(int64x2_t __p0) {
  uint64x2_t __ret;
  __ret = (uint64x2_t) __builtin_neon_vcgezq_v((int8x16_t)__p0, 51);
  return __ret;
}
#else
__ai uint64x2_t vcgezq_s64(int64x2_t __p0) {
  int64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint64x2_t __ret;
  __ret = (uint64x2_t) __builtin_neon_vcgezq_v((int8x16_t)__rev0, 51);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x8_t vcgezq_s16(int16x8_t __p0) {
  uint16x8_t __ret;
  __ret = (uint16x8_t) __builtin_neon_vcgezq_v((int8x16_t)__p0, 49);
  return __ret;
}
#else
__ai uint16x8_t vcgezq_s16(int16x8_t __p0) {
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __ret;
  __ret = (uint16x8_t) __builtin_neon_vcgezq_v((int8x16_t)__rev0, 49);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x8_t vcgez_s8(int8x8_t __p0) {
  uint8x8_t __ret;
  __ret = (uint8x8_t) __builtin_neon_vcgez_v((int8x8_t)__p0, 16);
  return __ret;
}
#else
__ai uint8x8_t vcgez_s8(int8x8_t __p0) {
  int8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __ret;
  __ret = (uint8x8_t) __builtin_neon_vcgez_v((int8x8_t)__rev0, 16);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x1_t vcgez_f64(float64x1_t __p0) {
  uint64x1_t __ret;
  __ret = (uint64x1_t) __builtin_neon_vcgez_v((int8x8_t)__p0, 19);
  return __ret;
}
#else
__ai uint64x1_t vcgez_f64(float64x1_t __p0) {
  uint64x1_t __ret;
  __ret = (uint64x1_t) __builtin_neon_vcgez_v((int8x8_t)__p0, 19);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x2_t vcgez_f32(float32x2_t __p0) {
  uint32x2_t __ret;
  __ret = (uint32x2_t) __builtin_neon_vcgez_v((int8x8_t)__p0, 18);
  return __ret;
}
#else
__ai uint32x2_t vcgez_f32(float32x2_t __p0) {
  float32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint32x2_t __ret;
  __ret = (uint32x2_t) __builtin_neon_vcgez_v((int8x8_t)__rev0, 18);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x2_t vcgez_s32(int32x2_t __p0) {
  uint32x2_t __ret;
  __ret = (uint32x2_t) __builtin_neon_vcgez_v((int8x8_t)__p0, 18);
  return __ret;
}
#else
__ai uint32x2_t vcgez_s32(int32x2_t __p0) {
  int32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint32x2_t __ret;
  __ret = (uint32x2_t) __builtin_neon_vcgez_v((int8x8_t)__rev0, 18);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x1_t vcgez_s64(int64x1_t __p0) {
  uint64x1_t __ret;
  __ret = (uint64x1_t) __builtin_neon_vcgez_v((int8x8_t)__p0, 19);
  return __ret;
}
#else
__ai uint64x1_t vcgez_s64(int64x1_t __p0) {
  uint64x1_t __ret;
  __ret = (uint64x1_t) __builtin_neon_vcgez_v((int8x8_t)__p0, 19);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x4_t vcgez_s16(int16x4_t __p0) {
  uint16x4_t __ret;
  __ret = (uint16x4_t) __builtin_neon_vcgez_v((int8x8_t)__p0, 17);
  return __ret;
}
#else
__ai uint16x4_t vcgez_s16(int16x4_t __p0) {
  int16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint16x4_t __ret;
  __ret = (uint16x4_t) __builtin_neon_vcgez_v((int8x8_t)__rev0, 17);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64_t vcgezd_s64(int64_t __p0) {
  int64_t __ret;
  __ret = (int64_t) __builtin_neon_vcgezd_s64(__p0);
  return __ret;
}
#else
__ai int64_t vcgezd_s64(int64_t __p0) {
  int64_t __ret;
  __ret = (int64_t) __builtin_neon_vcgezd_s64(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64_t vcgezd_f64(float64_t __p0) {
  uint64_t __ret;
  __ret = (uint64_t) __builtin_neon_vcgezd_f64(__p0);
  return __ret;
}
#else
__ai uint64_t vcgezd_f64(float64_t __p0) {
  uint64_t __ret;
  __ret = (uint64_t) __builtin_neon_vcgezd_f64(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32_t vcgezs_f32(float32_t __p0) {
  uint32_t __ret;
  __ret = (uint32_t) __builtin_neon_vcgezs_f32(__p0);
  return __ret;
}
#else
__ai uint32_t vcgezs_f32(float32_t __p0) {
  uint32_t __ret;
  __ret = (uint32_t) __builtin_neon_vcgezs_f32(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vcgtq_u64(uint64x2_t __p0, uint64x2_t __p1) {
  uint64x2_t __ret;
  __ret = (uint64x2_t)(__p0 > __p1);
  return __ret;
}
#else
__ai uint64x2_t vcgtq_u64(uint64x2_t __p0, uint64x2_t __p1) {
  uint64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  uint64x2_t __ret;
  __ret = (uint64x2_t)(__rev0 > __rev1);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vcgtq_f64(float64x2_t __p0, float64x2_t __p1) {
  uint64x2_t __ret;
  __ret = (uint64x2_t)(__p0 > __p1);
  return __ret;
}
#else
__ai uint64x2_t vcgtq_f64(float64x2_t __p0, float64x2_t __p1) {
  float64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  uint64x2_t __ret;
  __ret = (uint64x2_t)(__rev0 > __rev1);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vcgtq_s64(int64x2_t __p0, int64x2_t __p1) {
  uint64x2_t __ret;
  __ret = (uint64x2_t)(__p0 > __p1);
  return __ret;
}
#else
__ai uint64x2_t vcgtq_s64(int64x2_t __p0, int64x2_t __p1) {
  int64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  uint64x2_t __ret;
  __ret = (uint64x2_t)(__rev0 > __rev1);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x1_t vcgt_u64(uint64x1_t __p0, uint64x1_t __p1) {
  uint64x1_t __ret;
  __ret = (uint64x1_t)(__p0 > __p1);
  return __ret;
}
#else
__ai uint64x1_t vcgt_u64(uint64x1_t __p0, uint64x1_t __p1) {
  uint64x1_t __ret;
  __ret = (uint64x1_t)(__p0 > __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x1_t vcgt_f64(float64x1_t __p0, float64x1_t __p1) {
  uint64x1_t __ret;
  __ret = (uint64x1_t)(__p0 > __p1);
  return __ret;
}
#else
__ai uint64x1_t vcgt_f64(float64x1_t __p0, float64x1_t __p1) {
  uint64x1_t __ret;
  __ret = (uint64x1_t)(__p0 > __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x1_t vcgt_s64(int64x1_t __p0, int64x1_t __p1) {
  uint64x1_t __ret;
  __ret = (uint64x1_t)(__p0 > __p1);
  return __ret;
}
#else
__ai uint64x1_t vcgt_s64(int64x1_t __p0, int64x1_t __p1) {
  uint64x1_t __ret;
  __ret = (uint64x1_t)(__p0 > __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64_t vcgtd_s64(int64_t __p0, int64_t __p1) {
  int64_t __ret;
  __ret = (int64_t) __builtin_neon_vcgtd_s64(__p0, __p1);
  return __ret;
}
#else
__ai int64_t vcgtd_s64(int64_t __p0, int64_t __p1) {
  int64_t __ret;
  __ret = (int64_t) __builtin_neon_vcgtd_s64(__p0, __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64_t vcgtd_u64(uint64_t __p0, uint64_t __p1) {
  uint64_t __ret;
  __ret = (uint64_t) __builtin_neon_vcgtd_u64(__p0, __p1);
  return __ret;
}
#else
__ai uint64_t vcgtd_u64(uint64_t __p0, uint64_t __p1) {
  uint64_t __ret;
  __ret = (uint64_t) __builtin_neon_vcgtd_u64(__p0, __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64_t vcgtd_f64(float64_t __p0, float64_t __p1) {
  uint64_t __ret;
  __ret = (uint64_t) __builtin_neon_vcgtd_f64(__p0, __p1);
  return __ret;
}
#else
__ai uint64_t vcgtd_f64(float64_t __p0, float64_t __p1) {
  uint64_t __ret;
  __ret = (uint64_t) __builtin_neon_vcgtd_f64(__p0, __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32_t vcgts_f32(float32_t __p0, float32_t __p1) {
  uint32_t __ret;
  __ret = (uint32_t) __builtin_neon_vcgts_f32(__p0, __p1);
  return __ret;
}
#else
__ai uint32_t vcgts_f32(float32_t __p0, float32_t __p1) {
  uint32_t __ret;
  __ret = (uint32_t) __builtin_neon_vcgts_f32(__p0, __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x16_t vcgtzq_s8(int8x16_t __p0) {
  uint8x16_t __ret;
  __ret = (uint8x16_t) __builtin_neon_vcgtzq_v((int8x16_t)__p0, 48);
  return __ret;
}
#else
__ai uint8x16_t vcgtzq_s8(int8x16_t __p0) {
  int8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __ret;
  __ret = (uint8x16_t) __builtin_neon_vcgtzq_v((int8x16_t)__rev0, 48);
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vcgtzq_f64(float64x2_t __p0) {
  uint64x2_t __ret;
  __ret = (uint64x2_t) __builtin_neon_vcgtzq_v((int8x16_t)__p0, 51);
  return __ret;
}
#else
__ai uint64x2_t vcgtzq_f64(float64x2_t __p0) {
  float64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint64x2_t __ret;
  __ret = (uint64x2_t) __builtin_neon_vcgtzq_v((int8x16_t)__rev0, 51);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vcgtzq_f32(float32x4_t __p0) {
  uint32x4_t __ret;
  __ret = (uint32x4_t) __builtin_neon_vcgtzq_v((int8x16_t)__p0, 50);
  return __ret;
}
#else
__ai uint32x4_t vcgtzq_f32(float32x4_t __p0) {
  float32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint32x4_t __ret;
  __ret = (uint32x4_t) __builtin_neon_vcgtzq_v((int8x16_t)__rev0, 50);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vcgtzq_s32(int32x4_t __p0) {
  uint32x4_t __ret;
  __ret = (uint32x4_t) __builtin_neon_vcgtzq_v((int8x16_t)__p0, 50);
  return __ret;
}
#else
__ai uint32x4_t vcgtzq_s32(int32x4_t __p0) {
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint32x4_t __ret;
  __ret = (uint32x4_t) __builtin_neon_vcgtzq_v((int8x16_t)__rev0, 50);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vcgtzq_s64(int64x2_t __p0) {
  uint64x2_t __ret;
  __ret = (uint64x2_t) __builtin_neon_vcgtzq_v((int8x16_t)__p0, 51);
  return __ret;
}
#else
__ai uint64x2_t vcgtzq_s64(int64x2_t __p0) {
  int64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint64x2_t __ret;
  __ret = (uint64x2_t) __builtin_neon_vcgtzq_v((int8x16_t)__rev0, 51);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x8_t vcgtzq_s16(int16x8_t __p0) {
  uint16x8_t __ret;
  __ret = (uint16x8_t) __builtin_neon_vcgtzq_v((int8x16_t)__p0, 49);
  return __ret;
}
#else
__ai uint16x8_t vcgtzq_s16(int16x8_t __p0) {
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __ret;
  __ret = (uint16x8_t) __builtin_neon_vcgtzq_v((int8x16_t)__rev0, 49);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x8_t vcgtz_s8(int8x8_t __p0) {
  uint8x8_t __ret;
  __ret = (uint8x8_t) __builtin_neon_vcgtz_v((int8x8_t)__p0, 16);
  return __ret;
}
#else
__ai uint8x8_t vcgtz_s8(int8x8_t __p0) {
  int8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __ret;
  __ret = (uint8x8_t) __builtin_neon_vcgtz_v((int8x8_t)__rev0, 16);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x1_t vcgtz_f64(float64x1_t __p0) {
  uint64x1_t __ret;
  __ret = (uint64x1_t) __builtin_neon_vcgtz_v((int8x8_t)__p0, 19);
  return __ret;
}
#else
__ai uint64x1_t vcgtz_f64(float64x1_t __p0) {
  uint64x1_t __ret;
  __ret = (uint64x1_t) __builtin_neon_vcgtz_v((int8x8_t)__p0, 19);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x2_t vcgtz_f32(float32x2_t __p0) {
  uint32x2_t __ret;
  __ret = (uint32x2_t) __builtin_neon_vcgtz_v((int8x8_t)__p0, 18);
  return __ret;
}
#else
__ai uint32x2_t vcgtz_f32(float32x2_t __p0) {
  float32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint32x2_t __ret;
  __ret = (uint32x2_t) __builtin_neon_vcgtz_v((int8x8_t)__rev0, 18);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x2_t vcgtz_s32(int32x2_t __p0) {
  uint32x2_t __ret;
  __ret = (uint32x2_t) __builtin_neon_vcgtz_v((int8x8_t)__p0, 18);
  return __ret;
}
#else
__ai uint32x2_t vcgtz_s32(int32x2_t __p0) {
  int32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint32x2_t __ret;
  __ret = (uint32x2_t) __builtin_neon_vcgtz_v((int8x8_t)__rev0, 18);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x1_t vcgtz_s64(int64x1_t __p0) {
  uint64x1_t __ret;
  __ret = (uint64x1_t) __builtin_neon_vcgtz_v((int8x8_t)__p0, 19);
  return __ret;
}
#else
__ai uint64x1_t vcgtz_s64(int64x1_t __p0) {
  uint64x1_t __ret;
  __ret = (uint64x1_t) __builtin_neon_vcgtz_v((int8x8_t)__p0, 19);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x4_t vcgtz_s16(int16x4_t __p0) {
  uint16x4_t __ret;
  __ret = (uint16x4_t) __builtin_neon_vcgtz_v((int8x8_t)__p0, 17);
  return __ret;
}
#else
__ai uint16x4_t vcgtz_s16(int16x4_t __p0) {
  int16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint16x4_t __ret;
  __ret = (uint16x4_t) __builtin_neon_vcgtz_v((int8x8_t)__rev0, 17);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64_t vcgtzd_s64(int64_t __p0) {
  int64_t __ret;
  __ret = (int64_t) __builtin_neon_vcgtzd_s64(__p0);
  return __ret;
}
#else
__ai int64_t vcgtzd_s64(int64_t __p0) {
  int64_t __ret;
  __ret = (int64_t) __builtin_neon_vcgtzd_s64(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64_t vcgtzd_f64(float64_t __p0) {
  uint64_t __ret;
  __ret = (uint64_t) __builtin_neon_vcgtzd_f64(__p0);
  return __ret;
}
#else
__ai uint64_t vcgtzd_f64(float64_t __p0) {
  uint64_t __ret;
  __ret = (uint64_t) __builtin_neon_vcgtzd_f64(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32_t vcgtzs_f32(float32_t __p0) {
  uint32_t __ret;
  __ret = (uint32_t) __builtin_neon_vcgtzs_f32(__p0);
  return __ret;
}
#else
__ai uint32_t vcgtzs_f32(float32_t __p0) {
  uint32_t __ret;
  __ret = (uint32_t) __builtin_neon_vcgtzs_f32(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vcleq_u64(uint64x2_t __p0, uint64x2_t __p1) {
  uint64x2_t __ret;
  __ret = (uint64x2_t)(__p0 <= __p1);
  return __ret;
}
#else
__ai uint64x2_t vcleq_u64(uint64x2_t __p0, uint64x2_t __p1) {
  uint64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  uint64x2_t __ret;
  __ret = (uint64x2_t)(__rev0 <= __rev1);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vcleq_f64(float64x2_t __p0, float64x2_t __p1) {
  uint64x2_t __ret;
  __ret = (uint64x2_t)(__p0 <= __p1);
  return __ret;
}
#else
__ai uint64x2_t vcleq_f64(float64x2_t __p0, float64x2_t __p1) {
  float64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  uint64x2_t __ret;
  __ret = (uint64x2_t)(__rev0 <= __rev1);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vcleq_s64(int64x2_t __p0, int64x2_t __p1) {
  uint64x2_t __ret;
  __ret = (uint64x2_t)(__p0 <= __p1);
  return __ret;
}
#else
__ai uint64x2_t vcleq_s64(int64x2_t __p0, int64x2_t __p1) {
  int64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  uint64x2_t __ret;
  __ret = (uint64x2_t)(__rev0 <= __rev1);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x1_t vcle_u64(uint64x1_t __p0, uint64x1_t __p1) {
  uint64x1_t __ret;
  __ret = (uint64x1_t)(__p0 <= __p1);
  return __ret;
}
#else
__ai uint64x1_t vcle_u64(uint64x1_t __p0, uint64x1_t __p1) {
  uint64x1_t __ret;
  __ret = (uint64x1_t)(__p0 <= __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x1_t vcle_f64(float64x1_t __p0, float64x1_t __p1) {
  uint64x1_t __ret;
  __ret = (uint64x1_t)(__p0 <= __p1);
  return __ret;
}
#else
__ai uint64x1_t vcle_f64(float64x1_t __p0, float64x1_t __p1) {
  uint64x1_t __ret;
  __ret = (uint64x1_t)(__p0 <= __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x1_t vcle_s64(int64x1_t __p0, int64x1_t __p1) {
  uint64x1_t __ret;
  __ret = (uint64x1_t)(__p0 <= __p1);
  return __ret;
}
#else
__ai uint64x1_t vcle_s64(int64x1_t __p0, int64x1_t __p1) {
  uint64x1_t __ret;
  __ret = (uint64x1_t)(__p0 <= __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64_t vcled_u64(uint64_t __p0, uint64_t __p1) {
  uint64_t __ret;
  __ret = (uint64_t) __builtin_neon_vcled_u64(__p0, __p1);
  return __ret;
}
#else
__ai uint64_t vcled_u64(uint64_t __p0, uint64_t __p1) {
  uint64_t __ret;
  __ret = (uint64_t) __builtin_neon_vcled_u64(__p0, __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64_t vcled_s64(int64_t __p0, int64_t __p1) {
  int64_t __ret;
  __ret = (int64_t) __builtin_neon_vcled_s64(__p0, __p1);
  return __ret;
}
#else
__ai int64_t vcled_s64(int64_t __p0, int64_t __p1) {
  int64_t __ret;
  __ret = (int64_t) __builtin_neon_vcled_s64(__p0, __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64_t vcled_f64(float64_t __p0, float64_t __p1) {
  uint64_t __ret;
  __ret = (uint64_t) __builtin_neon_vcled_f64(__p0, __p1);
  return __ret;
}
#else
__ai uint64_t vcled_f64(float64_t __p0, float64_t __p1) {
  uint64_t __ret;
  __ret = (uint64_t) __builtin_neon_vcled_f64(__p0, __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32_t vcles_f32(float32_t __p0, float32_t __p1) {
  uint32_t __ret;
  __ret = (uint32_t) __builtin_neon_vcles_f32(__p0, __p1);
  return __ret;
}
#else
__ai uint32_t vcles_f32(float32_t __p0, float32_t __p1) {
  uint32_t __ret;
  __ret = (uint32_t) __builtin_neon_vcles_f32(__p0, __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x16_t vclezq_s8(int8x16_t __p0) {
  uint8x16_t __ret;
  __ret = (uint8x16_t) __builtin_neon_vclezq_v((int8x16_t)__p0, 48);
  return __ret;
}
#else
__ai uint8x16_t vclezq_s8(int8x16_t __p0) {
  int8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __ret;
  __ret = (uint8x16_t) __builtin_neon_vclezq_v((int8x16_t)__rev0, 48);
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vclezq_f64(float64x2_t __p0) {
  uint64x2_t __ret;
  __ret = (uint64x2_t) __builtin_neon_vclezq_v((int8x16_t)__p0, 51);
  return __ret;
}
#else
__ai uint64x2_t vclezq_f64(float64x2_t __p0) {
  float64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint64x2_t __ret;
  __ret = (uint64x2_t) __builtin_neon_vclezq_v((int8x16_t)__rev0, 51);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vclezq_f32(float32x4_t __p0) {
  uint32x4_t __ret;
  __ret = (uint32x4_t) __builtin_neon_vclezq_v((int8x16_t)__p0, 50);
  return __ret;
}
#else
__ai uint32x4_t vclezq_f32(float32x4_t __p0) {
  float32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint32x4_t __ret;
  __ret = (uint32x4_t) __builtin_neon_vclezq_v((int8x16_t)__rev0, 50);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vclezq_s32(int32x4_t __p0) {
  uint32x4_t __ret;
  __ret = (uint32x4_t) __builtin_neon_vclezq_v((int8x16_t)__p0, 50);
  return __ret;
}
#else
__ai uint32x4_t vclezq_s32(int32x4_t __p0) {
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint32x4_t __ret;
  __ret = (uint32x4_t) __builtin_neon_vclezq_v((int8x16_t)__rev0, 50);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vclezq_s64(int64x2_t __p0) {
  uint64x2_t __ret;
  __ret = (uint64x2_t) __builtin_neon_vclezq_v((int8x16_t)__p0, 51);
  return __ret;
}
#else
__ai uint64x2_t vclezq_s64(int64x2_t __p0) {
  int64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint64x2_t __ret;
  __ret = (uint64x2_t) __builtin_neon_vclezq_v((int8x16_t)__rev0, 51);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x8_t vclezq_s16(int16x8_t __p0) {
  uint16x8_t __ret;
  __ret = (uint16x8_t) __builtin_neon_vclezq_v((int8x16_t)__p0, 49);
  return __ret;
}
#else
__ai uint16x8_t vclezq_s16(int16x8_t __p0) {
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __ret;
  __ret = (uint16x8_t) __builtin_neon_vclezq_v((int8x16_t)__rev0, 49);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x8_t vclez_s8(int8x8_t __p0) {
  uint8x8_t __ret;
  __ret = (uint8x8_t) __builtin_neon_vclez_v((int8x8_t)__p0, 16);
  return __ret;
}
#else
__ai uint8x8_t vclez_s8(int8x8_t __p0) {
  int8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __ret;
  __ret = (uint8x8_t) __builtin_neon_vclez_v((int8x8_t)__rev0, 16);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x1_t vclez_f64(float64x1_t __p0) {
  uint64x1_t __ret;
  __ret = (uint64x1_t) __builtin_neon_vclez_v((int8x8_t)__p0, 19);
  return __ret;
}
#else
__ai uint64x1_t vclez_f64(float64x1_t __p0) {
  uint64x1_t __ret;
  __ret = (uint64x1_t) __builtin_neon_vclez_v((int8x8_t)__p0, 19);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x2_t vclez_f32(float32x2_t __p0) {
  uint32x2_t __ret;
  __ret = (uint32x2_t) __builtin_neon_vclez_v((int8x8_t)__p0, 18);
  return __ret;
}
#else
__ai uint32x2_t vclez_f32(float32x2_t __p0) {
  float32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint32x2_t __ret;
  __ret = (uint32x2_t) __builtin_neon_vclez_v((int8x8_t)__rev0, 18);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x2_t vclez_s32(int32x2_t __p0) {
  uint32x2_t __ret;
  __ret = (uint32x2_t) __builtin_neon_vclez_v((int8x8_t)__p0, 18);
  return __ret;
}
#else
__ai uint32x2_t vclez_s32(int32x2_t __p0) {
  int32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint32x2_t __ret;
  __ret = (uint32x2_t) __builtin_neon_vclez_v((int8x8_t)__rev0, 18);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x1_t vclez_s64(int64x1_t __p0) {
  uint64x1_t __ret;
  __ret = (uint64x1_t) __builtin_neon_vclez_v((int8x8_t)__p0, 19);
  return __ret;
}
#else
__ai uint64x1_t vclez_s64(int64x1_t __p0) {
  uint64x1_t __ret;
  __ret = (uint64x1_t) __builtin_neon_vclez_v((int8x8_t)__p0, 19);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x4_t vclez_s16(int16x4_t __p0) {
  uint16x4_t __ret;
  __ret = (uint16x4_t) __builtin_neon_vclez_v((int8x8_t)__p0, 17);
  return __ret;
}
#else
__ai uint16x4_t vclez_s16(int16x4_t __p0) {
  int16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint16x4_t __ret;
  __ret = (uint16x4_t) __builtin_neon_vclez_v((int8x8_t)__rev0, 17);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64_t vclezd_s64(int64_t __p0) {
  int64_t __ret;
  __ret = (int64_t) __builtin_neon_vclezd_s64(__p0);
  return __ret;
}
#else
__ai int64_t vclezd_s64(int64_t __p0) {
  int64_t __ret;
  __ret = (int64_t) __builtin_neon_vclezd_s64(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64_t vclezd_f64(float64_t __p0) {
  uint64_t __ret;
  __ret = (uint64_t) __builtin_neon_vclezd_f64(__p0);
  return __ret;
}
#else
__ai uint64_t vclezd_f64(float64_t __p0) {
  uint64_t __ret;
  __ret = (uint64_t) __builtin_neon_vclezd_f64(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32_t vclezs_f32(float32_t __p0) {
  uint32_t __ret;
  __ret = (uint32_t) __builtin_neon_vclezs_f32(__p0);
  return __ret;
}
#else
__ai uint32_t vclezs_f32(float32_t __p0) {
  uint32_t __ret;
  __ret = (uint32_t) __builtin_neon_vclezs_f32(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vcltq_u64(uint64x2_t __p0, uint64x2_t __p1) {
  uint64x2_t __ret;
  __ret = (uint64x2_t)(__p0 < __p1);
  return __ret;
}
#else
__ai uint64x2_t vcltq_u64(uint64x2_t __p0, uint64x2_t __p1) {
  uint64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  uint64x2_t __ret;
  __ret = (uint64x2_t)(__rev0 < __rev1);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vcltq_f64(float64x2_t __p0, float64x2_t __p1) {
  uint64x2_t __ret;
  __ret = (uint64x2_t)(__p0 < __p1);
  return __ret;
}
#else
__ai uint64x2_t vcltq_f64(float64x2_t __p0, float64x2_t __p1) {
  float64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  uint64x2_t __ret;
  __ret = (uint64x2_t)(__rev0 < __rev1);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vcltq_s64(int64x2_t __p0, int64x2_t __p1) {
  uint64x2_t __ret;
  __ret = (uint64x2_t)(__p0 < __p1);
  return __ret;
}
#else
__ai uint64x2_t vcltq_s64(int64x2_t __p0, int64x2_t __p1) {
  int64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  uint64x2_t __ret;
  __ret = (uint64x2_t)(__rev0 < __rev1);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x1_t vclt_u64(uint64x1_t __p0, uint64x1_t __p1) {
  uint64x1_t __ret;
  __ret = (uint64x1_t)(__p0 < __p1);
  return __ret;
}
#else
__ai uint64x1_t vclt_u64(uint64x1_t __p0, uint64x1_t __p1) {
  uint64x1_t __ret;
  __ret = (uint64x1_t)(__p0 < __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x1_t vclt_f64(float64x1_t __p0, float64x1_t __p1) {
  uint64x1_t __ret;
  __ret = (uint64x1_t)(__p0 < __p1);
  return __ret;
}
#else
__ai uint64x1_t vclt_f64(float64x1_t __p0, float64x1_t __p1) {
  uint64x1_t __ret;
  __ret = (uint64x1_t)(__p0 < __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x1_t vclt_s64(int64x1_t __p0, int64x1_t __p1) {
  uint64x1_t __ret;
  __ret = (uint64x1_t)(__p0 < __p1);
  return __ret;
}
#else
__ai uint64x1_t vclt_s64(int64x1_t __p0, int64x1_t __p1) {
  uint64x1_t __ret;
  __ret = (uint64x1_t)(__p0 < __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64_t vcltd_u64(uint64_t __p0, uint64_t __p1) {
  uint64_t __ret;
  __ret = (uint64_t) __builtin_neon_vcltd_u64(__p0, __p1);
  return __ret;
}
#else
__ai uint64_t vcltd_u64(uint64_t __p0, uint64_t __p1) {
  uint64_t __ret;
  __ret = (uint64_t) __builtin_neon_vcltd_u64(__p0, __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64_t vcltd_s64(int64_t __p0, int64_t __p1) {
  int64_t __ret;
  __ret = (int64_t) __builtin_neon_vcltd_s64(__p0, __p1);
  return __ret;
}
#else
__ai int64_t vcltd_s64(int64_t __p0, int64_t __p1) {
  int64_t __ret;
  __ret = (int64_t) __builtin_neon_vcltd_s64(__p0, __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64_t vcltd_f64(float64_t __p0, float64_t __p1) {
  uint64_t __ret;
  __ret = (uint64_t) __builtin_neon_vcltd_f64(__p0, __p1);
  return __ret;
}
#else
__ai uint64_t vcltd_f64(float64_t __p0, float64_t __p1) {
  uint64_t __ret;
  __ret = (uint64_t) __builtin_neon_vcltd_f64(__p0, __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32_t vclts_f32(float32_t __p0, float32_t __p1) {
  uint32_t __ret;
  __ret = (uint32_t) __builtin_neon_vclts_f32(__p0, __p1);
  return __ret;
}
#else
__ai uint32_t vclts_f32(float32_t __p0, float32_t __p1) {
  uint32_t __ret;
  __ret = (uint32_t) __builtin_neon_vclts_f32(__p0, __p1);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x16_t vcltzq_s8(int8x16_t __p0) {
  uint8x16_t __ret;
  __ret = (uint8x16_t) __builtin_neon_vcltzq_v((int8x16_t)__p0, 48);
  return __ret;
}
#else
__ai uint8x16_t vcltzq_s8(int8x16_t __p0) {
  int8x16_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x16_t __ret;
  __ret = (uint8x16_t) __builtin_neon_vcltzq_v((int8x16_t)__rev0, 48);
  __ret = __builtin_shufflevector(__ret, __ret, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vcltzq_f64(float64x2_t __p0) {
  uint64x2_t __ret;
  __ret = (uint64x2_t) __builtin_neon_vcltzq_v((int8x16_t)__p0, 51);
  return __ret;
}
#else
__ai uint64x2_t vcltzq_f64(float64x2_t __p0) {
  float64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint64x2_t __ret;
  __ret = (uint64x2_t) __builtin_neon_vcltzq_v((int8x16_t)__rev0, 51);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vcltzq_f32(float32x4_t __p0) {
  uint32x4_t __ret;
  __ret = (uint32x4_t) __builtin_neon_vcltzq_v((int8x16_t)__p0, 50);
  return __ret;
}
#else
__ai uint32x4_t vcltzq_f32(float32x4_t __p0) {
  float32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint32x4_t __ret;
  __ret = (uint32x4_t) __builtin_neon_vcltzq_v((int8x16_t)__rev0, 50);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x4_t vcltzq_s32(int32x4_t __p0) {
  uint32x4_t __ret;
  __ret = (uint32x4_t) __builtin_neon_vcltzq_v((int8x16_t)__p0, 50);
  return __ret;
}
#else
__ai uint32x4_t vcltzq_s32(int32x4_t __p0) {
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint32x4_t __ret;
  __ret = (uint32x4_t) __builtin_neon_vcltzq_v((int8x16_t)__rev0, 50);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vcltzq_s64(int64x2_t __p0) {
  uint64x2_t __ret;
  __ret = (uint64x2_t) __builtin_neon_vcltzq_v((int8x16_t)__p0, 51);
  return __ret;
}
#else
__ai uint64x2_t vcltzq_s64(int64x2_t __p0) {
  int64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint64x2_t __ret;
  __ret = (uint64x2_t) __builtin_neon_vcltzq_v((int8x16_t)__rev0, 51);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x8_t vcltzq_s16(int16x8_t __p0) {
  uint16x8_t __ret;
  __ret = (uint16x8_t) __builtin_neon_vcltzq_v((int8x16_t)__p0, 49);
  return __ret;
}
#else
__ai uint16x8_t vcltzq_s16(int16x8_t __p0) {
  int16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint16x8_t __ret;
  __ret = (uint16x8_t) __builtin_neon_vcltzq_v((int8x16_t)__rev0, 49);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint8x8_t vcltz_s8(int8x8_t __p0) {
  uint8x8_t __ret;
  __ret = (uint8x8_t) __builtin_neon_vcltz_v((int8x8_t)__p0, 16);
  return __ret;
}
#else
__ai uint8x8_t vcltz_s8(int8x8_t __p0) {
  int8x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  uint8x8_t __ret;
  __ret = (uint8x8_t) __builtin_neon_vcltz_v((int8x8_t)__rev0, 16);
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x1_t vcltz_f64(float64x1_t __p0) {
  uint64x1_t __ret;
  __ret = (uint64x1_t) __builtin_neon_vcltz_v((int8x8_t)__p0, 19);
  return __ret;
}
#else
__ai uint64x1_t vcltz_f64(float64x1_t __p0) {
  uint64x1_t __ret;
  __ret = (uint64x1_t) __builtin_neon_vcltz_v((int8x8_t)__p0, 19);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x2_t vcltz_f32(float32x2_t __p0) {
  uint32x2_t __ret;
  __ret = (uint32x2_t) __builtin_neon_vcltz_v((int8x8_t)__p0, 18);
  return __ret;
}
#else
__ai uint32x2_t vcltz_f32(float32x2_t __p0) {
  float32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint32x2_t __ret;
  __ret = (uint32x2_t) __builtin_neon_vcltz_v((int8x8_t)__rev0, 18);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32x2_t vcltz_s32(int32x2_t __p0) {
  uint32x2_t __ret;
  __ret = (uint32x2_t) __builtin_neon_vcltz_v((int8x8_t)__p0, 18);
  return __ret;
}
#else
__ai uint32x2_t vcltz_s32(int32x2_t __p0) {
  int32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint32x2_t __ret;
  __ret = (uint32x2_t) __builtin_neon_vcltz_v((int8x8_t)__rev0, 18);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x1_t vcltz_s64(int64x1_t __p0) {
  uint64x1_t __ret;
  __ret = (uint64x1_t) __builtin_neon_vcltz_v((int8x8_t)__p0, 19);
  return __ret;
}
#else
__ai uint64x1_t vcltz_s64(int64x1_t __p0) {
  uint64x1_t __ret;
  __ret = (uint64x1_t) __builtin_neon_vcltz_v((int8x8_t)__p0, 19);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint16x4_t vcltz_s16(int16x4_t __p0) {
  uint16x4_t __ret;
  __ret = (uint16x4_t) __builtin_neon_vcltz_v((int8x8_t)__p0, 17);
  return __ret;
}
#else
__ai uint16x4_t vcltz_s16(int16x4_t __p0) {
  int16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  uint16x4_t __ret;
  __ret = (uint16x4_t) __builtin_neon_vcltz_v((int8x8_t)__rev0, 17);
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64_t vcltzd_s64(int64_t __p0) {
  int64_t __ret;
  __ret = (int64_t) __builtin_neon_vcltzd_s64(__p0);
  return __ret;
}
#else
__ai int64_t vcltzd_s64(int64_t __p0) {
  int64_t __ret;
  __ret = (int64_t) __builtin_neon_vcltzd_s64(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64_t vcltzd_f64(float64_t __p0) {
  uint64_t __ret;
  __ret = (uint64_t) __builtin_neon_vcltzd_f64(__p0);
  return __ret;
}
#else
__ai uint64_t vcltzd_f64(float64_t __p0) {
  uint64_t __ret;
  __ret = (uint64_t) __builtin_neon_vcltzd_f64(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32_t vcltzs_f32(float32_t __p0) {
  uint32_t __ret;
  __ret = (uint32_t) __builtin_neon_vcltzs_f32(__p0);
  return __ret;
}
#else
__ai uint32_t vcltzs_f32(float32_t __p0) {
  uint32_t __ret;
  __ret = (uint32_t) __builtin_neon_vcltzs_f32(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly64x2_t vcombine_p64(poly64x1_t __p0, poly64x1_t __p1) {
  poly64x2_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 0, 1);
  return __ret;
}
#else
__ai poly64x2_t vcombine_p64(poly64x1_t __p0, poly64x1_t __p1) {
  poly64x2_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 0, 1);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x2_t vcombine_f64(float64x1_t __p0, float64x1_t __p1) {
  float64x2_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 0, 1);
  return __ret;
}
#else
__ai float64x2_t vcombine_f64(float64x1_t __p0, float64x1_t __p1) {
  float64x2_t __ret;
  __ret = __builtin_shufflevector(__p0, __p1, 0, 1);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
#define vcopyq_lane_p8(__p0_0, __p1_0, __p2_0, __p3_0) __extension__ ({ \
  poly8x16_t __s0_0 = __p0_0; \
  poly8x8_t __s2_0 = __p2_0; \
  poly8x16_t __ret_0; \
  __ret_0 = vsetq_lane_p8(vget_lane_p8(__s2_0, __p3_0), __s0_0, __p1_0); \
  __ret_0; \
})
#else
#define vcopyq_lane_p8(__p0_1, __p1_1, __p2_1, __p3_1) __extension__ ({ \
  poly8x16_t __s0_1 = __p0_1; \
  poly8x8_t __s2_1 = __p2_1; \
  poly8x16_t __rev0_1;  __rev0_1 = __builtin_shufflevector(__s0_1, __s0_1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  poly8x8_t __rev2_1;  __rev2_1 = __builtin_shufflevector(__s2_1, __s2_1, 7, 6, 5, 4, 3, 2, 1, 0); \
  poly8x16_t __ret_1; \
  __ret_1 = __noswap_vsetq_lane_p8(__noswap_vget_lane_p8(__rev2_1, __p3_1), __rev0_1, __p1_1); \
  __ret_1 = __builtin_shufflevector(__ret_1, __ret_1, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret_1; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vcopyq_lane_p16(__p0_2, __p1_2, __p2_2, __p3_2) __extension__ ({ \
  poly16x8_t __s0_2 = __p0_2; \
  poly16x4_t __s2_2 = __p2_2; \
  poly16x8_t __ret_2; \
  __ret_2 = vsetq_lane_p16(vget_lane_p16(__s2_2, __p3_2), __s0_2, __p1_2); \
  __ret_2; \
})
#else
#define vcopyq_lane_p16(__p0_3, __p1_3, __p2_3, __p3_3) __extension__ ({ \
  poly16x8_t __s0_3 = __p0_3; \
  poly16x4_t __s2_3 = __p2_3; \
  poly16x8_t __rev0_3;  __rev0_3 = __builtin_shufflevector(__s0_3, __s0_3, 7, 6, 5, 4, 3, 2, 1, 0); \
  poly16x4_t __rev2_3;  __rev2_3 = __builtin_shufflevector(__s2_3, __s2_3, 3, 2, 1, 0); \
  poly16x8_t __ret_3; \
  __ret_3 = __noswap_vsetq_lane_p16(__noswap_vget_lane_p16(__rev2_3, __p3_3), __rev0_3, __p1_3); \
  __ret_3 = __builtin_shufflevector(__ret_3, __ret_3, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret_3; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vcopyq_lane_u8(__p0_4, __p1_4, __p2_4, __p3_4) __extension__ ({ \
  uint8x16_t __s0_4 = __p0_4; \
  uint8x8_t __s2_4 = __p2_4; \
  uint8x16_t __ret_4; \
  __ret_4 = vsetq_lane_u8(vget_lane_u8(__s2_4, __p3_4), __s0_4, __p1_4); \
  __ret_4; \
})
#else
#define vcopyq_lane_u8(__p0_5, __p1_5, __p2_5, __p3_5) __extension__ ({ \
  uint8x16_t __s0_5 = __p0_5; \
  uint8x8_t __s2_5 = __p2_5; \
  uint8x16_t __rev0_5;  __rev0_5 = __builtin_shufflevector(__s0_5, __s0_5, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint8x8_t __rev2_5;  __rev2_5 = __builtin_shufflevector(__s2_5, __s2_5, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint8x16_t __ret_5; \
  __ret_5 = __noswap_vsetq_lane_u8(__noswap_vget_lane_u8(__rev2_5, __p3_5), __rev0_5, __p1_5); \
  __ret_5 = __builtin_shufflevector(__ret_5, __ret_5, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret_5; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vcopyq_lane_u32(__p0_6, __p1_6, __p2_6, __p3_6) __extension__ ({ \
  uint32x4_t __s0_6 = __p0_6; \
  uint32x2_t __s2_6 = __p2_6; \
  uint32x4_t __ret_6; \
  __ret_6 = vsetq_lane_u32(vget_lane_u32(__s2_6, __p3_6), __s0_6, __p1_6); \
  __ret_6; \
})
#else
#define vcopyq_lane_u32(__p0_7, __p1_7, __p2_7, __p3_7) __extension__ ({ \
  uint32x4_t __s0_7 = __p0_7; \
  uint32x2_t __s2_7 = __p2_7; \
  uint32x4_t __rev0_7;  __rev0_7 = __builtin_shufflevector(__s0_7, __s0_7, 3, 2, 1, 0); \
  uint32x2_t __rev2_7;  __rev2_7 = __builtin_shufflevector(__s2_7, __s2_7, 1, 0); \
  uint32x4_t __ret_7; \
  __ret_7 = __noswap_vsetq_lane_u32(__noswap_vget_lane_u32(__rev2_7, __p3_7), __rev0_7, __p1_7); \
  __ret_7 = __builtin_shufflevector(__ret_7, __ret_7, 3, 2, 1, 0); \
  __ret_7; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vcopyq_lane_u64(__p0_8, __p1_8, __p2_8, __p3_8) __extension__ ({ \
  uint64x2_t __s0_8 = __p0_8; \
  uint64x1_t __s2_8 = __p2_8; \
  uint64x2_t __ret_8; \
  __ret_8 = vsetq_lane_u64(vget_lane_u64(__s2_8, __p3_8), __s0_8, __p1_8); \
  __ret_8; \
})
#else
#define vcopyq_lane_u64(__p0_9, __p1_9, __p2_9, __p3_9) __extension__ ({ \
  uint64x2_t __s0_9 = __p0_9; \
  uint64x1_t __s2_9 = __p2_9; \
  uint64x2_t __rev0_9;  __rev0_9 = __builtin_shufflevector(__s0_9, __s0_9, 1, 0); \
  uint64x2_t __ret_9; \
  __ret_9 = __noswap_vsetq_lane_u64(__noswap_vget_lane_u64(__s2_9, __p3_9), __rev0_9, __p1_9); \
  __ret_9 = __builtin_shufflevector(__ret_9, __ret_9, 1, 0); \
  __ret_9; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vcopyq_lane_u16(__p0_10, __p1_10, __p2_10, __p3_10) __extension__ ({ \
  uint16x8_t __s0_10 = __p0_10; \
  uint16x4_t __s2_10 = __p2_10; \
  uint16x8_t __ret_10; \
  __ret_10 = vsetq_lane_u16(vget_lane_u16(__s2_10, __p3_10), __s0_10, __p1_10); \
  __ret_10; \
})
#else
#define vcopyq_lane_u16(__p0_11, __p1_11, __p2_11, __p3_11) __extension__ ({ \
  uint16x8_t __s0_11 = __p0_11; \
  uint16x4_t __s2_11 = __p2_11; \
  uint16x8_t __rev0_11;  __rev0_11 = __builtin_shufflevector(__s0_11, __s0_11, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint16x4_t __rev2_11;  __rev2_11 = __builtin_shufflevector(__s2_11, __s2_11, 3, 2, 1, 0); \
  uint16x8_t __ret_11; \
  __ret_11 = __noswap_vsetq_lane_u16(__noswap_vget_lane_u16(__rev2_11, __p3_11), __rev0_11, __p1_11); \
  __ret_11 = __builtin_shufflevector(__ret_11, __ret_11, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret_11; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vcopyq_lane_s8(__p0_12, __p1_12, __p2_12, __p3_12) __extension__ ({ \
  int8x16_t __s0_12 = __p0_12; \
  int8x8_t __s2_12 = __p2_12; \
  int8x16_t __ret_12; \
  __ret_12 = vsetq_lane_s8(vget_lane_s8(__s2_12, __p3_12), __s0_12, __p1_12); \
  __ret_12; \
})
#else
#define vcopyq_lane_s8(__p0_13, __p1_13, __p2_13, __p3_13) __extension__ ({ \
  int8x16_t __s0_13 = __p0_13; \
  int8x8_t __s2_13 = __p2_13; \
  int8x16_t __rev0_13;  __rev0_13 = __builtin_shufflevector(__s0_13, __s0_13, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  int8x8_t __rev2_13;  __rev2_13 = __builtin_shufflevector(__s2_13, __s2_13, 7, 6, 5, 4, 3, 2, 1, 0); \
  int8x16_t __ret_13; \
  __ret_13 = __noswap_vsetq_lane_s8(__noswap_vget_lane_s8(__rev2_13, __p3_13), __rev0_13, __p1_13); \
  __ret_13 = __builtin_shufflevector(__ret_13, __ret_13, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret_13; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vcopyq_lane_f32(__p0_14, __p1_14, __p2_14, __p3_14) __extension__ ({ \
  float32x4_t __s0_14 = __p0_14; \
  float32x2_t __s2_14 = __p2_14; \
  float32x4_t __ret_14; \
  __ret_14 = vsetq_lane_f32(vget_lane_f32(__s2_14, __p3_14), __s0_14, __p1_14); \
  __ret_14; \
})
#else
#define vcopyq_lane_f32(__p0_15, __p1_15, __p2_15, __p3_15) __extension__ ({ \
  float32x4_t __s0_15 = __p0_15; \
  float32x2_t __s2_15 = __p2_15; \
  float32x4_t __rev0_15;  __rev0_15 = __builtin_shufflevector(__s0_15, __s0_15, 3, 2, 1, 0); \
  float32x2_t __rev2_15;  __rev2_15 = __builtin_shufflevector(__s2_15, __s2_15, 1, 0); \
  float32x4_t __ret_15; \
  __ret_15 = __noswap_vsetq_lane_f32(__noswap_vget_lane_f32(__rev2_15, __p3_15), __rev0_15, __p1_15); \
  __ret_15 = __builtin_shufflevector(__ret_15, __ret_15, 3, 2, 1, 0); \
  __ret_15; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vcopyq_lane_s32(__p0_16, __p1_16, __p2_16, __p3_16) __extension__ ({ \
  int32x4_t __s0_16 = __p0_16; \
  int32x2_t __s2_16 = __p2_16; \
  int32x4_t __ret_16; \
  __ret_16 = vsetq_lane_s32(vget_lane_s32(__s2_16, __p3_16), __s0_16, __p1_16); \
  __ret_16; \
})
#else
#define vcopyq_lane_s32(__p0_17, __p1_17, __p2_17, __p3_17) __extension__ ({ \
  int32x4_t __s0_17 = __p0_17; \
  int32x2_t __s2_17 = __p2_17; \
  int32x4_t __rev0_17;  __rev0_17 = __builtin_shufflevector(__s0_17, __s0_17, 3, 2, 1, 0); \
  int32x2_t __rev2_17;  __rev2_17 = __builtin_shufflevector(__s2_17, __s2_17, 1, 0); \
  int32x4_t __ret_17; \
  __ret_17 = __noswap_vsetq_lane_s32(__noswap_vget_lane_s32(__rev2_17, __p3_17), __rev0_17, __p1_17); \
  __ret_17 = __builtin_shufflevector(__ret_17, __ret_17, 3, 2, 1, 0); \
  __ret_17; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vcopyq_lane_s64(__p0_18, __p1_18, __p2_18, __p3_18) __extension__ ({ \
  int64x2_t __s0_18 = __p0_18; \
  int64x1_t __s2_18 = __p2_18; \
  int64x2_t __ret_18; \
  __ret_18 = vsetq_lane_s64(vget_lane_s64(__s2_18, __p3_18), __s0_18, __p1_18); \
  __ret_18; \
})
#else
#define vcopyq_lane_s64(__p0_19, __p1_19, __p2_19, __p3_19) __extension__ ({ \
  int64x2_t __s0_19 = __p0_19; \
  int64x1_t __s2_19 = __p2_19; \
  int64x2_t __rev0_19;  __rev0_19 = __builtin_shufflevector(__s0_19, __s0_19, 1, 0); \
  int64x2_t __ret_19; \
  __ret_19 = __noswap_vsetq_lane_s64(__noswap_vget_lane_s64(__s2_19, __p3_19), __rev0_19, __p1_19); \
  __ret_19 = __builtin_shufflevector(__ret_19, __ret_19, 1, 0); \
  __ret_19; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vcopyq_lane_s16(__p0_20, __p1_20, __p2_20, __p3_20) __extension__ ({ \
  int16x8_t __s0_20 = __p0_20; \
  int16x4_t __s2_20 = __p2_20; \
  int16x8_t __ret_20; \
  __ret_20 = vsetq_lane_s16(vget_lane_s16(__s2_20, __p3_20), __s0_20, __p1_20); \
  __ret_20; \
})
#else
#define vcopyq_lane_s16(__p0_21, __p1_21, __p2_21, __p3_21) __extension__ ({ \
  int16x8_t __s0_21 = __p0_21; \
  int16x4_t __s2_21 = __p2_21; \
  int16x8_t __rev0_21;  __rev0_21 = __builtin_shufflevector(__s0_21, __s0_21, 7, 6, 5, 4, 3, 2, 1, 0); \
  int16x4_t __rev2_21;  __rev2_21 = __builtin_shufflevector(__s2_21, __s2_21, 3, 2, 1, 0); \
  int16x8_t __ret_21; \
  __ret_21 = __noswap_vsetq_lane_s16(__noswap_vget_lane_s16(__rev2_21, __p3_21), __rev0_21, __p1_21); \
  __ret_21 = __builtin_shufflevector(__ret_21, __ret_21, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret_21; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vcopy_lane_p8(__p0_22, __p1_22, __p2_22, __p3_22) __extension__ ({ \
  poly8x8_t __s0_22 = __p0_22; \
  poly8x8_t __s2_22 = __p2_22; \
  poly8x8_t __ret_22; \
  __ret_22 = vset_lane_p8(vget_lane_p8(__s2_22, __p3_22), __s0_22, __p1_22); \
  __ret_22; \
})
#else
#define vcopy_lane_p8(__p0_23, __p1_23, __p2_23, __p3_23) __extension__ ({ \
  poly8x8_t __s0_23 = __p0_23; \
  poly8x8_t __s2_23 = __p2_23; \
  poly8x8_t __rev0_23;  __rev0_23 = __builtin_shufflevector(__s0_23, __s0_23, 7, 6, 5, 4, 3, 2, 1, 0); \
  poly8x8_t __rev2_23;  __rev2_23 = __builtin_shufflevector(__s2_23, __s2_23, 7, 6, 5, 4, 3, 2, 1, 0); \
  poly8x8_t __ret_23; \
  __ret_23 = __noswap_vset_lane_p8(__noswap_vget_lane_p8(__rev2_23, __p3_23), __rev0_23, __p1_23); \
  __ret_23 = __builtin_shufflevector(__ret_23, __ret_23, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret_23; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vcopy_lane_p16(__p0_24, __p1_24, __p2_24, __p3_24) __extension__ ({ \
  poly16x4_t __s0_24 = __p0_24; \
  poly16x4_t __s2_24 = __p2_24; \
  poly16x4_t __ret_24; \
  __ret_24 = vset_lane_p16(vget_lane_p16(__s2_24, __p3_24), __s0_24, __p1_24); \
  __ret_24; \
})
#else
#define vcopy_lane_p16(__p0_25, __p1_25, __p2_25, __p3_25) __extension__ ({ \
  poly16x4_t __s0_25 = __p0_25; \
  poly16x4_t __s2_25 = __p2_25; \
  poly16x4_t __rev0_25;  __rev0_25 = __builtin_shufflevector(__s0_25, __s0_25, 3, 2, 1, 0); \
  poly16x4_t __rev2_25;  __rev2_25 = __builtin_shufflevector(__s2_25, __s2_25, 3, 2, 1, 0); \
  poly16x4_t __ret_25; \
  __ret_25 = __noswap_vset_lane_p16(__noswap_vget_lane_p16(__rev2_25, __p3_25), __rev0_25, __p1_25); \
  __ret_25 = __builtin_shufflevector(__ret_25, __ret_25, 3, 2, 1, 0); \
  __ret_25; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vcopy_lane_u8(__p0_26, __p1_26, __p2_26, __p3_26) __extension__ ({ \
  uint8x8_t __s0_26 = __p0_26; \
  uint8x8_t __s2_26 = __p2_26; \
  uint8x8_t __ret_26; \
  __ret_26 = vset_lane_u8(vget_lane_u8(__s2_26, __p3_26), __s0_26, __p1_26); \
  __ret_26; \
})
#else
#define vcopy_lane_u8(__p0_27, __p1_27, __p2_27, __p3_27) __extension__ ({ \
  uint8x8_t __s0_27 = __p0_27; \
  uint8x8_t __s2_27 = __p2_27; \
  uint8x8_t __rev0_27;  __rev0_27 = __builtin_shufflevector(__s0_27, __s0_27, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint8x8_t __rev2_27;  __rev2_27 = __builtin_shufflevector(__s2_27, __s2_27, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint8x8_t __ret_27; \
  __ret_27 = __noswap_vset_lane_u8(__noswap_vget_lane_u8(__rev2_27, __p3_27), __rev0_27, __p1_27); \
  __ret_27 = __builtin_shufflevector(__ret_27, __ret_27, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret_27; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vcopy_lane_u32(__p0_28, __p1_28, __p2_28, __p3_28) __extension__ ({ \
  uint32x2_t __s0_28 = __p0_28; \
  uint32x2_t __s2_28 = __p2_28; \
  uint32x2_t __ret_28; \
  __ret_28 = vset_lane_u32(vget_lane_u32(__s2_28, __p3_28), __s0_28, __p1_28); \
  __ret_28; \
})
#else
#define vcopy_lane_u32(__p0_29, __p1_29, __p2_29, __p3_29) __extension__ ({ \
  uint32x2_t __s0_29 = __p0_29; \
  uint32x2_t __s2_29 = __p2_29; \
  uint32x2_t __rev0_29;  __rev0_29 = __builtin_shufflevector(__s0_29, __s0_29, 1, 0); \
  uint32x2_t __rev2_29;  __rev2_29 = __builtin_shufflevector(__s2_29, __s2_29, 1, 0); \
  uint32x2_t __ret_29; \
  __ret_29 = __noswap_vset_lane_u32(__noswap_vget_lane_u32(__rev2_29, __p3_29), __rev0_29, __p1_29); \
  __ret_29 = __builtin_shufflevector(__ret_29, __ret_29, 1, 0); \
  __ret_29; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vcopy_lane_u64(__p0_30, __p1_30, __p2_30, __p3_30) __extension__ ({ \
  uint64x1_t __s0_30 = __p0_30; \
  uint64x1_t __s2_30 = __p2_30; \
  uint64x1_t __ret_30; \
  __ret_30 = vset_lane_u64(vget_lane_u64(__s2_30, __p3_30), __s0_30, __p1_30); \
  __ret_30; \
})
#else
#define vcopy_lane_u64(__p0_31, __p1_31, __p2_31, __p3_31) __extension__ ({ \
  uint64x1_t __s0_31 = __p0_31; \
  uint64x1_t __s2_31 = __p2_31; \
  uint64x1_t __ret_31; \
  __ret_31 = __noswap_vset_lane_u64(__noswap_vget_lane_u64(__s2_31, __p3_31), __s0_31, __p1_31); \
  __ret_31; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vcopy_lane_u16(__p0_32, __p1_32, __p2_32, __p3_32) __extension__ ({ \
  uint16x4_t __s0_32 = __p0_32; \
  uint16x4_t __s2_32 = __p2_32; \
  uint16x4_t __ret_32; \
  __ret_32 = vset_lane_u16(vget_lane_u16(__s2_32, __p3_32), __s0_32, __p1_32); \
  __ret_32; \
})
#else
#define vcopy_lane_u16(__p0_33, __p1_33, __p2_33, __p3_33) __extension__ ({ \
  uint16x4_t __s0_33 = __p0_33; \
  uint16x4_t __s2_33 = __p2_33; \
  uint16x4_t __rev0_33;  __rev0_33 = __builtin_shufflevector(__s0_33, __s0_33, 3, 2, 1, 0); \
  uint16x4_t __rev2_33;  __rev2_33 = __builtin_shufflevector(__s2_33, __s2_33, 3, 2, 1, 0); \
  uint16x4_t __ret_33; \
  __ret_33 = __noswap_vset_lane_u16(__noswap_vget_lane_u16(__rev2_33, __p3_33), __rev0_33, __p1_33); \
  __ret_33 = __builtin_shufflevector(__ret_33, __ret_33, 3, 2, 1, 0); \
  __ret_33; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vcopy_lane_s8(__p0_34, __p1_34, __p2_34, __p3_34) __extension__ ({ \
  int8x8_t __s0_34 = __p0_34; \
  int8x8_t __s2_34 = __p2_34; \
  int8x8_t __ret_34; \
  __ret_34 = vset_lane_s8(vget_lane_s8(__s2_34, __p3_34), __s0_34, __p1_34); \
  __ret_34; \
})
#else
#define vcopy_lane_s8(__p0_35, __p1_35, __p2_35, __p3_35) __extension__ ({ \
  int8x8_t __s0_35 = __p0_35; \
  int8x8_t __s2_35 = __p2_35; \
  int8x8_t __rev0_35;  __rev0_35 = __builtin_shufflevector(__s0_35, __s0_35, 7, 6, 5, 4, 3, 2, 1, 0); \
  int8x8_t __rev2_35;  __rev2_35 = __builtin_shufflevector(__s2_35, __s2_35, 7, 6, 5, 4, 3, 2, 1, 0); \
  int8x8_t __ret_35; \
  __ret_35 = __noswap_vset_lane_s8(__noswap_vget_lane_s8(__rev2_35, __p3_35), __rev0_35, __p1_35); \
  __ret_35 = __builtin_shufflevector(__ret_35, __ret_35, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret_35; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vcopy_lane_f32(__p0_36, __p1_36, __p2_36, __p3_36) __extension__ ({ \
  float32x2_t __s0_36 = __p0_36; \
  float32x2_t __s2_36 = __p2_36; \
  float32x2_t __ret_36; \
  __ret_36 = vset_lane_f32(vget_lane_f32(__s2_36, __p3_36), __s0_36, __p1_36); \
  __ret_36; \
})
#else
#define vcopy_lane_f32(__p0_37, __p1_37, __p2_37, __p3_37) __extension__ ({ \
  float32x2_t __s0_37 = __p0_37; \
  float32x2_t __s2_37 = __p2_37; \
  float32x2_t __rev0_37;  __rev0_37 = __builtin_shufflevector(__s0_37, __s0_37, 1, 0); \
  float32x2_t __rev2_37;  __rev2_37 = __builtin_shufflevector(__s2_37, __s2_37, 1, 0); \
  float32x2_t __ret_37; \
  __ret_37 = __noswap_vset_lane_f32(__noswap_vget_lane_f32(__rev2_37, __p3_37), __rev0_37, __p1_37); \
  __ret_37 = __builtin_shufflevector(__ret_37, __ret_37, 1, 0); \
  __ret_37; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vcopy_lane_s32(__p0_38, __p1_38, __p2_38, __p3_38) __extension__ ({ \
  int32x2_t __s0_38 = __p0_38; \
  int32x2_t __s2_38 = __p2_38; \
  int32x2_t __ret_38; \
  __ret_38 = vset_lane_s32(vget_lane_s32(__s2_38, __p3_38), __s0_38, __p1_38); \
  __ret_38; \
})
#else
#define vcopy_lane_s32(__p0_39, __p1_39, __p2_39, __p3_39) __extension__ ({ \
  int32x2_t __s0_39 = __p0_39; \
  int32x2_t __s2_39 = __p2_39; \
  int32x2_t __rev0_39;  __rev0_39 = __builtin_shufflevector(__s0_39, __s0_39, 1, 0); \
  int32x2_t __rev2_39;  __rev2_39 = __builtin_shufflevector(__s2_39, __s2_39, 1, 0); \
  int32x2_t __ret_39; \
  __ret_39 = __noswap_vset_lane_s32(__noswap_vget_lane_s32(__rev2_39, __p3_39), __rev0_39, __p1_39); \
  __ret_39 = __builtin_shufflevector(__ret_39, __ret_39, 1, 0); \
  __ret_39; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vcopy_lane_s64(__p0_40, __p1_40, __p2_40, __p3_40) __extension__ ({ \
  int64x1_t __s0_40 = __p0_40; \
  int64x1_t __s2_40 = __p2_40; \
  int64x1_t __ret_40; \
  __ret_40 = vset_lane_s64(vget_lane_s64(__s2_40, __p3_40), __s0_40, __p1_40); \
  __ret_40; \
})
#else
#define vcopy_lane_s64(__p0_41, __p1_41, __p2_41, __p3_41) __extension__ ({ \
  int64x1_t __s0_41 = __p0_41; \
  int64x1_t __s2_41 = __p2_41; \
  int64x1_t __ret_41; \
  __ret_41 = __noswap_vset_lane_s64(__noswap_vget_lane_s64(__s2_41, __p3_41), __s0_41, __p1_41); \
  __ret_41; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vcopy_lane_s16(__p0_42, __p1_42, __p2_42, __p3_42) __extension__ ({ \
  int16x4_t __s0_42 = __p0_42; \
  int16x4_t __s2_42 = __p2_42; \
  int16x4_t __ret_42; \
  __ret_42 = vset_lane_s16(vget_lane_s16(__s2_42, __p3_42), __s0_42, __p1_42); \
  __ret_42; \
})
#else
#define vcopy_lane_s16(__p0_43, __p1_43, __p2_43, __p3_43) __extension__ ({ \
  int16x4_t __s0_43 = __p0_43; \
  int16x4_t __s2_43 = __p2_43; \
  int16x4_t __rev0_43;  __rev0_43 = __builtin_shufflevector(__s0_43, __s0_43, 3, 2, 1, 0); \
  int16x4_t __rev2_43;  __rev2_43 = __builtin_shufflevector(__s2_43, __s2_43, 3, 2, 1, 0); \
  int16x4_t __ret_43; \
  __ret_43 = __noswap_vset_lane_s16(__noswap_vget_lane_s16(__rev2_43, __p3_43), __rev0_43, __p1_43); \
  __ret_43 = __builtin_shufflevector(__ret_43, __ret_43, 3, 2, 1, 0); \
  __ret_43; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vcopyq_laneq_p8(__p0_44, __p1_44, __p2_44, __p3_44) __extension__ ({ \
  poly8x16_t __s0_44 = __p0_44; \
  poly8x16_t __s2_44 = __p2_44; \
  poly8x16_t __ret_44; \
  __ret_44 = vsetq_lane_p8(vgetq_lane_p8(__s2_44, __p3_44), __s0_44, __p1_44); \
  __ret_44; \
})
#else
#define vcopyq_laneq_p8(__p0_45, __p1_45, __p2_45, __p3_45) __extension__ ({ \
  poly8x16_t __s0_45 = __p0_45; \
  poly8x16_t __s2_45 = __p2_45; \
  poly8x16_t __rev0_45;  __rev0_45 = __builtin_shufflevector(__s0_45, __s0_45, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  poly8x16_t __rev2_45;  __rev2_45 = __builtin_shufflevector(__s2_45, __s2_45, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  poly8x16_t __ret_45; \
  __ret_45 = __noswap_vsetq_lane_p8(__noswap_vgetq_lane_p8(__rev2_45, __p3_45), __rev0_45, __p1_45); \
  __ret_45 = __builtin_shufflevector(__ret_45, __ret_45, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret_45; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vcopyq_laneq_p16(__p0_46, __p1_46, __p2_46, __p3_46) __extension__ ({ \
  poly16x8_t __s0_46 = __p0_46; \
  poly16x8_t __s2_46 = __p2_46; \
  poly16x8_t __ret_46; \
  __ret_46 = vsetq_lane_p16(vgetq_lane_p16(__s2_46, __p3_46), __s0_46, __p1_46); \
  __ret_46; \
})
#else
#define vcopyq_laneq_p16(__p0_47, __p1_47, __p2_47, __p3_47) __extension__ ({ \
  poly16x8_t __s0_47 = __p0_47; \
  poly16x8_t __s2_47 = __p2_47; \
  poly16x8_t __rev0_47;  __rev0_47 = __builtin_shufflevector(__s0_47, __s0_47, 7, 6, 5, 4, 3, 2, 1, 0); \
  poly16x8_t __rev2_47;  __rev2_47 = __builtin_shufflevector(__s2_47, __s2_47, 7, 6, 5, 4, 3, 2, 1, 0); \
  poly16x8_t __ret_47; \
  __ret_47 = __noswap_vsetq_lane_p16(__noswap_vgetq_lane_p16(__rev2_47, __p3_47), __rev0_47, __p1_47); \
  __ret_47 = __builtin_shufflevector(__ret_47, __ret_47, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret_47; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vcopyq_laneq_u8(__p0_48, __p1_48, __p2_48, __p3_48) __extension__ ({ \
  uint8x16_t __s0_48 = __p0_48; \
  uint8x16_t __s2_48 = __p2_48; \
  uint8x16_t __ret_48; \
  __ret_48 = vsetq_lane_u8(vgetq_lane_u8(__s2_48, __p3_48), __s0_48, __p1_48); \
  __ret_48; \
})
#else
#define vcopyq_laneq_u8(__p0_49, __p1_49, __p2_49, __p3_49) __extension__ ({ \
  uint8x16_t __s0_49 = __p0_49; \
  uint8x16_t __s2_49 = __p2_49; \
  uint8x16_t __rev0_49;  __rev0_49 = __builtin_shufflevector(__s0_49, __s0_49, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint8x16_t __rev2_49;  __rev2_49 = __builtin_shufflevector(__s2_49, __s2_49, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint8x16_t __ret_49; \
  __ret_49 = __noswap_vsetq_lane_u8(__noswap_vgetq_lane_u8(__rev2_49, __p3_49), __rev0_49, __p1_49); \
  __ret_49 = __builtin_shufflevector(__ret_49, __ret_49, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret_49; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vcopyq_laneq_u32(__p0_50, __p1_50, __p2_50, __p3_50) __extension__ ({ \
  uint32x4_t __s0_50 = __p0_50; \
  uint32x4_t __s2_50 = __p2_50; \
  uint32x4_t __ret_50; \
  __ret_50 = vsetq_lane_u32(vgetq_lane_u32(__s2_50, __p3_50), __s0_50, __p1_50); \
  __ret_50; \
})
#else
#define vcopyq_laneq_u32(__p0_51, __p1_51, __p2_51, __p3_51) __extension__ ({ \
  uint32x4_t __s0_51 = __p0_51; \
  uint32x4_t __s2_51 = __p2_51; \
  uint32x4_t __rev0_51;  __rev0_51 = __builtin_shufflevector(__s0_51, __s0_51, 3, 2, 1, 0); \
  uint32x4_t __rev2_51;  __rev2_51 = __builtin_shufflevector(__s2_51, __s2_51, 3, 2, 1, 0); \
  uint32x4_t __ret_51; \
  __ret_51 = __noswap_vsetq_lane_u32(__noswap_vgetq_lane_u32(__rev2_51, __p3_51), __rev0_51, __p1_51); \
  __ret_51 = __builtin_shufflevector(__ret_51, __ret_51, 3, 2, 1, 0); \
  __ret_51; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vcopyq_laneq_u64(__p0_52, __p1_52, __p2_52, __p3_52) __extension__ ({ \
  uint64x2_t __s0_52 = __p0_52; \
  uint64x2_t __s2_52 = __p2_52; \
  uint64x2_t __ret_52; \
  __ret_52 = vsetq_lane_u64(vgetq_lane_u64(__s2_52, __p3_52), __s0_52, __p1_52); \
  __ret_52; \
})
#else
#define vcopyq_laneq_u64(__p0_53, __p1_53, __p2_53, __p3_53) __extension__ ({ \
  uint64x2_t __s0_53 = __p0_53; \
  uint64x2_t __s2_53 = __p2_53; \
  uint64x2_t __rev0_53;  __rev0_53 = __builtin_shufflevector(__s0_53, __s0_53, 1, 0); \
  uint64x2_t __rev2_53;  __rev2_53 = __builtin_shufflevector(__s2_53, __s2_53, 1, 0); \
  uint64x2_t __ret_53; \
  __ret_53 = __noswap_vsetq_lane_u64(__noswap_vgetq_lane_u64(__rev2_53, __p3_53), __rev0_53, __p1_53); \
  __ret_53 = __builtin_shufflevector(__ret_53, __ret_53, 1, 0); \
  __ret_53; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vcopyq_laneq_u16(__p0_54, __p1_54, __p2_54, __p3_54) __extension__ ({ \
  uint16x8_t __s0_54 = __p0_54; \
  uint16x8_t __s2_54 = __p2_54; \
  uint16x8_t __ret_54; \
  __ret_54 = vsetq_lane_u16(vgetq_lane_u16(__s2_54, __p3_54), __s0_54, __p1_54); \
  __ret_54; \
})
#else
#define vcopyq_laneq_u16(__p0_55, __p1_55, __p2_55, __p3_55) __extension__ ({ \
  uint16x8_t __s0_55 = __p0_55; \
  uint16x8_t __s2_55 = __p2_55; \
  uint16x8_t __rev0_55;  __rev0_55 = __builtin_shufflevector(__s0_55, __s0_55, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint16x8_t __rev2_55;  __rev2_55 = __builtin_shufflevector(__s2_55, __s2_55, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint16x8_t __ret_55; \
  __ret_55 = __noswap_vsetq_lane_u16(__noswap_vgetq_lane_u16(__rev2_55, __p3_55), __rev0_55, __p1_55); \
  __ret_55 = __builtin_shufflevector(__ret_55, __ret_55, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret_55; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vcopyq_laneq_s8(__p0_56, __p1_56, __p2_56, __p3_56) __extension__ ({ \
  int8x16_t __s0_56 = __p0_56; \
  int8x16_t __s2_56 = __p2_56; \
  int8x16_t __ret_56; \
  __ret_56 = vsetq_lane_s8(vgetq_lane_s8(__s2_56, __p3_56), __s0_56, __p1_56); \
  __ret_56; \
})
#else
#define vcopyq_laneq_s8(__p0_57, __p1_57, __p2_57, __p3_57) __extension__ ({ \
  int8x16_t __s0_57 = __p0_57; \
  int8x16_t __s2_57 = __p2_57; \
  int8x16_t __rev0_57;  __rev0_57 = __builtin_shufflevector(__s0_57, __s0_57, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  int8x16_t __rev2_57;  __rev2_57 = __builtin_shufflevector(__s2_57, __s2_57, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  int8x16_t __ret_57; \
  __ret_57 = __noswap_vsetq_lane_s8(__noswap_vgetq_lane_s8(__rev2_57, __p3_57), __rev0_57, __p1_57); \
  __ret_57 = __builtin_shufflevector(__ret_57, __ret_57, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret_57; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vcopyq_laneq_f32(__p0_58, __p1_58, __p2_58, __p3_58) __extension__ ({ \
  float32x4_t __s0_58 = __p0_58; \
  float32x4_t __s2_58 = __p2_58; \
  float32x4_t __ret_58; \
  __ret_58 = vsetq_lane_f32(vgetq_lane_f32(__s2_58, __p3_58), __s0_58, __p1_58); \
  __ret_58; \
})
#else
#define vcopyq_laneq_f32(__p0_59, __p1_59, __p2_59, __p3_59) __extension__ ({ \
  float32x4_t __s0_59 = __p0_59; \
  float32x4_t __s2_59 = __p2_59; \
  float32x4_t __rev0_59;  __rev0_59 = __builtin_shufflevector(__s0_59, __s0_59, 3, 2, 1, 0); \
  float32x4_t __rev2_59;  __rev2_59 = __builtin_shufflevector(__s2_59, __s2_59, 3, 2, 1, 0); \
  float32x4_t __ret_59; \
  __ret_59 = __noswap_vsetq_lane_f32(__noswap_vgetq_lane_f32(__rev2_59, __p3_59), __rev0_59, __p1_59); \
  __ret_59 = __builtin_shufflevector(__ret_59, __ret_59, 3, 2, 1, 0); \
  __ret_59; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vcopyq_laneq_s32(__p0_60, __p1_60, __p2_60, __p3_60) __extension__ ({ \
  int32x4_t __s0_60 = __p0_60; \
  int32x4_t __s2_60 = __p2_60; \
  int32x4_t __ret_60; \
  __ret_60 = vsetq_lane_s32(vgetq_lane_s32(__s2_60, __p3_60), __s0_60, __p1_60); \
  __ret_60; \
})
#else
#define vcopyq_laneq_s32(__p0_61, __p1_61, __p2_61, __p3_61) __extension__ ({ \
  int32x4_t __s0_61 = __p0_61; \
  int32x4_t __s2_61 = __p2_61; \
  int32x4_t __rev0_61;  __rev0_61 = __builtin_shufflevector(__s0_61, __s0_61, 3, 2, 1, 0); \
  int32x4_t __rev2_61;  __rev2_61 = __builtin_shufflevector(__s2_61, __s2_61, 3, 2, 1, 0); \
  int32x4_t __ret_61; \
  __ret_61 = __noswap_vsetq_lane_s32(__noswap_vgetq_lane_s32(__rev2_61, __p3_61), __rev0_61, __p1_61); \
  __ret_61 = __builtin_shufflevector(__ret_61, __ret_61, 3, 2, 1, 0); \
  __ret_61; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vcopyq_laneq_s64(__p0_62, __p1_62, __p2_62, __p3_62) __extension__ ({ \
  int64x2_t __s0_62 = __p0_62; \
  int64x2_t __s2_62 = __p2_62; \
  int64x2_t __ret_62; \
  __ret_62 = vsetq_lane_s64(vgetq_lane_s64(__s2_62, __p3_62), __s0_62, __p1_62); \
  __ret_62; \
})
#else
#define vcopyq_laneq_s64(__p0_63, __p1_63, __p2_63, __p3_63) __extension__ ({ \
  int64x2_t __s0_63 = __p0_63; \
  int64x2_t __s2_63 = __p2_63; \
  int64x2_t __rev0_63;  __rev0_63 = __builtin_shufflevector(__s0_63, __s0_63, 1, 0); \
  int64x2_t __rev2_63;  __rev2_63 = __builtin_shufflevector(__s2_63, __s2_63, 1, 0); \
  int64x2_t __ret_63; \
  __ret_63 = __noswap_vsetq_lane_s64(__noswap_vgetq_lane_s64(__rev2_63, __p3_63), __rev0_63, __p1_63); \
  __ret_63 = __builtin_shufflevector(__ret_63, __ret_63, 1, 0); \
  __ret_63; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vcopyq_laneq_s16(__p0_64, __p1_64, __p2_64, __p3_64) __extension__ ({ \
  int16x8_t __s0_64 = __p0_64; \
  int16x8_t __s2_64 = __p2_64; \
  int16x8_t __ret_64; \
  __ret_64 = vsetq_lane_s16(vgetq_lane_s16(__s2_64, __p3_64), __s0_64, __p1_64); \
  __ret_64; \
})
#else
#define vcopyq_laneq_s16(__p0_65, __p1_65, __p2_65, __p3_65) __extension__ ({ \
  int16x8_t __s0_65 = __p0_65; \
  int16x8_t __s2_65 = __p2_65; \
  int16x8_t __rev0_65;  __rev0_65 = __builtin_shufflevector(__s0_65, __s0_65, 7, 6, 5, 4, 3, 2, 1, 0); \
  int16x8_t __rev2_65;  __rev2_65 = __builtin_shufflevector(__s2_65, __s2_65, 7, 6, 5, 4, 3, 2, 1, 0); \
  int16x8_t __ret_65; \
  __ret_65 = __noswap_vsetq_lane_s16(__noswap_vgetq_lane_s16(__rev2_65, __p3_65), __rev0_65, __p1_65); \
  __ret_65 = __builtin_shufflevector(__ret_65, __ret_65, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret_65; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vcopy_laneq_p8(__p0_66, __p1_66, __p2_66, __p3_66) __extension__ ({ \
  poly8x8_t __s0_66 = __p0_66; \
  poly8x16_t __s2_66 = __p2_66; \
  poly8x8_t __ret_66; \
  __ret_66 = vset_lane_p8(vgetq_lane_p8(__s2_66, __p3_66), __s0_66, __p1_66); \
  __ret_66; \
})
#else
#define vcopy_laneq_p8(__p0_67, __p1_67, __p2_67, __p3_67) __extension__ ({ \
  poly8x8_t __s0_67 = __p0_67; \
  poly8x16_t __s2_67 = __p2_67; \
  poly8x8_t __rev0_67;  __rev0_67 = __builtin_shufflevector(__s0_67, __s0_67, 7, 6, 5, 4, 3, 2, 1, 0); \
  poly8x16_t __rev2_67;  __rev2_67 = __builtin_shufflevector(__s2_67, __s2_67, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  poly8x8_t __ret_67; \
  __ret_67 = __noswap_vset_lane_p8(__noswap_vgetq_lane_p8(__rev2_67, __p3_67), __rev0_67, __p1_67); \
  __ret_67 = __builtin_shufflevector(__ret_67, __ret_67, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret_67; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vcopy_laneq_p16(__p0_68, __p1_68, __p2_68, __p3_68) __extension__ ({ \
  poly16x4_t __s0_68 = __p0_68; \
  poly16x8_t __s2_68 = __p2_68; \
  poly16x4_t __ret_68; \
  __ret_68 = vset_lane_p16(vgetq_lane_p16(__s2_68, __p3_68), __s0_68, __p1_68); \
  __ret_68; \
})
#else
#define vcopy_laneq_p16(__p0_69, __p1_69, __p2_69, __p3_69) __extension__ ({ \
  poly16x4_t __s0_69 = __p0_69; \
  poly16x8_t __s2_69 = __p2_69; \
  poly16x4_t __rev0_69;  __rev0_69 = __builtin_shufflevector(__s0_69, __s0_69, 3, 2, 1, 0); \
  poly16x8_t __rev2_69;  __rev2_69 = __builtin_shufflevector(__s2_69, __s2_69, 7, 6, 5, 4, 3, 2, 1, 0); \
  poly16x4_t __ret_69; \
  __ret_69 = __noswap_vset_lane_p16(__noswap_vgetq_lane_p16(__rev2_69, __p3_69), __rev0_69, __p1_69); \
  __ret_69 = __builtin_shufflevector(__ret_69, __ret_69, 3, 2, 1, 0); \
  __ret_69; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vcopy_laneq_u8(__p0_70, __p1_70, __p2_70, __p3_70) __extension__ ({ \
  uint8x8_t __s0_70 = __p0_70; \
  uint8x16_t __s2_70 = __p2_70; \
  uint8x8_t __ret_70; \
  __ret_70 = vset_lane_u8(vgetq_lane_u8(__s2_70, __p3_70), __s0_70, __p1_70); \
  __ret_70; \
})
#else
#define vcopy_laneq_u8(__p0_71, __p1_71, __p2_71, __p3_71) __extension__ ({ \
  uint8x8_t __s0_71 = __p0_71; \
  uint8x16_t __s2_71 = __p2_71; \
  uint8x8_t __rev0_71;  __rev0_71 = __builtin_shufflevector(__s0_71, __s0_71, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint8x16_t __rev2_71;  __rev2_71 = __builtin_shufflevector(__s2_71, __s2_71, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint8x8_t __ret_71; \
  __ret_71 = __noswap_vset_lane_u8(__noswap_vgetq_lane_u8(__rev2_71, __p3_71), __rev0_71, __p1_71); \
  __ret_71 = __builtin_shufflevector(__ret_71, __ret_71, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret_71; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vcopy_laneq_u32(__p0_72, __p1_72, __p2_72, __p3_72) __extension__ ({ \
  uint32x2_t __s0_72 = __p0_72; \
  uint32x4_t __s2_72 = __p2_72; \
  uint32x2_t __ret_72; \
  __ret_72 = vset_lane_u32(vgetq_lane_u32(__s2_72, __p3_72), __s0_72, __p1_72); \
  __ret_72; \
})
#else
#define vcopy_laneq_u32(__p0_73, __p1_73, __p2_73, __p3_73) __extension__ ({ \
  uint32x2_t __s0_73 = __p0_73; \
  uint32x4_t __s2_73 = __p2_73; \
  uint32x2_t __rev0_73;  __rev0_73 = __builtin_shufflevector(__s0_73, __s0_73, 1, 0); \
  uint32x4_t __rev2_73;  __rev2_73 = __builtin_shufflevector(__s2_73, __s2_73, 3, 2, 1, 0); \
  uint32x2_t __ret_73; \
  __ret_73 = __noswap_vset_lane_u32(__noswap_vgetq_lane_u32(__rev2_73, __p3_73), __rev0_73, __p1_73); \
  __ret_73 = __builtin_shufflevector(__ret_73, __ret_73, 1, 0); \
  __ret_73; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vcopy_laneq_u64(__p0_74, __p1_74, __p2_74, __p3_74) __extension__ ({ \
  uint64x1_t __s0_74 = __p0_74; \
  uint64x2_t __s2_74 = __p2_74; \
  uint64x1_t __ret_74; \
  __ret_74 = vset_lane_u64(vgetq_lane_u64(__s2_74, __p3_74), __s0_74, __p1_74); \
  __ret_74; \
})
#else
#define vcopy_laneq_u64(__p0_75, __p1_75, __p2_75, __p3_75) __extension__ ({ \
  uint64x1_t __s0_75 = __p0_75; \
  uint64x2_t __s2_75 = __p2_75; \
  uint64x2_t __rev2_75;  __rev2_75 = __builtin_shufflevector(__s2_75, __s2_75, 1, 0); \
  uint64x1_t __ret_75; \
  __ret_75 = __noswap_vset_lane_u64(__noswap_vgetq_lane_u64(__rev2_75, __p3_75), __s0_75, __p1_75); \
  __ret_75; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vcopy_laneq_u16(__p0_76, __p1_76, __p2_76, __p3_76) __extension__ ({ \
  uint16x4_t __s0_76 = __p0_76; \
  uint16x8_t __s2_76 = __p2_76; \
  uint16x4_t __ret_76; \
  __ret_76 = vset_lane_u16(vgetq_lane_u16(__s2_76, __p3_76), __s0_76, __p1_76); \
  __ret_76; \
})
#else
#define vcopy_laneq_u16(__p0_77, __p1_77, __p2_77, __p3_77) __extension__ ({ \
  uint16x4_t __s0_77 = __p0_77; \
  uint16x8_t __s2_77 = __p2_77; \
  uint16x4_t __rev0_77;  __rev0_77 = __builtin_shufflevector(__s0_77, __s0_77, 3, 2, 1, 0); \
  uint16x8_t __rev2_77;  __rev2_77 = __builtin_shufflevector(__s2_77, __s2_77, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint16x4_t __ret_77; \
  __ret_77 = __noswap_vset_lane_u16(__noswap_vgetq_lane_u16(__rev2_77, __p3_77), __rev0_77, __p1_77); \
  __ret_77 = __builtin_shufflevector(__ret_77, __ret_77, 3, 2, 1, 0); \
  __ret_77; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vcopy_laneq_s8(__p0_78, __p1_78, __p2_78, __p3_78) __extension__ ({ \
  int8x8_t __s0_78 = __p0_78; \
  int8x16_t __s2_78 = __p2_78; \
  int8x8_t __ret_78; \
  __ret_78 = vset_lane_s8(vgetq_lane_s8(__s2_78, __p3_78), __s0_78, __p1_78); \
  __ret_78; \
})
#else
#define vcopy_laneq_s8(__p0_79, __p1_79, __p2_79, __p3_79) __extension__ ({ \
  int8x8_t __s0_79 = __p0_79; \
  int8x16_t __s2_79 = __p2_79; \
  int8x8_t __rev0_79;  __rev0_79 = __builtin_shufflevector(__s0_79, __s0_79, 7, 6, 5, 4, 3, 2, 1, 0); \
  int8x16_t __rev2_79;  __rev2_79 = __builtin_shufflevector(__s2_79, __s2_79, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  int8x8_t __ret_79; \
  __ret_79 = __noswap_vset_lane_s8(__noswap_vgetq_lane_s8(__rev2_79, __p3_79), __rev0_79, __p1_79); \
  __ret_79 = __builtin_shufflevector(__ret_79, __ret_79, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret_79; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vcopy_laneq_f32(__p0_80, __p1_80, __p2_80, __p3_80) __extension__ ({ \
  float32x2_t __s0_80 = __p0_80; \
  float32x4_t __s2_80 = __p2_80; \
  float32x2_t __ret_80; \
  __ret_80 = vset_lane_f32(vgetq_lane_f32(__s2_80, __p3_80), __s0_80, __p1_80); \
  __ret_80; \
})
#else
#define vcopy_laneq_f32(__p0_81, __p1_81, __p2_81, __p3_81) __extension__ ({ \
  float32x2_t __s0_81 = __p0_81; \
  float32x4_t __s2_81 = __p2_81; \
  float32x2_t __rev0_81;  __rev0_81 = __builtin_shufflevector(__s0_81, __s0_81, 1, 0); \
  float32x4_t __rev2_81;  __rev2_81 = __builtin_shufflevector(__s2_81, __s2_81, 3, 2, 1, 0); \
  float32x2_t __ret_81; \
  __ret_81 = __noswap_vset_lane_f32(__noswap_vgetq_lane_f32(__rev2_81, __p3_81), __rev0_81, __p1_81); \
  __ret_81 = __builtin_shufflevector(__ret_81, __ret_81, 1, 0); \
  __ret_81; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vcopy_laneq_s32(__p0_82, __p1_82, __p2_82, __p3_82) __extension__ ({ \
  int32x2_t __s0_82 = __p0_82; \
  int32x4_t __s2_82 = __p2_82; \
  int32x2_t __ret_82; \
  __ret_82 = vset_lane_s32(vgetq_lane_s32(__s2_82, __p3_82), __s0_82, __p1_82); \
  __ret_82; \
})
#else
#define vcopy_laneq_s32(__p0_83, __p1_83, __p2_83, __p3_83) __extension__ ({ \
  int32x2_t __s0_83 = __p0_83; \
  int32x4_t __s2_83 = __p2_83; \
  int32x2_t __rev0_83;  __rev0_83 = __builtin_shufflevector(__s0_83, __s0_83, 1, 0); \
  int32x4_t __rev2_83;  __rev2_83 = __builtin_shufflevector(__s2_83, __s2_83, 3, 2, 1, 0); \
  int32x2_t __ret_83; \
  __ret_83 = __noswap_vset_lane_s32(__noswap_vgetq_lane_s32(__rev2_83, __p3_83), __rev0_83, __p1_83); \
  __ret_83 = __builtin_shufflevector(__ret_83, __ret_83, 1, 0); \
  __ret_83; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vcopy_laneq_s64(__p0_84, __p1_84, __p2_84, __p3_84) __extension__ ({ \
  int64x1_t __s0_84 = __p0_84; \
  int64x2_t __s2_84 = __p2_84; \
  int64x1_t __ret_84; \
  __ret_84 = vset_lane_s64(vgetq_lane_s64(__s2_84, __p3_84), __s0_84, __p1_84); \
  __ret_84; \
})
#else
#define vcopy_laneq_s64(__p0_85, __p1_85, __p2_85, __p3_85) __extension__ ({ \
  int64x1_t __s0_85 = __p0_85; \
  int64x2_t __s2_85 = __p2_85; \
  int64x2_t __rev2_85;  __rev2_85 = __builtin_shufflevector(__s2_85, __s2_85, 1, 0); \
  int64x1_t __ret_85; \
  __ret_85 = __noswap_vset_lane_s64(__noswap_vgetq_lane_s64(__rev2_85, __p3_85), __s0_85, __p1_85); \
  __ret_85; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vcopy_laneq_s16(__p0_86, __p1_86, __p2_86, __p3_86) __extension__ ({ \
  int16x4_t __s0_86 = __p0_86; \
  int16x8_t __s2_86 = __p2_86; \
  int16x4_t __ret_86; \
  __ret_86 = vset_lane_s16(vgetq_lane_s16(__s2_86, __p3_86), __s0_86, __p1_86); \
  __ret_86; \
})
#else
#define vcopy_laneq_s16(__p0_87, __p1_87, __p2_87, __p3_87) __extension__ ({ \
  int16x4_t __s0_87 = __p0_87; \
  int16x8_t __s2_87 = __p2_87; \
  int16x4_t __rev0_87;  __rev0_87 = __builtin_shufflevector(__s0_87, __s0_87, 3, 2, 1, 0); \
  int16x8_t __rev2_87;  __rev2_87 = __builtin_shufflevector(__s2_87, __s2_87, 7, 6, 5, 4, 3, 2, 1, 0); \
  int16x4_t __ret_87; \
  __ret_87 = __noswap_vset_lane_s16(__noswap_vgetq_lane_s16(__rev2_87, __p3_87), __rev0_87, __p1_87); \
  __ret_87 = __builtin_shufflevector(__ret_87, __ret_87, 3, 2, 1, 0); \
  __ret_87; \
})
#endif

#ifdef __LITTLE_ENDIAN__
__ai poly64x1_t vcreate_p64(uint64_t __p0) {
  poly64x1_t __ret;
  __ret = (poly64x1_t)(__p0);
  return __ret;
}
#else
__ai poly64x1_t vcreate_p64(uint64_t __p0) {
  poly64x1_t __ret;
  __ret = (poly64x1_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x1_t vcreate_f64(uint64_t __p0) {
  float64x1_t __ret;
  __ret = (float64x1_t)(__p0);
  return __ret;
}
#else
__ai float64x1_t vcreate_f64(uint64_t __p0) {
  float64x1_t __ret;
  __ret = (float64x1_t)(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32_t vcvts_f32_s32(int32_t __p0) {
  float32_t __ret;
  __ret = (float32_t) __builtin_neon_vcvts_f32_s32(__p0);
  return __ret;
}
#else
__ai float32_t vcvts_f32_s32(int32_t __p0) {
  float32_t __ret;
  __ret = (float32_t) __builtin_neon_vcvts_f32_s32(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32_t vcvts_f32_u32(uint32_t __p0) {
  float32_t __ret;
  __ret = (float32_t) __builtin_neon_vcvts_f32_u32(__p0);
  return __ret;
}
#else
__ai float32_t vcvts_f32_u32(uint32_t __p0) {
  float32_t __ret;
  __ret = (float32_t) __builtin_neon_vcvts_f32_u32(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x2_t vcvt_f32_f64(float64x2_t __p0) {
  float32x2_t __ret;
  __ret = (float32x2_t) __builtin_neon_vcvt_f32_f64((int8x16_t)__p0, 9);
  return __ret;
}
#else
__ai float32x2_t vcvt_f32_f64(float64x2_t __p0) {
  float64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float32x2_t __ret;
  __ret = (float32x2_t) __builtin_neon_vcvt_f32_f64((int8x16_t)__rev0, 9);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
__ai float32x2_t __noswap_vcvt_f32_f64(float64x2_t __p0) {
  float32x2_t __ret;
  __ret = (float32x2_t) __builtin_neon_vcvt_f32_f64((int8x16_t)__p0, 9);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64_t vcvtd_f64_s64(int64_t __p0) {
  float64_t __ret;
  __ret = (float64_t) __builtin_neon_vcvtd_f64_s64(__p0);
  return __ret;
}
#else
__ai float64_t vcvtd_f64_s64(int64_t __p0) {
  float64_t __ret;
  __ret = (float64_t) __builtin_neon_vcvtd_f64_s64(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64_t vcvtd_f64_u64(uint64_t __p0) {
  float64_t __ret;
  __ret = (float64_t) __builtin_neon_vcvtd_f64_u64(__p0);
  return __ret;
}
#else
__ai float64_t vcvtd_f64_u64(uint64_t __p0) {
  float64_t __ret;
  __ret = (float64_t) __builtin_neon_vcvtd_f64_u64(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x2_t vcvtq_f64_u64(uint64x2_t __p0) {
  float64x2_t __ret;
  __ret = (float64x2_t) __builtin_neon_vcvtq_f64_v((int8x16_t)__p0, 51);
  return __ret;
}
#else
__ai float64x2_t vcvtq_f64_u64(uint64x2_t __p0) {
  uint64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float64x2_t __ret;
  __ret = (float64x2_t) __builtin_neon_vcvtq_f64_v((int8x16_t)__rev0, 51);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x2_t vcvtq_f64_s64(int64x2_t __p0) {
  float64x2_t __ret;
  __ret = (float64x2_t) __builtin_neon_vcvtq_f64_v((int8x16_t)__p0, 35);
  return __ret;
}
#else
__ai float64x2_t vcvtq_f64_s64(int64x2_t __p0) {
  int64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float64x2_t __ret;
  __ret = (float64x2_t) __builtin_neon_vcvtq_f64_v((int8x16_t)__rev0, 35);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x1_t vcvt_f64_u64(uint64x1_t __p0) {
  float64x1_t __ret;
  __ret = (float64x1_t) __builtin_neon_vcvt_f64_v((int8x8_t)__p0, 19);
  return __ret;
}
#else
__ai float64x1_t vcvt_f64_u64(uint64x1_t __p0) {
  float64x1_t __ret;
  __ret = (float64x1_t) __builtin_neon_vcvt_f64_v((int8x8_t)__p0, 19);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x1_t vcvt_f64_s64(int64x1_t __p0) {
  float64x1_t __ret;
  __ret = (float64x1_t) __builtin_neon_vcvt_f64_v((int8x8_t)__p0, 3);
  return __ret;
}
#else
__ai float64x1_t vcvt_f64_s64(int64x1_t __p0) {
  float64x1_t __ret;
  __ret = (float64x1_t) __builtin_neon_vcvt_f64_v((int8x8_t)__p0, 3);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x2_t vcvt_f64_f32(float32x2_t __p0) {
  float64x2_t __ret;
  __ret = (float64x2_t) __builtin_neon_vcvt_f64_f32((int8x8_t)__p0, 42);
  return __ret;
}
#else
__ai float64x2_t vcvt_f64_f32(float32x2_t __p0) {
  float32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float64x2_t __ret;
  __ret = (float64x2_t) __builtin_neon_vcvt_f64_f32((int8x8_t)__rev0, 42);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
__ai float64x2_t __noswap_vcvt_f64_f32(float32x2_t __p0) {
  float64x2_t __ret;
  __ret = (float64x2_t) __builtin_neon_vcvt_f64_f32((int8x8_t)__p0, 42);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float16x8_t vcvt_high_f16_f32(float16x4_t __p0, float32x4_t __p1) {
  float16x8_t __ret;
  __ret = vcombine_f16(__p0, vcvt_f16_f32(__p1));
  return __ret;
}
#else
__ai float16x8_t vcvt_high_f16_f32(float16x4_t __p0, float32x4_t __p1) {
  float16x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  float32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  float16x8_t __ret;
  __ret = __noswap_vcombine_f16(__rev0, __noswap_vcvt_f16_f32(__rev1));
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x4_t vcvt_high_f32_f16(float16x8_t __p0) {
  float32x4_t __ret;
  __ret = vcvt_f32_f16(vget_high_f16(__p0));
  return __ret;
}
#else
__ai float32x4_t vcvt_high_f32_f16(float16x8_t __p0) {
  float16x8_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 7, 6, 5, 4, 3, 2, 1, 0);
  float32x4_t __ret;
  __ret = __noswap_vcvt_f32_f16(__noswap_vget_high_f16(__rev0));
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x4_t vcvt_high_f32_f64(float32x2_t __p0, float64x2_t __p1) {
  float32x4_t __ret;
  __ret = vcombine_f32(__p0, vcvt_f32_f64(__p1));
  return __ret;
}
#else
__ai float32x4_t vcvt_high_f32_f64(float32x2_t __p0, float64x2_t __p1) {
  float32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  float32x4_t __ret;
  __ret = __noswap_vcombine_f32(__rev0, __noswap_vcvt_f32_f64(__rev1));
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x2_t vcvt_high_f64_f32(float32x4_t __p0) {
  float64x2_t __ret;
  __ret = vcvt_f64_f32(vget_high_f32(__p0));
  return __ret;
}
#else
__ai float64x2_t vcvt_high_f64_f32(float32x4_t __p0) {
  float32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  float64x2_t __ret;
  __ret = __noswap_vcvt_f64_f32(__noswap_vget_high_f32(__rev0));
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
#define vcvts_n_f32_u32(__p0, __p1) __extension__ ({ \
  uint32_t __s0 = __p0; \
  float32_t __ret; \
  __ret = (float32_t) __builtin_neon_vcvts_n_f32_u32(__s0, __p1); \
  __ret; \
})
#else
#define vcvts_n_f32_u32(__p0, __p1) __extension__ ({ \
  uint32_t __s0 = __p0; \
  float32_t __ret; \
  __ret = (float32_t) __builtin_neon_vcvts_n_f32_u32(__s0, __p1); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vcvts_n_f32_s32(__p0, __p1) __extension__ ({ \
  int32_t __s0 = __p0; \
  float32_t __ret; \
  __ret = (float32_t) __builtin_neon_vcvts_n_f32_s32(__s0, __p1); \
  __ret; \
})
#else
#define vcvts_n_f32_s32(__p0, __p1) __extension__ ({ \
  int32_t __s0 = __p0; \
  float32_t __ret; \
  __ret = (float32_t) __builtin_neon_vcvts_n_f32_s32(__s0, __p1); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vcvtq_n_f64_u64(__p0, __p1) __extension__ ({ \
  uint64x2_t __s0 = __p0; \
  float64x2_t __ret; \
  __ret = (float64x2_t) __builtin_neon_vcvtq_n_f64_v((int8x16_t)__s0, __p1, 51); \
  __ret; \
})
#else
#define vcvtq_n_f64_u64(__p0, __p1) __extension__ ({ \
  uint64x2_t __s0 = __p0; \
  uint64x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  float64x2_t __ret; \
  __ret = (float64x2_t) __builtin_neon_vcvtq_n_f64_v((int8x16_t)__rev0, __p1, 51); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vcvtq_n_f64_s64(__p0, __p1) __extension__ ({ \
  int64x2_t __s0 = __p0; \
  float64x2_t __ret; \
  __ret = (float64x2_t) __builtin_neon_vcvtq_n_f64_v((int8x16_t)__s0, __p1, 35); \
  __ret; \
})
#else
#define vcvtq_n_f64_s64(__p0, __p1) __extension__ ({ \
  int64x2_t __s0 = __p0; \
  int64x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  float64x2_t __ret; \
  __ret = (float64x2_t) __builtin_neon_vcvtq_n_f64_v((int8x16_t)__rev0, __p1, 35); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vcvt_n_f64_u64(__p0, __p1) __extension__ ({ \
  uint64x1_t __s0 = __p0; \
  float64x1_t __ret; \
  __ret = (float64x1_t) __builtin_neon_vcvt_n_f64_v((int8x8_t)__s0, __p1, 19); \
  __ret; \
})
#else
#define vcvt_n_f64_u64(__p0, __p1) __extension__ ({ \
  uint64x1_t __s0 = __p0; \
  float64x1_t __ret; \
  __ret = (float64x1_t) __builtin_neon_vcvt_n_f64_v((int8x8_t)__s0, __p1, 19); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vcvt_n_f64_s64(__p0, __p1) __extension__ ({ \
  int64x1_t __s0 = __p0; \
  float64x1_t __ret; \
  __ret = (float64x1_t) __builtin_neon_vcvt_n_f64_v((int8x8_t)__s0, __p1, 3); \
  __ret; \
})
#else
#define vcvt_n_f64_s64(__p0, __p1) __extension__ ({ \
  int64x1_t __s0 = __p0; \
  float64x1_t __ret; \
  __ret = (float64x1_t) __builtin_neon_vcvt_n_f64_v((int8x8_t)__s0, __p1, 3); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vcvtd_n_f64_u64(__p0, __p1) __extension__ ({ \
  uint64_t __s0 = __p0; \
  float64_t __ret; \
  __ret = (float64_t) __builtin_neon_vcvtd_n_f64_u64(__s0, __p1); \
  __ret; \
})
#else
#define vcvtd_n_f64_u64(__p0, __p1) __extension__ ({ \
  uint64_t __s0 = __p0; \
  float64_t __ret; \
  __ret = (float64_t) __builtin_neon_vcvtd_n_f64_u64(__s0, __p1); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vcvtd_n_f64_s64(__p0, __p1) __extension__ ({ \
  int64_t __s0 = __p0; \
  float64_t __ret; \
  __ret = (float64_t) __builtin_neon_vcvtd_n_f64_s64(__s0, __p1); \
  __ret; \
})
#else
#define vcvtd_n_f64_s64(__p0, __p1) __extension__ ({ \
  int64_t __s0 = __p0; \
  float64_t __ret; \
  __ret = (float64_t) __builtin_neon_vcvtd_n_f64_s64(__s0, __p1); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vcvts_n_s32_f32(__p0, __p1) __extension__ ({ \
  float32_t __s0 = __p0; \
  int32_t __ret; \
  __ret = (int32_t) __builtin_neon_vcvts_n_s32_f32(__s0, __p1); \
  __ret; \
})
#else
#define vcvts_n_s32_f32(__p0, __p1) __extension__ ({ \
  float32_t __s0 = __p0; \
  int32_t __ret; \
  __ret = (int32_t) __builtin_neon_vcvts_n_s32_f32(__s0, __p1); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vcvtq_n_s64_f64(__p0, __p1) __extension__ ({ \
  float64x2_t __s0 = __p0; \
  int64x2_t __ret; \
  __ret = (int64x2_t) __builtin_neon_vcvtq_n_s64_v((int8x16_t)__s0, __p1, 35); \
  __ret; \
})
#else
#define vcvtq_n_s64_f64(__p0, __p1) __extension__ ({ \
  float64x2_t __s0 = __p0; \
  float64x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  int64x2_t __ret; \
  __ret = (int64x2_t) __builtin_neon_vcvtq_n_s64_v((int8x16_t)__rev0, __p1, 35); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vcvt_n_s64_f64(__p0, __p1) __extension__ ({ \
  float64x1_t __s0 = __p0; \
  int64x1_t __ret; \
  __ret = (int64x1_t) __builtin_neon_vcvt_n_s64_v((int8x8_t)__s0, __p1, 3); \
  __ret; \
})
#else
#define vcvt_n_s64_f64(__p0, __p1) __extension__ ({ \
  float64x1_t __s0 = __p0; \
  int64x1_t __ret; \
  __ret = (int64x1_t) __builtin_neon_vcvt_n_s64_v((int8x8_t)__s0, __p1, 3); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vcvtd_n_s64_f64(__p0, __p1) __extension__ ({ \
  float64_t __s0 = __p0; \
  int64_t __ret; \
  __ret = (int64_t) __builtin_neon_vcvtd_n_s64_f64(__s0, __p1); \
  __ret; \
})
#else
#define vcvtd_n_s64_f64(__p0, __p1) __extension__ ({ \
  float64_t __s0 = __p0; \
  int64_t __ret; \
  __ret = (int64_t) __builtin_neon_vcvtd_n_s64_f64(__s0, __p1); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vcvts_n_u32_f32(__p0, __p1) __extension__ ({ \
  float32_t __s0 = __p0; \
  uint32_t __ret; \
  __ret = (uint32_t) __builtin_neon_vcvts_n_u32_f32(__s0, __p1); \
  __ret; \
})
#else
#define vcvts_n_u32_f32(__p0, __p1) __extension__ ({ \
  float32_t __s0 = __p0; \
  uint32_t __ret; \
  __ret = (uint32_t) __builtin_neon_vcvts_n_u32_f32(__s0, __p1); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vcvtq_n_u64_f64(__p0, __p1) __extension__ ({ \
  float64x2_t __s0 = __p0; \
  uint64x2_t __ret; \
  __ret = (uint64x2_t) __builtin_neon_vcvtq_n_u64_v((int8x16_t)__s0, __p1, 51); \
  __ret; \
})
#else
#define vcvtq_n_u64_f64(__p0, __p1) __extension__ ({ \
  float64x2_t __s0 = __p0; \
  float64x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  uint64x2_t __ret; \
  __ret = (uint64x2_t) __builtin_neon_vcvtq_n_u64_v((int8x16_t)__rev0, __p1, 51); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vcvt_n_u64_f64(__p0, __p1) __extension__ ({ \
  float64x1_t __s0 = __p0; \
  uint64x1_t __ret; \
  __ret = (uint64x1_t) __builtin_neon_vcvt_n_u64_v((int8x8_t)__s0, __p1, 19); \
  __ret; \
})
#else
#define vcvt_n_u64_f64(__p0, __p1) __extension__ ({ \
  float64x1_t __s0 = __p0; \
  uint64x1_t __ret; \
  __ret = (uint64x1_t) __builtin_neon_vcvt_n_u64_v((int8x8_t)__s0, __p1, 19); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vcvtd_n_u64_f64(__p0, __p1) __extension__ ({ \
  float64_t __s0 = __p0; \
  uint64_t __ret; \
  __ret = (uint64_t) __builtin_neon_vcvtd_n_u64_f64(__s0, __p1); \
  __ret; \
})
#else
#define vcvtd_n_u64_f64(__p0, __p1) __extension__ ({ \
  float64_t __s0 = __p0; \
  uint64_t __ret; \
  __ret = (uint64_t) __builtin_neon_vcvtd_n_u64_f64(__s0, __p1); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32_t vcvts_s32_f32(float32_t __p0) {
  int32_t __ret;
  __ret = (int32_t) __builtin_neon_vcvts_s32_f32(__p0);
  return __ret;
}
#else
__ai int32_t vcvts_s32_f32(float32_t __p0) {
  int32_t __ret;
  __ret = (int32_t) __builtin_neon_vcvts_s32_f32(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64_t vcvtd_s64_f64(float64_t __p0) {
  int64_t __ret;
  __ret = (int64_t) __builtin_neon_vcvtd_s64_f64(__p0);
  return __ret;
}
#else
__ai int64_t vcvtd_s64_f64(float64_t __p0) {
  int64_t __ret;
  __ret = (int64_t) __builtin_neon_vcvtd_s64_f64(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x2_t vcvtq_s64_f64(float64x2_t __p0) {
  int64x2_t __ret;
  __ret = (int64x2_t) __builtin_neon_vcvtq_s64_v((int8x16_t)__p0, 35);
  return __ret;
}
#else
__ai int64x2_t vcvtq_s64_f64(float64x2_t __p0) {
  float64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  int64x2_t __ret;
  __ret = (int64x2_t) __builtin_neon_vcvtq_s64_v((int8x16_t)__rev0, 35);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64x1_t vcvt_s64_f64(float64x1_t __p0) {
  int64x1_t __ret;
  __ret = (int64x1_t) __builtin_neon_vcvt_s64_v((int8x8_t)__p0, 3);
  return __ret;
}
#else
__ai int64x1_t vcvt_s64_f64(float64x1_t __p0) {
  int64x1_t __ret;
  __ret = (int64x1_t) __builtin_neon_vcvt_s64_v((int8x8_t)__p0, 3);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32_t vcvts_u32_f32(float32_t __p0) {
  uint32_t __ret;
  __ret = (uint32_t) __builtin_neon_vcvts_u32_f32(__p0);
  return __ret;
}
#else
__ai uint32_t vcvts_u32_f32(float32_t __p0) {
  uint32_t __ret;
  __ret = (uint32_t) __builtin_neon_vcvts_u32_f32(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64_t vcvtd_u64_f64(float64_t __p0) {
  uint64_t __ret;
  __ret = (uint64_t) __builtin_neon_vcvtd_u64_f64(__p0);
  return __ret;
}
#else
__ai uint64_t vcvtd_u64_f64(float64_t __p0) {
  uint64_t __ret;
  __ret = (uint64_t) __builtin_neon_vcvtd_u64_f64(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x2_t vcvtq_u64_f64(float64x2_t __p0) {
  uint64x2_t __ret;
  __ret = (uint64x2_t) __builtin_neon_vcvtq_u64_v((int8x16_t)__p0, 51);
  return __ret;
}
#else
__ai uint64x2_t vcvtq_u64_f64(float64x2_t __p0) {
  float64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  uint64x2_t __ret;
  __ret = (uint64x2_t) __builtin_neon_vcvtq_u64_v((int8x16_t)__rev0, 51);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64x1_t vcvt_u64_f64(float64x1_t __p0) {
  uint64x1_t __ret;
  __ret = (uint64x1_t) __builtin_neon_vcvt_u64_v((int8x8_t)__p0, 19);
  return __ret;
}
#else
__ai uint64x1_t vcvt_u64_f64(float64x1_t __p0) {
  uint64x1_t __ret;
  __ret = (uint64x1_t) __builtin_neon_vcvt_u64_v((int8x8_t)__p0, 19);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32_t vcvtas_s32_f32(float32_t __p0) {
  int32_t __ret;
  __ret = (int32_t) __builtin_neon_vcvtas_s32_f32(__p0);
  return __ret;
}
#else
__ai int32_t vcvtas_s32_f32(float32_t __p0) {
  int32_t __ret;
  __ret = (int32_t) __builtin_neon_vcvtas_s32_f32(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64_t vcvtad_s64_f64(float64_t __p0) {
  int64_t __ret;
  __ret = (int64_t) __builtin_neon_vcvtad_s64_f64(__p0);
  return __ret;
}
#else
__ai int64_t vcvtad_s64_f64(float64_t __p0) {
  int64_t __ret;
  __ret = (int64_t) __builtin_neon_vcvtad_s64_f64(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32_t vcvtas_u32_f32(float32_t __p0) {
  uint32_t __ret;
  __ret = (uint32_t) __builtin_neon_vcvtas_u32_f32(__p0);
  return __ret;
}
#else
__ai uint32_t vcvtas_u32_f32(float32_t __p0) {
  uint32_t __ret;
  __ret = (uint32_t) __builtin_neon_vcvtas_u32_f32(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64_t vcvtad_u64_f64(float64_t __p0) {
  uint64_t __ret;
  __ret = (uint64_t) __builtin_neon_vcvtad_u64_f64(__p0);
  return __ret;
}
#else
__ai uint64_t vcvtad_u64_f64(float64_t __p0) {
  uint64_t __ret;
  __ret = (uint64_t) __builtin_neon_vcvtad_u64_f64(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32_t vcvtms_s32_f32(float32_t __p0) {
  int32_t __ret;
  __ret = (int32_t) __builtin_neon_vcvtms_s32_f32(__p0);
  return __ret;
}
#else
__ai int32_t vcvtms_s32_f32(float32_t __p0) {
  int32_t __ret;
  __ret = (int32_t) __builtin_neon_vcvtms_s32_f32(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64_t vcvtmd_s64_f64(float64_t __p0) {
  int64_t __ret;
  __ret = (int64_t) __builtin_neon_vcvtmd_s64_f64(__p0);
  return __ret;
}
#else
__ai int64_t vcvtmd_s64_f64(float64_t __p0) {
  int64_t __ret;
  __ret = (int64_t) __builtin_neon_vcvtmd_s64_f64(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32_t vcvtms_u32_f32(float32_t __p0) {
  uint32_t __ret;
  __ret = (uint32_t) __builtin_neon_vcvtms_u32_f32(__p0);
  return __ret;
}
#else
__ai uint32_t vcvtms_u32_f32(float32_t __p0) {
  uint32_t __ret;
  __ret = (uint32_t) __builtin_neon_vcvtms_u32_f32(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64_t vcvtmd_u64_f64(float64_t __p0) {
  uint64_t __ret;
  __ret = (uint64_t) __builtin_neon_vcvtmd_u64_f64(__p0);
  return __ret;
}
#else
__ai uint64_t vcvtmd_u64_f64(float64_t __p0) {
  uint64_t __ret;
  __ret = (uint64_t) __builtin_neon_vcvtmd_u64_f64(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32_t vcvtns_s32_f32(float32_t __p0) {
  int32_t __ret;
  __ret = (int32_t) __builtin_neon_vcvtns_s32_f32(__p0);
  return __ret;
}
#else
__ai int32_t vcvtns_s32_f32(float32_t __p0) {
  int32_t __ret;
  __ret = (int32_t) __builtin_neon_vcvtns_s32_f32(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64_t vcvtnd_s64_f64(float64_t __p0) {
  int64_t __ret;
  __ret = (int64_t) __builtin_neon_vcvtnd_s64_f64(__p0);
  return __ret;
}
#else
__ai int64_t vcvtnd_s64_f64(float64_t __p0) {
  int64_t __ret;
  __ret = (int64_t) __builtin_neon_vcvtnd_s64_f64(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32_t vcvtns_u32_f32(float32_t __p0) {
  uint32_t __ret;
  __ret = (uint32_t) __builtin_neon_vcvtns_u32_f32(__p0);
  return __ret;
}
#else
__ai uint32_t vcvtns_u32_f32(float32_t __p0) {
  uint32_t __ret;
  __ret = (uint32_t) __builtin_neon_vcvtns_u32_f32(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64_t vcvtnd_u64_f64(float64_t __p0) {
  uint64_t __ret;
  __ret = (uint64_t) __builtin_neon_vcvtnd_u64_f64(__p0);
  return __ret;
}
#else
__ai uint64_t vcvtnd_u64_f64(float64_t __p0) {
  uint64_t __ret;
  __ret = (uint64_t) __builtin_neon_vcvtnd_u64_f64(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int32_t vcvtps_s32_f32(float32_t __p0) {
  int32_t __ret;
  __ret = (int32_t) __builtin_neon_vcvtps_s32_f32(__p0);
  return __ret;
}
#else
__ai int32_t vcvtps_s32_f32(float32_t __p0) {
  int32_t __ret;
  __ret = (int32_t) __builtin_neon_vcvtps_s32_f32(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai int64_t vcvtpd_s64_f64(float64_t __p0) {
  int64_t __ret;
  __ret = (int64_t) __builtin_neon_vcvtpd_s64_f64(__p0);
  return __ret;
}
#else
__ai int64_t vcvtpd_s64_f64(float64_t __p0) {
  int64_t __ret;
  __ret = (int64_t) __builtin_neon_vcvtpd_s64_f64(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint32_t vcvtps_u32_f32(float32_t __p0) {
  uint32_t __ret;
  __ret = (uint32_t) __builtin_neon_vcvtps_u32_f32(__p0);
  return __ret;
}
#else
__ai uint32_t vcvtps_u32_f32(float32_t __p0) {
  uint32_t __ret;
  __ret = (uint32_t) __builtin_neon_vcvtps_u32_f32(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai uint64_t vcvtpd_u64_f64(float64_t __p0) {
  uint64_t __ret;
  __ret = (uint64_t) __builtin_neon_vcvtpd_u64_f64(__p0);
  return __ret;
}
#else
__ai uint64_t vcvtpd_u64_f64(float64_t __p0) {
  uint64_t __ret;
  __ret = (uint64_t) __builtin_neon_vcvtpd_u64_f64(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32_t vcvtxd_f32_f64(float64_t __p0) {
  float32_t __ret;
  __ret = (float32_t) __builtin_neon_vcvtxd_f32_f64(__p0);
  return __ret;
}
#else
__ai float32_t vcvtxd_f32_f64(float64_t __p0) {
  float32_t __ret;
  __ret = (float32_t) __builtin_neon_vcvtxd_f32_f64(__p0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x2_t vcvtx_f32_f64(float64x2_t __p0) {
  float32x2_t __ret;
  __ret = (float32x2_t) __builtin_neon_vcvtx_f32_v((int8x16_t)__p0, 42);
  return __ret;
}
#else
__ai float32x2_t vcvtx_f32_f64(float64x2_t __p0) {
  float64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float32x2_t __ret;
  __ret = (float32x2_t) __builtin_neon_vcvtx_f32_v((int8x16_t)__rev0, 42);
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
__ai float32x2_t __noswap_vcvtx_f32_f64(float64x2_t __p0) {
  float32x2_t __ret;
  __ret = (float32x2_t) __builtin_neon_vcvtx_f32_v((int8x16_t)__p0, 42);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x4_t vcvtx_high_f32_f64(float32x2_t __p0, float64x2_t __p1) {
  float32x4_t __ret;
  __ret = vcombine_f32(__p0, vcvtx_f32_f64(__p1));
  return __ret;
}
#else
__ai float32x4_t vcvtx_high_f32_f64(float32x2_t __p0, float64x2_t __p1) {
  float32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  float32x4_t __ret;
  __ret = __noswap_vcombine_f32(__rev0, __noswap_vcvtx_f32_f64(__rev1));
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x2_t vdivq_f64(float64x2_t __p0, float64x2_t __p1) {
  float64x2_t __ret;
  __ret = __p0 / __p1;
  return __ret;
}
#else
__ai float64x2_t vdivq_f64(float64x2_t __p0, float64x2_t __p1) {
  float64x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float64x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  float64x2_t __ret;
  __ret = __rev0 / __rev1;
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x4_t vdivq_f32(float32x4_t __p0, float32x4_t __p1) {
  float32x4_t __ret;
  __ret = __p0 / __p1;
  return __ret;
}
#else
__ai float32x4_t vdivq_f32(float32x4_t __p0, float32x4_t __p1) {
  float32x4_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 3, 2, 1, 0);
  float32x4_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 3, 2, 1, 0);
  float32x4_t __ret;
  __ret = __rev0 / __rev1;
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float64x1_t vdiv_f64(float64x1_t __p0, float64x1_t __p1) {
  float64x1_t __ret;
  __ret = __p0 / __p1;
  return __ret;
}
#else
__ai float64x1_t vdiv_f64(float64x1_t __p0, float64x1_t __p1) {
  float64x1_t __ret;
  __ret = __p0 / __p1;
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
__ai float32x2_t vdiv_f32(float32x2_t __p0, float32x2_t __p1) {
  float32x2_t __ret;
  __ret = __p0 / __p1;
  return __ret;
}
#else
__ai float32x2_t vdiv_f32(float32x2_t __p0, float32x2_t __p1) {
  float32x2_t __rev0;  __rev0 = __builtin_shufflevector(__p0, __p0, 1, 0);
  float32x2_t __rev1;  __rev1 = __builtin_shufflevector(__p1, __p1, 1, 0);
  float32x2_t __ret;
  __ret = __rev0 / __rev1;
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0);
  return __ret;
}
#endif

#ifdef __LITTLE_ENDIAN__
#define vdupb_lane_p8(__p0, __p1) __extension__ ({ \
  poly8x8_t __s0 = __p0; \
  poly8_t __ret; \
  __ret = (poly8_t) __builtin_neon_vdupb_lane_i8((int8x8_t)__s0, __p1); \
  __ret; \
})
#else
#define vdupb_lane_p8(__p0, __p1) __extension__ ({ \
  poly8x8_t __s0 = __p0; \
  poly8x8_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 7, 6, 5, 4, 3, 2, 1, 0); \
  poly8_t __ret; \
  __ret = (poly8_t) __builtin_neon_vdupb_lane_i8((int8x8_t)__rev0, __p1); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vduph_lane_p16(__p0, __p1) __extension__ ({ \
  poly16x4_t __s0 = __p0; \
  poly16_t __ret; \
  __ret = (poly16_t) __builtin_neon_vduph_lane_i16((int8x8_t)__s0, __p1); \
  __ret; \
})
#else
#define vduph_lane_p16(__p0, __p1) __extension__ ({ \
  poly16x4_t __s0 = __p0; \
  poly16x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  poly16_t __ret; \
  __ret = (poly16_t) __builtin_neon_vduph_lane_i16((int8x8_t)__rev0, __p1); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vdupb_lane_u8(__p0, __p1) __extension__ ({ \
  uint8x8_t __s0 = __p0; \
  uint8_t __ret; \
  __ret = (uint8_t) __builtin_neon_vdupb_lane_i8((int8x8_t)__s0, __p1); \
  __ret; \
})
#else
#define vdupb_lane_u8(__p0, __p1) __extension__ ({ \
  uint8x8_t __s0 = __p0; \
  uint8x8_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint8_t __ret; \
  __ret = (uint8_t) __builtin_neon_vdupb_lane_i8((int8x8_t)__rev0, __p1); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vdups_lane_u32(__p0, __p1) __extension__ ({ \
  uint32x2_t __s0 = __p0; \
  uint32_t __ret; \
  __ret = (uint32_t) __builtin_neon_vdups_lane_i32((int8x8_t)__s0, __p1); \
  __ret; \
})
#else
#define vdups_lane_u32(__p0, __p1) __extension__ ({ \
  uint32x2_t __s0 = __p0; \
  uint32x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  uint32_t __ret; \
  __ret = (uint32_t) __builtin_neon_vdups_lane_i32((int8x8_t)__rev0, __p1); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vdupd_lane_u64(__p0, __p1) __extension__ ({ \
  uint64x1_t __s0 = __p0; \
  uint64_t __ret; \
  __ret = (uint64_t) __builtin_neon_vdupd_lane_i64((int8x8_t)__s0, __p1); \
  __ret; \
})
#else
#define vdupd_lane_u64(__p0, __p1) __extension__ ({ \
  uint64x1_t __s0 = __p0; \
  uint64_t __ret; \
  __ret = (uint64_t) __builtin_neon_vdupd_lane_i64((int8x8_t)__s0, __p1); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vduph_lane_u16(__p0, __p1) __extension__ ({ \
  uint16x4_t __s0 = __p0; \
  uint16_t __ret; \
  __ret = (uint16_t) __builtin_neon_vduph_lane_i16((int8x8_t)__s0, __p1); \
  __ret; \
})
#else
#define vduph_lane_u16(__p0, __p1) __extension__ ({ \
  uint16x4_t __s0 = __p0; \
  uint16x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  uint16_t __ret; \
  __ret = (uint16_t) __builtin_neon_vduph_lane_i16((int8x8_t)__rev0, __p1); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vdupb_lane_s8(__p0, __p1) __extension__ ({ \
  int8x8_t __s0 = __p0; \
  int8_t __ret; \
  __ret = (int8_t) __builtin_neon_vdupb_lane_i8((int8x8_t)__s0, __p1); \
  __ret; \
})
#else
#define vdupb_lane_s8(__p0, __p1) __extension__ ({ \
  int8x8_t __s0 = __p0; \
  int8x8_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 7, 6, 5, 4, 3, 2, 1, 0); \
  int8_t __ret; \
  __ret = (int8_t) __builtin_neon_vdupb_lane_i8((int8x8_t)__rev0, __p1); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vdupd_lane_f64(__p0, __p1) __extension__ ({ \
  float64x1_t __s0 = __p0; \
  float64_t __ret; \
  __ret = (float64_t) __builtin_neon_vdupd_lane_f64((int8x8_t)__s0, __p1); \
  __ret; \
})
#else
#define vdupd_lane_f64(__p0, __p1) __extension__ ({ \
  float64x1_t __s0 = __p0; \
  float64_t __ret; \
  __ret = (float64_t) __builtin_neon_vdupd_lane_f64((int8x8_t)__s0, __p1); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vdups_lane_f32(__p0, __p1) __extension__ ({ \
  float32x2_t __s0 = __p0; \
  float32_t __ret; \
  __ret = (float32_t) __builtin_neon_vdups_lane_f32((int8x8_t)__s0, __p1); \
  __ret; \
})
#else
#define vdups_lane_f32(__p0, __p1) __extension__ ({ \
  float32x2_t __s0 = __p0; \
  float32x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  float32_t __ret; \
  __ret = (float32_t) __builtin_neon_vdups_lane_f32((int8x8_t)__rev0, __p1); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vdups_lane_s32(__p0, __p1) __extension__ ({ \
  int32x2_t __s0 = __p0; \
  int32_t __ret; \
  __ret = (int32_t) __builtin_neon_vdups_lane_i32((int8x8_t)__s0, __p1); \
  __ret; \
})
#else
#define vdups_lane_s32(__p0, __p1) __extension__ ({ \
  int32x2_t __s0 = __p0; \
  int32x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  int32_t __ret; \
  __ret = (int32_t) __builtin_neon_vdups_lane_i32((int8x8_t)__rev0, __p1); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vdupd_lane_s64(__p0, __p1) __extension__ ({ \
  int64x1_t __s0 = __p0; \
  int64_t __ret; \
  __ret = (int64_t) __builtin_neon_vdupd_lane_i64((int8x8_t)__s0, __p1); \
  __ret; \
})
#else
#define vdupd_lane_s64(__p0, __p1) __extension__ ({ \
  int64x1_t __s0 = __p0; \
  int64_t __ret; \
  __ret = (int64_t) __builtin_neon_vdupd_lane_i64((int8x8_t)__s0, __p1); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vduph_lane_s16(__p0, __p1) __extension__ ({ \
  int16x4_t __s0 = __p0; \
  int16_t __ret; \
  __ret = (int16_t) __builtin_neon_vduph_lane_i16((int8x8_t)__s0, __p1); \
  __ret; \
})
#else
#define vduph_lane_s16(__p0, __p1) __extension__ ({ \
  int16x4_t __s0 = __p0; \
  int16x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  int16_t __ret; \
  __ret = (int16_t) __builtin_neon_vduph_lane_i16((int8x8_t)__rev0, __p1); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vdup_lane_p64(__p0, __p1) __extension__ ({ \
  poly64x1_t __s0 = __p0; \
  poly64x1_t __ret; \
  __ret = __builtin_shufflevector(__s0, __s0, __p1); \
  __ret; \
})
#else
#define vdup_lane_p64(__p0, __p1) __extension__ ({ \
  poly64x1_t __s0 = __p0; \
  poly64x1_t __ret; \
  __ret = __builtin_shufflevector(__s0, __s0, __p1); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vdupq_lane_p64(__p0, __p1) __extension__ ({ \
  poly64x1_t __s0 = __p0; \
  poly64x2_t __ret; \
  __ret = __builtin_shufflevector(__s0, __s0, __p1, __p1); \
  __ret; \
})
#else
#define vdupq_lane_p64(__p0, __p1) __extension__ ({ \
  poly64x1_t __s0 = __p0; \
  poly64x2_t __ret; \
  __ret = __builtin_shufflevector(__s0, __s0, __p1, __p1); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vdupq_lane_f64(__p0, __p1) __extension__ ({ \
  float64x1_t __s0 = __p0; \
  float64x2_t __ret; \
  __ret = __builtin_shufflevector(__s0, __s0, __p1, __p1); \
  __ret; \
})
#else
#define vdupq_lane_f64(__p0, __p1) __extension__ ({ \
  float64x1_t __s0 = __p0; \
  float64x2_t __ret; \
  __ret = __builtin_shufflevector(__s0, __s0, __p1, __p1); \
  __ret = __builtin_shufflevector(__ret, __ret, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vdupq_lane_f16(__p0, __p1) __extension__ ({ \
  float16x4_t __s0 = __p0; \
  float16x8_t __ret; \
  __ret = __builtin_shufflevector(__s0, __s0, __p1, __p1, __p1, __p1, __p1, __p1, __p1, __p1); \
  __ret; \
})
#else
#define vdupq_lane_f16(__p0, __p1) __extension__ ({ \
  float16x4_t __s0 = __p0; \
  float16x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  float16x8_t __ret; \
  __ret = __builtin_shufflevector(__rev0, __rev0, __p1, __p1, __p1, __p1, __p1, __p1, __p1, __p1); \
  __ret = __builtin_shufflevector(__ret, __ret, 7, 6, 5, 4, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vdup_lane_f64(__p0, __p1) __extension__ ({ \
  float64x1_t __s0 = __p0; \
  float64x1_t __ret; \
  __ret = __builtin_shufflevector(__s0, __s0, __p1); \
  __ret; \
})
#else
#define vdup_lane_f64(__p0, __p1) __extension__ ({ \
  float64x1_t __s0 = __p0; \
  float64x1_t __ret; \
  __ret = __builtin_shufflevector(__s0, __s0, __p1); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vdup_lane_f16(__p0, __p1) __extension__ ({ \
  float16x4_t __s0 = __p0; \
  float16x4_t __ret; \
  __ret = __builtin_shufflevector(__s0, __s0, __p1, __p1, __p1, __p1); \
  __ret; \
})
#else
#define vdup_lane_f16(__p0, __p1) __extension__ ({ \
  float16x4_t __s0 = __p0; \
  float16x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  float16x4_t __ret; \
  __ret = __builtin_shufflevector(__rev0, __rev0, __p1, __p1, __p1, __p1); \
  __ret = __builtin_shufflevector(__ret, __ret, 3, 2, 1, 0); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vdupb_laneq_p8(__p0, __p1) __extension__ ({ \
  poly8x16_t __s0 = __p0; \
  poly8_t __ret; \
  __ret = (poly8_t) __builtin_neon_vdupb_laneq_i8((int8x16_t)__s0, __p1); \
  __ret; \
})
#else
#define vdupb_laneq_p8(__p0, __p1) __extension__ ({ \
  poly8x16_t __s0 = __p0; \
  poly8x16_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  poly8_t __ret; \
  __ret = (poly8_t) __builtin_neon_vdupb_laneq_i8((int8x16_t)__rev0, __p1); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vduph_laneq_p16(__p0, __p1) __extension__ ({ \
  poly16x8_t __s0 = __p0; \
  poly16_t __ret; \
  __ret = (poly16_t) __builtin_neon_vduph_laneq_i16((int8x16_t)__s0, __p1); \
  __ret; \
})
#else
#define vduph_laneq_p16(__p0, __p1) __extension__ ({ \
  poly16x8_t __s0 = __p0; \
  poly16x8_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 7, 6, 5, 4, 3, 2, 1, 0); \
  poly16_t __ret; \
  __ret = (poly16_t) __builtin_neon_vduph_laneq_i16((int8x16_t)__rev0, __p1); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vdupb_laneq_u8(__p0, __p1) __extension__ ({ \
  uint8x16_t __s0 = __p0; \
  uint8_t __ret; \
  __ret = (uint8_t) __builtin_neon_vdupb_laneq_i8((int8x16_t)__s0, __p1); \
  __ret; \
})
#else
#define vdupb_laneq_u8(__p0, __p1) __extension__ ({ \
  uint8x16_t __s0 = __p0; \
  uint8x16_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint8_t __ret; \
  __ret = (uint8_t) __builtin_neon_vdupb_laneq_i8((int8x16_t)__rev0, __p1); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vdups_laneq_u32(__p0, __p1) __extension__ ({ \
  uint32x4_t __s0 = __p0; \
  uint32_t __ret; \
  __ret = (uint32_t) __builtin_neon_vdups_laneq_i32((int8x16_t)__s0, __p1); \
  __ret; \
})
#else
#define vdups_laneq_u32(__p0, __p1) __extension__ ({ \
  uint32x4_t __s0 = __p0; \
  uint32x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  uint32_t __ret; \
  __ret = (uint32_t) __builtin_neon_vdups_laneq_i32((int8x16_t)__rev0, __p1); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vdupd_laneq_u64(__p0, __p1) __extension__ ({ \
  uint64x2_t __s0 = __p0; \
  uint64_t __ret; \
  __ret = (uint64_t) __builtin_neon_vdupd_laneq_i64((int8x16_t)__s0, __p1); \
  __ret; \
})
#else
#define vdupd_laneq_u64(__p0, __p1) __extension__ ({ \
  uint64x2_t __s0 = __p0; \
  uint64x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  uint64_t __ret; \
  __ret = (uint64_t) __builtin_neon_vdupd_laneq_i64((int8x16_t)__rev0, __p1); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vduph_laneq_u16(__p0, __p1) __extension__ ({ \
  uint16x8_t __s0 = __p0; \
  uint16_t __ret; \
  __ret = (uint16_t) __builtin_neon_vduph_laneq_i16((int8x16_t)__s0, __p1); \
  __ret; \
})
#else
#define vduph_laneq_u16(__p0, __p1) __extension__ ({ \
  uint16x8_t __s0 = __p0; \
  uint16x8_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 7, 6, 5, 4, 3, 2, 1, 0); \
  uint16_t __ret; \
  __ret = (uint16_t) __builtin_neon_vduph_laneq_i16((int8x16_t)__rev0, __p1); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vdupb_laneq_s8(__p0, __p1) __extension__ ({ \
  int8x16_t __s0 = __p0; \
  int8_t __ret; \
  __ret = (int8_t) __builtin_neon_vdupb_laneq_i8((int8x16_t)__s0, __p1); \
  __ret; \
})
#else
#define vdupb_laneq_s8(__p0, __p1) __extension__ ({ \
  int8x16_t __s0 = __p0; \
  int8x16_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); \
  int8_t __ret; \
  __ret = (int8_t) __builtin_neon_vdupb_laneq_i8((int8x16_t)__rev0, __p1); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vdupd_laneq_f64(__p0, __p1) __extension__ ({ \
  float64x2_t __s0 = __p0; \
  float64_t __ret; \
  __ret = (float64_t) __builtin_neon_vdupd_laneq_f64((int8x16_t)__s0, __p1); \
  __ret; \
})
#else
#define vdupd_laneq_f64(__p0, __p1) __extension__ ({ \
  float64x2_t __s0 = __p0; \
  float64x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  float64_t __ret; \
  __ret = (float64_t) __builtin_neon_vdupd_laneq_f64((int8x16_t)__rev0, __p1); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vdups_laneq_f32(__p0, __p1) __extension__ ({ \
  float32x4_t __s0 = __p0; \
  float32_t __ret; \
  __ret = (float32_t) __builtin_neon_vdups_laneq_f32((int8x16_t)__s0, __p1); \
  __ret; \
})
#else
#define vdups_laneq_f32(__p0, __p1) __extension__ ({ \
  float32x4_t __s0 = __p0; \
  float32x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  float32_t __ret; \
  __ret = (float32_t) __builtin_neon_vdups_laneq_f32((int8x16_t)__rev0, __p1); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vdups_laneq_s32(__p0, __p1) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  int32_t __ret; \
  __ret = (int32_t) __builtin_neon_vdups_laneq_i32((int8x16_t)__s0, __p1); \
  __ret; \
})
#else
#define vdups_laneq_s32(__p0, __p1) __extension__ ({ \
  int32x4_t __s0 = __p0; \
  int32x4_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 3, 2, 1, 0); \
  int32_t __ret; \
  __ret = (int32_t) __builtin_neon_vdups_laneq_i32((int8x16_t)__rev0, __p1); \
  __ret; \
})
#endif

#ifdef __LITTLE_ENDIAN__
#define vdupd_laneq_s64(__p0, __p1) __extension__ ({ \
  int64x2_t __s0 = __p0; \
  int64_t __ret; \
  __ret = (int64_t) __builtin_neon_vdupd_laneq_i64((int8x16_t)__s0, __p1); \
  __ret; \
})
#else
#define vdupd_laneq_s64(__p0, __p1) __extension__ ({ \
  int64x2_t __s0 = __p0; \
  int64x2_t __rev0;  __rev0 = __builtin_shufflevector(__s0, __s0, 1, 0); \
  int64_t __ret; \
  __ret = (int64_t) __builtin_neon_vdupd_laneq_i64((int8